[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyse et modÃ©lisation dâ€™agroÃ©cosystÃ¨mes sur R",
    "section": "",
    "text": "PrÃ©face\nCe cours a pour objectif de former les Ã©tudiants graduÃ©s en gÃ©nie agroenvironnemental, gÃ©nie civil, gÃ©nie Ã©cologique, agronomie, biologie, foresterie et Ã©cologie en analyse et modÃ©lisation de systÃ¨mes vivants. Les sujets traitÃ©s sont lâ€™introduction au langage de programmation R, lâ€™analyse statistique descriptive, la visualisation, la modÃ©lisation infÃ©rentielle, prÃ©dictive et dÃ©terministe.\nCe manuel est basÃ© sur le cours Analyse et modÃ©lisation dâ€™agroÃ©cosystÃ¨mes de Essi Parent (voir licence au bas de la page). La version proposÃ©e ici tient compte des mises Ã  jour des diffÃ©rents outils prÃ©sentÃ©s dans le manuel original. Elle est construite au format Quarto par Charles Frenette-ValliÃ¨res et AndrÃ©s Felipe Silva DimatÃ©\nVoici la liste des modifications principales apportÃ©es jusquâ€™Ã  prÃ©sent par rapport Ã  la version originale :"
  },
  {
    "objectID": "index.html#table-des-matiÃ¨res",
    "href": "index.html#table-des-matiÃ¨res",
    "title": "Analyse et modÃ©lisation dâ€™agroÃ©cosystÃ¨mes sur R",
    "section": "Table des matiÃ¨res",
    "text": "Table des matiÃ¨res\n\nIntroduction\nLa science des donnÃ©es avec R\nOrganisation des donnÃ©es et opÃ©rations sur des tableaux\nVisualisation\nScience ouverte et reproductibilitÃ©\nIntroduction Ã  Python\nBiostatistiques\nIntroduction Ã  lâ€™analyse bayÃ©sienne en Ã©cologie\nExplorer R\nAssociation, partitionnement et ordination\nDÃ©tection de valeurs aberrantes et imputation de donnÃ©es manquantes\nLes sÃ©ries temporelles\nIntroduction Ã  lâ€™autoapprentissage\nLes donnÃ©es gÃ©ospatiales\nModÃ©lisation de mÃ©canismes Ã©cologiques\n\n\nAnalyse et modÃ©lisation dâ€™agroÃ©cosystÃ¨mes de Essi Parent est mis Ã  disposition selon les termes de la licence Creative Commons Attribution - Pas dâ€™Utilisation Commerciale - Partage dans les MÃªmes Conditions 4.0 International\n\nFondÃ©(e) sur une Å“uvre Ã  https://github.com/essicolo/ecologie-mathematique-R/."
  },
  {
    "objectID": "01-intro.html#dÃ©finitions",
    "href": "01-intro.html#dÃ©finitions",
    "title": "1Â  Introduction",
    "section": "\n1.1 DÃ©finitions",
    "text": "1.1 DÃ©finitions\nLes mathÃ©matiques confÃ¨rent aux humains une capacitÃ© dâ€™abstraction suffisamment complexe pour leur permettre de toucher les Ã©toiles et les atomes, de comprendre le passÃ© et de prÃ©dire le futur, de toucher lâ€™infini et de goÃ»ter Ã  lâ€™Ã©ternitÃ©. Ã€ partir des maths, on a pu crÃ©er des outils de calcul qui permettent de projeter des images de lâ€™univers, bien au-delÃ  de la Voie lactÃ©e. Mais apprÃ©hender le vivant, tout prÃ¨s de nous, demeure une tÃ¢che complexe.\n\n\n\n\nFigureÂ 1.2: Domaines scientifiques de lâ€™Ã©cologie mathÃ©matique.\n\n\n\nLâ€™Ã©cologie mathÃ©matique couvre un large spectre de domaines (FigureÂ 1.2), mais peut Ãªtre divisÃ©e en deux branches: lâ€™Ã©cologie thÃ©orique et lâ€™Ã©cologie quantitative (Legendre et Legendre, 2012). Alors que lâ€™Ã©cologie thÃ©orique sâ€™intÃ©resse Ã  lâ€™expression mathÃ©matique des mÃ©canismes Ã©cologiques, lâ€™Ã©cologie quantitative, plus empirique, en Ã©tudie principalement les phÃ©nomÃ¨nes. La modÃ©lisation Ã©cologique vise Ã  prÃ©voir une situation selon des conditions donnÃ©es. Faisant partie Ã  la fois de lâ€™Ã©cologie thÃ©orique et de lâ€™Ã©cologie quantitative, elle superpose souvent des mÃ©canismes de lâ€™Ã©cologie thÃ©orique et des phÃ©nomÃ¨nes empiriques de lâ€™Ã©cologie quantitative. Lâ€™Ã©cologie numÃ©rique comprend la branche descriptive de lâ€™Ã©cologie quantitative, câ€™est-Ã -dire quâ€™elle sâ€™intÃ©resse Ã  Ã©valuer des effets Ã  partir de donnÃ©es empiriques. Lâ€™exploration des donnÃ©es dans le but dâ€™y dÃ©couvrir des structures passe souvent par des techniques multivariÃ©es comme la classification hiÃ©rarchique ou la rÃ©duction dâ€™axe (par exemple, lâ€™analyse en composantes principales), qui sont davantage heuristiques (dans notre cas, bioheuristique) que statistiques. Les tests dâ€™hypothÃ¨ses et lâ€™analyse des probabilitÃ©s, quant Ã  eux, relÃ¨vent de la biostatistique.\nLe gÃ©nie Ã©cologique, une discipline intimement liÃ©e Ã  lâ€™Ã©cologie mathÃ©matique, est vouÃ© Ã  lâ€™analyse, la modÃ©lisation, la conception et la construction de systÃ¨mes vivants dans le but de rÃ©soudre de maniÃ¨re efficace des problÃ¨mes liÃ©s Ã  lâ€™Ã©cologie et Ã  une panoplie de domaines qui lui sont raccordÃ©s. Lâ€™agriculture est lâ€™un de ces domaines. Câ€™est dâ€™emblÃ©e la discipline qui sera prisÃ©e dans ce manuel. NÃ©anmoins, les principes qui seront discutÃ©s sont transfÃ©rables Ã  lâ€™Ã©cologie gÃ©nÃ©rale."
  },
  {
    "objectID": "01-intro.html#Ã -qui-sadresse-ce-manuel",
    "href": "01-intro.html#Ã -qui-sadresse-ce-manuel",
    "title": "1Â  Introduction",
    "section": "\n1.2 Ã€ qui sâ€™adresse ce manuel?",
    "text": "1.2 Ã€ qui sâ€™adresse ce manuel?\nLe cours vise Ã  introduire des Ã©tudiant.e.s graduÃ©.e.s en agronomie, biologie, Ã©cologie, sols, gÃ©nie agroenvironnemental, gÃ©nie civil et gÃ©nie Ã©cologique Ã  lâ€™analyse et la modÃ©lisation dans leur domaine, tant pour les appuyer pour leurs travaux de recherche que pour leur fournir une trousse dâ€™outils Ã©mancipatrice pour leur cheminement professionnel. Plus spÃ©cifiquement, vous serez accompagnÃ© Ã  dÃ©couvrir diffÃ©rents outils numÃ©riques qui vous permettront dâ€™apprÃ©hender vos donnÃ©es, dâ€™en faire Ã©merger lâ€™information et de construire des modÃ¨les. Lâ€™objectif de ce cours nâ€™est pas de vous former en mathÃ©matiques, mais de vous aider Ã  les utiliser. En ce sens, câ€™est un cours de pilotage, pas un cours de mÃ©canique. Vous ferez tout de mÃªme un peu de mÃ©canique pour mieux comprendre les rÃ©actions de notre machine.\nBien que des connaissances en programmation et en statistiques aideront grandement les Ã©tudiant.e.s Ã  apprÃ©hender ce document, une littÃ©ratie informatique nâ€™est pas requise. Dans tous les cas, quiconque voudra tirer profit de ce manuel devra faire preuve dâ€™autonomie. Vous serez guidÃ©s vers des ressources et des rÃ©fÃ©rences, mais je vous suggÃ¨re vivement de dÃ©velopper votre propre bibliothÃ¨que adaptÃ©e Ã  vos besoins et Ã  votre maniÃ¨re de comprendre."
  },
  {
    "objectID": "01-intro.html#les-logiciels-libres",
    "href": "01-intro.html#les-logiciels-libres",
    "title": "1Â  Introduction",
    "section": "\n1.3 Les logiciels libres",
    "text": "1.3 Les logiciels libres\nTous les outils numÃ©riques qui sont proposÃ©s dans ce cours sont des logiciels libres:\n\nÂ« Logiciel libre Â» [free software] dÃ©signe des logiciels qui respectent la libertÃ© des utilisateurs. En gros, cela veut dire que les utilisateurs ont la libertÃ© dâ€™exÃ©cuter, copier, distribuer, Ã©tudier, modifier et amÃ©liorer ces logiciels. Ainsi, Â« logiciel libre Â» fait rÃ©fÃ©rence Ã  la libertÃ©, pas au prix1 (pour comprendre ce concept, vous devez penser Ã  Â« libertÃ© dâ€™expression Â», pas Ã  Â« entrÃ©e libre Â»). - Projet GNU\n\nDonc: codes sources ouverts, dÃ©veloppement souvent communautaire, gratuitÃ©. Plusieurs raisons Ã©thiques, principalement liÃ©es au contrÃ´le de lâ€™environnement virtuel par les utilisateurs et les communautÃ©s, peuvent justifier lâ€™utilisation de logiciels libres. Plusieurs raisons pratiques justifient aussi cette orientation. Les logiciels libres vous permettent de transporter vos outils avec vous, dâ€™une entreprise Ã  lâ€™autre, au bureau, ou Ã  la maison, et ce, sans vous soucier dâ€™acheter de coÃ»teuses licences.\nIl existe tout de mÃªme des risques liÃ©s aux possibles erreurs dans les codes des logiciels communautaires. Ces risques sont dâ€™ailleurs les mÃªmes que ceux liÃ©s aux logiciels propriÃ©taires. Pour les scientifiques, une erreur peut mener Ã  une Ã©tude retirÃ©e de la littÃ©rature et mÃªme, potentiellement, des politiques publiques mal avisÃ©es. Pour les ingÃ©nieurs, les consÃ©quences pourraient Ãªtre dramatiques. Mais retenez quâ€™en toute circonstance, comme professionnel.le, vous Ãªtes responsable des outils que vous utilisez: vous devez vous assurer de la bonne qualitÃ© dâ€™un logiciel, quâ€™il soit propriÃ©taire ou communautaire.\nAlors que la qualitÃ© des logiciels propriÃ©taires est gÃ©nÃ©ralement suivie par audits, celle des logiciels libres est plutÃ´t soumise Ã  la vigilance communautaire. Chaque approche a ses avantages et inconvÃ©nients, mais elles ne sont pas exclusives. Ainsi, les logiciels libres peuvent Ãªtre auditÃ©s Ã  lâ€™externe par quiconque dÃ©cide de le faire. DiffÃ©rentes entreprises, souvent concurrentes, participent tant Ã  cette vigilance quâ€™au dÃ©veloppement des logiciels libres: elles en sont mÃªme souvent les instigatrices (comme RStudio, Anaconda et Enthought).\nPar ailleurs, ce manuel est distribuÃ© librement sous licence Creative commons, selon les termes suivants.\n\n\nAnalyse et modÃ©lisation dâ€™agroÃ©cosystÃ¨mes de Essi Parent est mis Ã  disposition selon les termes de la licence Creative Commons Attribution - Pas dâ€™Utilisation Commerciale - Partage dans les MÃªmes Conditions 4.0 International\n\nFondÃ©(e) sur une Å“uvre Ã  https://github.com/essicolo/ecologie-mathematique-R/."
  },
  {
    "objectID": "01-intro.html#langage-de-programmation",
    "href": "01-intro.html#langage-de-programmation",
    "title": "1Â  Introduction",
    "section": "\n1.4 Langage de programmation",
    "text": "1.4 Langage de programmation\n\n1.4.1 R\nCe cours est basÃ© sur le langage R. En plus dâ€™Ãªtre libre, R est un langage de programmation dynamique largement utilisÃ© dans le monde universitaire, et dont lâ€™utilisation sâ€™Ã©tend de maniÃ¨re soutenue hors des tours dâ€™ivoire.\n\nR is also the name of a popular programming language used by a growing number of data analysts inside corporations and academia. It is becoming their lingua franca partly because data mining has entered a golden age, whether being used to set ad prices, find new drugs more quickly or fine-tune financial models. New York Times, janvier 2009\n\nSon dÃ©veloppement est supportÃ© par la R Foundation for Statistical Computing, basÃ©e Ã  lâ€™UniversitÃ© de Vienne. Ã‰galement, lâ€™Ã©quipe de RStudio contribue largement au dÃ©veloppement de modules gÃ©nÃ©riques. R est principalement utilisÃ© pour le calcul statistique, mais les rÃ©cents dÃ©veloppements le rendent un outil de choix pour tout ce qui entoure la science des donnÃ©es, de lâ€™interaction avec les bases de donnÃ©es au dÃ©ploiement dâ€™outils dâ€™intelligence artificielle en passant par la visualisation. Une fois implÃ©mentÃ© avec des modules de calcul scientifique spÃ©cialisÃ©s en biologie, en Ã©cologie et en agronomie (que nous couvrirons au long du cours), R devient un outil de calcul convivial, rapide et fiable.\n\n1.4.2 Pourquoi pas Python?\nLa premiÃ¨re mouture de ce cours se fondait sur le langage Python. Tout comme R, Python est un langage de programmation dynamique prisÃ© pour le calcul scientifique. Python est un langage gÃ©nÃ©rique apprÃ©ciÃ© pour sa polyvalence et sa simplicitÃ©. Python est utilisÃ© autant pour crÃ©er des logiciels ou des sites web que pour le calcul scientifique. Ainsi, Python peut Ãªtre utilisÃ© en interopÃ©rabilitÃ© avec une panoplie de logiciels libres, comme QGIS pour la cartographie et FreeCAD pour le dessin technique. Il est particuliÃ¨rement apprÃ©ciÃ© en ingÃ©nierie pour ses modules de calcul par Ã©lÃ©ments finis (e.g.Â FeNICS) et en bioinformatique pour ses outils liÃ©s au sÃ©quenÃ§age (scikit-bio), mais ses lacunes en analyse statistique, en particulier en statistiques multivariÃ©es mâ€™ont amenÃ© Ã  favoriser R.\nBien que leurs possibilitÃ©s se superposent largement, ce serait une erreur dâ€™aborder R et Python comme des langages rivaux. Les deux langages sâ€™expriment de maniÃ¨re similaire et sâ€™inspirent mutuellement: apprendre Ã  travailler avec lâ€™un revient Ã  apprendre lâ€™autre. Les spÃ©cialistes en calcul scientifique tendent Ã  apprendre Ã  travailler avec plus dâ€™un langage de programmation. Par ailleurs, il existe de plus en plus des moyens de travailler en R et en Python dans un mÃªme flux de travail. Lâ€™interface de calcul RStudio, que nous utiliserons pendant le cours, permet dâ€™inclure des blocs de code en Python.\nDans la version mise Ã  jour du manuel, une courte introduction facultative Ã  Python est proposÃ©e.\n\n1.4.3 Pourquoi pas Matlab?\nParce quâ€™on est en 2024.\n\n1.4.4 Etâ€¦ SAS?\nParce quâ€™on est Ã  lâ€™universitÃ©.\n\n1.4.5 Mais pourquoi pas ______ ?\nDâ€™autres langages, comme Julia, Scala, Javascript et mÃªme Ruby sont utilisÃ©s en calcul scientifique. Ils sont nÃ©anmoins moins garnis et moins documentÃ©s que R. Des langages de plus bas niveau, comme Fortran et C++, viennent souvent appuyer les fonctions des autres langages: ces langages sont plus ardus Ã  utiliser au jour le jour, mais leur rapiditÃ© de calcul est imbattable."
  },
  {
    "objectID": "01-intro.html#contenu-du-manuel",
    "href": "01-intro.html#contenu-du-manuel",
    "title": "1Â  Introduction",
    "section": "\n1.5 Contenu du manuel",
    "text": "1.5 Contenu du manuel\nJe favorise une approche intuitive aux dÃ©veloppements mathÃ©matiques. Nous aborderons lâ€™analyse et la modÃ©lisation infÃ©rentielle, prÃ©dictive et mÃ©canistique appliquÃ©e aux agroÃ©cosystÃ¨mes.\nChapitreÂ 2 - Introduction au langage de programmation R. Quâ€™est-ce que R? Comment lâ€™aborder? Quelles sont les fonctionnalitÃ©s de base et comment tirer profit de tout lâ€™Ã©cosystÃ¨me de programmation?\nChapitreÂ 3 - Organisation des donnÃ©es et opÃ©rations sur des tableaux. Les tableaux permettent dâ€™enchÃ¢sser lâ€™information dans un format prÃªt-Ã -porter pour R. Comment les importer, les exporter, les filtrer, et en faire des sommaires?\nChapitreÂ 4 - Visualisation. Comment prÃ©senter lâ€™information contenue dans un long tableau en un seul coup dâ€™oeil?\nChapitreÂ 5 - Le travail collaboratif, le suivi de version et la science ouverte. Ce chapitre offre une introduction Ã  lâ€™utilisation des outils de calcul collaboratif, ainsi quâ€™un aperÃ§u du systÃ¨me de suivi de version git et de son utilisation sur GitHub.\nChapitreÂ 6 - Introduction Ã  Python (section facultative). Une trÃ¨s brÃ¨ve introduction au langage de programmation Python. Ce contenu est externe au cours et est lÃ  pour vous fournir des rÃ©fÃ©rences si vous souhaitez explorer ce langage dans le futur.\nChapitreÂ 7 - Biostatistiques. Il est audacieux de ne consacrer quâ€™un seul chapitre sur ce vaste sujet. Nous irons Ã  lâ€™essentielâ€¦ pour vous donner les outils qui permettront dâ€™approfondir le sujet.\nChapitreÂ 8 - Biostatistiques bayÃ©siennes (section facultative). Une trÃ¨s brÃ¨ve introduction pour qui est intÃ©ressÃ© Ã  lâ€™analyse bayÃ©sienne.\n?sec-expl - Explorer R. La science des donnÃ©es Ã©volue rapidement. Vous gagnerez Ã  vous tenir au courrant de son Ã©volution, et immanquablement vous vous buterez sur des opÃ©rations qui vous sembleront insolubles. Ce chapitre vous accompagnera Ã  rester Ã  jour sur le dÃ©veloppement de R, Ã  poser de bonnes questions et proposera des modules intÃ©ressants en Ã©cologie mathÃ©matique.\nChapitreÂ 10 - Association, partitionnement et ordination. Les Ã©cosystÃ¨mes diffÃ¨rent, mais en quoi sont-ils semblables, et en quoi dffÃ¨rent-ils? Ces questions importantes peuvent Ãªtre abordÃ©s par lâ€™Ã©cologie numÃ©rique, domaine dâ€™Ã©tude au sein duquel lâ€™association, le partitionnement et lâ€™ordination sont des outils prÃ©dominants.\n?sec-imput - DÃ©tection de valeurs aberrantes et imputation. Une donnÃ©e aberrante sortira du lot, pour une raison ou pour une autre. Comment les dÃ©tecter de maniÃ¨re systÃ©matique? Dâ€™autre part, que faire lorsquâ€™une donnÃ©e est manquante? Peut-on lâ€™imputer? Comment?\n?sec-temps - Les sÃ©ries temporelles. Les capteurs modernes permettent de gÃ©nÃ©rer des donnÃ©es en fonction du temps. Que ce soit des donnÃ©es mÃ©tÃ©orologiques enregistrÃ©es quotidiennement ou des donnÃ©es de teneur en eau enregistrÃ©es au 5 secondes, les donnÃ©es en fonction du temps forment un signal. Comment analyser ces signaux?\n?sec-ia - Lâ€™autoapprentissage. Les applications de lâ€™intelligence artificielle ne sont limitÃ©es que par votre imagination. Encore faut-il lâ€™utiliserâ€¦ intelligemment.\n?sec-geo - Les donnÃ©es spatiales. Ce chapitre porte sur lâ€™utilisation de R comme systÃ¨me dâ€™information gÃ©ographique de base. Nous utiliserons aussi lâ€™autoapprentissage et les modÃ¨les dÃ©terministes comme outils dâ€™interpolation spatial.\n?sec-model - La modÃ©lisation mÃ©canistique (section facultative). Les modÃ¨les sont des maquettes simplifiÃ©es. Comment utiliser les Ã©quations diffÃ©rentielles ordinaires pour crÃ©er ces maquettes?\nSi les chapitres 3 Ã  5 peuvent Ãªtre considÃ©rÃ©s comme fondamentaux pour bien maÃ®triser R, les autres peuvent Ãªtre feuilletÃ©s Ã  la piÃ¨ce, bien quâ€™ils forment une suite logique.\nChaque chapitre de ce manuel est rÃ©digÃ© en format Quarto, dans un environnement RStudio. Pour exÃ©cuter les commandes, vous pourrez soit les copier-coller dans R (ou RStudio), soit tÃ©lÃ©charger les fichiers-sources et exÃ©cuter les blocs de code.\nLe manuel original Ã©tait rÃ©digÃ© au format R Markdown et est toujours disponible Ã  lâ€™adresse suivante."
  },
  {
    "objectID": "01-intro.html#objectifs-gÃ©nÃ©raux",
    "href": "01-intro.html#objectifs-gÃ©nÃ©raux",
    "title": "1Â  Introduction",
    "section": "\n1.6 Objectifs gÃ©nÃ©raux",
    "text": "1.6 Objectifs gÃ©nÃ©raux\nÃ€ la fin du cours, vous serez en mesure:\n\nde programmer en langage R\ndâ€™importer, de manipuler (sÃ©lection des colonnes, filtres, sommaires statistiques) et dâ€™exporter des tableaux\nde gÃ©nÃ©rer des graphiques dâ€™utilisation commune\nde vous assurer que vos calculs soient auditables et reproductibles dans une perspective de science ouverte\ndâ€™apprÃ©hender des donnÃ©es Ã©cologiques et agronomiques Ã  lâ€™aide de tests statistiques frÃ©quentiels\ndâ€™explorer par vous-mÃªme les possibilitÃ©s offertes par la communautÃ© de dÃ©veloppement de modules R\ndâ€™explorer les donnÃ©es Ã  lâ€™aide des outils de lâ€™Ã©cologie numÃ©rique (association, partitionnement et ordination)\ndâ€™imputer des donnÃ©es manquantes dans un tableau et de dÃ©tecter des valeurs aberrantes\nde crÃ©er un modÃ¨le dâ€™autoapprentissage\ndâ€™effectuer une analyse de sÃ©rie temporelle\ndâ€™interpoler des donnÃ©es spatiales\nde modÃ©liser des Ã©quations diffÃ©rentielles ordinaires"
  },
  {
    "objectID": "01-intro.html#lectures-complÃ©mentaires",
    "href": "01-intro.html#lectures-complÃ©mentaires",
    "title": "1Â  Introduction",
    "section": "\n1.7 Lectures complÃ©mentaires",
    "text": "1.7 Lectures complÃ©mentaires\n\n1.7.1 Ã‰cologie mathÃ©matique\n\n\nHow to be a quantitative ecologist. Jason Mathipoulos vous prend par la main pour dÃ©couvrir les notions de mathÃ©matiques fondamentales en Ã©cologie, appliquÃ©es avec le langage R.\n\n\nNumerical ecology. Lâ€™ouvrage hautement dÃ©taillÃ© des frÃ¨res Legendre est non seulement fondamental, mais aussi fondateur dâ€™une science qui Ã©volue encore aujourdâ€™hui: lâ€™analyse des donnÃ©es Ã©cologiques.\n\nA practical guide to ecological modelling. Soetaert et Herman portent une attention particuliÃ¨re Ã  la prÃ©sentation des principes de modÃ©lisation dans un langage accessible - ce qui est rarement le cas dans le domaine de la modÃ©lisation. Les modÃ¨les prÃ©sentÃ©s concernent principalement les bilans de masse, en termes de systÃ¨mes de rÃ©actions chimiques et de relations biologiques.\n\nModÃ©lisation mathÃ©matique en Ã©cologie. Rare livre en modÃ©lisation Ã©cologique publiÃ© en franÃ§ais, la premiÃ¨re partie sâ€™attarde aux concepts mathÃ©matiques, alors que la deuxiÃ¨me planche Ã  les appliquer. Si le haut niveau dâ€™abstraction de la premiÃ¨re partie vous rebute, nâ€™hÃ©sitez pas dÃ©buter par la seconde partie et de vous rÃ©fÃ©rer Ã  la premiÃ¨re au besoin.\n\nA new ecology: systems perspective. Principalement grÃ¢ce au soleil, la Terre forme un ensemble de gradients dâ€™Ã©nergie qui se dÃ©clinent en des systÃ¨mes dâ€™une Ã©tonnante complexitÃ©. Câ€™est ainsi que le regrettÃ© Sven Erik JÃ¸rgensen (1934-2016, FigureÂ 1.3)) et ses collaborateurs dÃ©crivent les Ã©cosystÃ¨mes dans cet ouvrage qui fait suite aux travaux fondateurs de Howard Thomas Odum.\nEcological engineering. Principle and Practice.\nEcological processes handbook.\nModeling complex ecological dynamics\n\n\n\n\n\nFigureÂ 1.3: Sven Erik JÃ¸rgensen, Source: Elsevier.\n\n\n\n\n1.7.2 Programmation\n\n\nR for data science (2e). Lâ€™analyse de donnÃ©es est une branche importante de lâ€™Ã©cologie mathÃ©matique. Ce manuel traite des matrices et la manipulation de donnÃ©es chapitre 3), de la visualisation (chapitre 4) ainsi que de lâ€™apprentissage automatique (chapitre 14). R for data science (2e) repasse ces sujets plus en profondeur. En particulier, lâ€™ouvrage de Garrett Grolemund, Hadley Wickham et Mine Ã‡etinkaya-Rundel offre une introduction au module graphique ggplot2 et Ã  tidyverse.\n\nNumerical ecology with R. Daniel Borcard enseigne lâ€™Ã©cologie numÃ©rique Ã  lâ€™UniversitÃ© de MontrÃ©al. Son cours est condensÃ© dans ce livre recettes vouÃ© Ã  lâ€™application des principes lourdement dÃ©crits dans Numerical ecology.\n\n1.7.3 Divers\n\n\nThe truthful art. Dans cet ouvrage, Alberto Cairo sâ€™intÃ©resse Ã  lâ€™utilisation des donnÃ©es et de leurs prÃ©sentations pour fournir une information adÃ©quate Ã  diffÃ©rents publics."
  },
  {
    "objectID": "01-intro.html#besoin-daide",
    "href": "01-intro.html#besoin-daide",
    "title": "1Â  Introduction",
    "section": "\n1.8 Besoin dâ€™aide?",
    "text": "1.8 Besoin dâ€™aide?\nLes ouvrages de rÃ©fÃ©rence reconnus vous offrent des bases solides sur lesquelles vous pouvez vous appuyer dans vos travaux. Mais au-delÃ  des principes, au jour le jour, vous vous buterez immanquablement Ã  toutes sortes de petits problÃ¨mes. Quel module utiliser pour cette tÃ¢che prÃ©cise? Que veut dire ce message dâ€™erreur? Comment interprÃ©ter ce rÃ©sultat? Pour tous les petits accrocs du quotidien en calcul scientifique, internet offre de nombreuses ressources qui sont trÃ¨s hÃ©tÃ©rogÃ¨nes en qualitÃ©. Vous apprendrez Ã  reconnaÃ®tre les ressources fiables Ã  celles qui sont douteuses. Les plateformes basÃ©es sur Stack Exchange, comme Stack Overflow et Cross Validated, mâ€™ont souvent Ã©tÃ© dâ€™une aide prÃ©cieuse. Vous aurez avantage Ã  vous construire une petite banque dâ€™information avec un logiciel de prise de notes en collectant des liens, en prenant en notes certaines recettes et en suivant des sites dâ€™intÃ©rÃªt avec des flux RSS."
  },
  {
    "objectID": "01-intro.html#Ã -propos-de-lauteur",
    "href": "01-intro.html#Ã -propos-de-lauteur",
    "title": "1Â  Introduction",
    "section": "\n1.9 Ã€ propos de lâ€™auteur",
    "text": "1.9 Ã€ propos de lâ€™auteur\nJe mâ€™appelle Essi Parent. Je suis ingÃ©nieur Ã©cologue et professeur adjoint au DÃ©partement des sols et de gÃ©nie agroalimentaire de lâ€™UniversitÃ© Laval, QuÃ©bec, Canada. Je crois que la science est le meilleur moyen dâ€™apprÃ©hender le monde pour prendre des dÃ©cisions avisÃ©es."
  },
  {
    "objectID": "01-intro.html#un-cours-complÃ©mentaire-Ã -dautres-cours",
    "href": "01-intro.html#un-cours-complÃ©mentaire-Ã -dautres-cours",
    "title": "1Â  Introduction",
    "section": "\n1.10 Un cours complÃ©mentaire Ã  dâ€™autres cours",
    "text": "1.10 Un cours complÃ©mentaire Ã  dâ€™autres cours\nCe cours a Ã©tÃ© dÃ©veloppÃ© pour ouvrir des perspectives mathÃ©matiques en Ã©cologie et en agronomie Ã  la FSAA de lâ€™UniversitÃ© Laval. Il est complÃ©mentaire Ã  certains cours offerts dans dâ€™autres institutions acadÃ©miques au QuÃ©bec, dont ceux-ci.\n\n\nBIO2041. Biostatistiques 1, UniversitÃ© de MontrÃ©al\n\nBIO2042. Biostatistiques 2, UniversitÃ© de MontrÃ©al\n\nBIO109. Introduction Ã  la programmation scientifique, UniversitÃ© de Sherbrooke\n\nBIO500. MÃ©thodes en Ã©cologie computationnelle, UniversitÃ© de Sherbrooke."
  },
  {
    "objectID": "01-intro.html#contribuer-au-manuel",
    "href": "01-intro.html#contribuer-au-manuel",
    "title": "1Â  Introduction",
    "section": "\n1.11 Contribuer au manuel",
    "text": "1.11 Contribuer au manuel\nJe suis ouvert aux commentaires et suggestions. Pour contribuer directement, dirigez-vous sur le dÃ©pÃ´t du manuel sur GitHub, puis ouvrez une Issue pour en discuter. CrÃ©ez une nouvelle branche (fork), effectuez les modifications, puis lancer une requÃªte de fusion (pull request)."
  },
  {
    "objectID": "02-R.html#statistiques-ou-science-des-donnÃ©es",
    "href": "02-R.html#statistiques-ou-science-des-donnÃ©es",
    "title": "2Â  La science des donnÃ©es avec R",
    "section": "\n2.1 Statistiques ou science des donnÃ©es?",
    "text": "2.1 Statistiques ou science des donnÃ©es?\nSelon Whitlock et Schluter (2015), la statistique est lâ€™Ã©tude des mÃ©thodes pour dÃ©crire et mesurer des aspects de la nature Ã  partir dâ€™Ã©chantillon. Pour Grolemund et Wickham (2023), la science des donnÃ©es est une discipline excitante permettant de transformer des donnÃ©es brutes en comprÃ©hension, perspectives et connaissances. Oui, excitante! La diffÃ©rence entre les deux champs dâ€™expertise est subtile, et certaines personnes nâ€™y voient quâ€™une diffÃ©rence de ton.\n\n\n\nData Science is statistics on a Mac.\n\nâ€” Big Data Borat (@BigDataBorat) 27 aoÃ»t 2013\n\nConfinÃ©es Ã  ses applications traditionnelles, les statistiques sont davantage vouÃ©es Ã  la dÃ©finition de dispositifs expÃ©rimentaux et Ã  lâ€™exÃ©cution de tests dâ€™hypothÃ¨ses, alors que la science des donnÃ©es est moins linÃ©aire, en particulier dans sa phase dâ€™analyse, oÃ¹ de nouvelles questions (donc de nouvelles hypothÃ¨ses) peuvent Ãªtre posÃ©es au fur et Ã  mesure de lâ€™analyse. Cela arrive gÃ©nÃ©ralement davantage lorsque lâ€™on fait face Ã  de nombreuses observations sur lesquelles de nombreux paramÃ¨tres sont mesurÃ©s.\nLa quantitÃ© de donnÃ©es et de mesures auxquelles nous avons aujourdâ€™hui accÃ¨s grÃ¢ce aux technologies de mesure et de stockage relativement peu dispendieux rend la science des donnÃ©es une discipline particuliÃ¨rement attrayante, pour ne pas dire sexy."
  },
  {
    "objectID": "02-R.html#dÃ©buter-en-r",
    "href": "02-R.html#dÃ©buter-en-r",
    "title": "2Â  La science des donnÃ©es avec R",
    "section": "\n2.2 DÃ©buter en R",
    "text": "2.2 DÃ©buter en R\nR est un langage de programmation dÃ©rivÃ© du langage S, qui fut initialement lancÃ© en 1976.\n\n\n\n\nFigureÂ 2.2: Logo officiel du language R.\n\n\n\nR figure parmi les langages de programmation les plus utilisÃ©s au monde. Bien quâ€™il soit basÃ© sur les langages statiques C et Fortran, R est un langage dynamique, câ€™est-Ã -dire que le code peut Ãªtre exÃ©cutÃ© ligne par ligne ou bloc par bloc: un avantage majeur pour des activitÃ©s qui nÃ©cessitent des interactions frÃ©quentes. Bien que R soit surtout utilisÃ© pour le calcul statistique, il sâ€™impose de plus en plus comme outil privilÃ©giÃ© en sciences des donnÃ©es en raison des rÃ©cents dÃ©veloppements de modules dâ€™analyse, de modÃ©lisation et de visualisation, dont plusieurs seront utilisÃ©s dans ce manuel.\nUn langage de programmation sâ€™apprend un peu comme une langue. Au dÃ©but, un code R peut sembler incomprÃ©hensible. Et face Ã  son clavier, on ne sait pas trop comment exprimer ce que lâ€™on dÃ©sire. Au fur et Ã  mesure de lâ€™apprentissage, les symboles, les fonctions et le style deviennent de plus en plus familiers et on apprend tranquillement Ã  traduire en code ce que lâ€™on dÃ©sire effectuer. Comme une langue sâ€™apprend en la parlant dans la vie de tous les jours, un language de programmation sâ€™apprend avantageusement en solutionnant vos propres problÃ¨mes (FigureÂ 2.3).\n\n\n\n\nFigureÂ 2.3: R avant et maintenant, Illustration de Allison Horst"
  },
  {
    "objectID": "02-R.html#prÃ©parer-son-flux-de-travail",
    "href": "02-R.html#prÃ©parer-son-flux-de-travail",
    "title": "2Â  La science des donnÃ©es avec R",
    "section": "\n2.3 PrÃ©parer son flux de travail",
    "text": "2.3 PrÃ©parer son flux de travail\nIl existe de nombreuses maniÃ¨res dâ€™utiliser R. Parmi celles-ci, jâ€™en couvrirai 3:\n\nInstallation classique (installation suggÃ©rÃ©e)\nInstallation avec Anaconda\nUtilisation infonuagique\n\n\n2.3.1 Installation classique\nInstallation suggÃ©rÃ©e. Sur Windows ou Mac, dirigez-vous ici, tÃ©lÃ©chargez et installez. Sur Linux, ouvrez votre gestionnaire dâ€™application, chercher r-base (Ubuntu, Debian), R-base (openSuse) ou R-core (Fedora) et installez-le (assurez-vous que les librairies suivantes sont aussi installÃ©es: gcc, gcc-fortran, gcc-c++ et make), vous aurez peut-Ãªtre besoin dâ€™installer des librairies supplÃ©mentaires pour faire fonctionner certains modules.\n\nNote. Les modules prÃ©sentÃ©s dans ce cours devraient Ãªtre disponibles sur Linux, Windows et Mac. Ce nâ€™est pas le cas pour tous les modules R. La plupart fonctionnent nÃ©anmoins sur Linux, dont les systÃ¨mes dâ€™opÃ©ration (je recommande Ubuntu ou lâ€™une de ses dÃ©rivÃ©es comme elementary OS) sont de bonnes options pour le calcul scientifique.\n\nÃ€ cette Ã©tape, R devrait fonctionner dans un interprÃ©teur de commande . Si vous lancez R dans un terminal (chercher cmd dans le menu si vous Ãªtes sur Windows), vous obtiendrez quelque chose comme ceci.\n\n\n\n\nFigureÂ 2.4: R dans le terminal.\n\n\n\nLe symbole &gt; indique que R attend que vos instructions. Vous voilÃ  dans un Ã©tat mÃ©ditatif devant lâ€™indÃ©chiffrable vide du terminal ğŸ˜µ. Ne vous en faites pas: nous commencerons bientÃ´t Ã  jaser avec R.\nAvant cela, installons-nous au salon. Afin de travailler dans un environnement de travail plus confortable, je recommande lâ€™installation de lâ€™interface RStudio, gratuite et open source: tÃ©lÃ©chargez lâ€™installateur et suivez les instructions. RStudio ressemble Ã  ceci.\n\n\n\n\nFigureÂ 2.5: FenÃªtre de RStudio.\n\n\n\nEn haut Ã  droite se trouve un menu Project (None). Il sâ€™agit dâ€™un menu de vos projets. Je recommande dâ€™utiliser ces projets avec RStudio, qui vous permettront de mieux gÃ©rer vos sessions de travail, en particulier en lien avec les chemins vers vos donnÃ©es, graphiques, etc., que vous pouvez gÃ©rer relativement Ã  lâ€™emplacement de votre dossier de projet plutÃ´t quâ€™Ã  lâ€™emplacement des fichiers sur votre machine: nous verrons plus en dÃ©tails au chapitreÂ 5.\n\nEn haut Ã  gauche, vous avez vos feuilles de calcul, qui apparaÃ®tront en tant quâ€™onglets. Une feuille de calcul R script est une sÃ©rie de commandes que vous lancez en sÃ©quence. Il peut aussi sâ€™agir dâ€™un document Quarto si vous choisissez de travailler ainsi. Ce format vous permettra de dâ€™Ã©crire du texte en format Markdown entre des blocs de code. Il est question du format Quarto au chapitreÂ 5).\nEn bas Ã  gauche apparaÃ®t la Console, oÃ¹ vous voyez les commandes envoyÃ©es Ã  R ainsi que ses sorties.\nEn haut Ã  droite, les diffÃ©rents onglets indiquent oÃ¹ vous en Ãªtes dans vos calculs. En particulier, la liste sous Environment indique les objets qui ont Ã©tÃ© gÃ©nÃ©rÃ©s ou chargÃ©s jusquâ€™alors.\nEn bas Ã  droite, on retrouve des onglets de nature variÃ©s. Files contient les sous-dossiers et fichiers du dossier de projets. Plots est lâ€™endroit oÃ¹ apparaÃ®tront vos graphiques. Packages contient la liste des modules dÃ©jÃ  installÃ©s, ainsi quâ€™un outil de gestion des modules pour leur installation, leur dÃ©sinstallation et leur mise Ã  jour. Help affiche les fiches dâ€™aide des fonctions (pour obtenir de lâ€™aide sur une fonction dans RStudio, surlignez la fonction dans votre feuille de calcul, puis appuyez sur F1). Enfin, lâ€™onglet Viewer affichera les sorties HTML, en particulier les graphiques interactifs que vous gÃ©nÃ©rerez par exemple avec le module plotly, ou alors le rendu de votre fichier Quarto. Si votre environnement de travail Ã©tait un avion, R serait le moteur et RStudio serait le cockpit!\n\n\n\n\n\nFigureÂ 2.6: ScÃ¨ne de Fifi Brindacier (Astrid Lindgren, 1945).\n\n\n\n\n2.3.2 Installation avec Anaconda\nSi vous cherchez une trousse complÃ¨te dâ€™analyse de donnÃ©es, comprenant R et Python, vous pourrez prÃ©fÃ©rer Anaconda. Une fois installÃ©e, vous pourrez isoler un environnement de travail sur R, ou mÃªme isoler des environnements de travail particuliers pour vos projets. Une maniÃ¨re conviviale de crÃ©er des environnements de travail est de passer par lâ€™interface Anaconda navigator, que vous lancerez soit dans le menu Windows, soit en ligne de commande anaconda-navigator sous Mac et Linux, puis dâ€™installer r-essentials, rstudio et jupyterlab dans lâ€™onglet Environment. Vous pourrez aussi installer RStudio et Jupyter lab via lâ€™onglet Home de Anaconda navigator. Dans lâ€™environnement de base, installez le package nb_conda_kernels pour vous assurer que tous les noyaux (R, Python, etc.) installÃ©s dans les environnements de travail soient automatiquement accessibles dans Jupyter. Si vous dÃ©sirez utiliser dans Jupyter la version de R installÃ©e avec lâ€™installation classique, rÃ©fÃ©rez-vous au guide prÃ©sentÃ© en extra au bas de la page.\n\n\n\n\nFigureÂ 2.7: Anaconda navigator.\n\n\n\nJupyter lab est une interface notebook semblable Ã  Quarto - les format Jupyter (*.ipynb) et Quarto (*.qmd) sont par ailleurs convertibles grÃ¢ce au module jupytext. Lâ€™utilisation de R en Anaconda nâ€™est pas tout Ã  fait au point, et pourrait poser problÃ¨me pour lâ€™installation de certains modules. Si vous optez pour cette option, prÃ©parez-vous Ã  avoir Ã  bidouiller un peu. Plusieurs prÃ©fÃ¨rent Jupyter Ã  RStudio (ce nâ€™est pas mon cas).\n\n2.3.3 Utilisation infonuagique\nPas besoin dâ€™avoir une machine super puissante pour travailler en R. Il existe une multitude de services infonuagiques (dans le cloud) vous permettant de lancer vos calculs sur des serveurs plutÃ´t que sur votre Chromebook ou votre vieux laptop dÃ©glinguÃ©. Certains services sont gratuits, et dâ€™autres souvent plus Ã©laborÃ©s sont payants. Vous pouvez utiliser gratuitement Azure Notebooks ou un tour de passe-passe pour faire fonctionner Google colab en R. Une option gratuite de CoCalc vient avec un agressant bandeau rouge vif qui disparait avec lâ€™option payante.\nÃ€ mon avis, le service Nextjournal est celui dâ€™entre tous qui possÃ¨de en ce moment les meilleures qualitÃ©s dans sa version gratuite. Vous pourrez y travailler en mode collaboratif, comme dans Google docs. En outre, vous pouvez lancer ces notes de cours en les important dans Nextjournal. Vous devrez toutefois dÃ©poser les donnÃ©es dans lâ€™interface, puis Ã  chaque session installer les modules spÃ©cialisÃ©s. Le service gratuit offre peu de puissance de calcul, mais pour effectuer les applications de base, Ã§a devrait Ãªtre suffisant. La vidÃ©o ci-dessous monter comment importer les notes de cours dans Nextjournal.\nVideo"
  },
  {
    "objectID": "02-R.html#premiers-pas-avec-r",
    "href": "02-R.html#premiers-pas-avec-r",
    "title": "2Â  La science des donnÃ©es avec R",
    "section": "\n2.4 Premiers pas avec R",
    "text": "2.4 Premiers pas avec R\nR ne fonctionne pas avec des menus, en faisant danser une souris sous une musique de clics. Vous devrez donc entrer des commandes avec votre clavier, que vous apprendrez par cÅ“ur au fur et Ã  mesure, ou que vous retrouverez en lanÃ§ant des recherches sur internet. Par expÃ©rience personnelle, lorsque je travaille avec R, jâ€™ai toujours un navigateur ouvert prÃªt Ã  recevoir une question.\nLes Ã©tapes qui suivent sont des premiers pas. Elles ne feront pas de vous des ceintures noires de la programmation. La plupart des utilisateurs de R ont appris en se pratiquant sur leurs donnÃ©es, en se butant sur des obstacles, en apprenant comment les surmonter ou les contournerâ€¦\nPour lâ€™instant, ouvrez seulement un interprÃ©teur de commande, et lancez R. Voyons si R est aussi libre quâ€™on le prÃ©tend.\n\nâ€œLa libertÃ©, câ€™est la libertÃ© de dire que deux et deux font quatre. Si cela est accordÃ©, tout le reste suit.â€ - George Orwell, 1984\n\n\n2 + 2\n\n[1] 4\n\n\nEt voilÃ .\n\nLes opÃ©rations mathÃ©matiques sont effectuÃ©es telles que lâ€™on devrait sâ€™attendre.\n\n67.1 - 43.3\n\n[1] 23.8\n\n2 * 4\n\n[1] 8\n\n1 / 2\n\n[1] 0.5\n\n\nLâ€™exposant peut Ãªtre notÃ© ^, comme câ€™est le cas dans Excel, ou ** comme câ€™est le cas en Python.\n\n2^4\n\n[1] 16\n\n\n\n2**4\n\n[1] 16\n\n\n\n1 / 2 # utilisez des espaces de part et d'autre des opÃ©rateurs (sauf pour l'exposant) pour Ã©claircir le code\n\n[1] 0.5\n\n\nR ne lit pas ce qui suit le caractÃ¨re #. Cela vous laisse lâ€™opportunitÃ© de commenter un code comprenant une sÃ©quence de plusieurs lignes. Remarquez Ã©galement que la derniÃ¨re opÃ©ration comporte des espaces entre les nombres et lâ€™opÃ©rateur /. Dans ce cas (ce nâ€™est pas toujours le cas), les espaces ne signifient rien: ils aident seulement Ã  Ã©claircir le code. Il existe des guides pour lâ€™Ã©criture de code en R. Je recommande fortement de suivre mÃ©ticuleusement le guide de style de tidyverse.\nAssigner des objets Ã  des variables est fondamental en programmation. En R, on assigne traditionnellement avec la flÃ¨che &lt;-, mais vous verrez parfois le =, qui est davantage utilisÃ© comme standard dans dâ€™autres langages de programmation. Par exemple.\n\na &lt;- 3\n\nTruc. Essayez dâ€™inverser la flÃ¨che, e.g.Â 3 -&gt; a.\nTechniquement, a pointe vers le nombre entier 3. ConsÃ©quemment, on peut effectuer des opÃ©rations sur a.\n\na * 6\n\n[1] 18\n\n\n\nA + 2\n\nLe message dâ€™erreur nous dit que A nâ€™est pas dÃ©fini. Sa version minuscule, a, lâ€™est pourtant. La raison est que R considÃ¨re la case dans la dÃ©finition des objets. Utiliser la mauvaise case mÃ¨ne donc Ã  des erreurs.\nNote. Les messages dâ€™erreur ne sont pas toujours clairs, mais vous apprendrez Ã  les comprendre. Dans tous les cas, ils sont fait pour vous aider. Lisez-les attentivement!\nEn gÃ©nÃ©ral, le nom dâ€™une variable doit toujours commencer par une lettre, et ne doit pas contenir de caractÃ¨res rÃ©servÃ©s (espaces, +, *). Dans la dÃ©finition des variables, plusieurs utilisent des symboles . pour dÃ©limiter les mots, mais la barre de soulignement _ est Ã  prÃ©fÃ©rer. En effet, dans dâ€™autres langages de programmation comme Python, le . a une autre signification: son utilisation est Ã  Ã©viter autant que possible. De mÃªme, Ã©vitez lâ€™utilisation de majuscules pour nommer vos objets (voir le guide de style de tidyverse pour nommer les objets).\nNote. Ã€ ce stade, vous serez probablement plus Ã  lâ€™aise de copier-coller ces commandes dans votre terminal.\n\nrendement_arbre &lt;- 50 # pomme/arbre\nnombre_arbre &lt;- 300 # arbre\nnombre_pomme &lt;- rendement_arbre * nombre_arbre\nnombre_pomme\n\n[1] 15000\n\n\nComme chez la plupart des langages de programmation, R respecte les conventions des prioritÃ©s des opÃ©rations mathÃ©atiques.\n\n10 - 9^0.5 * 2\n\n[1] 4\n\n\n\n2.4.1 Types de donnÃ©es\nJusquâ€™Ã  maintenant, nous nâ€™avons utilisÃ© que des nombres entiers (integer ou int) et des nombres rÃ©els (numeric ou float64). R inclut dâ€™autres types. La chaÃ®ne de caractÃ¨re (string ou character) contient un ou plusieurs symboles. Elle est dÃ©finie entre des doubles guillemets \" \" ou des apostrophes ' '. Il nâ€™existe pas de standard sur lâ€™utilisation de lâ€™un ou de lâ€™autre, mais en rÃ¨gle gÃ©nÃ©rale, on utilise les apostrophes pour les expressions courtes, contenant un simple mot ou une sÃ©quence de lettres, et les guillemets pour les phrases. Une raison pour cela: les guillemets sont utiles pour insÃ©rer des apostrophes dans une chaÃ®ne de caractÃ¨re.\n\na &lt;- \"L'ours\"\nb &lt;- \"polaire\"\npaste(a, b)\n\n[1] \"L'ours polaire\"\n\n\nOn colle a et b avec la fonction paste. Notez que lâ€™objet a a Ã©tÃ© dÃ©fini prÃ©cÃ©demment. Il est possible en R de rÃ©assigner une variable, mais cela peut porter Ã  confusion, jusquâ€™Ã  gÃ©nÃ©rer des erreurs de calcul si une variable nâ€™est pas assignÃ©e Ã  lâ€™objet auquel on voulait rÃ©fÃ©rer.\nCombien de caractÃ¨res contient la chaÃ®ne \"L'ours polaire\"? R sait compter. Demandons-lui.\n\nc &lt;- paste(a, b)\nnchar(c)\n\n[1] 14\n\n\nQuatorze, câ€™est bien cela (comptez â€œLâ€™ours polaireâ€, en incluant lâ€™espace). Comme paste, nchar est une fonction incluse par dÃ©faut dans lâ€™environnement de travail de R: plus prÃ©cisÃ©ment, ces fonctions sont incluses dans le module base, inclut par dÃ©faut lorsque R est lancÃ©. La fonction est appelÃ©e en Ã©crivant nchar(). Mais une fonction de quoi? Des arguments, qui se trouvent entre les parenthÃ¨ses. Dans ce cas, il y a un seul argument: c.\nEn calcul scientifique, il est courant de lancer des requÃªtes dÃ©terminant si un rÃ©sultat est vrai ou faux.\n\na &lt;- 17\na &lt; 10\n\n[1] FALSE\n\na &gt; 10\n\n[1] TRUE\n\na == 10\n\n[1] FALSE\n\na != 10\n\n[1] TRUE\n\na == 17\n\n[1] TRUE\n\n!(a == 17)\n\n[1] FALSE\n\n\nJe viens dâ€™introduire un nouveau type de donnÃ©e: les donnÃ©es boolÃ©ennes (boolean, ou logical), qui ne peuvent prendre que deux Ã©tats - TRUE ou FALSE. En mÃªme temps, jâ€™ai utilisÃ© la fonction print parce que dans mon carnet, seule la derniÃ¨re opÃ©ration permet dâ€™afficher le rÃ©sultat. Si lâ€™on veut forcer une sortie, on utilise print. Puis, on a vu plus haut que le symbole = est rÃ©servÃ© pour assigner des objets: pour les tests dâ€™Ã©galitÃ©, on utilise le double Ã©gal, ==, ou != pour la non-Ã©galitÃ©. Enfin, pour inverser une donnÃ©e de type boolÃ©enne, on utilise le point dâ€™exclamation !.\n\n2.4.2 Les collections de donnÃ©es\nLes exercices prÃ©cÃ©dents ont permis de prÃ©senter les types de donnÃ©es offerts par dÃ©faut sur R qui sont les plus importants pour le calcul scientifique: int (integer, ou nombre entier), numeric (nombre rÃ©el), character (string, ou chaÃ®ne de caractÃ¨re) et logical (boolÃ©en). Dâ€™autres sâ€™ajouteront tout au long du cours, comme les catÃ©gories (factor) et les unitÃ©s de temps (date-heure).\nLorsque lâ€™on procÃ¨de Ã  des opÃ©rations de calcul en science, nous utilisons rarement des valeurs uniques. Nous prÃ©fÃ©rons les organiser et les traiter en collections. Par dÃ©faut, R offre quatre types importants de collections: les vecteurs, les matrices, les listes et les tableaux.\n\n2.4.2.1 Vecteurs\nDâ€™abord, les vecteurs sont une sÃ©rie de variables de mÃªme type. Un vecteur est dÃ©limitÃ© par la fonction c( ) (c pour concatÃ©nation). Les Ã©lÃ©ments de la liste sont sÃ©parÃ©s par des virgules.\n\nespece &lt;- c(\"Petromyzon marinus\", \"Lepisosteus osseus\", \"Amia calva\", \"Hiodon tergisus\")\nespece\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\n\nPour accÃ©der aux Ã©lÃ©ments dâ€™une liste, one appelle la liste suivie de la position de lâ€™objet dÃ©sirÃ© entre crochets.\n\nespece[1]\n\n[1] \"Petromyzon marinus\"\n\nespece[2]\n\n[1] \"Lepisosteus osseus\"\n\nespece[1:3]\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n\nespece[c(1, 3)]\n\n[1] \"Petromyzon marinus\" \"Amia calva\"        \n\n\nOn peut noter que le premier Ã©lÃ©ment de la liste est notÃ© 1, et non 0 comme câ€™est le cas de la plupart de langages. Le raccourcis 1:3 crÃ©e une liste de nombres entiers de 1 Ã  3 inclusivement, câ€™est-Ã -dire lâ€™Ã©quivalent de c(1, 2, 3). En effet, on crÃ©e une liste dâ€™indices pour soutirer des Ã©lÃ©ments dâ€™une liste. On peut utiliser le symbole de soustraction pour retirer un ou plusieurs Ã©lÃ©ments dâ€™un vecteur.\n\nespece[-c(1, 3)]\n\n[1] \"Lepisosteus osseus\" \"Hiodon tergisus\"   \n\n\nPour ajouter un Ã©lÃ©ment Ã  notre liste, on peut utiliser la fonction c( ).\n\nespece &lt;- c(espece, \"Cyprinus carpio\")\nespece\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"    \"Cyprinus carpio\"   \n\n\nNotez que lâ€™on efface lâ€™objet espece par une concatÃ©nation de lâ€™objet espece, prÃ©cÃ©demment dÃ©finie, et dâ€™un autre Ã©lÃ©ment.\nEn lanÃ§ant espece[3] &lt;- \"Lepomis gibbosus\", il est possible de changer un Ã©lÃ©ment de la liste.\n\nespece[3] &lt;- \"Lepomis gibbosus\"\nespece\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Lepomis gibbosus\"  \n[4] \"Hiodon tergisus\"    \"Cyprinus carpio\"   \n\n\n\n2.4.2.2 Matrices\nUne matrice est un vecteur de dimension plus Ã©levÃ©e que 1. En Ã©cologie, on dÃ©passe rarement la deuxiÃ¨me dimension, quoi que les matrices en N dimensions soient courantes en modÃ©lisation mathÃ©matique. Je ne considÃ©rerai pour le moment que des matrices 2D. Comme câ€™est la cas des vecteurs, les matrices contiennent des valeurs de mÃªme type. En R, on peut attribuer aux matrices 2D des noms de ligne et de colonne.\n\nmat &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), \n              ncol = 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\n\ncolnames(mat) &lt;- c(\"A\", \"B\", \"C\")\nrownames(mat) &lt;- c(\"site_1\", \"site_2\", \"site_3\", \"site_4\")\nmat\n\n       A B  C\nsite_1 1 5  9\nsite_2 2 6 10\nsite_3 3 7 11\nsite_4 4 8 12\n\n\nOn peut soutirer les noms de colonne et les noms de ligne. Le rÃ©sultat est un vecteur.\n\ncolnames(mat)\n\n[1] \"A\" \"B\" \"C\"\n\nrownames(mat)\n\n[1] \"site_1\" \"site_2\" \"site_3\" \"site_4\"\n\n\n\n2.4.2.3 Listes\nLes listes sont des collections hÃ©tÃ©rogÃ¨nes dans lesquelles on peut placer les objets dÃ©sirÃ©s, sans distinction: elles peuvent mÃªme inclure dâ€™autres listes. Chacun des Ã©lÃ©ments de la liste peut Ãªtre identifiÃ© par une clÃ©.\n\nma_liste &lt;- list(\n  especes = c(\n    \"Petromyzon marinus\", \"Lepisosteus osseus\",\n    \"Amia calva\", \"Hiodon tergisus\"\n  ),\n  site = \"A101\",\n  stations_meteos = c(\"746583\", \"783786\", \"856363\")\n)\nma_liste\n\n$especes\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\n$site\n[1] \"A101\"\n\n$stations_meteos\n[1] \"746583\" \"783786\" \"856363\"\n\n\nLes Ã©lÃ©ments de la liste peuvent Ãªtre soutirÃ©s par le nom de la clÃ© ou par lâ€™indice, de cette maniÃ¨re.\n\nma_liste$especes\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\nma_liste[[1]]\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\n\nExercice. AccÃ©der au deuxiÃ¨me Ã©lÃ©ment du vecteur dâ€™espÃ¨ces dans la liste ma_liste.\n\n2.4.2.4 Tableaux\nEnfin, le type de collection de donnÃ©es le plus important est le tableau, ou data.frame. Techniquement, il sâ€™agit dâ€™une liste composÃ©e de vecteurs de mÃªme longueur. Chaque colonne peut ainsi prendre un type de donnÃ©e indÃ©pendamment des autres colonnes.\n\ntableau &lt;- data.frame(\n  espece = c(\n    \"Petromyzon marinus\", \"Lepisosteus osseus\",\n    \"Amia calva\", \"Hiodon tergisus\"\n  ),\n  poids = c(10, 13, 21, 4),\n  longueur = c(35, 44, 50, 8)\n)\ntableau\n\n              espece poids longueur\n1 Petromyzon marinus    10       35\n2 Lepisosteus osseus    13       44\n3         Amia calva    21       50\n4    Hiodon tergisus     4        8\n\n\nEn programmation classique en R (nous verrons plus loin la mÃ©thode tidyverse), les Ã©lÃ©ments dâ€™un tableau se manipulent comme ceux dâ€™une matrice et les colonnes peuvent Ãªtre appelÃ©s comme les Ã©lÃ©ments dâ€™une liste.\n\ntableau[, 2:3]\n\n  poids longueur\n1    10       35\n2    13       44\n3    21       50\n4     4        8\n\ntableau$poids\n\n[1] 10 13 21  4\n\n\nVous verrez aussi, quoi que rarement, ce format, qui Ã  la diffÃ©rence du format $ gÃ©nÃ¨re un tableau.\n\ntableau[\"poids\"]\n\n  poids\n1    10\n2    13\n3    21\n4     4\n\n\nLe tableau est le format de collection Ã  privilÃ©gier pour manipuler des donnÃ©es. RÃ©cemment, le format de tableau tibble a Ã©tÃ© crÃ©Ã© par lâ€™Ã©quipe de RStudio pour offrir un format plus moderne.\n\n2.4.3 Les fonctions\nLorsque vous Ã©crivez une commande suivit de parenthÃ¨ses, comme data.frame(especes = ...), vous demandez Ã  R de passer Ã  lâ€™action en appelant une fonction. De maniÃ¨re trÃ¨s gÃ©nÃ©rale, une fonction transforme quelque chose en quelque chose dâ€™autre (FigureÂ 2.8).\n\n\n\n\nFigureÂ 2.8: SchÃ©ma simplifiÃ© dâ€™une fonction.\n\n\n\nPar exemple, la fonction mean() prend une collection de nombre comme entrÃ©e, puis en sort vous devinez quoi.\n\nmean(tableau$poids)\n\n[1] 12\n\n\nLes entrÃ©es sont appelÃ©s les arguments de la fonction. Leur dÃ©finition est toujours disponible dans la documentation.\nExercice. Familiarisez-vous avec la documentation de R en lanÃ§ant ?mean. Truc: si vous avez pris de lâ€™avance et que vous travaillez dÃ©jÃ  en RStudio, mettez le terme en surbrillance, puis appuyez sur F1.\nVous verrez dans la documentation que la fonction mean() demande trois arguments, x, trim et na.rm. Or nous avons seulement placÃ© un vecteur, sans spÃ©cifier dâ€™argument!\nEn effet. En lâ€™absence dâ€™une dÃ©finition des arguments, R supposera que les arguments dans la parenthÃ¨se, sÃ©parÃ©s par une virgule, sont prÃ©sentÃ©s dans le mÃªme ordre que celui spÃ©cifiÃ© dans la dÃ©finition de la fonction (celle qui est prÃ©sentÃ©e dans le fichier dâ€™aide). Dans le cas qui nous intÃ©resse, mean(tableau$poids) est Ã©quivalent Ã  mean(x = tableau$poids).\nMaintenant, selon la fiche dâ€™aide, lâ€™argument na.rm est un valeur logique spÃ©cifiant si oui (TRUE) ou non (FALSE) les valeurs manquantes doivent Ãªtre considÃ©rÃ©es (une moyenne dâ€™un vecteur comprenant au moins un NA sera de NA). En ne spÃ©cifiant rien, R prend la valeur par dÃ©faut, telle que spÃ©cifiÃ©e dans la documentation. Il en va de mÃªme pour lâ€™argument trim, qui permet dâ€™Ã©laguer des valeurs extrÃªmes. Dans la fiche dâ€™aide, mean(x, trim = 0, na.rm = FALSE, ...) signifie que par dÃ©faut, lâ€™argument x est vide (il doit donc Ãªtre spÃ©cifiÃ©), lâ€™argument trim est de 0 et lâ€™argument na.rm est FALSE.\n\nmean(c(6, 1, 7, 4, 9, NA, 1))\n\n[1] NA\n\nmean(c(6, 1, 7, 4, 9, NA, 1), na.rm = TRUE)\n\n[1] 4.666667\n\n\nVous nâ€™Ãªtes pas emprisonnÃ© par les fonctions offertes par R. Vous pouvez installer des modules qui complÃ¨tent les fonctions de base de R: on le verra un peu plus loin dans ce chapitre. Mais pour lâ€™instant, voyons comment vous pouvez crÃ©er vos propres fonctions. Disons que vous voulez crÃ©er une fonction qui calcule la sortie de \\(x^3-2y+a\\). Pour obtenir la rÃ©ponse, on a besoin des arguments x, y et a. La sortie de la fonction est ici triviale: la rÃ©ponse de lâ€™Ã©quation. Lâ€™opÃ©ration function permet de prendre Ã§a en charge.\n\noperation_f &lt;- function(x, y, a = 10) {\n  return(x^3 - 2 * y + a)\n}\n\nNotez que a a une valeur par dÃ©faut. La sortie de la fonction est ce qui se trouve entre les parenthÃ¨ses de return. Vous pouvez maintenant utiliser la fonction operation_f au besoin.\n\noperation_f(x = 2, y = 3, a = 1)\n\n[1] 3\n\n\nUne telle fonction est peu utile. Mais lâ€™utilisation de fonctions personnalisÃ©es vous permettra dâ€™Ã©viter de rÃ©pÃ©ter la mÃªme opÃ©ration plusieurs fois dans un flux de travail, en Ã©vitant de gÃ©nÃ©rer trop de code, donc aussi de potentielles erreurs. Personnellement, jâ€™utilise les fonctions surtout pour gÃ©nÃ©rer des graphiques personnalisÃ©s.\nExercice. Afin dâ€™acquÃ©rir de lâ€™autonomie, vous devrez Ãªtre en mesure de trouver le nom des commandes dont vous avez besoin pour effectuer la tÃ¢che que vous dÃ©sirez effectuer. Cela peut causer des frustrations, mais vous vous sentirez toujours plus Ã  lâ€™aise avec R jour aprÃ¨s jour. Lâ€™exercice ici est de trouver par vous-mÃªme la commande qui vous permettra mesurer la longueur dâ€™un vecteur.\n\n2.4.4 Les boucles\nLes boucles permettent dâ€™effectuer une mÃªme suite dâ€™opÃ©rations sur plusieurs objets. Pour faire suite Ã  notre exemple, nous dÃ©sirons obtenir le rÃ©sultat de lâ€™opÃ©ration f pour des paramÃ¨tres que nous enregistrons dans ce tableau.\n\nparams &lt;- data.frame(\n  x = c(2, 4, 1, 5, 6),\n  y = c(3, 4, 8, 1, 0),\n  a = c(6, 1, 8, 2, 5)\n)\nparams\n\n  x y a\n1 2 3 6\n2 4 4 1\n3 1 8 8\n4 5 1 2\n5 6 0 5\n\n\nNous crÃ©ons un vecteur vide, puis nous effectuons une itÃ©ration ligne par ligne en remplissant le vecteur.\n\noperation_res &lt;- c()\nfor (i in 1:nrow(params)) {\n  operation_res[i] &lt;- operation_f(x = params[i, 1], y = params[i, 2], a = params[i, 3])\n}\noperation_res\n\n[1]   8  57  -7 125 221\n\n\nEn faisant varier i sur des valeurs du vecteur donnÃ© par la sÃ©quence de nombres entiers de 1 au nombre de ligne du tableau de paramÃ¨tres, nous demandons Ã  R dâ€™effectuer la suite dâ€™opÃ©ration entre les accolades {}. Ã€ chaque boucle, i prend une valeur de la sÃ©quence. i est utilisÃ© ici comme indice de la ligne Ã  soutirer du tableau params, qui correspond Ã  lâ€™indice dans le vecteur operation_res.\nAinsi, chaque rÃ©sultat est calculÃ© dans lâ€™ordre des lignes du tableau de paramÃ¨tres et lâ€™on pourra trÃ¨s bien y coller nos rÃ©sultats:\n\nparams$resultats &lt;- operation_res\nparams\n\n  x y a resultats\n1 2 3 6         8\n2 4 4 1        57\n3 1 8 8        -7\n4 5 1 2       125\n5 6 0 5       221\n\n\nNotez que puisque la colonne resultat nâ€™existe pas dans le tableau params, R crÃ©e automatiquement une nouvelle colonne.\nLes boucles for vous permettront par exemple de gÃ©nÃ©rer en peu de temps 10, 100, 1000 graphiques (autant que vous voulez), chacun issu de simulations obtenues Ã  partir de conditions initiales diffÃ©rentes, et de les enregistrer dans un rÃ©pertoire sur votre ordinateur. Un travail qui pourrait prendre des semaines sur Excel peut Ãªtre effectuÃ© en R en quelques secondes.\nUn second outil est disponible pour les itÃ©rations: les boucles while. Elles effectuent une opÃ©ration tant quâ€™un critÃ¨re nâ€™est pas atteint. Elles sont utiles pour les opÃ©rations oÃ¹ lâ€™on cherche une convergence. Je les couvre rapidement puisquâ€™elles sont rarement utilisÃ©es dans les flux de travail courants. En voici un petit exemple.\n\nx &lt;- 100\nwhile (x &gt; 1.1) {\n  x &lt;- sqrt(x)\n  print(x)\n}\n\n[1] 10\n[1] 3.162278\n[1] 1.778279\n[1] 1.333521\n[1] 1.154782\n[1] 1.074608\n\n\nNous avons initiÃ© x Ã  une valeur de 100. Puis, tant que (while) le test x &gt; 1.1 est vrai, attribuer Ã  x la nouvelle valeur calculÃ©e en extrayant la racine de la valeur prÃ©cÃ©dente de x. Enfin, indiquer la valeur avec print.\n\n2.4.5 Conditions: if, else if, else\n\n\nSi la condition 1 est remplie, effectuer une suite dâ€™instructions 1. Si la condition 1 nâ€™est pas remplie, et si la condition 2 est remplie, effectuer la suite dâ€™instructions 2. Sinon, effectuer la suite dâ€™instruction 3.\n\nVoilÃ  comment on exprime une suite de conditions. Prenons lâ€™exemple simple dâ€™une discrÃ©tisation dâ€™une valeur continue. Si \\(x&lt;10\\), il est classÃ© comme faible. Si \\(10 \\leq x &lt;20\\), il est classÃ© comme moyen. Si \\(x \\geq 20\\), il est classÃ© comme Ã©levÃ©. PlaÃ§ons cette classification dans une fonction.\n\nclassification &lt;- function(x, lim1 = 10, lim2 = 20) {\n  if (x &lt; lim1) {\n    categorie &lt;- \"faible\"\n  } else if (x &lt; lim2) {\n    categorie &lt;- \"moyen\"\n  } else {\n    categorie &lt;- \"Ã©levÃ©\"\n  }\n  return(categorie)\n}\nclassification(-10)\n\n[1] \"faible\"\n\nclassification(15.4)\n\n[1] \"moyen\"\n\nclassification(1000)\n\n[1] \"Ã©levÃ©\"\n\n\nUne condition est dÃ©finie avec le if, suivi du test Ã  vrai ou faux entre parenthÃ¨ses. Si le test retourne un vrai (TRUE), lâ€™instruction entre accolades est exÃ©cutÃ©e. Si elle est fausse, on passe au suivant.\nExercice. Explorer les commandes ifelse et cut et rÃ©flÃ©chissez Ã  la maniÃ¨re quâ€™elles pourraient Ãªtre utilisÃ©es pour effectuer une discrÃ©tisation plus efficacement quâ€™avec les if et les else.\n\n2.4.6 Installer et charger un module\nLa plupart des opÃ©rations dâ€™ordre gÃ©nÃ©ral (comme les racines carrÃ©es, les tests statistiques, la gestion de matrices et de tableau, les graphiques, etc.) sont accessibles grÃ¢ce aux modules de base de R, qui sont installÃ©s et chargÃ©s par dÃ©faut lors du dÃ©marrage de R. Des Ã©quipes de travail ont nÃ©anmoins dÃ©veloppÃ© plusieurs modules pour rÃ©pondre Ã  leurs besoins spÃ©cialisÃ©s, et les ont laissÃ©s disponibles au grand public dans des modules que vous pouvez installer dâ€™un dÃ©pÃ´t CRAN (le AppStore de R), dâ€™un dÃ©pÃ´t Anaconda (le AppStore de Anaconda, si vous utilisez cette plate-forme), dâ€™un dÃ©pÃ´t Github (dÃ©pÃ´ts dÃ©centralisÃ©s), etc.\nRStudio possÃ¨de un pratique bouton Install qui vous permet dâ€™y inscrire une liste de modules. Le navigateur anaconda offre aussi une interface dâ€™installation. La commande R pour installer un module est install.packages(\"ggplot2\"), si par exemple vous dÃ©sirez installer ggplot2, le module graphique par excellence en R. Câ€™est la commande que RStudio lancera tout seul si vous lui demandez dâ€™installer ggplot2.\nLes modules sont lâ€™Ã©quivalent des applications spÃ©cialisÃ©es que vous installez sur un tÃ©lÃ©phone mobile. Pour les utiliser, il faut les ouvrir.\nGÃ©nÃ©ralement, jâ€™ouvre toutes les applications nÃ©cessaires Ã  mon flux de travail au tout dÃ©but de ma feuille de calcul (la prochaine cellule retournera un message dâ€™erreur si les packages ne sont pas installÃ©s).\n\nlibrary(\"tidyverse\") # mÃ©ta-package qui charge entre autres dplyr et ggplot2\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.4.4     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.0\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"vegan\")\n\nLoading required package: permute\nLoading required package: lattice\nThis is vegan 2.6-4\n\nlibrary(\"nlme\")\n\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nLes modules sont installÃ©s sur votre ordinateur Ã  un endroit que vous pourrez retrouver avec la commande .libPaths()\nExercice. Ã€ partir dâ€™ici jusquâ€™Ã  la fin du cours, nous utiliserons RStudio. Ouvrez-le et familiarisez-vous avec lâ€™interface! Quelques petits trucs:\n\npour lancer une ligne, placez votre curseur sur la ligne, puis appuyez sur Ctrl+Enter\npour lancer une partie de code prÃ©cise, mettez le en surbrillance, puis Ctrl+Enter\nutilisez toujours le gestionnaire de projets, en haut Ã  droite!\ninstallez le module tidyverse\n\nlancez data(\"iris\") pour obtenir un tableau dâ€™exercice, puis cliquez sur lâ€™objet dans la fenÃªtre environnement"
  },
  {
    "objectID": "02-R.html#enfin",
    "href": "02-R.html#enfin",
    "title": "2Â  La science des donnÃ©es avec R",
    "section": "\n2.5 Enfinâ€¦",
    "text": "2.5 Enfinâ€¦\nComme une langue, on nâ€™apprend Ã  sâ€™exprimer en un langage informatique quâ€™en se mettant Ã  lâ€™Ã©preuve, ce que vous ferez tout au long de ce cours. Pour vous encourager, voici quelques trucs pour apprendre Ã  coder en R.\n\n\nR nâ€™aime pas lâ€™ambiguÃ¯tÃ©. Une simple virgule mal placÃ©e et il ne sait plus quoi faire. Cela peut Ãªtre frustrant au dÃ©but, mais cette rigiditÃ© est nÃ©cessaire pour effectuer du calcul scientifique.\n\nLe copier-coller est votre ami. En gardant Ã  lâ€™esprit que vous Ãªtre responsable de votre code et que vous respectez les droits dâ€™auteur, nâ€™ayez pas peur de copier-coller des lignes de code et de personnaliser par la suite.\n\nLâ€™erreur que vous obtenez: dâ€™autres lâ€™ont obtenue avant vous. Le site de question-rÃ©ponse stackoverflow est une ressource inestimable oÃ¹ des gens ayant postÃ© des questions ont reÃ§u des rÃ©ponses dâ€™experts (les meilleures rÃ©ponses et les meilleures questions apparaissent en premier). Apprenez Ã  chercher intelligemment des rÃ©ponses en formulant prÃ©cisÃ©ment vos questions!\n\nÃ‰tudiez et pratiquez. Les messages dâ€™erreur en R sont courants, mÃªme chez les personnes expÃ©rimentÃ©es. La meilleure maniÃ¨re dâ€™apprendre une langue est de la parler, dâ€™Ã©tudier ses susceptibilitÃ©s, de les tester dans une conversation, etc."
  },
  {
    "objectID": "02-R.html#petit-truc",
    "href": "02-R.html#petit-truc",
    "title": "2Â  La science des donnÃ©es avec R",
    "section": "\n2.6 Petit truc!",
    "text": "2.6 Petit truc!\nRStudio peut Ãªtre implÃ©mentÃ© avec des extensions. Lâ€™une dâ€™elle permet dâ€™ajuster votre style de code. Par exemple, vous voulez vous assurer que toutes les allocations sont bien effectuÃ©es avec des &lt;- et non pas des =, quâ€™il y a bien des espaces de part et dâ€™autre de &lt;-, que les retours de lignes sont bien placÃ©s, etc. Installez le module styler, et des options apparaÃ®tront dans le menu Addins comme Ã  la FigureÂ 2.9.\n\n\n\n\nFigureÂ 2.9: Lâ€™extension styler permet de formater votre code dans un style particulier"
  },
  {
    "objectID": "02-R.html#extra-jupyter",
    "href": "02-R.html#extra-jupyter",
    "title": "2Â  La science des donnÃ©es avec R",
    "section": "\n2.7 Extra: Utiliser R avec Jupyter",
    "text": "2.7 Extra: Utiliser R avec Jupyter\nPour utiliser R dans Jupyter notebook ou Jupyter lab, vous devez installer le module IRkernel dans la version de R que vous dÃ©sirez utiliser avec Jupyter, puis de lancer la commande IRkernel::installspec(). La prochaine fois que vous ouvrirez Jupyter, le noyau de R devrait apparaÃ®tre.\nJe nâ€™ai aucune expÃ©rience sur Mac, mais semble-t-il cela fonctionne comme en Linux. Ouvrez R Ã  partir dâ€™un terminal (R + Enter), puis lancez IRkernel::installspec() aprÃ¨s avoir installÃ© IRkernel. Si vous travaillez en Windows, il vous faudra lancer R par son chemin complet dans lâ€™invite de commande de Anaconda (Anaconda Powershell Prompt). Par exemple, ouvrir Anaconda Powershell Prompt, puis, si votre installation de R se trouve dans C:\\Program Files\\R\\R-3.6.2,\n(base) PS C:\\Users\\fifi&gt; cd \"C:\\Program Files\\R\\R-3.6.2\\bin\"\n(base) PS C:\\Program Files\\R\\R-3.6.2\\bin&gt; .\\R.exe\n\nR version 3.6.2 (2019-12-12) -- \"Dark and Stormy Night\"\nCopyright (C) 2019 The R Foundation for Statistical Computing\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\n\nR est un logiciel libre livrÃ© sans AUCUNE GARANTIE.\nVous pouvez le redistribuer sous certaines conditions.\nTapez 'license()' ou 'licence()' pour plus de dÃ©tails.\n\nR est un projet collaboratif avec de nombreux contributeurs.\nTapez 'contributors()' pour plus d'information et\n'citation()' pour la faÃ§on de le citer dans les publications.\n\nTapez 'demo()' pour des dÃ©monstrations, 'help()' pour l'aide\nen ligne ou 'help.start()' pour obtenir l'aide au format HTML.\nTapez 'q()' pour quitter R.\n\n&gt; install.packages(\"IRkernel\")\nInstallation du package dans 'C:/Users/fifi/Documents/R/win-library/3.6'\n(car 'lib' n'est pas spÃ©cifiÃ©)\n--- SVP sÃ©lectionner un miroir CRAN pour cette session ---\nessai de l'URL 'https://cloud.r-project.org/bin/windows/contrib/3.6/IRkernel_1.1.zip'\nContent type 'application/zip' length 138696 bytes (135 KB)\ndownloaded 135 KB\n\nle package 'IRkernel' a Ã©tÃ© dÃ©compressÃ© et les sommes MD5 ont Ã©tÃ© vÃ©rifiÃ©es avec succÃ©s\n\nLes packages binaires tÃ©lÃ©chargÃ©s sont dans\n       C:\\Users\\fifi\\AppData\\Local\\Temp\\Rtmp6xJtB3\\downloaded_packages\n\n&gt; IRkernel::installspec()\n[InstallKernelSpec] Installed kernelspec ir in C:\\Users\\fifi\\AppData\\Roaming\\jupyter\\kernels\\ir\n&gt; qui()"
  },
  {
    "objectID": "03-tableaux.html#les-collections-de-donnÃ©es",
    "href": "03-tableaux.html#les-collections-de-donnÃ©es",
    "title": "3Â  Organisation des donnÃ©es et opÃ©rations sur des tableaux",
    "section": "\n3.1 Les collections de donnÃ©es",
    "text": "3.1 Les collections de donnÃ©es\nDans le chapitreÂ 2, nous avons survolÃ© diffÃ©rents types dâ€™objets : rÃ©els, entiers, chaÃ®nes de caractÃ¨res et boolÃ©ens. Les donnÃ©es peuvent appartenir Ã  dâ€™autres types : dates, catÃ©gories ordinales (ordonnÃ©es : faible, moyen, Ã©levÃ©) et nominales (non ordonnÃ©es : espÃ¨ces, cultivars, couleurs, unitÃ© pÃ©dologique, etc.). Comme mentionnÃ© en dÃ©but de chapitre, une donnÃ©e est une valeur associÃ©e Ã  une variable. Les donnÃ©es peuvent Ãªtre organisÃ©es en collections.\nNous avons aussi vu au chapitreÂ 2 que la maniÃ¨re privilÃ©giÃ©e dâ€™organiser des donnÃ©es Ã©tait sous forme de tableaux. De maniÃ¨re gÃ©nÃ©rale, un tableau de donnÃ©es est une organisation de donnÃ©es en deux dimensions, comportant des lignes et des colonnes. Il est prÃ©fÃ©rable de respecter la convention selon laquelle les lignes sont des observations et les colonnes sont des variables. Ainsi, un tableau est une liste de vecteurs de mÃªme longueur, chaque vecteur reprÃ©sentant une variable. Chaque variable est libre de prendre le type de donnÃ©es appropriÃ©. La position dâ€™une donnÃ©e dans le vecteur correspond Ã  une observation. Lorsque les vecteurs sont posÃ©s les uns Ã  cÃ´tÃ© des autres, la position dans le vecteur devient une ligne qui dÃ©finit les valeurs des variables dâ€™une observation.\nImaginez que vous consignez des donnÃ©es mÃ©tÃ©orologiques comme les prÃ©cipitations totales ou la tempÃ©rature moyenne pour chaque jour, pendant une semaine sur les sites A, B et C. Chaque site possÃ¨de ses propres caractÃ©ristiques, comme la position en longitude-latitude. Il est redondant de rÃ©pÃ©ter la position du site pour chaque jour de la semaine. Vous prÃ©fÃ©rerez crÃ©er deux tableaux : un pour dÃ©crire vos observations, et un autre pour dÃ©crire les sites. De cette maniÃ¨re, vous crÃ©ez une collection de tableaux interreliÃ©s : une base de donnÃ©es. Nous couvrirons cette notion un peu plus loin. R peut soutirer des donnÃ©es des bases de donnÃ©es grÃ¢ce au module DBI, qui nâ€™est pas couvert Ã  ce stade de dÃ©veloppement du cours.\nDans R, les donnÃ©es structurÃ©es en tableaux, ainsi que les opÃ©rations sur les tableaux, peuvent Ãªtre gÃ©rÃ©es grÃ¢ce aux modules readr, dplyr et tidyr, tous des modules faisant partie du mÃ©ta-module tidyverse, qui est un genre de Microsoft Office sur R : plusieurs modules fonctionnant en interopÃ©rabilitÃ©. Mais avant de se lancer dans lâ€™utilisation de ces modules, voyons quelques rÃ¨gles Ã  suivre pour bien structurer ses donnÃ©es en format tidy, un jargon du tidyverse qui signifie proprement organisÃ©."
  },
  {
    "objectID": "03-tableaux.html#organiser-un-tableau-de-donnÃ©es",
    "href": "03-tableaux.html#organiser-un-tableau-de-donnÃ©es",
    "title": "3Â  Organisation des donnÃ©es et opÃ©rations sur des tableaux",
    "section": "\n3.2 Organiser un tableau de donnÃ©es",
    "text": "3.2 Organiser un tableau de donnÃ©es\nAfin de repÃ©rer chaque cellule dâ€™un tableau, on attribue Ã  chaque ligne et Ã  chaque colonne un identifiant unique, que lâ€™on nomme indice pour les lignes et entÃªte pour les colonnes.\n\nRÃ¨gle no 1. Une variable par colonne, une observation par ligne, une valeur par cellule.\n\nLes unitÃ©s expÃ©rimentales sont dÃ©crites par une ou plusieurs variables, par des chiffres ou des lettres. Chaque variable devrait Ãªtre prÃ©sente en une seule colonne, et chaque ligne devrait correspondre Ã  une unitÃ© expÃ©rimentale oÃ¹ ces variables ont Ã©tÃ© mesurÃ©es. La rÃ¨gle parait simple, mais elle est rarement respectÃ©e. Prenez par exemple le tableau suivant.\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.4.4     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.0\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nTableÂ 3.1: Rendements obtenus sur les sites expÃ©rimentaux selon les traitements.\n\nSite\nTraitement A\nTraitement B\nTraitement C\n\n\n\nSainte-Souris\n4.1\n8.2\n6.8\n\n\nSainte-Fourmi\n5.8\n5.9\nNA\n\n\nSaint-Ours\n2.9\n3.4\n4.6\n\n\n\n\n\n\nQuâ€™est-ce qui cloche avec ce tableau? Chaque ligne est une observation, mais contient plusieurs observations dâ€™une mÃªme variable, le rendement, qui devient Ã©talÃ© sur plusieurs colonnes. Ã€ bien y penser, le type de traitement est une variable et le rendement en est une autre:\n\n\n\n\nTableÂ 3.2: Rendements obtenus sur les sites expÃ©rimentaux selon les traitements.\n\nSite\nTraitement\nRendement\n\n\n\nSainte-Souris\nTraitement A\n4.1\n\n\nSainte-Souris\nTraitement B\n8.2\n\n\nSainte-Souris\nTraitement C\n6.8\n\n\nSainte-Fourmi\nTraitement A\n5.8\n\n\nSainte-Fourmi\nTraitement B\n5.9\n\n\nSainte-Fourmi\nTraitement C\nNA\n\n\nSaint-Ours\nTraitement A\n2.9\n\n\nSaint-Ours\nTraitement B\n3.4\n\n\nSaint-Ours\nTraitement C\n4.6\n\n\n\n\n\n\nPlus prÃ©cisÃ©ment, lâ€™expression Ã  bien y penser suggÃ¨re une rÃ©flexion sur la signification des donnÃ©es. Certaines variables peuvent parfois Ãªtre intÃ©grÃ©es dans une mÃªme colonne, parfois pas. Par exemple, les concentrations en cuivre, zinc et plomb dans un sol contaminÃ© peuvent Ãªtre placÃ©es dans la mÃªme colonne â€œConcentrationâ€ ou dÃ©clinÃ©es en plusieurs colonnes Cu, Zn et Pb. La premiÃ¨re version trouvera son utilitÃ© pour crÃ©er des graphiques (chapitreÂ 4), alors que la deuxiÃ¨me favorise le traitement statistique (chapitreÂ 7). Il est possible de passer dâ€™un format Ã  lâ€™autre grÃ¢ce Ã  la fonction pivot_longer() et pivot_wider() du module tidyr.\n\nRÃ¨gle no 2. Un tableau par unitÃ© observationnelle: ne pas rÃ©pÃ©ter les informations.\n\nReprenons la mÃªme expÃ©rience. Supposons que vous mesurez la prÃ©cipitation Ã  lâ€™Ã©chelle du site.\n\n\n\n\nTableÂ 3.3: Rendements et prÃ©cipitations obtenus sur les sites expÃ©rimentaux selon les traitements.\n\nSite\nTraitement\nRendement\nPrÃ©cipitations\n\n\n\nSainte-Souris\nTraitement A\n4.1\n813\n\n\nSainte-Souris\nTraitement B\n8.2\n813\n\n\nSainte-Souris\nTraitement C\n6.8\n813\n\n\nSainte-Fourmi\nTraitement A\n5.8\n642\n\n\nSainte-Fourmi\nTraitement B\n5.9\n642\n\n\nSainte-Fourmi\nTraitement C\nNA\n642\n\n\nSaint-Ours\nTraitement A\n2.9\n1028\n\n\nSaint-Ours\nTraitement B\n3.4\n1028\n\n\nSaint-Ours\nTraitement C\n4.6\n1028\n\n\n\n\n\n\nSegmenter lâ€™information en deux tableaux serait prÃ©fÃ©rable.\n\n\n\n\nTableÂ 3.4: PrÃ©cipitations sur les sites expÃ©rimentaux.\n\nSite\nPrÃ©cipitations\n\n\n\nSainte-Souris\n813\n\n\nSainte-Fourmi\n642\n\n\nSaint-Ours\n1028\n\n\n\n\n\n\nLes tableaux TableÂ 3.2 et TableÂ 3.4, ensemble, forment une base de donnÃ©es (collection organisÃ©e de tableaux). Les opÃ©rations de fusion entre les tableaux peuvent Ãªtre effectuÃ©es grÃ¢ce aux fonctions de jointure (left_join(), par exemple) du module tidyr. Une jointure de TableÂ 3.4 vers TableÂ 3.2 donnera le tableau TableÂ 3.3.\n\nRÃ¨gle no 3. Ne pas bousiller les donnÃ©es.\n\nPar exemple.\n\n\nAjouter des commentaires dans des cellules. Si une cellule mÃ©rite dâ€™Ãªtre commentÃ©e, il est prÃ©fÃ©rable de placer les commentaires soit dans un fichier dÃ©crivant le tableau de donnÃ©es, soit dans une colonne de commentaire juxtaposÃ©e Ã  la colonne de la variable Ã  commenter. Par exemple, si vous nâ€™avez pas mesurÃ© le pH pour une observation, nâ€™Ã©crivez pas â€œÃ©chantillon contaminÃ©â€ dans la cellule, mais annoter dans un fichier dâ€™explication que lâ€™Ã©chantillon no X a Ã©tÃ© contaminÃ©. Si les commentaires sont systÃ©matiques, il peut Ãªtre pratique de les inscrire dans une colonne commentaire_pH.\n\nInscription non systÃ©matique. Il arrive souvent que des catÃ©gories dâ€™une variable ou que des valeurs manquantes soient annotÃ©es diffÃ©remment. Il arrive mÃªme que le sÃ©parateur dÃ©cimal soit non systÃ©matique, parfois notÃ© par un point, parfois par une virgule. Par exemple, une fois importÃ©s dans votre session, les catÃ©gories St-Ours et Saint-Ours seront traitÃ©es comme deux catÃ©gories distinctes. De mÃªme, les cellules correspondant Ã  des valeurs manquantes ne devraient pas Ãªtre inscrites parfois avec une cellule vide, parfois avec un point, parfois avec un tiret ou avec la mention NA. Le plus simple est de laisser systÃ©matiquement ces cellules vides.\n\nInclure des notes dans un tableau. La rÃ¨gle â€œune colonne, une variableâ€ nâ€™est pas respectÃ©e si on ajoute des notes un peu nâ€™importe oÃ¹ sous ou Ã  cÃ´tÃ© du tableau.\n\nAjouter des sommaires. Si vous ajoutez une ligne sous un tableau comprenant la moyenne de chaque colonne, quâ€™est-ce qui arrivera lorsque vous importerez votre tableau dans votre session de travail? La ligne sera considÃ©rÃ©e comme une observation supplÃ©mentaire.\n\nInclure une hiÃ©rarchie dans les entÃªtes. Afin de consigner des donnÃ©es de texture du sol, comprenant la proportion de sable, de limon et dâ€™argile, vous organisez votre entÃªte en plusieurs lignes. Une ligne pour la catÃ©gorie de donnÃ©e, Texture, fusionnÃ©e sur trois colonnes, puis trois colonnes intitulÃ©es Sable, Limon et Argile. Votre tableau est joli, mais il ne pourra pas Ãªtre importÃ© conformÃ©ment dans un votre session de calcul : on recherche une entÃªte unique par colonne. Votre tableau de donnÃ©es devrait plutÃ´t porter les entÃªtes Texture sable, Texture limon et Texture argile. Un conseil : rÃ©server le travail esthÃ©tique Ã  la toute fin dâ€™un flux de travail."
  },
  {
    "objectID": "03-tableaux.html#formats-de-tableau",
    "href": "03-tableaux.html#formats-de-tableau",
    "title": "3Â  Organisation des donnÃ©es et opÃ©rations sur des tableaux",
    "section": "\n3.3 Formats de tableau",
    "text": "3.3 Formats de tableau\nPlusieurs outils sont Ã  votre disposition pour crÃ©er des tableaux. Je vous prÃ©sente ici les plus communs.\n\n3.3.1 xls ou xlsx\n\nMicrosoft Excel est un logiciel de type tableur, ou chiffrier Ã©lectronique. Lâ€™ancien format xls a Ã©tÃ© remplacÃ© par le format xlsx avec lâ€™arrivÃ©e de Microsoft Office 2010. Il sâ€™agit dâ€™un format propriÃ©taire, dont lâ€™alternative libre la plus connue est le format ods, popularisÃ© par la suite bureautique LibreOffice. Les formats xls, xlsx ou ods sont davantage utilisÃ©s comme outils de calcul que dâ€™entreposage de donnÃ©es. Ils contiennent des formules, des graphiques, du formatage de cellule, etc. Je ne les recommande pas pour stocker des donnÃ©es.\n\n3.3.2 csv\n\nLe format csv, pour comma separated values, est un fichier texte, que vous pouvez ouvrir avec nâ€™importe quel Ã©diteur de texte brut (Bloc note, VSCode, Notepad++, etc.). Chaque colonne doit Ãªtre dÃ©limitÃ©e par un caractÃ¨re cohÃ©rent (conventionnellement une virgule, mais en franÃ§ais un point-virgule ou une tabulation pour Ã©viter la confusion avec le sÃ©parateur dÃ©cimal) et chaque ligne du tableau est un retour de ligne. Il est possible dâ€™ouvrir et dâ€™Ã©diter les fichiers csv dans un Ã©diteur texte, mais il est plus pratique de les ouvrir avec des tableurs (LibreOffice Calc, Microsoft Excel, Google Sheets, etc.).\nEncodage des fichiers texte. Puisque le format csv est un fichier texte, un souci particulier doit Ãªtre portÃ© sur la maniÃ¨re dont le texte est encodÃ©. Les caractÃ¨res accentuÃ©s pourraient Ãªtre importÃ©s incorrectement si vous importez votre tableau en spÃ©cifiant le mauvais encodage. Pour les fichiers en langues occidentales, lâ€™encodage UTF-8 devrait Ãªtre utilisÃ©. Toutefois, par dÃ©faut, Excel utilise un encodage de Microsoft. Si le csv a Ã©tÃ© gÃ©nÃ©rÃ© par Excel, il est prÃ©fÃ©rable de lâ€™ouvrir avec votre Ã©diteur texte et de lâ€™enregistrer dans lâ€™encodage UTF-8.\n\n3.3.3 json\n\nComme le format csv, le format json indique un fichier en texte clair. En permettant des structures de tableaux emboÃ®tÃ©s et en ne demandant pas que chaque colonne ait la mÃªme longueur, le format json permet plus de souplesse que le format csv, mais il est plus compliquÃ© Ã  consulter et prend davantage dâ€™espace sur le disque que le csv. Il est utilisÃ© davantage pour le partage de donnÃ©es des applications web, mais en ce qui concerne la matiÃ¨re du cours, ce format est surtout utilisÃ© pour les donnÃ©es gÃ©orÃ©fÃ©rencÃ©es. Lâ€™encodage est gÃ©rÃ© de la mÃªme maniÃ¨re quâ€™un fichier csv.\n\n3.3.4 SQLite\nSQLite est une application pour les bases de donnÃ©es relationnelles de type SQL qui nâ€™a pas besoin de serveur pour fonctionner. Les bases de donnÃ©es SQLite sont encodÃ©s dans des fichiers portant lâ€™extension db, qui peuvent Ãªtre facilement partagÃ©s.\n\n3.3.5 Suggestion\nEn csv pour les petits tableaux, en sqlite pour les bases de donnÃ©es plus complexes. Ce cours se concentre toutefois sur les donnÃ©es de type csv."
  },
  {
    "objectID": "03-tableaux.html#entreposer-ses-donnÃ©es",
    "href": "03-tableaux.html#entreposer-ses-donnÃ©es",
    "title": "3Â  Organisation des donnÃ©es et opÃ©rations sur des tableaux",
    "section": "\n3.4 Entreposer ses donnÃ©es",
    "text": "3.4 Entreposer ses donnÃ©es\nLa maniÃ¨re la plus sÃ©curisÃ©e pour entreposer ses donnÃ©es est de les confiner dans une base de donnÃ©es sÃ©curisÃ©e sur un serveur sÃ©curisÃ© dans un environnement sÃ©curisÃ© et dâ€™encrypter les communications. Câ€™est aussiâ€¦ la maniÃ¨re la moins accessible. Des espaces de stockage nuagiques, comme Dropbox ou dâ€™autres options similaires, peuvent Ãªtre pratiques pour les backups et le partage des donnÃ©es avec une Ã©quipe de travail (qui risque en retour de bousiller vos donnÃ©es). Le suivi de version est possible chez certains fournisseurs dâ€™espace de stockage. Mais pour un suivi de version plus rigoureux, les espaces de dÃ©veloppement (comme GitHub et GitLab) sont plus appropriÃ©s (couverts au chapitreÂ 5). Dans tous les cas, il est important de garder (1) des copies anciennes pour y revenir en cas dâ€™erreurs et (2) un petit fichier dÃ©crivant les changements effectuÃ©s sur les donnÃ©es."
  },
  {
    "objectID": "03-tableaux.html#manipuler-des-donnÃ©es-en-mode-tidyverse",
    "href": "03-tableaux.html#manipuler-des-donnÃ©es-en-mode-tidyverse",
    "title": "3Â  Organisation des donnÃ©es et opÃ©rations sur des tableaux",
    "section": "\n3.5 Manipuler des donnÃ©es en mode tidyverse",
    "text": "3.5 Manipuler des donnÃ©es en mode tidyverse\nLe mÃ©ta-module tidyverse regroupe une collection de prÃ©cieux modules pour lâ€™analyse de donnÃ©es en R. Il permet dâ€™importer des donnÃ©es dans votre session de travail avec readr, de les explorer avec le module de visualisation ggplot2, de les transformer avec tidyr et dplyr et de les exporter avec readr. Les tableaux de classe data.frame, comme ceux de la plus moderne classe tibble, peuvent Ãªtre manipulÃ©s Ã  travers le flux de travail pour lâ€™analyse et la modÃ©lisation. Comme ce sera le cas pour le chapitre sur la visualisation, ce chapitre est loin de couvrir les nombreuses fonctionnalitÃ©s qui sont offertes dans le tidyverse.\n\n3.5.1 Importer vos donnÃ©es dans votre session de travail\nSupposons que vous avez bien organisÃ© vos donnÃ©es en mode tidy. Pour les importer dans votre session et commencer Ã  les inspecter, vous lancerez une des commandes du module readr, dÃ©crites dans la documentation dÃ©diÃ©e.\n\n\nread_csv() si le sÃ©parateur de colonne est une virgule\n\nread_csv2() si le sÃ©parateur de colonne est un point-virgule et que le sÃ©parateur dÃ©cimal est une virgule\n\nread_tsv() si le sÃ©parateur de colonne est une tabulation\n\nread_table() si le sÃ©parateur de colonne est un espace blanc\n\nread_delim() si le sÃ©parateur de colonne est un autre caractÃ¨re (comme le point-virgule) que vous spÃ©cifierez dans lâ€™argument delim = \";\"\n\n\nLes principaux arguments sont les suivants.\n\n\nfile: le chemin vers le fichier. Ce chemin peut aussi bien Ãªtre une adresse locale (data/â€¦) quâ€™une adresse internet (https://â€¦).\n\ndelim: le symbole dÃ©limitant les colonnes dans le cas de read_delim.\n\ncol_names: si TRUE, la premiÃ¨re ligne est lâ€™entÃªte du tableau, sinon FALSE. Si vous spÃ©cifiez un vecteur numÃ©rique, ce sont les numÃ©ros des lignes utilisÃ©es pour le nom de lâ€™entÃªte. Si vous utilisez un vecteur de caractÃ¨res, ce sont les noms des colonnes que vous dÃ©sirez donner Ã  votre tableau.\n\nna: le symbole spÃ©cifiant une valeur manquante. Lâ€™argument na='' signifie que les cellules vides sont des donnÃ©es manquantes. Si les valeurs manquantes ne sont pas uniformes, vous pouvez les indiquer dans un vecteur, par exemple na = c(\"\", \"NA\", \"NaN\", \".\", \"-\").\n\nlocal: cet argument prend une fonction local() qui peut inclure des arguments de format de temps, mais aussi dâ€™encodage (voir documentation)\n\nDâ€™autres arguments peuvent Ãªtre spÃ©cifiÃ©s au besoin, et les rÃ©pÃ©ter ici dupliquerait lâ€™information de la documentation de la fonction read_csv de readr.\nJe dÃ©conseille dâ€™importer des donnÃ©es en format xls ou xlsx. Si toutefois cela vous convient, je vous rÃ©fÃ¨re au module readxl.\nLâ€™aide-mÃ©moire de readr (FigureÂ 3.1) est Ã  afficher prÃ¨s de soi.\n\n\n\n\nFigureÂ 3.1: Aide-mÃ©moire de readr, Source: https://rstudio.github.io/cheatsheets/data-import.pdf\n\n\n\nNous allons charger des donnÃ©es de culture de la chicoutÃ© (Rubus chamaemorus), un petit fruit nordique, tirÃ© de Parent et al.Â (2013). Ouvrons dâ€™abord le fichier pour vÃ©rifier les sÃ©parateurs de colonnes et de dÃ©cimales (FigureÂ 3.2).\n\n\n\n\nFigureÂ 3.2: AperÃ§u brut dâ€™un fichier csv.\n\n\n\nLe sÃ©parateur de colonnes est un point-virgule et le dÃ©cimal est une virgule.\nAvec Atom, mon Ã©diteur texte prÃ©fÃ©rÃ© (il y en a dâ€™autres), je vais dans Edit &gt; Select Encoding et jâ€™obtiens bien le UTF-8 (FigureÂ 3.3).\n\n\n\n\nFigureÂ 3.3: Changer lâ€™encodage dâ€™un fichier csv.\n\n\n\nNous allons donc utiliser read_csv2() avec ses arguments par dÃ©faut.\n\nlibrary(\"tidyverse\")\nchicoute &lt;- read_csv2(\"data/chicoute.csv\")\n\nâ„¹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 90 Columns: 31\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \";\"\nchr  (5): CodeTourbiere, Ordre, Traitement, DemiParcelle, SousTraitement\ndbl (26): ID, Site, Latitude_m, Longitude_m, Rendement_g_5m2, TotalRamet_nom...\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nQuelques commandes utiles inspecter le tableau:\n\n\nhead() prÃ©sente lâ€™entÃªte du tableau, soit ses 6 premiÃ¨res lignes\n\nstr() et glimpse() prÃ©sentent les variables du tableau et leur type - glimpse()est la fonction tidyverse et str() est la fonction classique (je prÃ©fÃ¨re str())\n\nsummary() prÃ©sente des statistiques de base du tableau\n\nnames() ou colnames() sort les noms des colonnes sous forme dâ€™un vecteur\n\ndim() donne les dimensions du tableau, ncol() son nombre de colonnes et nrow() son nombre de lignes\n\nskim est une fonction du module skimr montrant un portrait graphique et numÃ©rique du tableau\n\nExtra 1. Plusieurs modules ne se trouvent pas dans les dÃ©pÃ´ts CRAN, mais sont disponibles sur GitHub. Pour les installer, installez dâ€™abord le module devtools disponible sur CRAN. Vous pourrez alors installer les packages de GitHub comme on le fait avec le package skimr.\nExtra 2. Lorsque je dÃ©sire utiliser une fonction, mais sans charger le module dans la session, jâ€™utilise la notation module::fonction. Comme dans ce cas, pour skimr.\n\nskimr::skim(chicoute)\n\n\nData summary\n\n\nName\nchicoute\n\n\nNumber of rows\n90\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n26\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nCodeTourbiere\n0\n1.00\n1\n4\n0\n12\n0\n\n\nOrdre\n0\n1.00\n1\n2\n0\n20\n0\n\n\nTraitement\n50\n0.44\n6\n11\n0\n2\n0\n\n\nDemiParcelle\n50\n0.44\n4\n5\n0\n2\n0\n\n\nSousTraitement\n50\n0.44\n1\n7\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n45.50\n26.12\n1.00\n23.25\n45.50\n67.75\n90.00\nâ–‡â–‡â–‡â–‡â–‡\n\n\nSite\n0\n1.00\n6.33\n5.49\n1.00\n2.00\n4.00\n9.00\n20.00\nâ–‡â–ƒâ–â–â–\n\n\nLatitude_m\n0\n1.00\n5701839.86\n1915.50\n5695688.00\n5701868.50\n5702129.00\n5702537.00\n5706394.00\nâ–â–‚â–…â–‡â–\n\n\nLongitude_m\n0\n1.00\n485295.54\n6452.33\n459873.00\n485927.00\n486500.00\n486544.75\n491955.00\nâ–â–â–â–‚â–‡\n\n\nRendement_g_5m2\n50\n0.44\n13.33\n21.56\n0.00\n0.00\n0.95\n15.63\n72.44\nâ–‡â–â–â–â–\n\n\nTotalRamet_nombre_m2\n0\n1.00\n251.26\n156.06\n40.74\n122.70\n212.92\n347.80\n651.90\nâ–‡â–‡â–ƒâ–‚â–‚\n\n\nTotalVegetatif_nombre_m2\n4\n0.96\n199.02\n139.13\n22.92\n86.26\n161.25\n263.78\n580.60\nâ–‡â–‡â–‚â–‚â–\n\n\nTotalFloral_nombre_m2\n4\n0.96\n52.08\n40.41\n4.80\n22.92\n43.00\n69.52\n198.62\nâ–‡â–…â–‚â–â–\n\n\nTotalMale_nombre_m2\n4\n0.96\n24.40\n26.87\n0.00\n3.30\n15.28\n36.51\n104.41\nâ–‡â–‚â–‚â–â–\n\n\nTotalFemelle_nombre_m2\n4\n0.96\n27.53\n29.83\n2.55\n10.34\n17.19\n31.96\n187.17\nâ–‡â–â–â–â–\n\n\nFemelleFruit_nombre_m2\n18\n0.80\n19.97\n23.79\n0.40\n7.64\n11.46\n22.83\n157.88\nâ–‡â–‚â–â–â–\n\n\nFemelleAvorte_nombre_m2\n4\n0.96\n8.49\n14.52\n0.00\n1.27\n3.07\n10.14\n76.80\nâ–‡â–â–â–â–\n\n\nSterileFleur_nombre_m2\n4\n0.96\n0.26\n0.71\n0.00\n0.00\n0.00\n0.00\n3.82\nâ–‡â–â–â–â–\n\n\nC_pourc\n0\n1.00\n50.28\n1.61\n46.72\n49.14\n50.45\n51.58\n53.83\nâ–ƒâ–†â–…â–‡â–\n\n\nN_pourc\n0\n1.00\n2.20\n0.40\n1.53\n1.89\n2.12\n2.58\n3.10\nâ–ƒâ–‡â–ƒâ–ƒâ–‚\n\n\nP_pourc\n0\n1.00\n0.14\n0.04\n0.07\n0.12\n0.14\n0.16\n0.23\nâ–ƒâ–†â–‡â–‚â–‚\n\n\nK_pourc\n0\n1.00\n0.89\n0.27\n0.35\n0.69\n0.86\n1.13\n1.54\nâ–ƒâ–‡â–‡â–‡â–\n\n\nCa_pourc\n0\n1.00\n0.39\n0.10\n0.19\n0.32\n0.37\n0.44\n0.88\nâ–…â–‡â–‚â–â–\n\n\nMg_pourc\n0\n1.00\n0.50\n0.08\n0.36\n0.45\n0.48\n0.52\n0.86\nâ–‡â–‡â–‚â–â–\n\n\nS_pourc\n0\n1.00\n0.13\n0.04\n0.07\n0.11\n0.13\n0.14\n0.28\nâ–…â–‡â–‚â–â–\n\n\nB_pourc\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nâ–‚â–…â–ƒâ–‡â–ƒ\n\n\nCu_pourc\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nâ–‡â–â–â–â–\n\n\nZn_pourc\n0\n1.00\n0.01\n0.00\n0.00\n0.01\n0.01\n0.01\n0.02\nâ–‡â–‡â–‚â–â–\n\n\nMn_pourc\n0\n1.00\n0.03\n0.03\n0.00\n0.01\n0.03\n0.05\n0.10\nâ–‡â–…â–ƒâ–‚â–\n\n\nFe_pourc\n0\n1.00\n0.02\n0.01\n0.01\n0.01\n0.01\n0.02\n0.05\nâ–‡â–‚â–â–â–\n\n\nAl_pourc\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\nâ–‡â–…â–â–â–\n\n\n\n\n\nExercice. Inspectez le tableau.\n\n3.5.2 Comment sÃ©lectionner et filtrer des donnÃ©es ?\nOn utilise le terme sÃ©lectionner lorsque lâ€™on dÃ©sire choisir une ou plusieurs lignes et colonnes dâ€™un tableau (la plupart du temps des colonnes). Lâ€™action de filtrer signifie de sÃ©lectionner des lignes selon certains critÃ¨res.\n\n3.5.2.1 SÃ©lectionner\nVoici 4 maniÃ¨res de sÃ©lectionner une colonne en R.\n\nUne mÃ©thode rapide mais peu expressive consiste Ã  indiquer les valeurs numÃ©riques de lâ€™indice de la colonne entre des crochets. Il sâ€™agit dâ€™appeler le tableau suivi de crochets. Lâ€™intÃ©rieur des crochets comprend deux Ã©lÃ©ments sÃ©parÃ©s par une virgule. Le premier Ã©lÃ©ment sert Ã  filtrer selon lâ€™indice, le deuxiÃ¨me sert Ã  sÃ©lectionner selon lâ€™indice. Ainsi:\n\n\nchicoute[, 1]: sÃ©lectionner la premiÃ¨re colonne\n\nchicoute[, 1:10]: sÃ©lectionner les 10 premiÃ¨res colonnes\n\nchicoute[, c(2, 4, 5)]: sÃ©lectionner les colonnes 2, 4 et 5\n\nchicoute[c(10, 13, 20), c(2, 4, 5)]: sÃ©lectionner les colonnes 2, 4 et 5 et les lignes 10, 13 et 20.\n\n\nUne autre mÃ©thode rapide, mais plus expressive, consiste Ã  appeler le tableau, suivi du symbole $, puis le nom de la colonne, e.g.Â chicoute$Site.\n\n\nTruc. La plupart des IDE, comme RStudio, peuvent vous proposer des colonnes dans une liste. AprÃ¨s avoir saisi le $, taper sur la touche de tabulation: vous pourrez sÃ©lectionner la colonne dans une liste dÃ©filante (FigureÂ 3.4).\n\n\n\n\n\nFigureÂ 3.4: AutocomplÃ©tion dans RStudio.\n\n\n\n\nVous pouvez aussi inscrire le nom de la colonne, ou du vecteur des colonnes, entre des crochets suivant le nom du tableau, câ€™est-Ã -dire chicoute[c(\"Site\", \"Latitude_m\", \"Longitude_m\")].\nEnfin, dans une sÃ©quence dâ€™opÃ©rations en mode pipeline (chaque opÃ©ration est mise Ã  la suite de la prÃ©cÃ©dente en plaÃ§ant le pipe |&gt; entre chacune), il peut Ãªtre prÃ©fÃ©rable de sÃ©lectionner des colonnes avec la fonction select(), i.e.\n\n\nchicoute |&gt; \n  select(Site, Latitude_m, Longitude_m)\n\n\nNote sur le mode pipeline : Le pipe |&gt; a Ã©tÃ© introduit dans R-base en 2021. Auparavant, on utilisait la fonction %&gt;% introduite dans le module magrittr, inclus dans tidyverse. La plupart du temps, les deux fonctionnent sensiblement de la mÃªme faÃ§on, mais il existe quelques diffÃ©rences dans leur interaction avec certaines fonctions. Puisque |&gt; fait partie de R-base, je vous suggÃ¨re de lâ€™utiliser par dÃ©faut, mais il est fort probable que vous trouviez lâ€™ancienne version %&gt;% lors de vos recherches sur internet (ou mÃªme dans ce guide si jâ€™oublie dâ€™effectuer les modifications). Pour insÃ©rer un pipe, il suffit dâ€™utiliser le raccourci clavier Ctrl + Shift + M. Vous pouvez modifier la forme par dÃ©faut dans les options de RStudio, comme sur la FigureÂ 3.5.\n\n\n\n\n\nFigureÂ 3.5: Modifier le pipe par dÃ©faut dans RStudio.\n\n\n\nLa fonction select() permet aussi de travailler en exclusion. Ainsi pour enlever des colonnes, on placera un - (signe de soustraction) devant le nom de la colonne.\nâš ï¸ Attention. Plusieurs modules utilisent la fonction select (et filter, plus bas). Lorsque vous lancez select et que vous obtenez un message dâ€™erreur comme\nError in select(., ends_with(\"pourc\")) : \n  argument inutilisÃ© (ends_with(\"pourc\"))\nil se pourrait bien que R utilise la fonction select dâ€™un autre module. Pour spÃ©cifier que vous dÃ©sirez la fonction select du module dplyr, spÃ©cifiez dplyr::select.\nDâ€™autre arguments de select() permettent une sÃ©lection rapide. Par exemple, pour obtenir les colonnes contenant des pourcentages:\n\nchicoute |&gt; \n  select(ends_with(\"pourc\")) |&gt; \n  head(3)\n\n# A tibble: 3 Ã— 13\n  C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1    51.5    1.72  0.108     1.21    0.435    0.470  0.0976 0.00258 0.000175\n2    51.3    2.18  0.0985    1.22    0.337    0.439  0.0996 0.00258 0.000407\n3    50.6    2.12  0.0708    1.05    0.373    0.420  0.104  0.00258 0.000037\n# â„¹ 4 more variables: Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;,\n#   Al_pourc &lt;dbl&gt;\n\n\n\n3.5.2.2 Filtrer\nComme câ€™est le cas de la sÃ©lection, on pourra filtrer un tableau de plusieurs maniÃ¨res. Jâ€™ai dÃ©jÃ  prÃ©sentÃ© comment filtrer selon les indices des lignes. Les autres maniÃ¨res reposent nÃ©anmoins sur une opÃ©ration logique ==, &lt;, &gt; ou %in% (le %in% signifie se trouve parmi et peut Ãªtre suivi dâ€™un vecteur de valeurs que lâ€™on dÃ©sire accepter).\nLes conditions boolÃ©ennes peuvent Ãªtre combinÃ©es avec les opÃ©rateurs et, &, et ou, |. Pour rappel,\n\n\nOpÃ©ration\nRÃ©sultat\n\n\n\nVrai et Vrai\nVrai\n\n\nVrai et Faux\nFaux\n\n\nFaux et Faux\nFaux\n\n\nVrai ou Vrai\nVrai\n\n\nVrai ou Faux\nVrai\n\n\nFaux ou Faux\nFaux\n\n\n\n\nLa mÃ©thode classique consiste Ã  appliquer une opÃ©ration logique entre les crochets, par exemple chicoute[chicoute$CodeTourbiere == \"BEAU\", ]\n\nLa mÃ©thode tidyverse, plus pratique en mode pipeline, passe par la fonction filter(), i.e.\n\nchicoute |&gt; \n  filter(CodeTourbiere == \"BEAU\")\nCombiner le tout.\n\nchicoute |&gt; \n  filter(Ca_pourc &lt; 0.4 & CodeTourbiere %in% c(\"BEAU\", \"MB\", \"WTP\")) |&gt; \n  select(contains(\"pourc\"))\n\n# A tibble: 4 Ã— 13\n  C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1    51.3    2.18  0.0985   1.22     0.337    0.439  0.0996 0.00258 0.000407\n2    50.6    2.12  0.0708   1.05     0.373    0.420  0.104  0.00258 0.000037\n3    53.8    2.04  0.115    0.947    0.333    0.472  0.106  0.00258 0.000037\n4    52.6    2.11  0.0847   0.913    0.328    0.376  0.111  0.00296 0.000037\n# â„¹ 4 more variables: Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;,\n#   Al_pourc &lt;dbl&gt;\n\n\n\n3.5.3 Le format long et le format large\nDans le tableau chicoute, chaque Ã©lÃ©ment possÃ¨de sa propre colonne. Si lâ€™on voulait mettre en graphique les boxplot des facettes de concentrations dâ€™azote, de phosphore et de potassium dans les diffÃ©rentes tourbiÃ¨res, il faudrait obtenir une seule colonne de concentrations.\nPour ce faire, nous utiliserons la fonction pivot_longer(). Lâ€™argument obligatoire (excluant le tableau, qui est implicite dans la chaÃ®ne dâ€™opÃ©rations), est cols, le nom des colonnes Ã  allonger. Pour obtenir des noms de colonnes allongÃ©es personnalisÃ©es, on spÃ©cifie le nom des variables consistant aux anciens noms de colonnes avec names_to et celui de la nouvelle colonne contenant les valeurs dans values_to. La suite consiste Ã  dÃ©crire les colonnes Ã  inclure ou Ã  exclure. Dans le cas qui suit, jâ€™exclue CodeTourbiere de la refonte et jâ€™utilise slice_sample() pour prÃ©senter un Ã©chantillon alÃ©atoire du rÃ©sultat. Notez la ligne comprenant la fonction mutate, que lâ€™on verra plus loin. Cette fonction ajoute une colonne au tableau. Dans ce cas-ci, jâ€™ajoute une colonne constituÃ©e dâ€™une sÃ©quence de nombres allant de 1 au nombre de lignes du tableau (il y en a 90). Cet identifiant unique pour chaque ligne permettra de reconstituer par la suite le tableau initial.\n\nchicoute_long &lt;- chicoute |&gt; \n  select(CodeTourbiere, N_pourc, P_pourc, K_pourc) |&gt; \n  mutate(ID = 1:n())  |&gt;  # mutate ajoute une colonne au tableau\n  # pour l'identifiant, on peut aussi utiliser la commande cur_group_rows()\n  pivot_longer(cols = contains(\"pourc\"), names_to = \"nutrient\", values_to = \"concentration\")\nchicoute_long |&gt;  slice_sample(n = 10)\n\n# A tibble: 10 Ã— 4\n   CodeTourbiere    ID nutrient concentration\n   &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 SSP              58 N_pourc         1.98  \n 2 2                26 P_pourc         0.158 \n 3 BS2              62 N_pourc         2.44  \n 4 MB               32 P_pourc         0.117 \n 5 BEAU              4 P_pourc         0.0909\n 6 MB               35 P_pourc         0.0847\n 7 2                25 K_pourc         1.24  \n 8 1                70 N_pourc         2.47  \n 9 1                77 P_pourc         0.138 \n10 MB               34 N_pourc         2.35  \n\n\nLâ€™opÃ©ration inverse est pivot_wider(), avec laquelle nous sÃ©lectionnons une colonne spÃ©cifiant les nouvelles colonnes Ã  construire (names_from) ainsi que les valeurs Ã  placer dans ces colonnes (values_from).\n\nchicoute_large &lt;- chicoute_long |&gt; \n  pivot_wider(names_from = nutrient, values_from = concentration)\nchicoute_large |&gt;  slice_sample(n = 10)\n\n# A tibble: 10 Ã— 5\n   CodeTourbiere    ID N_pourc P_pourc K_pourc\n   &lt;chr&gt;         &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 NTP              51    2.05  0.104    0.398\n 2 2                24    2.72  0.181    1.14 \n 3 1                76    2.13  0.140    0.683\n 4 BEAU              4    1.95  0.0909   1.19 \n 5 NBM              50    2.42  0.156    0.825\n 6 2                22    2.92  0.226    1.22 \n 7 MR               37    1.90  0.129    0.958\n 8 BEAU              1    1.72  0.108    1.21 \n 9 BEAU              5    2.04  0.115    0.947\n10 1                78    2.31  0.156    0.833\n\n\n\n3.5.4 Combiner des tableaux\nNous avons introduit plus haut la notion de base de donnÃ©es. Nous voudrions peut-Ãªtre utiliser le code des tourbiÃ¨res pour inclure leur nom, le type dâ€™essai menÃ© Ã  ces tourbiÃ¨res, etc. Importons dâ€™abord le tableau des noms liÃ©s aux codes.\n\ntourbieres &lt;- read_csv2(\"data/chicoute_tourbieres.csv\")\n\nâ„¹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 11 Columns: 4\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \";\"\nchr (4): Tourbiere, CodeTourbiere, Type, TypeCulture\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntourbieres\n\n# A tibble: 11 Ã— 4\n   Tourbiere               CodeTourbiere Type        TypeCulture\n   &lt;chr&gt;                   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;      \n 1 Beaulieu                BEAU          calibration naturel    \n 2 Brador Path             BP            calibration naturel    \n 3 Lichen (BS2E)           2             validation  cultive sec\n 4 Mannys Brook            MB            calibration naturel    \n 5 Middle Bay Road         MR            calibration naturel    \n 6 North Est of Smelt Pond NESP          calibration naturel    \n 7 North of Blue Moon      NBM           calibration naturel    \n 8 South of Smelt Pond     SSP           calibration naturel    \n 9 Sphaigne (BS2F)         BS2           validation  cultive sec\n10 Sphaigne (BS2F)         1             calibration naturel    \n11 West of Trout Pond      WTP           calibration naturel    \n\n\nNotre information est organisÃ©e en deux tableaux, liÃ©s par la colonne CodeTourbiere. Comment fusionner lâ€™information pour quâ€™elle puisse Ãªtre utilisÃ©e dans son ensemble? La fonction left_join effectue cette opÃ©ration typique avec les bases de donnÃ©es.\n\nchicoute_merge &lt;- left_join(x = chicoute, y = tourbieres, by = \"CodeTourbiere\")\n# ou bien chicoute |&gt;  left_join(y = tourbieres, by = \"CodeTourbiere\")\nchicoute_merge |&gt;  slice_head(n = 4)\n\n# A tibble: 4 Ã— 34\n     ID CodeTourbiere Ordre  Site Traitement DemiParcelle SousTraitement\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;         \n1     1 BEAU          A         1 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n2     2 BEAU          A         2 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n3     3 BEAU          A         3 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n4     4 BEAU          A         4 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n# â„¹ 27 more variables: Latitude_m &lt;dbl&gt;, Longitude_m &lt;dbl&gt;,\n#   Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;,\n#   TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;,\n#   TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;,\n#   FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;,\n#   SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, P_pourc &lt;dbl&gt;,\n#   K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, S_pourc &lt;dbl&gt;, â€¦\n\n\nDâ€™autres types de jointures sont possibles, et dÃ©crites en dÃ©tails dans la documentation.\nGarrick Aden-Buie a prÃ©parÃ© de jolies animations pour dÃ©crire les diffÃ©rents types de jointures.\nleft_join(x, y) colle y Ã  x seulement ce qui dans y correspond Ã  ce que lâ€™on trouve dans x.\n\nright_join(x, y) colle y Ã  x seulement ce qui dans x correspond Ã  ce que lâ€™on trouve dans y.\n\ninner_join(x, y) colle x et y en excluant les lignes oÃ¹ au moins une variable de jointure est absente dans x et y.\n\nfull_join(x, y)garde toutes les lignes et les colonnes de x et y.\n\n\n3.5.5 OpÃ©rations sur les tableaux\nLes tableaux peuvent Ãªtre segmentÃ©s en Ã©lÃ©ments sur lesquels on calculera ce qui nous chante.\nOn pourrait vouloir obtenir :\n\nla somme avec la function sum()\n\nla moyenne avec la function mean() ou la mÃ©diane avec la fonction median()\n\nlâ€™Ã©cart-type avec la function sd()\n\nles maximum et minimum avec les fonctions min() et max()\n\nun dÃ©compte dâ€™occurrence avec la fonction n() ou count()\n\n\nPar exemple,\n\nmean(chicoute$Rendement_g_5m2, na.rm = TRUE)\n\n[1] 13.32851\n\n\nEn mode classique, pour effectuer des opÃ©rations sur des tableaux, on utilisera la fonction apply(). Cette fonction prend, comme arguments, le tableau, lâ€™axe (opÃ©ration par ligne = 1, opÃ©ration par colonne = 2), puis la fonction Ã  appliquer.\n\napply(chicoute |&gt;  select(contains(\"pourc\")), 2, mean)\n\n     C_pourc      N_pourc      P_pourc      K_pourc     Ca_pourc     Mg_pourc \n5.027911e+01 2.199411e+00 1.388959e-01 8.887000e-01 3.884391e-01 4.980142e-01 \n     S_pourc      B_pourc     Cu_pourc     Zn_pourc     Mn_pourc     Fe_pourc \n1.347177e-01 3.090922e-03 4.089891e-04 6.662155e-03 3.345239e-02 1.514885e-02 \n    Al_pourc \n2.694979e-03 \n\n\nLes opÃ©rations peuvent aussi Ãªtre effectuÃ©es par ligne, par exemple une somme (je garde seulement les 10 premiers rÃ©sultats).\n\napply(chicoute |&gt;  select(contains(\"pourc\")), 1, sum)[1:10]\n\n [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991\n [9] 55.06295 55.16774\n\n\nLa fonction Ã  appliquer peut Ãªtre personnalisÃ©e, par exemple:\n\napply(\n  chicoute |&gt;  select(contains(\"pourc\")), 2,\n  function(x) (prod(x))^(1 / length(x))\n)\n\n     C_pourc      N_pourc      P_pourc      K_pourc     Ca_pourc     Mg_pourc \n50.253429104  2.165246915  0.133754530  0.846193827  0.376192724  0.491763884 \n     S_pourc      B_pourc     Cu_pourc     Zn_pourc     Mn_pourc     Fe_pourc \n 0.129900753  0.003014675  0.000000000  0.006408775  0.024140327  0.014351745 \n    Al_pourc \n 0.002450982 \n\n\nVous reconnaissez cette fonction? Câ€™Ã©tait la moyenne gÃ©omÃ©trique (la fonction prod() Ã©tant le produit dâ€™un vecteur).\nEn mode tidyverse, on aura besoin principalement des fonction suivantes:\n\n\ngroup_by() pour effectuer des opÃ©rations par groupe, lâ€™opÃ©ration group_by() sÃ©pare le tableau en plusieurs petits tableaux, en attendant de les recombiner. Câ€™est un peu lâ€™Ã©quivalent des facettes avec le module de visualisation ggplot2, que nous explorons au chapitreÂ 4.\n\nsummarise() pour rÃ©duire plusieurs valeurs en une seule, il applique un calcul sur le tableau ou sâ€™il y a lieu sur chaque petit tableau segmentÃ©. Il en existe quelques variantes.\n\n\nsummarise_all() applique la fonction Ã  toutes les colonnes\n\nsummarise_at() applique la fonction aux colonnes spÃ©cifiÃ©es\n\nsummarise_if() applique la fonction aux colonnes qui ressortent comme TRUE selon une opÃ©ration boolÃ©enne\n\n\n\nmutate() pour ajouter une nouvelle colonne\n\nSi lâ€™on dÃ©sire ajouter une colonne Ã  un tableau, par exemple le sommaire calculÃ© avec summarise(). Ã€ lâ€™inverse, la fonction transmute() retournera seulement le rÃ©sultat, sans le tableau Ã  partir duquel il a Ã©tÃ© calculÃ©. De mÃªme que summarise(), mutate() et transmute() possÃ¨dent leurs Ã©quivalents _all(), _at() et _if().\n\n\n\narrange() pour rÃ©ordonner le tableau\n\nCette fonction est parfois utile lors de la mise en page de tableaux ou de graphiques. Il ne sâ€™agit pas dâ€™une opÃ©ration sur un tableau, mais plutÃ´t un changement dâ€™affichage en changeant lâ€™ordre dâ€™apparition des donnÃ©es.\n\n\n\nCes opÃ©rations sont dÃ©crites dans lâ€™aide-mÃ©moire Data Transformation Cheat Sheet (FigureÂ 3.6).\n\n\n\n\nFigureÂ 3.6: Aide-mÃ©moire pour la transformation des donnÃ©es, https://rstudio.github.io/cheatsheets/data-transformation.pdf\n\n\n\nPour effectuer des statistiques par colonne, on utilisera summarise pour des statistiques effectuÃ©es sur une seule colonne. summarise peut prendre le nombre dÃ©sirÃ© de statistiques dont la sortie est un scalaire.\n\nchicoute |&gt; \n  summarise(\n    moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE),\n    ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)\n  )\n\n# A tibble: 1 Ã— 2\n  moyenne ecart_type\n    &lt;dbl&gt;      &lt;dbl&gt;\n1    52.1       40.4\n\n\nSi lâ€™on dÃ©sire un sommaire sur toutes les variables sÃ©lectionnÃ©es, on utilisera summarise_all(). Pour spÃ©cifier que lâ€™on dÃ©sire la moyenne et lâ€™Ã©cart-type, on inscrit les noms des fonctions dans list().\n\nchicoute |&gt; \n  select(contains(\"pourc\")) |&gt; \n  summarise_all(list(mean, sd))\n\n# A tibble: 1 Ã— 26\n  C_pourc_fn1 N_pourc_fn1 P_pourc_fn1 K_pourc_fn1 Ca_pourc_fn1 Mg_pourc_fn1\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1        50.3        2.20       0.139       0.889        0.388        0.498\n# â„¹ 20 more variables: S_pourc_fn1 &lt;dbl&gt;, B_pourc_fn1 &lt;dbl&gt;,\n#   Cu_pourc_fn1 &lt;dbl&gt;, Zn_pourc_fn1 &lt;dbl&gt;, Mn_pourc_fn1 &lt;dbl&gt;,\n#   Fe_pourc_fn1 &lt;dbl&gt;, Al_pourc_fn1 &lt;dbl&gt;, C_pourc_fn2 &lt;dbl&gt;,\n#   N_pourc_fn2 &lt;dbl&gt;, P_pourc_fn2 &lt;dbl&gt;, K_pourc_fn2 &lt;dbl&gt;,\n#   Ca_pourc_fn2 &lt;dbl&gt;, Mg_pourc_fn2 &lt;dbl&gt;, S_pourc_fn2 &lt;dbl&gt;,\n#   B_pourc_fn2 &lt;dbl&gt;, Cu_pourc_fn2 &lt;dbl&gt;, Zn_pourc_fn2 &lt;dbl&gt;,\n#   Mn_pourc_fn2 &lt;dbl&gt;, Fe_pourc_fn2 &lt;dbl&gt;, Al_pourc_fn2 &lt;dbl&gt;\n\n\nOn utilisera group_by() pour segmenter le tableau, et ainsi obtenir des statistiques pour chaque groupe.\n\nchicoute |&gt; \n  group_by(CodeTourbiere) |&gt; \n  summarise(\n    moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE),\n    ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)\n  )\n\n# A tibble: 12 Ã— 3\n   CodeTourbiere moyenne ecart_type\n   &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n 1 1                72.1      32.7 \n 2 2                37.1      32.9 \n 3 BEAU            149.       53.2 \n 4 BP               60.4      30.6 \n 5 BS2              27.2      15.5 \n 6 MB               64.7      40.8 \n 7 MR               35.1      10.5 \n 8 NBM              35.1      16.6 \n 9 NESP             21.4       4.88\n10 NTP              47.6      15.9 \n11 SSP              25.7      11.1 \n12 WTP              50.2      28.3 \n\n\nDans le cas de summarise_all, les rÃ©sultats sâ€™affichent de la mÃªme maniÃ¨re.\n\nchicoute |&gt; \n  group_by(CodeTourbiere) |&gt; \n  select(N_pourc, P_pourc, K_pourc) |&gt; \n  summarise_all(list(mean, sd))\n\nAdding missing grouping variables: `CodeTourbiere`\n\n\n# A tibble: 12 Ã— 7\n   CodeTourbiere N_pourc_fn1 P_pourc_fn1 K_pourc_fn1 N_pourc_fn2 P_pourc_fn2\n   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1                    2.26      0.156        0.880      0.250      0.0193 \n 2 2                    2.76      0.181        1.12       0.178      0.0283 \n 3 BEAU                 2.00      0.0967       1.12       0.179      0.0172 \n 4 BP                   2.05      0.158        0.747      0.161      0.00625\n 5 BS2                  2.08      0.103        1.12       0.420      0.0218 \n 6 MB                   2.15      0.109        0.675      0.114      0.0165 \n 7 MR                   1.99      0.127        0.830      0.0802     0.0131 \n 8 NBM                  2.01      0.127        0.854      0.310      0.0202 \n 9 NESP                 1.76      0.135        0.945      0.149      0.0108 \n10 NTP                  1.83      0.0873       0.402      0.166      0.0103 \n11 SSP                  1.83      0.130        0.700      0.160      0.00383\n12 WTP                  1.79      0.0811       0.578      0.132      0.00587\n# â„¹ 1 more variable: K_pourc_fn2 &lt;dbl&gt;\n\n\nPour obtenir des statistiques Ã  chaque ligne, mieux vaut utiliser apply(), tel que vu prÃ©cÃ©demment. Le point, ., reprÃ©sente le tableau dans une fonction qui nâ€™a pas Ã©tÃ© conÃ§ue pour fonctionner de facto avec dplyr.\n\nchicoute |&gt; \n  select(contains(\"pourc\")) |&gt; \n  apply(1, sum)\n\n [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991\n [9] 55.06295 55.16774 56.41123 55.47917 55.43537 55.79175 55.44561 54.85448\n[17] 54.34262 55.03075 54.40533 51.89319 54.70172 54.62176 54.30250 53.86976\n[25] 53.44731 53.86244 52.43280 54.34978 53.96756 51.46672 55.44267 54.70350\n[33] 55.30711 56.16200 56.64710 55.95499 54.76370 54.32775 54.95419 53.37094\n[41] 53.07855 53.04541 52.09520 52.40456 51.92376 53.33248 56.56405 56.35004\n[49] 56.27185 55.56986 53.81654 55.39638 55.51961 54.88098 54.74774 51.08921\n[57] 51.31462 53.46819 53.15640 52.82020 57.78038 57.94636 56.65558 56.28845\n[65] 55.54463 56.51751 55.36497 56.00594 55.64247 56.56967 56.81674 55.87070\n[73] 55.72308 56.14116 56.42611 55.35650 54.90469 54.03674 53.42991 53.99334\n[81] 53.09085 53.23222 53.28212 53.63192 53.48102 52.31131 51.72026 51.10534\n[89] 51.49055 51.59297\n\n\nPrenons ce tableau des espÃ¨ces menacÃ©es issu de lâ€™Union internationale pour la conservation de la nature distribuÃ© par lâ€™OCDE.\n\nlibrary(\"tidyverse\")\nespeces_menacees &lt;- read_csv(\"data/WILD_LIFE_14012020030114795.csv\")\n\nNous exÃ©cutons le pipeline suivant.\n\nespeces_menacees |&gt; \n  dplyr::filter(IUCN == \"CRITICAL\", SPEC == \"VASCULAR_PLANT\") |&gt; \n  dplyr::select(Country, Value) |&gt; \n  dplyr::group_by(Country) |&gt; \n  dplyr::summarise(n_critical_plants = sum(Value)) |&gt; \n  dplyr::arrange(desc(n_critical_plants)) |&gt; \n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 Ã— 2\n   Country         n_critical_plants\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 United States                1222\n 2 Japan                         525\n 3 Canada                        315\n 4 Czech Republic                284\n 5 Spain                         271\n 6 Belgium                       253\n 7 Austria                       172\n 8 Slovak Republic               155\n 9 Australia                     148\n10 Italy                         128\n\n\nCe pipeline consiste Ã :\nprendre le tableau especes_menacees, puis\n  \nfiltrer pour n'obtenir que les espÃ¨ces critiques dans la catÃ©gorie des plantes vasculaires, puis\n  \nsÃ©lectionner les colonnes des pays et des valeurs (nombre d'espÃ¨ces), puis\n\nsegmenter le tableau en plusieurs tableaux selon le pays, puis\n\nappliquer la fonction sum pour chacun de ces petits tableaux (et recombiner ces sommaires), puis\n\ntrier les pays en nombre dÃ©croissant de dÃ©compte d'espÃ¨ces, puis\n\nafficher le top 10\nNotez quâ€™il aurait aussi Ã©tÃ© possible dâ€™utiliser la fonction dplyr::slice_max(n_critical_plants, n = 10) pour afficher directement le top 10, sans faire le tri prÃ©alable.\n\n3.5.6 Exemple (difficile)\nPour revenir Ã  notre tableau chicoute, imaginez que vous aviez une station mÃ©tÃ©o (station_A) situÃ©e aux coordonnÃ©es (490640, 5702453) et que vous dÃ©siriez calculer la distance entre lâ€™observation et la station. Prenez du temps pour rÃ©flÃ©chir Ã  la maniÃ¨re dont vous procÃ©derezâ€¦\n\nOn pourra crÃ©er une fonction qui mesure la distance entre un point x, y et les coordonnÃ©es de la station Aâ€¦\n\ndist_station_A &lt;- function(x, y) {\n  return(sqrt((x - 490640)^2 + (y - 5702453)^2))\n}\n\nâ€¦ puis ajouter une colonne avec mutate grÃ¢ce Ã  une fonction prenant les arguments x et y spÃ©cifiÃ©s.\n\nchicoute |&gt; \n  mutate(dist = dist_station_A(x = Longitude_m, y = Latitude_m)) |&gt; \n  select(ID, CodeTourbiere, Longitude_m, Latitude_m, dist) |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 Ã— 5\n      ID CodeTourbiere Longitude_m Latitude_m    dist\n   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1    63 BS2                486545    5702197 4103.  \n 2    11 2                  486522    5702582 4120.  \n 3    72 1                  486488    5702129 4165.  \n 4    15 2                  486498    5702643 4146.  \n 5    18 2                  486501    5702627 4143.  \n 6    88 WTP                487060    5700775 3954.  \n 7    64 BS2                486530    5702199 4118.  \n 8    73 1                  486488    5702129 4165.  \n 9    76 1                  486465    5702112 4189.  \n10     2 BEAU               490634    5702452    6.08\n\n\nNous pourrions procÃ©der de la mÃªme maniÃ¨re pour fusionner des donnÃ©es climatiques. Le tableau chicoute ne possÃ¨de pas dâ€™indicateurs climatiques, mais il est possible de les soutirer de stations mÃ©tÃ©o placÃ©es prÃ¨s des sites. Ces donnÃ©es ne sont pas disponibles pour le tableau de la chicoutÃ©, alors jâ€™utiliserai des donnÃ©es fictives pour lâ€™exemple.\nVoici ce qui pourrait Ãªtre fait.\n\nCrÃ©er un tableau des stations mÃ©tÃ©o ainsi que des indices mÃ©tÃ©orologiques associÃ©s Ã  ces stations.\nLier chaque site Ã  une station (Ã  la main oÃ¹ selon la plus petite distance entre le site et la station).\nFusionner les indices climatiques aux sites, puis les sites aux mesures de rendement.\n\nCes opÃ©rations demandent habituellement du tÃ¢tonnement. Il serait surprenant que mÃªme une personne expÃ©rimentÃ©e soit en mesure de compiler ces opÃ©rations sans obtenir de message dâ€™erreur, et retravailler jusquâ€™Ã  obtenir le rÃ©sultat souhaitÃ©. Lâ€™objectif de cette section est de vous prÃ©senter un flux de travail que vous pourriez Ãªtre amenÃ©s Ã  effectuer et de fournir quelques Ã©lÃ©ments nouveaux pour mener Ã  bien une opÃ©ration. Il peut Ãªtre frustrant de ne pas saisir toutes les opÃ©rations: passez Ã  travers cette section sans jugement. Si vous devez vous frotter Ã  un problÃ¨me semblable, vous saurez que vous trouverez dans ce manuel une recette intÃ©ressante.\n\nmes_stations &lt;- data.frame(\n  Station = c(\"A\", \"B\", \"C\"),\n  Longitude_m = c(490640, 484870, 485929),\n  Latitude_m = c(5702453, 5701870, 5696421),\n  t_moy_C = c(13.8, 18.2, 16.30),\n  prec_tot_mm = c(687, 714, 732)\n)\nmes_stations\n\n  Station Longitude_m Latitude_m t_moy_C prec_tot_mm\n1       A      490640    5702453    13.8         687\n2       B      484870    5701870    18.2         714\n3       C      485929    5696421    16.3         732\n\n\nLa fonction suivante calcule la distance entre des coordonnÃ©es x et y et chaque station dâ€™un tableau de stations, puis retourne le nom de la station dont la distance est la moindre.\n\ndist_station &lt;- function(x, y, stations_df) {\n  # stations est le tableau des stations Ã  trois colonnes\n  # 1iere: nom de la station\n  # 2ieme: longitude\n  # 3ieme: latitude\n  distance &lt;- c()\n  for (i in 1:nrow(stations_df)) {\n    distance[i] &lt;- sqrt((x - stations_df[i, 2])^2 + (y - stations_df[i, 3])^2)\n  }\n  nom_station &lt;- as.character(stations_df$Station[which.min(distance)])\n  return(nom_station)\n}\n\nTestons la fonction avec des coordonnÃ©es.\n\ndist_station(x = 459875, y = 5701988, stations_df = mes_stations)\n\n[1] \"B\"\n\n\nNous appliquons cette fonction Ã  toutes les lignes du tableau, puis en retournons un Ã©chantillon.\n\nchicoute |&gt; \n  rowwise() |&gt; \n  mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) |&gt; \n  select(ID, CodeTourbiere, Longitude_m, Latitude_m, Station) |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 90 Ã— 5\n# Rowwise: \n      ID CodeTourbiere Longitude_m Latitude_m Station\n   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;  \n 1     1 BEAU               490627    5702454 A      \n 2     2 BEAU               490634    5702452 A      \n 3     3 BEAU               490638    5702461 A      \n 4     4 BEAU               490647    5702453 A      \n 5     5 BEAU               490654    5702445 A      \n 6     6 BP                 484865    5706394 B      \n 7     7 BP                 484054    5706307 B      \n 8     8 BP                 484742    5702280 B      \n 9     9 BP                 484761    5706324 B      \n10    10 BP                 484780    5706364 B      \n# â„¹ 80 more rows\n\n\nCela semble fonctionner. On peut y ajouter un left_join() pour joindre les donnÃ©es mÃ©tÃ©o au tableau principal.\n\nchicoute_weather &lt;- chicoute |&gt; \n  rowwise() |&gt; \n  mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) |&gt; \n  left_join(y = mes_stations, by = \"Station\")\nchicoute_weather |&gt;  slice_sample(n = 10)\n\n# A tibble: 90 Ã— 36\n# Rowwise: \n      ID CodeTourbiere Ordre  Site Traitement DemiParcelle SousTraitement\n   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;         \n 1     1 BEAU          A         1 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 2     2 BEAU          A         2 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 3     3 BEAU          A         3 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 4     4 BEAU          A         4 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 5     5 BEAU          A         5 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 6     6 BP            H         1 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 7     7 BP            H         2 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 8     8 BP            H         3 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 9     9 BP            H         4 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n10    10 BP            H         5 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n# â„¹ 80 more rows\n# â„¹ 29 more variables: Latitude_m.x &lt;dbl&gt;, Longitude_m.x &lt;dbl&gt;,\n#   Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;,\n#   TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;,\n#   TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;,\n#   FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;,\n#   SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, â€¦\n\n\n\n3.5.7 Exporter un tableau\nSimplement avec write_csv().\n\nwrite_csv(chicoute_weather, \"data/chicoute_weather.csv\")\n\n\n3.5.8 Aller plus loin dans le tidyverse\nLe livre R for data science (2e), de Hadley Wickham et Garrett Grolemund (couverture Ã  la FigureÂ 3.7), est un incontournable.\n\n\n\n\nFigureÂ 3.7: Couverture du libre de Hadley Wickham, Mine Ã‡etinkaya-Rundel et Garrett Grolemund, Source: https://r4ds.hadley.nz/"
  },
  {
    "objectID": "03-tableaux.html#rÃ©fÃ©rences",
    "href": "03-tableaux.html#rÃ©fÃ©rences",
    "title": "3Â  Organisation des donnÃ©es et opÃ©rations sur des tableaux",
    "section": "\n3.6 RÃ©fÃ©rences",
    "text": "3.6 RÃ©fÃ©rences\nParent L.E., Parent, S.Ã‰., Herbert-Gentile, V., Naess, K. et Lapointe, L. 2013. Mineral Balance Plasticity of Cloudberry (Rubus chamaemorus) in Quebec-Labrador Bogs. American Journal of Plant Sciences, 4, 1508-1520. DOI: 10.4236/ajps.2013.47183"
  },
  {
    "objectID": "04-visualisation.html#pourquoi-explorer",
    "href": "04-visualisation.html#pourquoi-explorer",
    "title": "4Â  Visualisation",
    "section": "\n4.1 Pourquoi explorer graphiquement?",
    "text": "4.1 Pourquoi explorer graphiquement?\nLa plupart des graphiques que vous gÃ©nÃ©rerez ne seront pas destinÃ©s Ã  Ãªtre publiÃ©s. Ils viseront probablement dâ€™abord Ã  explorer des donnÃ©es. Cela vous permettra de mettre en Ã©vidence de nouvelles perspectives.\nPrenons par exemple deux variables, \\(X\\) et \\(Y\\). Vous calculez leur moyenne, Ã©cart-type et la corrÃ©lation entre les deux variables (nous verrons les statistiques plus en dÃ©tail dans un prochain chapitre).\n\nlibrary(\"tidyverse\")\ndatasaurus &lt;- read_tsv(\"data/DatasaurusDozen.tsv\")\n\ncor_datasaurus &lt;- datasaurus |&gt; \n  group_by(dataset) |&gt; \n  summarise(cor = cor(x = x, y = y, method = \"pearson\"))\n\ndatasaurus |&gt; \n  group_by(dataset) |&gt; \n  summarise_all(list(mean = mean, sd = sd)) |&gt; \n  left_join(cor_datasaurus, by = \"dataset\")\n\n# A tibble: 13 Ã— 6\n   dataset    x_mean y_mean  x_sd  y_sd     cor\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 away         54.3   47.8  16.8  26.9 -0.0641\n 2 bullseye     54.3   47.8  16.8  26.9 -0.0686\n 3 circle       54.3   47.8  16.8  26.9 -0.0683\n 4 dino         54.3   47.8  16.8  26.9 -0.0645\n 5 dots         54.3   47.8  16.8  26.9 -0.0603\n 6 h_lines      54.3   47.8  16.8  26.9 -0.0617\n 7 high_lines   54.3   47.8  16.8  26.9 -0.0685\n 8 slant_down   54.3   47.8  16.8  26.9 -0.0690\n 9 slant_up     54.3   47.8  16.8  26.9 -0.0686\n10 star         54.3   47.8  16.8  26.9 -0.0630\n11 v_lines      54.3   47.8  16.8  26.9 -0.0694\n12 wide_lines   54.3   47.8  16.8  26.9 -0.0666\n13 x_shape      54.3   47.8  16.8  26.9 -0.0656\n\n\nLes moyennes, Ã©carts-types et corrÃ©lations sont Ã  peu prÃ¨s les mÃªmes pour tous les groupes. Peut-on conclure que tous les groupes sont semblables? Pas encore.\nPour dÃ©montrer que ces statistiques ne vous apprendront pas grand chose sur la structure des donnÃ©es, Matejka et Fitzmaurice (2017) ont gÃ©nÃ©rÃ© 12 jeux de donnÃ©es \\(X\\) et \\(Y\\), ayant chacun pratiquement les mÃªmes statistiques. Mais avec des structures bien diffÃ©rentes (FigureÂ 4.2)!\n\n\n\n\nFigureÂ 4.2: Animation montrant la progression du jeu de donnÃ©es Datasaurus pour toutes les formes visÃ©es. Source: Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing"
  },
  {
    "objectID": "04-visualisation.html#publier-un-graphique",
    "href": "04-visualisation.html#publier-un-graphique",
    "title": "4Â  Visualisation",
    "section": "\n4.2 Publier un graphique",
    "text": "4.2 Publier un graphique\nVous voilÃ  sensibilisÃ©s Ã  lâ€™importance dâ€™explorer les donnÃ©es graphiquement. Mais ce qui ultimement Ã©manera dâ€™un projet sera le rapport que vous dÃ©poserez, lâ€™article scientifique que vous ferez publier ou le billet de blogue que vous partagerez sur les rÃ©seaux sociaux. Les graphiques inclus dans vos publications mÃ©ritent une attention particuliÃ¨re pour que votre audience puisse comprendre les dÃ©couvertes et perspectives offertes par vos travaux. Pour ce faire, un graphique doit rÃ©pondre honnÃªtement Ã  la question posÃ©e tout en Ã©tant attrayant.\n\n4.2.1 Cinq qualitÃ©s dâ€™un bon graphique\nAlberto Cairo, chercheur spÃ©cialisÃ© en visualisation de donnÃ©es, a fait paraÃ®tre en 2016 le livre The Truthful art. Il note cinq qualitÃ©s dâ€™une visualisation bien conÃ§ue (les citations de cette section proviennent de ma traduction de Alberto Cairo, The Truthful Art (2016), p.Â 45.).\n\n1- Elle est vÃ©ritable, puisquâ€™elle est basÃ©e sur une recherche exhaustive et honnÃªte.\n\nCela vaut autant pour les graphiques que pour lâ€™analyse de donnÃ©es. Il sâ€™agit froidement de prÃ©senter les donnÃ©es selon lâ€™interprÃ©tation la plus exacte. Les piÃ¨ges Ã  Ã©viter sont le picorage de cerises et la surinterprÃ©tation des donnÃ©es. Le picorage, câ€™est lorsquâ€™on rÃ©duit les perspectives afin de soutenir un argumentaire. Par exemple, retirer des donnÃ©es dâ€™une rÃ©gion ou dâ€™une dÃ©cennie qui rendraient factice une conclusion fixÃ©e a priori. Ceci vaut autant pour les graphiques que pour les statistiques (nous parlerons du p-hacking au prochain chapitre). La surinterprÃ©tation, câ€™est lorsque lâ€™on saute rapidement aux conclusions: par exemple, que lâ€™on gÃ©nÃ¨re des corrÃ©lations, voire mÃªme des relations de causalitÃ©s Ã  partir de ce qui nâ€™est que du bruit de fond. Ã€ ce titre, lors dâ€™une confÃ©rence, Heather Krause insiste sur lâ€™importance de faire en sorte que les reprÃ©sentations graphiques rÃ©pondent correctement aux questions posÃ©es dans une Ã©tude (FigureÂ 4.3).\n\n\n\n\nFigureÂ 4.3: The F word: Protect your work from four hidden fallacies when working with data, une confÃ©rence de Heather Krause, 2018\n\n\n\n\n2- Elle est fonctionnelle, puisquâ€™elle constitue une reprÃ©sentation prÃ©cise des donnÃ©es, et quâ€™elle est construite de maniÃ¨re Ã  laisser les observateurs.trices prendre des initiatives consÃ©quentes.\n\nâ€œLa seule chose qui est pire quâ€™un diagramme en pointe de tarte, câ€™est dâ€™en prÃ©senter plusieursâ€ (Edward Tufte, designer, citÃ© par Alberto Cairo, 2016, p.Â 50). Choisir le bon graphique pour reprÃ©senter vos donnÃ©es est beaucoup moins une question de bon goÃ»t quâ€™une question de dÃ©marche rationnelle sur lâ€™objectif visÃ© par la prÃ©sentation dâ€™un graphique. Je prÃ©senterai des lignes guides pour sÃ©lectionner le type de graphique qui prÃ©sentera vos donnÃ©es de maniÃ¨re fonctionnelle en fonction de lâ€™objectif dâ€™un graphique (dâ€™ailleurs, avez-vous vraiment besoin dâ€™un graphique?).\n\n3- Elle est attrayante et intrigante, et mÃªme esthÃ©tiquement plaisante pour lâ€™audience visÃ©e - les scientifiques dâ€™abord, mais aussi le public en gÃ©nÃ©ral.\n\nEn sciences naturelles, la pensÃ©e rationnelle, la capacitÃ© Ã  organiser la connaissance et crÃ©er de nouvelles avenues sont des qualitÃ©s qui sont privilÃ©giÃ©es au talent artistique. Que vous ayez oÃ¹ non des aptitudes en art visuel, prÃ©sentez de lâ€™information, pas des dÃ©corations. Excel vous permet dâ€™ajouter une perspective 3D Ã  un diagramme en barres. La profondeur contient-elle de lâ€™information? Non. Cette dÃ©coration ne fait quâ€™ajouter de la confusion. Minimalisez, fournissez le plus dâ€™information possible avec le moins dâ€™Ã©lÃ©ments graphiques possibles. Câ€™est ce que vous proposent les guides graphiques que jâ€™introduirai plus loin.\n\n4- Elle est pertinente, puisquâ€™elle rÃ©vÃ¨le des Ã©vidences scientifiques autrement difficilement accessibles.\n\nIl sâ€™agit de susciter un eurÃªka, dans le sens quâ€™elle gÃ©nÃ¨re une idÃ©e, et parfois une initiative, en un coup dâ€™Å“il. Le graphique en bÃ¢ton de hockey est un exemple oÃ¹ lâ€™on a spontanÃ©ment une idÃ©e de la situation. Cette situation peut Ãªtre la prÃ©sence dâ€™un phÃ©nomÃ¨ne comme lâ€™augmentation de la tempÃ©rature globale, mais aussi lâ€™absence de phÃ©nomÃ¨nes pourtant attendus.\n\n5- Elle est instructive, parce que si lâ€™on saisit et accepte les Ã©vidences scientifiques quâ€™elle dÃ©crit, cela changera notre perception pour le mieux.\n\nEn prÃ©sentant cette qualitÃ©, Alberto Cairo voulait inciter ses lecteurs.trices Ã  choisir des sujets de discussion visuelle de maniÃ¨re Ã  participer Ã  un monde meilleur. En ce qui nous concerne, il sâ€™agit de bien sÃ©lectionner lâ€™information que lâ€™on dÃ©sire transmettre. Imaginez que vous avez travaillÃ© quelques jours pour crÃ©er un graphique, dont vous Ãªtes fier, mais vous (ou un collÃ¨gue hiÃ©rarchiquement favorisÃ©) vous rendez compte que le graphique soutient peu ou pas le propos ou lâ€™objectif de votre thÃ¨se/mÃ©moire/rapport/article. Si câ€™est bien le cas, vous feriez mieux de laisser tomber votre oeuvre et considÃ©rer votre dÃ©marche comme une occasion dâ€™apprentissage.\nAlberto Cairo rÃ©sume son livre The Truthful Art dans une entrevue avec le National Geographic."
  },
  {
    "objectID": "04-visualisation.html#choisir-type-graph",
    "href": "04-visualisation.html#choisir-type-graph",
    "title": "4Â  Visualisation",
    "section": "\n4.3 Choisir le type de graphique le plus appropriÃ©",
    "text": "4.3 Choisir le type de graphique le plus appropriÃ©\nDe nombreuses maniÃ¨res de prÃ©senter les donnÃ©es sont couramment utilisÃ©es, comme les nuages de points, les lignes, les histogrammes, les diagrammes en barres et en pointes de tarte. Les principaux types de graphiques seront couverts dans ce chapitre. Dâ€™autres types spÃ©cialisÃ©s seront couverts dans les chapitres appropriÃ©s (graphiques davantage orientÃ©s vers les statistiques, les biplots, les dendrogrammes, les diagrammes ternaires, les cartes, etc.).\nLa visualisation de donnÃ©es est aujourdâ€™hui devenue un mÃ©tier pour plusieurs personnes ayant des affinitÃ©s pour la science, les arts et la communication, dont certaines partagent leur expertise sur le web. Ã€ ce titre, le site from data to viz est Ã  conserver dans vos marque-pages. Il comprend des arbres dÃ©cisionnels qui vous guident vers les options appropriÃ©es pour prÃ©senter vos donnÃ©es, puis fournissent des exemples pour produire ces visualisations en R. Ã‰galement, je suggÃ¨re le site internet de Ann K. Emery, qui prÃ©sente des lignes guide pour prÃ©senter le graphique adÃ©quat selon les donnÃ©es en main. De nombreuses recettes sont Ã©galement proposÃ©es sur r-graph-gallery.com. En ce qui a trait aux couleurs, le choix nâ€™est pas anodin. Si vous avez le souci des dÃ©tails sur les Ã©lÃ©ments esthÃ©tiques de vos graphiques, je recommande la lecture de ce billet de blog de Lisa Charlotte Rost.\nRetenez nÃ©anmois que La couleur est une information. Les couleurs devraient Ãªtre sÃ©lectionnÃ©es dâ€™abord pour Ãªtre lisibles par les personnes ne percevant pas les couleurs (FigureÂ 4.4), selon le support (apte Ã  Ãªtre photocopiÃ©, lisible Ã  lâ€™Ã©cran, lisible sur des documents imprimÃ©s en noir et blanc) et selon le type de donnÃ©es. Vous pouvez aussi utiliser certains modules comme RColorBrewer comme expliquÃ© dans le billet suivant qui permet dâ€™adopter directement les palettes sÃ©lectionnÃ©es.\n\nDonnÃ©es continues ou catÃ©gorielles ordinales: gradient (transition graduelle dâ€™une couleur Ã  lâ€™autre), sÃ©quence (transition saccadÃ©e selon des groupes de donnÃ©es continues) ou divergentes (transition saccadÃ©e dâ€™une couleur Ã  lâ€™autre vers des couleurs divergentes, par exemple orange vers blanc vers bleu).\nDonnÃ©es catÃ©gorielles nominales: couleurs Ã©loignÃ©es dâ€™une catÃ©gorie Ã  une autre (plus il y a de catÃ©gories, plus les couleurs sont susceptibles de se ressembler).\n\n\n\n\n\nFigureÂ 4.4: Capture dâ€™Ã©cran de colorbrewer2.org, qui propose des palettes de couleurs pour crÃ©er des cartes, mais lâ€™information est pertinente pour tout type de graphique.\n\n\n\nLe Financial Times offre Ã©galement ce guide visuel (FigureÂ 4.5).\n\n\n\n\nFigureÂ 4.5: Guide de sÃ©lection de graphique du Financial Times\n\n\n\nCairo (2016) propose de procÃ©der en suivant ces Ã©tapes:\n\nRÃ©flÃ©chissez au message que vous dÃ©sirez transmettre: comparer les catÃ©gories \\(A\\) et \\(B\\), visualiser une transition ou un changement de \\(A\\) vers \\(B\\), prÃ©senter une relation entre \\(A\\) et \\(B\\) ou la distribution de \\(A\\) et \\(B\\) sur une carte.\nEssayez diffÃ©rentes reprÃ©sentations: si le message que vous dÃ©sirez transmettre a plusieurs volets, il se pourrait que vous ayez besoin de plus dâ€™un graphique.\nMettez de lâ€™ordre dans vos donnÃ©es. Câ€™Ã©tait le sujet du chapitreÂ 3.\nTestez le rÃ©sultat. â€œHÃ©, quâ€™est-ce que tu comprends de cela?â€ Si la personne hausse les Ã©paules, il va falloir rÃ©Ã©valuer votre stratÃ©gie."
  },
  {
    "objectID": "04-visualisation.html#choisir-son-outil-de-visualisation",
    "href": "04-visualisation.html#choisir-son-outil-de-visualisation",
    "title": "4Â  Visualisation",
    "section": "\n4.4 Choisir son outil de visualisation",
    "text": "4.4 Choisir son outil de visualisation\nLes modules et logiciels de visualisation sont basÃ©s sur des approches que lâ€™on pourrait placer sur un spectre allant de lâ€™impÃ©ratif au dÃ©claratif.\n\n4.4.1 Approche impÃ©rative\nSelon cette approche, vous indiquez comment placer lâ€™information dans un espace graphique. Vous indiquer les symboles, les couleurs, les types de ligne, etc. Peu de choses sont automatisÃ©es, ce qui laisse une grande flexibilitÃ©, mais demande de vouer beaucoup dâ€™Ã©nergie Ã  la maniÃ¨re de coder pour obtenir le graphique dÃ©sirÃ©. Le module graphique de Excel, ainsi que le module graphique de base de R, utilisent des approches impÃ©ratives.\n\n4.4.2 Approche dÃ©clarative\nLes stratÃ©gies dâ€™automatisation graphique se sont grandement amÃ©liorÃ©es au cours des derniÃ¨res annÃ©es. PlutÃ´t que de vouer vos Ã©nergies Ã  crÃ©er un graphique, il est maintenant possible de spÃ©cifier ce que lâ€™on veut prÃ©senter.\n\nLa visualisation dÃ©clarative vous permet de penser aux donnÃ©es et Ã  leurs relations, plutÃ´t que des dÃ©tails accessoires.\nJake Vanderplas, Declarative Statistical Visualization in Python with Altair (ma traduction)\n\nLâ€™approche dÃ©clarative passe souvent par une grammaire graphique, câ€™est-Ã -dire un langage qui explique ce que lâ€™on veut prÃ©senter - en mode impÃ©ratif, on spÃ©cifie plutÃ´t comment on veut prÃ©senter les donnÃ©es. Le module ggplot2 est le module dÃ©claratif par excellence en R."
  },
  {
    "objectID": "04-visualisation.html#visualisation-en-r",
    "href": "04-visualisation.html#visualisation-en-r",
    "title": "4Â  Visualisation",
    "section": "\n4.5 Visualisation en R",
    "text": "4.5 Visualisation en R\nEn R, votre trousse dâ€™outils de visualisation mÃ©riterait de comprendre les modules suivants.\n\n\nbase. Le module de base de R contient des fonctions graphiques trÃ¨s polyvalentes. Les axes sont gÃ©nÃ©rÃ©s automatiquement, on peut y ajouter des titres et des lÃ©gendes, on peut crÃ©er plusieurs graphiques sur une mÃªme figure, on peut y ajouter diffÃ©rentes gÃ©omÃ©tries (points, lignes et polygones), avec diffÃ©rents types de points ou de traits, diffÃ©rentes couleurs, etc. Les modules spÃ©cialisÃ©s viennent souvent avec leurs graphiques spÃ©cialisÃ©s, construits Ã  partir du module de base. En tant que module graphique impÃ©ratif, on peut tout faire ou presque (pas dâ€™interactivitÃ©), mais lâ€™Ã©criture du code est peut expressive.\n\nggplot2. Câ€™est le module graphique par excellence en R (et jâ€™ose dire: en calcul scientifique). ggplot2 se base sur une grammaire graphique. Ã€ partir dâ€™un tableau de donnÃ©es, une colonne peut dÃ©finir lâ€™axe des x, une autre lâ€™axe des y, une autre la couleur des points ou leur dimension. Une autre colonne dÃ©finissant des catÃ©gories peut segmenter la visualisation en plusieurs graphiques alignÃ©s horizontalement ou verticalement. Des extensions de ggplot2 permettent de gÃ©nÃ©rer des cartes (ggmap), des diagrammes ternaires (ggtern), des animations (gganimate), etc.\n\nplotly. plotly est un module graphique particuliÃ¨rement utile pour gÃ©nÃ©rer des graphiques interactifs. plotly offre une fonction toute simple pour rendre interactif un graphique ggplot2.\n\nNous survolerons rapidement le module de base, irons plus en profondeur avec ggplot2, puis je prÃ©senterai briÃ¨vement les graphiques interactifs avec plotly."
  },
  {
    "objectID": "04-visualisation.html#module-de-base-pour-les-graphiques",
    "href": "04-visualisation.html#module-de-base-pour-les-graphiques",
    "title": "4Â  Visualisation",
    "section": "\n4.6 Module de base pour les graphiques",
    "text": "4.6 Module de base pour les graphiques\nNous allons dâ€™abord survoler le module de base, en mode impÃ©ratif. La fonction de base pour les graphiques en R est plot(). Pour nous exercer avec cette fonction, chargeons dâ€™abord le tableau de donnÃ©es dâ€™exercice iris, publiÃ© en 1936 par le cÃ©lÃ¨bre biostatisticien Ronald Fisher.\n\ndata(\"iris\")\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nLe tableau iris contient 5 colonnes, les 4 premiÃ¨res dÃ©crivant les longueurs et largeurs des pÃ©tales et sÃ©pales de diffÃ©rentes espÃ¨ces dâ€™iris dont le nom apparaÃ®t Ã  la 5iÃ¨me colonne. Vous avez dÃ©jÃ  vu au chapitre prÃ©cÃ©dent comment extraire les colonnes dâ€™un tableau; une mÃ©thode consiste Ã  appeler le tableau, suivi du $, puis du nom de la colonne, par exemple iris$Species. Pour gÃ©nÃ©rer un graphique avec la fonction plot():\n\nplot(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\nPar dÃ©faut, le premier argument est le vecteur dÃ©finissant lâ€™axe des x et le deuxiÃ¨me est celui dÃ©finissant lâ€™axe des y. Vous rencontrerez souvent de telles utilisations dâ€™arguments implicites, mais je prÃ©fÃ¨re Ãªtre explicite en dÃ©finissant bien les arguments: plot(x = iris$Sepal.Length, y = iris$Petal.Length). Le graphique prÃ©cÃ©dent peut Ãªtre amplement personnalisÃ© en utilisant diffÃ©rents arguments (FigureÂ 4.6).\n\n\n\n\nFigureÂ 4.6: Ã‰lÃ©ments personnalisables dâ€™un graphique de base\n\n\n\nExercice. Utilisez ces arguments dans la cellule de code de la figure plot(iris$Sepal.Length, iris$Petal.Length).\nRemarquez que la fonction a dÃ©cidÃ© toute seule de crÃ©er un nuage de point. La fonction plot() est conÃ§ue pour crÃ©er le graphique appropriÃ© selon le type des donnÃ©es spÃ©cifiÃ©es: lignes, boxplot, etc. Si lâ€™on spÃ©cifiait les espÃ¨ces comme argument x:\n\nplot(x = iris$Species, y = iris$Petal.Length)\n\n\n\n# ou bien\n# iris |&gt; \n#   select(Species, Petal.Length) |&gt; \n#   plot()\n\nDe mÃªme, la fonction plot() appliquÃ©e Ã  un tableau de donnÃ©es gÃ©nÃ©rera une reprÃ©sentation bivariÃ©e.\n\nplot(iris)\n\n\n\n\nIl est possible dâ€™encoder des attributs grÃ¢ce Ã  des vecteurs de facteurs (catÃ©gories).\n\nplot(iris, col = iris$Species)\n\n\n\n\nLâ€™argument type = \"\" permet de personnaliser lâ€™apparence:\n\n\ntype = \"p\": points\n\ntype = \"l\": ligne\n\ntype = \"o\" et type = \"b\": ligne et points\n\ntype = \"n\": ne rien afficher\n\nCrÃ©ons un jeu de donnÃ©es.\n\ntime &lt;- seq(from = 0, to = 100, by = 10)\nheight &lt;- abs(time * 0.1 + rnorm(length(time), 0, 2)) \n# abs pour valeur absolue (changement de signe si nÃ©gatif)\nplot(x = time, y = height, type = \"b\", lty = 2, lwd = 1)\n\n\n\n\nLe type de ligne est spÃ©cifiÃ© par lâ€™argument lty (qui peut prendre un chiffre ou une chaÃ®ne de caractÃ¨res, i.e.Â 1 est Ã©quivalent de \"solid\", 2 de \"dashed\", 3 de \"dotted\", etc.) et la largeur du trait (valeur numÃ©rique), par lâ€™argument lwd.\nLa fonction hist() permet quant Ã  elle de crÃ©er des histogrammes. Parmi ses arguments, breaks est particuliÃ¨rement utile, car il permet dâ€™ajuster la segmentation des incrÃ©ments.\n\nhist(iris$Petal.Length, breaks = 60)\n\n\n\n\nExercice. Ajustez le titre de lâ€™axe des x, ainsi que les limites de lâ€™axe des x. ÃŠtes-vous en mesure de colorer lâ€™intÃ©rieur des barres en bleu?\nLa fonction plot() peut Ãªtre suivie de plusieurs autres couches comme des lignes (lines() ou abline()), des points (points()), du texte (text()), des polygones (polygon(), des lÃ©gendes (legend())), etc. On peut aussi personnaliser les couleurs, les types de points, les types de lignes, etc. Lâ€™exemple suivant ajoute une ligne au graphique. Ne prÃªtez pas trop attention aux fonctions predict() et lm() pour lâ€™instant: nous les verrons au chapitreÂ 7.\n\nplot(x = time, y = height)\nlines(x = time, y = predict(lm(height ~ time)))\n\n\n\n\nPour exporter un graphique, vous pouvez passer par le menu Export de RStudio. Mais pour des graphiques destinÃ©s Ã  Ãªtre publiÃ©s, je vous suggÃ¨re dâ€™exporter vos graphiques avec une haute rÃ©solution Ã  la suite de la commande png() (ou jpg() ou svg()).\n\nsvg(filename = \"images/mon-graphique.svg\", width = 3000, height = 2000)\n# png(filename = 'images/mon-graphique.png', width = 3000, height=2000, res=300)\nplot(\n  x = iris$Petal.Length,\n  y = iris$Sepal.Length,\n  col = iris$Species,\n  cex = 3, # dimension des points\n  pch = 16 # type de points\n)\ndev.off()\n\npng \n  2 \n\n\nLe format svg crÃ©e une version vectorielle du graphique, câ€™est-Ã -dire que lâ€™image exportÃ©e est un fichier contenant les formes, non pas les pixels. Cela vous permet dâ€™Ã©diter votre graphique dans un logiciel de dessin vectoriel (comme Inkscape).\nDans le bloc de code prÃ©cÃ©dent, jâ€™ai mis en commentaire (# ...) le format dâ€™image png, utile pour les images de type graphique, avec des changements de couleurs drastiques. Jâ€™y ai spÃ©cifiÃ© une haute rÃ©solution, Ã  300 pixels par pouce. Pour les photos, vous prÃ©fÃ©rerez le format jpg. Des Ã©diteurs demanderont peut-Ãªtre des formats vectoriels comme pdf ou eps. Si vous ne trouvez pas de moyen de modifier un aspect du graphique dans le code (bouger des Ã©tiquettes ou des lÃ©gendes, ajouter des Ã©lÃ©ments graphiques), vous pouvez exporter votre graphique en format svg et Ã©diter votre graphique dans Inkscape.\nLe module de base de R comprend une panoplie dâ€™autres particularitÃ©s que je ne couvrirai pas ici, en faveur du module ggplot2."
  },
  {
    "objectID": "04-visualisation.html#la-grammaire-graphique-ggplot2",
    "href": "04-visualisation.html#la-grammaire-graphique-ggplot2",
    "title": "4Â  Visualisation",
    "section": "\n4.7 La grammaire graphique ggplot2\n",
    "text": "4.7 La grammaire graphique ggplot2\n\nLe module esquisse est une extension de RStudio permettant de gÃ©nÃ©rer du code pour le module graphique ggplot2. La vidÃ©o suivant, oÃ¹ jâ€™utilise esquisse, montre ce en quoi consiste une grammaire graphique.\nVideo\nChaque colonne est un Ã©lÃ©ment graphique qui peut Ãªtre encodÃ© pour former la position en x, en y, la taille des points, leur couleur, ou mÃªme le panneau (facet). Mais quelle forme prendra le bidule positionnÃ©? Des points, lignes, boxplots, barres? Câ€™est ce que dÃ©finit une grammaire graphique. BriÃ¨vement, une grammaire graphique permet de schÃ©matiser des donnÃ©es avec des marqueurs (points, lignes, etc.) sur des attributs visuels (couleurs, dimension, forme). Cette approche permet de dÃ©gager 5 composantes.\n\n\nLes donnÃ©es. Votre tableau est bien sÃ»r un argument nÃ©cessaire pour gÃ©nÃ©rer le graphique.\n\nLes marqueurs. Un terme abstrait pour dÃ©signer les points, les lignes, les polygones, les barres, les flÃ¨ches, etc. En ggplot2, ce sont des gÃ©omÃ©tries, par exemple geom_point() pour dÃ©finir une gÃ©omÃ©trie de points.\n\nLes attributs encodÃ©s. La position, la dimension, la couleur ou la forme que prendront les gÃ©omÃ©tries. En ggplot2, on les nomme les aesthetics.\n\nLes attributs globaux. Les attributs sont globaux lorsquâ€™ils sont constants (ils ne dÃ©pendent pas dâ€™une variable). Les valeurs par dÃ©faut conviennent gÃ©nÃ©ralement, mais certains attributs peuvent Ãªtre spÃ©cifiÃ©s: par exemple la forme ou la couleur des points, le type de ligne, etc.\n\nLes thÃ¨mes. Le thÃ¨me du graphique permet de personnaliser la maniÃ¨re dont le graphique est rendu. Il existe des thÃ¨mes prÃ©dÃ©finis, que vous pouvez ajuster, mais il est possible de crÃ©er vos propres thÃ¨mes (nous ne couvrirons pas cela dans ce cours).\n\n\n\n\n\nFigureÂ 4.7: CrÃ©er une oeuvre dâ€™art avec ggplot2, dessin de @allison_horst.\n\n\n\nLe flux de travail pour crÃ©er un graphique Ã  partir dâ€™une grammaire ressemble donc Ã  ceci:\nAvec mon tableau,\nCrÃ©er un marqueur (\nencoder(position X = colonne A,\nposition Y = colonne B,\ncouleur = colonne C),\nforme globale = 1)\nAvec un thÃ¨me noir et blanc\nLe module tidyverse installera des modules utilisÃ©s de maniÃ¨re rÃ©currente dans ce cours, comme ggplot2, dplyr, tidyr et readr. Je recommande de le charger au dÃ©but de vos sessions de travail.\n\nlibrary(\"tidyverse\")\n\nLâ€™approche tidyverse est une grammaire des donnÃ©es. Le module ggplot2, qui en fait partie, est une grammaire graphique (dâ€™oÃ¹ le gg de ggplot)."
  },
  {
    "objectID": "04-visualisation.html#mon-premier-ggplot",
    "href": "04-visualisation.html#mon-premier-ggplot",
    "title": "4Â  Visualisation",
    "section": "\n4.8 Mon premier ggplot",
    "text": "4.8 Mon premier ggplot\nPour notre premier exercice, je vais charger un tableau depuis le fichier de donnÃ©es abalone.data. Pour plus de dÃ©tails sur les tableaux de donnÃ©es, consultez le chapitreÂ 3. Le fichier de donnÃ©es porte sur un escargot de mer et comprend le sexe (M: mÃ¢le, F: femelle et I: enfant), des poids et dimensions des individus observÃ©s, et le nombre dâ€™anneaux comptÃ©s dans la coquille.\n\nabalone &lt;- read_csv(\"data/abalone.csv\")\n\nRows: 4177 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (1): Type\ndbl (8): LongestShell, Diameter, Height, WholeWeight, ShuckedWeight, Viscera...\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nInspectons lâ€™entÃªte du tableau avec la fonction head().\n\nhead(abalone)\n\n# A tibble: 6 Ã— 9\n  Type  LongestShell Diameter Height WholeWeight ShuckedWeight VisceraWeight\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 M            0.455    0.365  0.095       0.514        0.224         0.101 \n2 M            0.35     0.265  0.09        0.226        0.0995        0.0485\n3 F            0.53     0.42   0.135       0.677        0.256         0.142 \n4 M            0.44     0.365  0.125       0.516        0.216         0.114 \n5 I            0.33     0.255  0.08        0.205        0.0895        0.0395\n6 I            0.425    0.3    0.095       0.352        0.141         0.0775\n# â„¹ 2 more variables: ShellWeight &lt;dbl&gt;, Rings &lt;dbl&gt;\n\n\nSuivant la grammaire graphique ggplot2, on pourra crÃ©er ce graphique de points comprenant les attributs suivants.\n\n\ndata = abalone, le fichier de donnÃ©es.\n\nmapping = aes(...), spÃ©cifiÃ© comme attribut de la fonction ggplot(), cet encodage (ou aesthetic) reste lâ€™encodage par dÃ©faut pour tous les marqueurs du graphique. Toutefois, lâ€™encodage mapping = aes() peut aussi Ãªtre spÃ©cifiÃ© dans la fonction du marqueur (par exemple geom_point()). Dans lâ€™encodage global du graphique, on place en x la longueur de la coquille (x = LongestShell) et on place en y le poids de la coquille (y = ShellWeight).\nPour ajouter une fonction Ã  ggplot, comme une nouvelle couche de marqueur ou des Ã©lÃ©ments de thÃ¨me, on utilise le +. GÃ©nÃ©ralement, on change aussi de ligne.\nLe marqueur ajoutÃ© est un point, geom_point(), dans lequel on spÃ©cifie un encodage de couleur sur la variable Type (colour = Type) et un encodage de dimension du point sur la variable rings (size = Rings). Lâ€™attribut alpha = 0.5 se situe hors du mapping et de la fonction aes(): câ€™est un attribut identique pour tous les points.\n\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)\n\n\n\n\nIl existe plusieurs types de marqueurs:\n\n\ngeom_point() pour les points\n\ngeom_line() pour les lignes\n\ngeom_bar() pour les diagrammes en barre en dÃ©compte, geom_col en terme de grandeur et geom_histogram pour les histogrammes\n\ngeom_boxplot() pour les boxplots\n\ngeom_errorbar(), geom_pointrange() ou geom_crossbar() pour les marges dâ€™erreur\n\ngeom_map() pour les cartes\netc.\n\nIl existe plusieurs attributs dâ€™encodage:\n\nla position x, y et z (z pertinent notamment pour le marqueur geom_tile())\nla taille size\n\nla forme des points shape\n\nla couleur, qui peut Ãªtre discrÃ¨te ou continue :\n\n\ncolour, pour la couleur des contours\n\nfill, pour la couleur de remplissage\n\n\nle type de ligne linetype\n\nla transparence alpha\n\net dâ€™autres types spÃ©cialisÃ©s que vous retrouverez dans la documentation des marqueurs\n\nLes types de marqueurs et leurs encodages sont dÃ©crits dans la documentation de ggplot2, qui fournit des feuilles aide-mÃ©moire quâ€™il est commode dâ€™imprimer et dâ€™afficher prÃ¨s de soi (FigureÂ 4.8).\n\n\n\n\nFigureÂ 4.8: Aide-mÃ©moire de ggplot2, source: https://rstudio.github.io/cheatsheets/html/data-visualization.html\n\n\n\n\n4.8.0.1 Les facettes\nDans ggplot2, les facetttes sont un type spÃ©cial dâ€™encodage utilisÃ© pour dÃ©finir des grilles de graphiques. Elles prennent deux formes:\n\nLe collage, facet_wrap(). Une variable catÃ©gorielle est utilisÃ©e pour segmenter les graphiques en plusieurs graphiques, qui sont placÃ©s lâ€™un Ã  la suite de lâ€™autre dans un arrangement spÃ©cifiÃ© par un nombre de colonnes ou un nombre de lignes.\nLa grille, facet_grid(). Une ou deux variables segmentent les graphiques selon les colonnes et les lignes.\n\nLes facettes peuvent Ãªtre spÃ©cifiÃ©es nâ€™importe oÃ¹ dans la chaÃ®ne de commande de ggplot2 mais, conventionnellement, on les place tout de suite aprÃ¨s la fonction ggplot().\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  facet_wrap(~Type, ncol = 2) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)\n\n\n\n\nLa fonction cut() permet de discrÃ©tiser des variables continues en catÃ©gories ordonnÃ©es - les fonctions peuvent Ãªtre utilisÃ©es Ã  lâ€™intÃ©rieur de la fonction ggplot.\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  facet_grid(Type ~ cut(Rings, breaks = seq(0, 30, 5))) +\n  geom_point(mapping = aes(colour = Type), alpha = 0.5)\n\n\n\n\nPar dÃ©faut, les axes des facettes, ainsi que leurs dimensions, sont les mÃªmes. Une telle reprÃ©sentation permet de comparer les facets sur une mÃªme Ã©chelle. Les axes peuvent Ãªtre dÃ©finis selon les donnÃ©es avec lâ€™argument scales, tandis que lâ€™espace des facettes peut Ãªtre conditionnÃ© selon lâ€™argument space - pour plus de dÃ©tails, voir la fiche de documentation.\nExercice. Personnalisez le graphique avec les donnÃ©es abalone en remplaÃ§ant les variables et en rÃ©organisant les facettes.\n\n4.8.1 Plusieurs sources de donnÃ©es\nIl peut arriver que les donnÃ©es pour gÃ©nÃ©rer un graphique proviennent de plusieurs tableaux. Lorsquâ€™on ne spÃ©cifie pas la source du tableau dans un marqueur, la valeur par dÃ©faut est le tableau spÃ©cifiÃ© dans lâ€™amorce ggplot(). Il est nÃ©anmoins possible de dÃ©finir une source personnalisÃ©e pour chaque marqueur en spÃ©cifiant data = ... comme argument du marqueur.\n\nabalone_siteA &lt;- data.frame(\n  LongestShell = c(0.3, 0.8, 0.7),\n  ShellWeight = c(0.05, 0.81, 0.77)\n)\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +\n  geom_point(data = abalone_siteA, size = 8, shape = 4)\n\n\n\n\n\n4.8.2 Exporter avec style\nLe fond gris est une marque distinctive de ggplot2. Il nâ€™est toutefois pas apprÃ©ciÃ© de tout le monde. Dâ€™autres thÃ¨mes dits complets peuvent Ãªtre utilisÃ©s (liste des thÃ¨mes complets). Les thÃ¨mes complets sont appelÃ©s avant la fonction theme(), qui permet dâ€™effectuer des ajustements prÃ©cis dont la liste exhaustive se trouve dans la documentation de ggplot2.\nVous pouvez aussi personnaliser le titre des axes (xlab() et ylab()) ou du graphique (ggtitle()), ou bien tout spÃ©cifier dans une mÃªme fonction ou bien tout en mÃªme temps dans labs(x = \"...\", y = \"...\", title = \"...\"). Il est possible dâ€™utiliser des exposants dans le titre des axes avec la fonction expression(), par exemple labs(x = expression(\"Dose (kg ha\"^\"-1\"~\")\")) pour intituler lâ€™axe des x avec \\(Dose~(kg~ha^{-1})\\). Aussi convient parfois de spÃ©cifier les limites (xlim() et ylim(), ou expand_limits(x = c(0, 1), y = c(0, 1))).\nPour exporter un ggplot, on pourra utiliser les commandes de R png(), svg() ou pdf(), ou les outils de RStudio. Toutefois, ggplot2 offre la fonction ggsave(), que lâ€™on place en remorque du graphique, en spÃ©cifiant les dimensions (width et height) ainsi que la rÃ©solution (dpi). La rÃ©solution dâ€™un graphique destinÃ© Ã  la publication est typiquement de plus de 300 dpi.\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +\n  #xlab(\"Length (mm)\") +\n  #ylab(\"Shell weight (g)\") +\n  #ggtitle(\"Abalone\") + # prÃ©fÃ©rablement dans une mÃªme ligne\n  labs(x = \"Length (mm)\", y = \"Shell weight (g)\", title = \"Abalone\") +\n  xlim(c(0, 1)) +\n  theme_classic() +\n  theme(\n    axis.title = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.text.y = element_text(size = 20, angle = 90, hjust = 0.5),\n    legend.box = \"horizontal\"\n  )\n\n\n\nggsave(\"images/abalone.png\", width = 8, height = 8, dpi = 300)\n\nNous allons maintenant couvrir diffÃ©rents types de graphiques, accessibles selon diffÃ©rents marqueurs:\n\nles nuages de points\nles diagrammes en ligne\nles boxplots\nles histogrammes\nles diagrammes en barres\n\n4.8.3 Nuages de points\nLâ€™exemple prÃ©cÃ©dent est un nuage de points, que nous avons gÃ©nÃ©rÃ© avec le marqueur geom_point(), qui a dÃ©jÃ  Ã©tÃ© passablement introduit. Lâ€™exploration de ces donnÃ©es a permis de dÃ©tecter une croissance exponentielle du poids de la coquille en fonction de sa longueur. Il est clair que les abalones juvÃ©niles (Type I) sont plus petits et moins lourds, mais nous devrons probablement procÃ©der Ã  des tests statistiques pour vÃ©rifier sâ€™il y a des diffÃ©rences entre mÃ¢les et femelles.\nLe graphique Ã©tant trÃ¨s chargÃ©, nous avons utilisÃ© des stratÃ©gies pour lâ€™allÃ©ger en utilisant de la transparence et des facettes. Le marqueur geom_jitter() peut permettre de mieux apprÃ©cier la dispersion des points en ajoutant une dispersion randomisÃ©e en x ou en y.\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_jitter(mapping = aes(colour = Type, size = Rings), alpha = 0.5, width = 0.05, height = 0.1)\n\n\n\n\nDans ce cas-ci, Ã§a ne change pas beaucoup, mais retenons-le pour la suite.\n\n4.8.4 Diagrammes en lignes\nLes lignes sont utilisÃ©es pour exprimer des liens entre une suite dâ€™information. Dans la plupart des cas, il sâ€™agit dâ€™une suite dâ€™information dans le temps que lâ€™on appelle les sÃ©ries temporelles (plus sur ce sujet au chapitreÂ 12. En lâ€™occurrence, les lignes devraient Ãªtre Ã©vitÃ©es si la sÃ©quence entre les variables nâ€™est pas Ã©vidente. Nous allons utiliser un tableau de donnÃ©es de R portant sur la croissance des orangers.\n\ndata(\"Orange\")\nhead(Orange)\n\n  Tree  age circumference\n1    1  118            30\n2    1  484            58\n3    1  664            87\n4    1 1004           115\n5    1 1231           120\n6    1 1372           142\n\n\nLa premiÃ¨re colonne spÃ©cifie le numÃ©ro de lâ€™arbre mesurÃ©, la deuxiÃ¨me son Ã¢ge et la troisiÃ¨me sa circonfÃ©rence. Le marqueur geom_line() permet de tracer la tendance de la circonfÃ©rence selon lâ€™Ã¢ge. En encodant la couleur de la ligne Ã  lâ€™arbre, nous pourrons tracer une ligne pour chacun dâ€™entre eux.\n\nggplot(data = Orange, mapping = aes(x = age, y = circumference)) +\n  geom_line(aes(colour = Tree))\n\n\n\n\nLa lÃ©gende ne montre pas les numÃ©ros dâ€™arbre en ordre croissant. En effet, la lÃ©gende (tout comme les facettes) classe les catÃ©gories prioritairement selon lâ€™ordre des catÃ©gories si elles sont ordinales, ou par ordre alphabÃ©tique si les catÃ©gories sont nominales. Inspectons la colonne Tree en inspectant le tableau avec la commande str() - la commande glimpse() du tidyverse donne un sommaire moins complet que str().\n\nstr(Orange)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  35 obs. of  3 variables:\n $ Tree         : Ord.factor w/ 5 levels \"3\"&lt;\"1\"&lt;\"5\"&lt;\"2\"&lt;..: 2 2 2 2 2 2 2 4 4 4 ...\n $ age          : num  118 484 664 1004 1231 ...\n $ circumference: num  30 58 87 115 120 142 145 33 69 111 ...\n - attr(*, \"formula\")=Class 'formula'  language circumference ~ age | Tree\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Time since December 31, 1968\"\n  ..$ y: chr \"Trunk circumference\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(days)\"\n  ..$ y: chr \"(mm)\"\n\n\nEn effet, la colonne Tree est un facteur ordinal dont les niveaux sont dans le mÃªme ordre que celui la lÃ©gende.\n\n4.8.5 Les histogrammes\nNous avons vu les histogrammes dans la brÃ¨ve section sur les fonctions graphiques de base dans R: il sâ€™agit de segmenter lâ€™axe des x en incrÃ©ments, puis de prÃ©senter sur lâ€™axe de y le nombre de donnÃ©es que lâ€™on retrouve dans cet incrÃ©ment. Le marqueur Ã  utiliser est geom_histogram().\nRevenons Ã  nos escargots. Comment prÃ©senteriez-vous la longueur de la coquille selon la variable Type? Selon des couleurs ou des facettes? La couleur, dans le cas des histogrammes, est celle du pourtour des barres. Pour colorer lâ€™intÃ©rieur des barres, lâ€™argument Ã  utiliser est fill.\n\nggplot(data = abalone, mapping = aes(x = LongestShell)) +\n  geom_histogram(mapping = aes(fill = Type), colour = \"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOn nâ€™y voit pas grand chose. Essayons plutÃ´t les facettes.\n\nggplot(data = abalone, mapping = aes(x = LongestShell)) +\n  facet_grid(Type ~ .) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLes facettes permettent maintenant de bien distinguer la distribution des longueurs des juvÃ©niles. Lâ€™argument bins, tout comme lâ€™argument breaks du module graphique de base, permet de spÃ©cifier le nombre dâ€™incrÃ©ments, ce qui peut Ãªtre trÃ¨s utile en exploration de donnÃ©es.\n\nggplot(data = abalone, mapping = aes(x = LongestShell)) +\n  facet_grid(Type ~ .) +\n  geom_histogram(bins = 60, colour = \"white\")\n\n\n\n\nLe nombre dâ€™incrÃ©ments est un paramÃ¨tre quâ€™il ne faut pas sous-estimer. Ã€ preuve, ce tweet de @NicholasStrayer:\n\n\nHistograms are fantastic, but make sure your bin-width/number is chosen well. This is the exact same data, plotted with different bin-widths. Notice that the pattern doesn't necessarily get clearer as bin num increases. #dataviz pic.twitter.com/3MhSFwTVPH\n\nâ€” Nick Strayer (@NicholasStrayer) 7 aoÃ»t 2018\n\n\n4.8.6 Boxplots\nLes boxplots sont une autre maniÃ¨re de visualiser des distributions. Lâ€™astuce est de crÃ©er une boÃ®te qui sâ€™Ã©tend du premier quartile (valeur Ã  laquelle 25% des donnÃ©es ont une valeur infÃ©rieure) au troisiÃ¨me quartile (valeur Ã  laquelle 75% des donnÃ©es ont une valeur infÃ©rieure). Une barre Ã  lâ€™intÃ©rieur de cette boÃ®te est placÃ©e Ã  la mÃ©diane (qui est en fait le second quartile). De part et dâ€™autre de la boÃ®te, on retrouve des lignes spÃ©cifiant lâ€™Ã©tendue hors quartiles. Cette Ã©tendue peut Ãªtre dÃ©terminÃ©e de plusieurs maniÃ¨res, mais dans le cas de ggplot2, il sâ€™agit de 1.5 fois lâ€™Ã©tendue de la boÃ®te (lâ€™Ã©cart interquartile). Au-delÃ  de ces lignes, on retrouve les points reprÃ©sentant les valeurs extrÃªmes. Le marqueur Ã  utiliser est geom_boxplot(). Lâ€™encodage x est la variable catÃ©gorielle et lâ€™encodage y est la variable continue.\n\nggplot(data = abalone, mapping = aes(x = Type, y = LongestShell)) +\n  geom_boxplot()\n\n\n\n\nExercice. On suggÃ¨re parfois de prÃ©senter les mesures sur les boxplots. Utiliser geom_jitter() avec un bruit horizontal.\n\n4.8.7 Les diagrammes en barre\nLes diagrammes en barre reprÃ©sentent une variable continue associÃ©e Ã  une catÃ©gorie. Les barres sont gÃ©nÃ©ralement horizontales et ordonnÃ©es. Nous y reviendrons Ã  la fin de ce chapitre, mais retenez pour lâ€™instant que dans tous les cas, les diagrammes en barre doivent inclure le zÃ©ro pour Ã©viter les mauvaises interprÃ©tations.\nPour les diagrammes en barre, nous allons utiliser les donnÃ©es de lâ€™union internationale pour la conservation de la nature distribuÃ©es par lâ€™OCDE.\n\n# Certaines  colonnes de caractÃ¨res sont considÃ©rÃ©es comme boolÃ©ennes\n# mieux vaut dÃ©finir leur type pour s'assurer que le bon type\n# soit attribuÃ©\nespeces_menacees &lt;- read_csv(\"data/WILD_LIFE_14012020030114795.csv\",\n  col_types = list(\n    \"c\", \"c\", \"c\", \"c\",\n    \"c\", \"c\", \"c\", \"c\",\n    \"d\", \"c\", \"c\", \"c\",\n    \"d\", \"c\", \"c\"\n  )\n)\nhead(especes_menacees)\n\n# A tibble: 6 Ã— 15\n  IUCN       `IUCN Category`       SPEC  Species COU   Country `Unit Code` Unit \n  &lt;chr&gt;      &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;\n1 TOT_KNOWN  Total number of knowâ€¦ MAMMâ€¦ Mammals AUS   Austraâ€¦ NBR         Numbâ€¦\n2 ENDANGERED Number of endangeredâ€¦ MAMMâ€¦ Mammals AUS   Austraâ€¦ NBR         Numbâ€¦\n3 CRITICAL   Number of criticallyâ€¦ MAMMâ€¦ Mammals AUS   Austraâ€¦ NBR         Numbâ€¦\n4 VULNERABLE Number of vulnerableâ€¦ MAMMâ€¦ Mammals AUS   Austraâ€¦ NBR         Numbâ€¦\n5 THREATENED Total number of threâ€¦ MAMMâ€¦ Mammals AUS   Austraâ€¦ NBR         Numbâ€¦\n6 TOT_KNOWN  Total number of knowâ€¦ MAMMâ€¦ Mammals AUT   Austria NBR         Numbâ€¦\n# â„¹ 7 more variables: `PowerCode Code` &lt;dbl&gt;, PowerCode &lt;chr&gt;,\n#   `Reference Period Code` &lt;chr&gt;, `Reference Period` &lt;chr&gt;, Value &lt;dbl&gt;,\n#   `Flag Codes` &lt;chr&gt;, Flags &lt;chr&gt;\n\n\nLâ€™exercice consiste Ã  crÃ©er un diagramme en barres horizontales du nombre de plantes vasculaires menacÃ©es de maniÃ¨re critique pour les 10 pays qui en contiennent le plus. Je vais effectuer quelques opÃ©rations sur ce tableau afin dâ€™en arriver avec un tableau que nous pourrons convenablement mettre en graphique: si vous avez bien suivi le dernier chapitre, ces opÃ©rations devraient vous Ãªtre familiÃ¨res!\nNous allons filtrer le tableau pour obtenir le nombre de plantes vasculaires critiquement menacÃ©es, sÃ©lectionner seulement le pays et le nombre dâ€™espÃ¨ces, les grouper par pays, additionner toutes les espÃ¨ces pour chaque pays et enfin sÃ©lectionner et arranger les 10 premiers en ordre dÃ©croissant. Comme vous le voyez, la crÃ©ation de graphique est liÃ©e de prÃ¨s avec la manipulation des tableaux!\n\nespeces_crit &lt;- especes_menacees |&gt; \n  filter(IUCN == \"CRITICAL\", SPEC == \"VASCULAR_PLANT\") |&gt; \n  dplyr::select(Country, Value) |&gt; \n  group_by(Country) |&gt; \n  summarise(n_critical_species = sum(Value)) |&gt; \n  slice_max(n = 10, order_by = n_critical_species)\nespeces_crit\n\n# A tibble: 10 Ã— 2\n   Country         n_critical_species\n   &lt;chr&gt;                        &lt;dbl&gt;\n 1 United States                 1222\n 2 Japan                          525\n 3 Canada                         315\n 4 Czech Republic                 284\n 5 Spain                          271\n 6 Belgium                        253\n 7 Austria                        172\n 8 Slovak Republic                155\n 9 Australia                      148\n10 Italy                          128\n\n\nLe premier type de diagramme en barre que nous allons couvrir est obtenu par le marqueur geom_col().\n\nggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_col()\n\n\n\n\nCe graphique est perfectible. Les barres sont verticales et non ordonnÃ©es. Souvenons-nous que ggplot2 ordonne par ordre alphabÃ©tique si aucun autre ordre est spÃ©cifiÃ©. Nous pouvons changer lâ€™ordre en changeant lâ€™ordre des niveaux de la variable Country selon le nombre dâ€™espÃ¨ces grÃ¢ce Ã  la fonction fct_reorder.\n\nespeces_crit &lt;- especes_crit %&gt;%\n  mutate(Country = fct_reorder(Country, n_critical_species))\n\nPour faire pivoter le graphique, nous ajoutons coord_flip() Ã  la sÃ©quence.\n\nggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\nUne autre mÃ©thode, geom_bar(), est un raccourci permettant de compter le nombre dâ€™occurrence dâ€™une variable unique. Par exemple, dans le tableau abalone, le nombre de fois que chaque niveau de la variable Type.\n\nggplot(data = abalone, mapping = aes(x = Type)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\nPersonnellement, jâ€™aime bien passer par un diagramme en lignes avec le marqueur geom_segment(). Cela me donne la flexibilitÃ© pour dÃ©finir un largeur de trait et Ã©ventuellement dâ€™ajouter un point au bout pour en faire un diagramme en suÃ§on. Tenez, jâ€™en profite aussi pour y ajouter du texte (dÃ©calÃ© horizontalement) et Ã©tendre les limtes pour mâ€™assurer que les chiffres apparaissent bien.\n\nggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) +\n  geom_point(size = 5, colour = \"black\") +\n  geom_text(aes(label = n_critical_species), hjust = -0.5) + # si ce ne sont pas des valeurs entiÃ¨res, arrondir avec signif()\n  expand_limits(y = c(0, 1300)) +\n  coord_flip() +\n  theme_bw()\n\n\n\n\nLes diagrammes en barre peuvent Ãªtre placÃ©s en relation avec dâ€™autres. Reprenons notre manipulation de donnÃ©es prÃ©cÃ©dente, mais en incluant tous les pays, pour les trois niveaux dâ€™alerte, pour les poissons.\n\nespeces_pays_iucn &lt;- especes_menacees |&gt; \n  filter(IUCN %in% c(\"ENDANGERED\", \"VULNERABLE\", \"CRITICAL\"), SPEC == \"FISH_TOT\") |&gt; \n  dplyr::select(IUCN, Country, Value) |&gt; \n  group_by(Country, IUCN) |&gt; \n  summarise(n_species = sum(Value)) |&gt; \n  group_by(Country) |&gt; \n  mutate(n_tot = sum(n_species)) |&gt; \n  ungroup() |&gt;  # pour pouvoir modifier Country, non modifiable tant qu'elle est une variable de regroupement (voir group_by)\n  mutate(Country = fct_reorder(Country, n_tot))\n\n`summarise()` has grouped output by 'Country'. You can override using the\n`.groups` argument.\n\nhead(especes_pays_iucn)\n\n# A tibble: 6 Ã— 4\n  Country   IUCN       n_species n_tot\n  &lt;fct&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 Australia CRITICAL           8    48\n2 Australia ENDANGERED        16    48\n3 Australia VULNERABLE        24    48\n4 Austria   CRITICAL           6    39\n5 Austria   ENDANGERED        18    39\n6 Austria   VULNERABLE        15    39\n\n\nPour placer les barres les unes Ã  cÃ´tÃ© des autres, nous spÃ©cifions position = \"dodge\".\n\nggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +\n  geom_col(aes(fill = IUCN), position = \"dodge\") +\n  coord_flip()\n\n\n\n\nIl est parfois plus pratique dâ€™utiliser les facettes.\n\nggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +\n  facet_grid(IUCN ~ .) +\n  geom_col() +\n  coord_flip()\n\n\n\n\nPour perfectionner encore ce graphique, on pourrait rÃ©ordonner les facettes individuellement, mais ne nous Ã©garons par trop.\n\n4.8.8 Exporter un graphique\nPlus besoin dâ€™utiliser la fonction png() en mode ggplot2. Utilisons plutÃ´t ggsave().\n\nggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +\n  facet_grid(IUCN ~ .) +\n  geom_col(aes(fill = IUCN)) +\n  coord_flip()\n\n\n\nggsave(\"images/especes_pays_iucn.png\", width = 6, height = 8, dpi = 300)"
  },
  {
    "objectID": "04-visualisation.html#les-graphiques-comme-outil-dexploration-des-donnÃ©es",
    "href": "04-visualisation.html#les-graphiques-comme-outil-dexploration-des-donnÃ©es",
    "title": "4Â  Visualisation",
    "section": "\n4.9 Les graphiques comme outil dâ€™exploration des donnÃ©es",
    "text": "4.9 Les graphiques comme outil dâ€™exploration des donnÃ©es\n\n\n\n\nFigureÂ 4.9: Explorer les donnÃ©es avec ggplot2, dessin de @allison_horst.\n\n\n\nLa plupart des graphiques que vous crÃ©erez ne seront pas destinÃ©s Ã  Ãªtre publiÃ©s, mais serviront dâ€™outil dâ€™exploration des donnÃ©es. Le jeu de donnÃ©es datasaurus, prÃ©sentÃ© en dÃ©but de chapitre, permet de saisir lâ€™importance des outils graphiques pour bien comprendre les donnÃ©es.\n\ndatasaurus &lt;- read_tsv(\"data/DatasaurusDozen.tsv\")\n\nRows: 1846 Columns: 3\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \"\\t\"\nchr (1): dataset\ndbl (2): x, y\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(datasaurus)\n\n# A tibble: 6 Ã— 3\n  dataset     x     y\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 dino     55.4  97.2\n2 dino     51.5  96.0\n3 dino     46.2  94.5\n4 dino     42.8  91.4\n5 dino     40.8  88.3\n6 dino     38.7  84.9\n\n\nProjetons dâ€™abord les coordonnÃ©es x et y sur un graphique.\n\nggplot(data = datasaurus, mapping = aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\nCe graphique pourrait ressembler Ã  une distribution binormale, ou un coup de 12 dans une porte de grange. Mais on aperÃ§oit des donnÃ©es alignÃ©es, parfois de maniÃ¨re rectiligne, parfois en forme dâ€™ellipse. Le tableau datasaurus a une colonne dâ€™information supplÃ©mentaire. Utilisons-la comme catÃ©gorie pour gÃ©nÃ©rer des couleurs diffÃ©rentes.\n\nggplot(data = datasaurus, mapping = aes(x = x, y = y)) +\n  geom_point(mapping = aes(colour = dataset))\n\n\n\n\nCe nâ€™est pas vraiment plus clair. Il y a toutefois des formes qui se dÃ©gagent, comme des ellipses et des lignes. Et si je regarde bien, jâ€™y vois une Ã©toile. La catÃ©gorisation pourrait-elle Ãªtre mieux utilisÃ©e si on segmentait par facettes au lieu des couleurs?\n\nggplot(data = datasaurus, mapping = aes(x = x, y = y)) +\n  facet_wrap(~dataset, nrow = 2) +\n  geom_point(size = 0.5) +\n  coord_equal()\n\n\n\n\nVoilÃ ! Fait intÃ©ressant : ni les statistiques, ni les algorithmes de regroupement ne nous auraient Ã©tÃ© utiles pour diffÃ©rencier les groupes!\n\n4.9.1 Des graphiques interactifs!\nLes graphiques sont traditionnellement des images statiques. Toutefois, les graphiques nâ€™Ã©tant pas dÃ©pendants de supports papiers peuvent Ãªtre utilisÃ©s de maniÃ¨re diffÃ©rente, en ajoutant une couche dâ€™interaction. ConÃ§ue Ã  MontrÃ©al, plotly est un module graphique interactif en soi. Il peut Ãªtre utilisÃ© grÃ¢ce Ã  son outil web, tout comme il peut Ãªtre interfacÃ© avec R, Python, javascript, etc. Mais ce qui retient notre attention ici est son interface avec ggplot2.\nLes graphiques ggplot2 peuvent Ãªtre enregistrÃ©s en tant quâ€™objets. Il peuvent consÃ©quemment Ãªtre manipulÃ©s par des fonctions. La fonction ggplotly permet de rendre votre ggplot interactif.\n\nlibrary(\"plotly\")\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nespeces_crit_bar &lt;- ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) +\n  geom_point(size = 6) +\n  coord_flip()\nggplotly(especes_crit_bar)\n\n\n\n\n\nVous pouvez publier votre graphique plotly en ligne pour le partager ou lâ€™inclure dans une publication web. Il vous faudra crÃ©er un compte plotly, puis gÃ©nÃ©rer une clÃ© dâ€™utilisation dans Settings &gt; API Keys &gt; Generate key. Pour des raisons de sÃ©curitÃ©, la clÃ© du bloc ci-dessous ne fonctionnera pas. Jâ€™ai dÃ©sactivÃ© le bloc de code, mais le rÃ©sultat se trouve en suivant le lien gÃ©nÃ©rÃ© par plotly: https://plot.ly/~essicolo/152/.\nSys.setenv(\"plotly_username\"=\"essicolo\")\nSys.setenv(\"plotly_api_key\"=\"iavd1ycE2iiqOp9YD45I\")\n\nchart_link &lt;- api_create(x = ggplotly(especes_crit_bar), \n                         filename = \"public-graph\",\n                         sharing = \"public\",\n                         fileopt = \"overwrite\")\nchart_link\n\n4.9.2 Des extensions de ggplot2\n\nggplot2 est un module graphique Ã©lÃ©gant et polyvalent. Il a pourtant bien des limitations. Justement, le module est conÃ§u pour Ãªtre implÃ©mentÃ© avec des extensions. Vous en trouverez plusieurs sur exts.ggplot2.tidyverse.org, mais en trouverez de nombreuses autres en cherchant avec le terme ggplot2 sur github.com, probablement la plate-forme (voire un rÃ©seau social) de dÃ©veloppement de logiciels la plus utilisÃ©e dans le monde. En voici quelques unes.\n\n\nggthemr: spÃ©cifier un thÃ¨me graphique une seule fois dans votre session, et tout le reste suit.\n\ncowplot et patchwork permettent de crÃ©er des graphiques prÃªts pour la publication, par exemple en crÃ©ant des grilles de plusieurs ggplots, en les numÃ©rotant, etc.\nSi les thÃ¨mes de base ne vous conviennent pas, vous en trouverez dâ€™autres en installant ggthemes.\n\nggmap et ggspatial sont deux extensions pour crÃ©er des cartes. Un chapitre sur les donnÃ©es spatiales est en dÃ©veloppement.\n\nggtern permet de crÃ©er des diagrammes ternaires, qui sont utiles pour la visualisation de proportions incluant trois composantes, par exemple pour la granulomÃ©trie des sols.\n\nggprism permet de personnaliser les ggplots et leur donner un aspect similaire aux graphiques du logiciel statistique prism\n\n\n4.9.3 Aller plus loin avec ggplot2\n\n\n\nClaus O. Wilke est professeur en biologie intÃ©grative Ã  lâ€™UniversitÃ© du Texas Ã  Austin. Son livre Fundamentals of Data Visualization est un guide thÃ©orique et pratique pour la visualisation de donnÃ©es avec ggplot2.\nLe site data-to-viz.com vous accompagne dans le choix du graphique Ã  crÃ©er selon vos donnÃ©es.\nLe site r-graph-gallery.com offre des recettes pour crÃ©er des graphiques avec ggplot2.\nLe livre R Graphics Cookbook, disponible entiÃ¨rement en ligne, offre aussi des recettes pour rÃ©aliser diffÃ©rents graphiques."
  },
  {
    "objectID": "04-visualisation.html#extra-rÃ¨gles-particuliÃ¨res",
    "href": "04-visualisation.html#extra-rÃ¨gles-particuliÃ¨res",
    "title": "4Â  Visualisation",
    "section": "\n4.10 Extra: RÃ¨gles particuliÃ¨res",
    "text": "4.10 Extra: RÃ¨gles particuliÃ¨res\n\nLes mauvais graphiques peuvent survenir Ã  cause de lâ€™ignorance, bien sÃ»r, mais souvent ils existent pour la mÃªme raison que la boeuferie [bullhist] verbale ou Ã©crite. Parfois, les gens ne se soucient pas de la faÃ§on dont ils prÃ©sentent les donnÃ©es aussi longtemps que Ã§a appuie leurs arguments et, parfois, ils ne se soucient pas que Ã§a porte Ã  confusion tant quâ€™ils ont lâ€™air impressionnant. \\(-\\) Carl Bergstorm et Jevin West, Calling Bullshit Read-Along Week 6: Data Visualization\n\nUne reprÃ©sentation visuelle est un outil tranchant qui peut autant prÃ©senter un Ã©tat vÃ©ritable des donnÃ©es quâ€™une perspective trompeuse. Bien souvent, une ou plusieurs des 5 qualitÃ©s ne sont pas respectÃ©es. Les occasions dâ€™erreur ne manquent pas - jâ€™en ai fait mention dans la section Choisir le bon type de graphique. Maintenant, notons quelques rÃ¨gles particuliÃ¨res.\n\n4.10.1 Ne tronquez pas inutilement lâ€™axe des \\(y\\)\n\nTronquer lâ€™axe vertical peut amener Ã  porter de fausses conclusions.\n\n\n\n\nFigureÂ 4.10: Effets sur la perception dâ€™utiliser diffÃ©rentes rÃ©fÃ©rences. Source: Yau (2015), Real Chart Rules to Follow.\n\n\n\n\n\nEffets sur la perception dâ€™utiliser diffÃ©rentes rÃ©fÃ©rences. Source: Yau (2015), Real Chart Rules to Follow.\n\nLa rÃ¨gle semble simple: les diagrammes en barre (utilisÃ©s pour reprÃ©senter une grandeur) devraient toujours prÃ©senter le 0 et les diagrammes en ligne (utilisÃ©s pour prÃ©senter des tendances) ne requiÃ¨rent pas nÃ©cessairement le zÃ©ro (Bergstrom et West, Calling bullshit: Misleading axes on graphs. Mais le zÃ©ro nâ€™est pas toujours liÃ© Ã  une quantitÃ© particuliÃ¨re : par exemple, la tempÃ©rature ou un log-ratio. De plus, avec un diagramme en ligne, on pourra toujours magnifier des tendances en zoomant sur une variation somme toute mineure. On arrive donc moins Ã  une rÃ¨gle quâ€™une qualitÃ© dâ€™un bon graphique, en particulier la qualitÃ© no 1 de Cairo: offrir une reprÃ©sentation honnÃªte des donnÃ©es. Par exemple, Nathan Yau, auteur du blogue Flowing Data, propose de prÃ©senter des rÃ©sultats de maniÃ¨re relative Ã  la mesure initiale. Câ€™est dâ€™ailleurs ce qui a Ã©tÃ© fait pour gÃ©nÃ©rer le graphique de Michael Mann et al.Â au tout dÃ©but de ce chapitre Ã  la FigureÂ 4.1, oÃ¹ le zÃ©ro correspond Ã  la moyenne des tempÃ©ratures enregistrÃ©es entre 1961 et 1990.\nIl peut Ãªtre tentant de tronquer lâ€™axe des \\(y\\) lorsque lâ€™on dÃ©sire superposer deux axes verticaux. Souvent, lâ€™utilisation de plusieurs axes verticaux amÃ¨ne une perception de causalitÃ© dans des situations de fausses corrÃ©lations. On ne devrait pas utiliser plusieurs axes verticaux.\n\n4.10.2 Utilisez un encrage proportionnel\nCette rÃ¨gle a Ã©tÃ© proposÃ©e par Edward Tufte dans Visual Display of Quantitative Information. Une des raisons pour lesquelles on Ã©vite de tronquer lâ€™axe des \\(y\\) en particulier pour les diagrammes en barre est que lâ€™aire reprÃ©sentant une mesure (la quantitÃ© dâ€™â€œencreâ€ nÃ©cessaire pour la dessiner) devrait Ãªtre proportionnelle Ã  sa magnitude. Les diagrammes en barre sont particuliÃ¨rement sensibles Ã  cette rÃ¨gle, Ã©tant donnÃ©e que la largeur des barres peuvent amplifier lâ€™aire occupÃ©e. Deux solutions dans ce cas: (1) utiliser des barres minces ou (2) prÃ©fÃ©rer des â€œdiagrammes de pointsâ€ (dot charts, Ã  ne pas confondre aux nuages de points).\nLâ€™encrage a beau Ãªtre proportionnel, la difficultÃ© que les humains Ã©prouvent Ã  comparer la dimension des cercles, et a fortiori la dimension de parties de cercle, donne peu dâ€™avantage Ã  utiliser des diagrammes en pointe de tarte, souvent utilisÃ©s pour illustrer des proportions. Nathan Yau suggÃ¨re de les utiliser avec suspicions et dâ€™explorer dâ€™autres options.\n\nPour comparer deux proportions, une avenue intÃ©ressante est le diagramme en pente, suggÃ©rÃ© notamment par Ann K. Emery.\n\nPar extension, le diagramme en pente devient un diagramme en ligne lorsque plusieurs types de proportions sont comparÃ©es, ou lorsque des proportions Ã©voluent selon des donnÃ©es continues.\nDe la mÃªme maniÃ¨re, les diagrammes en bulles ne devraient pas Ãªtre reprÃ©sentatifs de la quantitÃ©, mais permettent plutÃ´t de contextualiser des donnÃ©es. Justement, le graphique tirÃ© des donnÃ©es de Gap minder prÃ©sentÃ© plus haut est une contextualisation: lâ€™aire dâ€™un cercle ne permet pas de saisir la population dâ€™un pays, mais de comparer grossiÃ¨rement la population dâ€™un pays par rapport aux autres.\n\n4.10.3 Publiez vos donnÃ©es\nVous avez peut-Ãªtre dÃ©jÃ  feuilletÃ© un article et voulu avoir accÃ¨s aux donnÃ©es incluses dans un graphique. Il existe des outils pour digitaliser des graphiques pour en extraire les donnÃ©es. Mais le processus est fastidieux, long, souvent peu prÃ©cis. De plus en plus, les chercheurs sont encouragÃ©s Ã  publier leurs donnÃ©es et leurs calculs. Matplotlib et Seaborn sont des outils graphiques classiques qui devraient Ãªtre accompagnÃ©s des donnÃ©es et calculs ayant servi Ã  les gÃ©nÃ©rer. Mais ce nâ€™est pas idÃ©al non plus. En revanche, les outils graphiques modernes comme Plotly et Altair peuvent Ãªtre exportÃ©s en code javascipt, qui contient toutes les informations sur les donnÃ©es et la maniÃ¨re de les reprÃ©senter graphiquement. Ce chapitre a pour objectif de vous familiariser avec les outils de base les plus communÃ©ment utilisÃ©s en calcul scientifique avec R, mais je vous encourage Ã  explorer la nouvelle gÃ©nÃ©ration dâ€™outils graphiques. Nous verrons Ã§a au chapitreÂ 5.\n\n4.10.4 Visitez Junk Charts de temps Ã  autre\nLe statisticien et blogueur Kaiser Fung sâ€™affaire quotidiennement Ã  proposer des amÃ©liorations Ã  de mauvais graphiques sur son blogue Junk Charts."
  },
  {
    "objectID": "05-github.html#un-code-reproductible",
    "href": "05-github.html#un-code-reproductible",
    "title": "5Â  Science ouverte et reproductibilitÃ©",
    "section": "\n5.1 Un code reproductible",
    "text": "5.1 Un code reproductible\n\n\n\n\nFigureÂ 5.2: A Guide to Reproducible Code in Ecology and Evolution, BES 2017\n\n\n\nLa British ecological society offre des lignes guide pour crÃ©er un flux de travail reproductible (BES, 2017). En outre, les principes suivants doivent Ãªtre respectÃ©s (ma traduction, avec ajouts).\n\nCommencez votre analyse Ã  partir dâ€™une copie des donnÃ©es brutes. Les donnÃ©es doivent Ãªtre fournies dans un format ouvert (csv, json, sqlite, etc.). Ã‰vitez de dÃ©marrer une analyse par un chiffrier Ã©lectronique ou un logiciel propriÃ©taire (qui nâ€™est pas open source). En ce sens, dÃ©marrer avec Excel (xls ou xlsx) est Ã  Ã©viter, tout comme le sont les donnÃ©es encodÃ©es pour SPSS ou SAS.\nToute opÃ©ration sur les donnÃ©es, que ce soit du nettoyage, des fusions, des transformations, etc. devrait Ãªtre effectuÃ©e avec du code, non pas manuellement. Sâ€™il sâ€™agit dâ€™une erreur de frappe dans un tableau, on peut dÃ©roger Ã  la rÃ¨gle. Mais sâ€™il sâ€™agit par exemple dâ€™Ã©liminer des outliers, ne supprimez pas des entrÃ©es de vos donnÃ©es brutes. De mÃªme, nâ€™effectuez pas de transformation de vos donnÃ©es brutes Ã  lâ€™extÃ©rieur du code. En somme, vos calculs devraient Ãªtre en mesure dâ€™Ãªtre lancÃ©s dâ€™un seul coup, sans opÃ©rations manuelles intermÃ©diaires.\nSÃ©parez vos opÃ©rations en unitÃ©s logiques thÃ©matiques. Par exemple, vous pourriez sÃ©parer votre code en parties: (i) charger, fusionner et nettoyer les donnÃ©es, (ii) analyser les donnÃ©es, (iii) crÃ©er des fichiers comme des tableaux et des figures.\nÃ‰liminez la duplication du code en crÃ©ant des fonctions personnalisÃ©es. Assurez-vous de commenter vos fonctions en dÃ©tails, expliquez ce qui est attendu comme entrÃ©es et comme sorties, ce quâ€™elles font et pourquoi.\nDocumentez votre code et vos donnÃ©es Ã  mÃªme les feuilles de calcul ou dans un fichier de documentation sÃ©parÃ©.\nTout fichier intermÃ©diaire devrait Ãªtre sÃ©parÃ© de vos donnÃ©es brutes.\n\n\n5.1.1 Structure dâ€™un projet\nUn projet de calcul devrait Ãªtre contenu en un seul dossier. Si vous nâ€™avez que quelques projets, il est assez facile de garder lâ€™info en mÃ©moire. Toutefois, en particulier en milieu dâ€™entreprise, il se pourrait fort bien que vous ayez Ã  mener plusieurs projets de front. Certaines entreprises crÃ©ent des numÃ©ros de projet: vous aurez avantage Ã  nommer vos dossiers avec ces numÃ©ros, incluant une brÃ¨ve description. Pour ma part, jâ€™ordonne mes projets chronologiquement par annÃ©e, avec un descriptif.\nğŸ“ 2019_abeille-canneberge\nNotez que je nâ€™utilise ni espace, ni caractÃ¨re spÃ©cial dans le nom du fichier, pour Ã©viter les erreurs potentielles avec des logiciels capricieux.\nÃ€ lâ€™intÃ©rieur du dossier racine du projet, jâ€™inclus lâ€™information gÃ©nÃ©rale: donnÃ©es source (souvent des fichiers Excel), manuscrit (mÃ©moire, thÃ¨se, article, etc.) documentation particuliÃ¨re (pour les articles, jâ€™utilise Zotero, un gestionnaire de rÃ©fÃ©rence), photos et, Ã©videmment, mon dossier de code (par exemple rstats).\n\nğŸ“‚ 2019_abeille-canneberge\n|-ğŸ“ documentation\n|-ğŸ“ manuscrit\n|-ğŸ“ photos\n|-ğŸ“ rstats\n|-ğŸ“ source\nSi vous rÃ©digez votre manuscrit Ã  mÃªme votre code (en Latex, Lyx, markdown, R markdown ou Quarto que nous verrons plus loin), vous pouvez trÃ¨s bien lâ€™inclure dans votre fichier de calcul.\nÃ€ lâ€™intÃ©rieur du fichier de calcul, vous aurez votre projet RStudio et vos feuilles de calcul sÃ©quencÃ©es. Jâ€™utilise 01-, et non pas 1- pour Ã©viter que le 10- suive le 1- dans le classement en ordre alpha-numÃ©rique au cas oÃ¹ jâ€™aurais plus de 10 feuilles de calcul. Jâ€™inclus un fichier README.md (extension md pour markdown), qui contient les informations gÃ©nÃ©rales de mes calculs. Les donnÃ©es brutes (csv) sont placÃ©es dans un dossier data, mes graphiques sont exportÃ©s dans un dossier images, mes tableaux sont exportÃ©s dans un dossier tables et mes fonctions externes sont exportÃ©es dans un dossier lib.\n\nğŸ“‚ rstats\n|-ğŸ“ data\n|-ğŸ“ images\n|-ğŸ“ lib\n|-ğŸ“ tables\nğŸ“„ bees.Rproj\nğŸ“„ 01_clean-data.R\nğŸ“„ 02_data-mining.R\nğŸ“„ 03_data-analysis.R\nğŸ“„ 04_data-modeling.R\nğŸ“„ README.md\nJe dÃ©cris les noms de fichiers dans la langue de communication utile pour le rendu final du projet, souvent en anglais lors de publications acadÃ©miques. Jâ€™Ã©vite les noms de fichier qui ne sont pas informatifs, par exemple 01.R ou Rplot1.png, ainsi que les majuscules, les caractÃ¨res spÃ©ciaux et les espaces comme dans DeuxiÃ¨me essai.R (le README.md est une exception).\nPour partager un dossier de projet sur R, on nâ€™a quâ€™Ã  le compresser (zip), puis Ã  lâ€™envoyer. Pour que le code fonctionne sur un autre ordinateur, les liens vers les fichiers de donnÃ©es Ã  importer ou les graphiques exportÃ©s doivent Ãªtre relatifs au fichier R ouvert dans votre projet, non pas le chemin complet sur votre ordinateur.\n\n\n\n\nFigureÂ 5.3: Retrouvez votre chemin, dessin de @Allison Horst\n\n\n\nTout comme la BSE, lâ€™organisme sans but lucratif rOpenSci offre un guide sur la reproductibilitÃ© (le rÃ©pertoire est maintenant archivÃ© et nâ€™est plus mis Ã  jour depuis 2022, mais vous y avez tout de mÃªme accÃ¨s en lecture seule).\n\n5.1.2 Les formats markdown\n\nUn code reproductible est un code bien dÃ©crit. La structure de projet prÃ©sentÃ©e prÃ©cÃ©demment propose de segmenter le code en plusieurs fichiers R. Cette maniÃ¨re de procÃ©der est optionnelle. Si le fichier de calcul nâ€™est pas trop encombrant, on pourra nâ€™en utiliser quâ€™un seul, par exemple stats.R. Ã€ lâ€™intÃ©rieur mÃªme des feuilles de calcul R, vous devrez commenter votre code pour en expliquer les Ã©tapes, par exemple:\n#############\n## Titre 1 ##\n#############\n\n# Titre 2\n## Titre 3\ndata &lt;- read_csv(\"data/abeilles.csv\") # commentaire particulier\nRStudio a dÃ©veloppÃ© une approche plus conviviale avec son format R markdown. Le langage markdown permet de formater un texte avec un minimum de dÃ©corations, et R markdown permet dâ€™intÃ©grer du texte et des codes. Le manuel original de notes de cours Ã©tait par ailleurs entiÃ¨rement Ã©crit en R markdown.\n\n\n\n\nFigureÂ 5.4: La magie de R markdown, dessin de @Allison Horst\n\n\n\nDepuis quelques annÃ©es, Quarto a fait son entrÃ©e en scÃ¨ne. Il sâ€™agit de la nouvelle version de R markdown qui se veut plus attrayante, accessible, stable et polyvalente. Quarto a la versatilitÃ© dâ€™utiliser, dans un seul document, des morceaux de code provenant de diffÃ©rents langages, puis de produire des fichiers sous diffÃ©rents formats en une seule Ã©tape. De plus, vous pouvez lâ€™utiliser directement en RStudio ou dans Jupyter. La version actuelle du manuel est montÃ©e Ã  lâ€™aide de Quarto, tout comme les diapositives prÃ©sentÃ©es dans les capsules vidÃ©o du cours.\n\n\n\n\n\n(a) La versatilitÃ© de Quarto\n\n\n\n\n\n\n\n(b) Les Ã©tapes de rendu de Quarto\n\n\n\nFigureÂ 5.5: Dessins de la prÃ©sentation Hello, Quarto par Julia Lowndes et Mine Ã‡etinkaya-Rundel, prÃ©sentÃ©e Ã  la confÃ©rence RStudio de 2022. IllustrÃ©s par @Allison Horst.\n\n\n\n5.1.2.1 Le langage markdown\nLe langage markdown est ce quâ€™on appelle â€œun langage de balises lÃ©gerâ€ qui vous permet dâ€™introduire dans votre texte brut des balises simples pour effectuer le formatage. Un fichier portant lâ€™extension .md ou .markdown est un fichier texte clair (que vous pouvez ouvrir et Ã©diter dans votre Ã©diteur texte prÃ©fÃ©rÃ©), tout comme un fichier .R. RStudio permet notamment dâ€™Ã©diter un fichier .md. Il existe aussi de nombreux Ã©diteurs de texte spÃ©cialisÃ©s en Ã©dition markdown - mon prÃ©fÃ©rÃ© est Typora. Les dÃ©corations (ou balises) principales en markdown sont les suivantes (les citations utilisÃ©es ci-aprÃ¨s sont tirÃ©es du roman Dune, de Frank Herbert).\nItalique. Pour accentuer en italique, balisez le texte avec des astÃ©risques simples *. Par exemple, â€œPourrais-je porter parmi vous le nom de *Paul-Muad'dib*?â€ devient â€œPourrais-je porter parmi vous le nom de Paul-Muadâ€™dib?â€\nGras. Pour accentuer en gras, balisez le texte avec des doubles astÃ©risques **. Par exemple, â€œL'espÃ©rance **ternit** l'observation.â€ devient â€œLâ€™espÃ©rance ternit lâ€™observationâ€.\nLargeur fixe. Pour un texte Ã  largeur fixe (signifiant du code), balisez le texte avec des accents graves. Par exemple, â€œQuel nom donnez-vous Ã  la petite `souris`, celle qui saute ?â€ devient â€œQuel nom donnez-vous Ã  la petite souris, celle qui saute?â€\nListes. Pour effectuer une liste numÃ©rotÃ©e, utilisez le chiffre 1. Par exemple,\n1. Paul\n1. Leto\n1. Alia\ndevient\n\nPaul\nJessica\nAlia\n\nDe mÃªme, pour une liste Ã  puces, changez le 1. par le - ou le *.\nEntÃªtes. Les titres sont prÃ©cÃ©dÃ©s par des #. Un # pour un titre 1, deux ## pour un titre 2, etc. Par exemple,\n\n# Imperium\n## Landsraad\n### Maison des AtrÃ©ides\n### Maison des Harkonnen\n## CHOAM\n# Guilde des navigateurs\nInsÃ©rera les titres appropriÃ©s (que je nâ€™insÃ¨re pas pour ne pas bousiller la structure de ce texte).\nLiens. Pour insÃ©rer des liens, le texte est entre crochets directement suivi du lien entre parenthÃ¨ses. Par exemple, â€œLongue vie aux [combattants](https://youtu.be/Cv87NJ2xX0k?t=59)â€ devient â€œLongue vie aux combattantsâ€.\nÃ‰quations. Les Ã©quations suivent la syntaxe Latex entre deux $$ pour les Ã©quations sur une ligne et entre des doubles $$ $$ pour les Ã©quations sur un paragraphe. Par exemple, $c = \\sqrt{a^2 + b^2}$ devient \\(c = \\sqrt{a^2 + b^2}\\).\nImages. Pour insÃ©rer une image, ![nom de l'image](images/spice-must-flow.png).\nUne liste exhaustive des balises markdown est disponible sous forme dâ€™aide-mÃ©moire. Lâ€™extension de RStudio remedy, installable de la mÃªme faÃ§on quâ€™un module, fera apparaÃ®tre une section REMEDY dans le menu Addins, oÃ¹ vous trouverez toutes sortes dâ€™options de formatage automatique (FigureÂ 5.6). Toutefois, vous verrez plus loin dans la section sur Quarto que dâ€™autres outils encore plus conviviaux existent dÃ©sormais.\n\n\n\n\nFigureÂ 5.6: Menu des extensions de RStudio, avec lâ€™extension remedy\n\n\n\n\n5.1.2.2 R markdown\nDans RStudio, ouvrez un R markdown par File &gt; New file &gt; R Markdown. Si le module rmarkdown nâ€™est pas installÃ©, RStudio vous demandera de lâ€™installer. Une fenÃªtre apparaÃ®tra.\n\n\n\n\nFigureÂ 5.7: Nouveau fichier R markdown\n\n\n\nLes options dâ€™exportation pourront Ãªtre modifiÃ©es par la suite.\nUn fichier dâ€™exemple sera crÃ©Ã©, et vous pourrez le modifier. Les parties de texte sont Ã©crits en markdown, et le code R est enchÃ¢ssÃ© entre les balises ```{r} et ```. Je nommerai ces parties de code des cellules ou des blocs de code. Vous pouvez utiliser le raccourci clavier Ctrl + Alt + I pour insÃ©rer rapidement un bloc de code.\nDes options de code peuvent Ãªtre utilisÃ©es Ã  lâ€™intÃ©rieur des accolades {r}. Par exemple\n\n\n{r, filtre-outliers} donne le nom filtre-outliers au bloc de code, qui permet nommÃ©ment de nommer les images crÃ©Ã©es dans le bloc de code.\n\n{r, eval = FALSE} permet dâ€™activer (TRUE, valeur par dÃ©faut) ou de dÃ©sactiver (FALSE) le calcul de la cellule.\n\n{r, echo = FALSE} permet de nâ€™afficher que la sortie de la cellule de code en nâ€™affichant pas le code, par exemple un graphique ou le sommaire dâ€™une rÃ©gression.\n\n{r, results = FALSE} permet de nâ€™afficher que le code, mais pas la sortie.\n\n{r, warning = FALSE, message = FALSE, error = FALSE} nâ€™affichera pas les avertissements, les messages automatiques et les messages dâ€™erreur.\n\n{r, fig.width = 10, fig.height = 5, fig.align = \"center\"} affichera les graphiques dans les dimensions voulues, alignÃ©e au centre (\"center\"), Ã  gauche (\"left\") ou Ã  droite (\"right\").\n\nNotez que vous pouvez exÃ©cuter rapidement du code sur une ligne avec la formulation `r `, par exemple la moyenne des nombres `\\r a&lt;-round(runif(4, 0, 10)); a` est de `\\r mean(a)`, en enlevant les \\ devant les r (ajoutÃ©es artificiellement pour Ã©viter que le code soit calculÃ©), sera la moyenne des nombres 7, 2, 2, 8 est de 4.75\nUne fois que vous serez satisfait de votre document, cliquer sur Knit  et le fichier de sortie sera gÃ©nÃ©rÃ©. Le guide qui permet de gÃ©nÃ©rer le fichier de sortie est tout en haut du fichier. Nous lâ€™appelons le YAML (acronyme rÃ©cursif de YAML Ainâ€™t Markup Language). Prenez le YAML suivant.\n---\ntitle: \"Dune\"\nauthor: \"Frank Herbert\"\ndate: \"1965-08-01\"\noutput: github_document\n---\nLe titre, lâ€™auteur et la date sont spÃ©cifiÃ©s. Pour indiquer la date courante, on peut simplement la gÃ©nÃ©rer avec R en remplaÃ§ant \"1965-08-01\" par 2024-02-29. La spÃ©cification output indique le type de document Ã  gÃ©nÃ©rer, par exemple html_document pour une page web, pdf_document pour un pdf, ou word_document pour un docx. Dans ce cas-ci, jâ€™indique github_document pour crÃ©er un fichier markdown comprenant notamment des liens relatifs vers les images des graphiques gÃ©nÃ©rÃ©s. Pourquoi un github_document? Câ€™est le sujet de la sectionÂ 5.2. Mais avant cela, je vous rÃ©fÃ¨re Ã  un autre aide-mÃ©moire.\n\n\n\n\nFigureÂ 5.8: Aide-mÃ©moire pour R Markdown, Source: RStudio\n\n\n\n\n5.1.3 Quarto markdown\nAvant dâ€™utiliser Quarto, il vous faudra lâ€™installer sur votre ordinateur. Lorsque ce sera fait, une fois RStudio redÃ©marrÃ©, vous devriez pouvoir crÃ©er un document Quarto par File &gt; New file &gt; Quarto Document. Toutefois, je vous recommande plutÃ´t de crÃ©er directement un nouveau projet, et de choisir le type de projet sâ€™appliquant Ã  votre situation (typiquement un projet Quarto, mais le manuel de cours par exemple est un livre Quarto).\n\n\n\n\nFigureÂ 5.9: CrÃ©ation dâ€™un nouveau projet Quarto.\n\n\n\nEn crÃ©ant votre projet dans un nouveau rÃ©pertoire, vous pouvez choisir le moteur (ici nous utilisons knitr, mais vous pourriez aussi choisir Jupyter si vous prÃ©fÃ©rez) et activer le suivi dâ€™environnement reproductible renv (vous en apprendrez plus sur renv Ã  la sectionÂ 5.3). De plus, vous pouvez directement crÃ©er un rÃ©pertoire git, ce qui facilitera les Ã©tapes dâ€™initialisation de votre rÃ©pertoire si vous souhaitez en faire un. Pour lâ€™instant, je vous suggÃ¨re de garder ces cases vides si vous souhaitez seulement utiliser Quarto.\n\n\n\n\nFigureÂ 5.10: Options de crÃ©ation dâ€™un nouveau projet Quarto.\n\n\n\nIl existe un guide trÃ¨s dÃ©taillÃ© sur lâ€™utilisation de Quarto sur leur site internet. Je vous suggÃ¨re pour bien dÃ©marrer la lecture suivante.\n\n5.1.3.1 Balises des blocs de code\nEn gÃ©nÃ©ral, vous pouvez utiliser les mÃªmes balises dans Quarto quâ€™avec R markdown. Les mÃªmes options peuvent Ãªtre utilisÃ©es Ã  lâ€™intÃ©rieur des accolades {r} de la mÃªme maniÃ¨re (par exemple : {r, important-figure, fig.width = 10, fig.height = 5, fig.align = \"center\"}), ou alors de faÃ§on plus claire avec le style YAML de la faÃ§on suivante :\n```{r}\n#| label: important-figure\n#| fig-width: 10\n#| fig-height: 5\n#| fig-align: center\n\nknitr::include_graphics(\"images/important-figure.png\")\n```\n\n5.1.3.2 Rendu de documents\nPour crÃ©er le rendu de votre document, au lieu dâ€™utiliser le bouton Knit, vous pouvez utiliser Render. Si vos options permettent la sortie de plusieurs formats de fichiers, vous verrez une liste dÃ©roulante vous permettant de choisir le format Ã  produire.\n\n\n\n\nFigureÂ 5.11: CrÃ©er un rendu de votre document Quarto (Tutorial: Hello Quarto).\n\n\n\nCe bouton est bien pratique lorsque vous avez un seul document simple .qmd dans votre dossier ou alors que vous voulez rapidement visualiser une sortie, mais il est prÃ©fÃ©rable de prendre lâ€™habitude dâ€™utiliser la commande quarto_render pour faire le rendu de tous les fichiers dans le dossier. Certaines options de cette commande vous permettent par exemple de faire le rendu dâ€™un seul format de fichier.\n```{r}\ninstall.packages(\"quarto\")\nquarto::quarto_render(\"index.qmd\")\n```\n\n5.1.3.3 Options YAML\nComme pour R markdown, nous utilisons le guide YAML pour spÃ©cifier les options de rendements de documents. On peut lâ€™utiliser directement en haut du fichier .qmd en en-tÃªte, mais en gÃ©nÃ©ral je recommande plutÃ´t dâ€™utiliser le fichier _quarto.yml qui devrait avoir Ã©tÃ© crÃ©Ã© dans votre dossier de projet lors de la crÃ©ation du projet. Vous pouvez y spÃ©cifier par exemple le dossier de sortie des fichiers crÃ©Ã©s, donner des paramÃ¨tres pour chaque format de fichiers de sortie, etc. Les informations dans ce fichier sont les consignes qui seront suivies lors de lâ€™interprÃ©tation et de la crÃ©ation de vos documents. Voici un exemple du contenu dâ€™un fichier YAML inspirÃ© du manuel des notes de cours.\nproject:\n  type: book\n  output-dir: docs\n  \nbook:\n  title: \"Titre livre\"\n  author:\n  - name: \"Nom auteur 1\"\n  - name: \"Nom auteur 2\"\n  date: today\n  date-format: iso\n  chapters:\n    - href: index.qmd\n      text: PrÃ©face\n    - 01-chap1.qmd\n    - 02-chap2.qmd\n  cover-image: images/cover.png\n  \nformat:\n  html:\n    theme: flatly\n    code-link: true\n    css: styles.css\n    toc: true\n  pdf:\n    documentclass: scrreprt\n    toc: true\n\n\nAide-mÃ©moire de Quarto\n\n\n5.1.3.4 Ã‰diteur visuel\nEnfin, un des grands avantages de Quarto en termes dâ€™accessibilitÃ© est lâ€™Ã©diteur visuel. Personnellement, je prÃ©fÃ¨re gÃ©nÃ©ralement travailler avec lâ€™Ã©diteur de code Source, qui me permet de mieux voir ce quâ€™il se passe exactement et dâ€™avoir un meilleur contrÃ´le sur le formatage. Toutefois, il mâ€™arrive de passer Ã  lâ€™Ã©diteur visuel lorsque jâ€™ignore comment introduire certains Ã©lÃ©ments.\n\n\n\n\nFigureÂ 5.12: Ã‰diteur visuel vs source (Tutorial: Hello Quarto).\n\n\n\nEn mode dâ€™Ã©dition visuelle, il est trÃ¨s facile dâ€™insÃ©rer un nouvel Ã©lÃ©ment : il vous suffit de taper / : une liste dÃ©roulante devrait alors apparaÃ®tre. Vous pouvez alors choisir un Ã©lÃ©ment dans la liste, ou dâ€™abord spÃ©cifier votre recherche (comme Ã  la FigureÂ 5.13).\n\n\n\n\nFigureÂ 5.13: InsÃ©rer une Ã©quation avec lâ€™Ã©diteur visuel. (Tutorial: Authoring).\n\n\n\nPour finir, il existe des modules vous permettant de crÃ©er directement un document Ã  partir de formats de rÃ©fÃ©rence. Par exemple, quarto-journals vous permet de crÃ©er rapidement des fichiers au format de votre revue prÃ©fÃ©rÃ©e. Puisque Quarto est plutÃ´t rÃ©cent, ces outils sont toujours en dÃ©veloppement et peuvent parfois Ãªtre imparfaits (ou inexistants pour votre revue), mais ils seront plus performants avec le temps et des revues devraient Ãªtre ajoutÃ©es."
  },
  {
    "objectID": "05-github.html#sec-intro-git",
    "href": "05-github.html#sec-intro-git",
    "title": "5Â  Science ouverte et reproductibilitÃ©",
    "section": "\n5.2 Introduction Ã  GitHub",
    "text": "5.2 Introduction Ã  GitHub\nLe systÃ¨me de suivi de version git (open source) a Ã©tÃ© crÃ©Ã© par Linus Torvalds, aussi connu pour avoir crÃ©Ã© Linux. git prend une photo de votre rÃ©pertoire de projet Ã  chaque fois que vous commettez un changement. Vous pourrez revenir sans problÃ¨me sur dâ€™anciennes versions si quelque chose tourne mal, et vous pourrez publier le rÃ©sultat final sur un service dâ€™hÃ©bergement utilisant git.\nIl existe plusieurs services pour rendre git utilisable en ligne, mais GitHub est dÃ©finitivement le plus utilisÃ© dâ€™entre tous. La plateforme GitHub est presque devenue un rÃ©seau social de dÃ©veloppement. GitHub, maintenant la propriÃ©tÃ© de Microsoft, nâ€™est en soi pas open source. Si comme moi vous avez un penchant pour lâ€™open source, je vous redirige vers la plateforme GitLab, qui fonctionne Ã  peu prÃ¨s de la mÃªme maniÃ¨re que GitHub, mais dans sa version gratuite GitLab vous octroie autant de rÃ©pertoires privÃ©s que vous dÃ©sirez. Seul hic, alors que la plateforme GitHub sera fort probablement toujours vivante dans plusieurs annÃ©es, on en est moins sÃ»r pour GitLab. Câ€™est pourquoi, en rÃ¨gle gÃ©nÃ©rale, jâ€™utilise GitHub Ã  des fins professionnelles mais GitLab Ã  des fins personnelles.\nPour suivre cette partie du cours, je vous invite Ã  crÃ©er un compte sur GitHub ou GitLab, Ã  votre choix. CrÃ©ez un nouveau dÃ©pÃ´t (New repository).\n\n\n\n\nFigureÂ 5.14: Nouveau dÃ©pÃ´t avec GitHub\n\n\n\n\n\n\n\nFigureÂ 5.15: Nouveau dÃ©pÃ´t avec GitLab\n\n\n\nPour utiliser git, vous pourrez toujours travailler en ligne de commande (câ€™est ce que je prÃ©fÃ¨re personnellement mais je ne le conseille pas nÃ©cessairement; vous trouverez toute la documentation nÃ©cessaire sur le site de GitHub). Il est aussi possible de travailler avec les outils intÃ©grÃ©s dans RStudio, mais je vous suggÃ¨re dâ€™utiliser GitHub Desktop (qui fonctionne aussi sur GitLab) - Ã©videmment, dâ€™autres logiciels similaires existent. Github Desktop vous permettra dâ€™abord de cloner un rÃ©pertoire en ligne. Le clonage vous permet de crÃ©er une copie locale (sur votre ordinateur) du rÃ©pertoire.\n\n\n\n\nFigureÂ 5.16: Cloner dÃ©pÃ´t avec GitHub\n\n\n\n\n\n\n\nFigureÂ 5.17: Cloner dÃ©pÃ´t avec GitLab\n\n\n\nUne fois que le dÃ©pÃ´t est clonÃ©, il est sur votre ordinateur. Lorsque vous effectuez un changement, vous devez commettre (commit), puis envoyer (push) vos changements vers le dÃ©pÃ´t en ligne. Pour que votre document markdown soit lisible par GitHub et GitLab, il doit Ãªtre exportÃ© au format html. Un fichier .md sera crÃ©Ã©, et inclura les dÃ©tails de votre feuille de calculs, images y compris!\nDe plus, puisque les fichiers HTML gÃ©nÃ©rÃ©s par Quarto remplacent certains paramÃ¨tres des fichiers HTML classiques, vous devez ajouter dans le dossier racine de votre projet un fichier vide .nojekyll Ã  partir de votre terminal. Ce fichier indiquera GitHub dâ€™ignorer les procÃ©dures Jekyll quâ€™il lance normalement sur les fichiers HTML (voir site web de Quarto pour plus de dÃ©tails).\n\n\n\n\n\n\n\nMac/Linux\n\n\nTerminal\n\ntouch .nojekyll\n\n\n\nWindows\n\n\nTerminal\n\ncopy NUL .nojekyll\n\n\n\n\n\n\n\n\nFigureÂ 5.18: Commettre et dÃ©ployer un dÃ©pÃ´t avec GitHub\n\n\n\nLâ€™interface de GitHub Desktop vous permet de revenir en arriÃ¨re en Ã©liminant des commits prÃ©cÃ©dents.\n\n\n\n\nFigureÂ 5.19: Revenir en arriÃ¨re avec GitHub desktop\n\n\n\nVous pourrez ajouter des collaborateurs Ã  votre dÃ©pÃ´t, pour que plusieurs personnes travaillent de front sur un mÃªme dÃ©pÃ´t. Il est aussi possible de crÃ©er une branche dâ€™un dÃ©pÃ´t, fusionner la branche de dÃ©veloppement avec la branche principale, commenter les codes, suggÃ©rer des changements, etc., mais cela sort du cadre dâ€™un cours sur la reproductibilitÃ©.\nSi vous souhaitez crÃ©er un site internet comme celui des notes de cours Ã  partir de votre rÃ©pertoire GitHub, vous devez activer votre page avec GitHub Pages, crÃ©er un lien, choisir la branche et le dossier contenant les fichiers HTML et publier.\n\n\n\n\nFigureÂ 5.20: Activer la page avec GitHub Pages\n\n\n\nEnfin, pour renvoyer un article vers votre matÃ©riel supplÃ©mentaire, insÃ©rez le lien dans la section mÃ©thodologie. Il peut sâ€™agir du lien complet, ou bien dâ€™un lien raccourci (avec par exemple tinyurl ou bitly). Par exemple,\n\nThe data and the R code used to compute the results are both available as supplementary material at https://git.io/fhHEj.\n\nNotez que RStudio offre une interface pour utiliser git via un onglet afichÃ© en haut Ã  droite dans lâ€™affichage par dÃ©faut. Ne lâ€™ayant jamais utilisÃ©, je ne me sens pas Ã  lâ€™aise dâ€™en suggÃ©rer lâ€™utilisation, mais libre Ã  vous dâ€™explorer cet outil et de vous lâ€™approprier!\n\n\n\n\nFigureÂ 5.21: Lâ€™outil Git de RStudio"
  },
  {
    "objectID": "05-github.html#sec-git-renv",
    "href": "05-github.html#sec-git-renv",
    "title": "5Â  Science ouverte et reproductibilitÃ©",
    "section": "\n5.3 Introduction Ã  renv\n",
    "text": "5.3 Introduction Ã  renv\n\nAlors que les modules sont continuellement mis Ã  jour, on doit sâ€™assurer que lâ€™on sache exactement quelle version a Ã©tÃ© utilisÃ©e si lâ€™on dÃ©sire Ãªtre strict sur la reproductibilitÃ©. Lorsque je rÃ©vise un article, je demande Ã  ce que le nom des modules utilisÃ©s et leur numÃ©ro de version soient explicitement citÃ©s et rÃ©fÃ©rencÃ©s. Par exemple, dans un article sur lâ€™analyse de compositions foliaires de laitues inoculÃ©es par une bactÃ©rie, jâ€™Ã©crivais:\n\nComputations were performed in the R statistical language version 3.4.1 (R Development Core Team, 2017). The main packages used in the data analysis workflow were the vegan package version 2.4-3 (Oksanen et al., 2017) for ordination, the compositions package version 1.40-1 (van den Boogaart and Tolosana-Delgado, 2013) for ilr transformations, the nlme version 3.1-131 (Pinheiro et al., 2017) package to compute the random experimental effect, the mvoutlier package version 2.0.8 (Filzmoser and Gschwandtner, 2017) for multivariate outlier detection, and the ggplot2 package version 2.2.1 (Wickham and Chang, 2017) for data visualization. The data and computations are publicly available at https://github.com/essicolo/Nicolas-et-al_Infected-lettuce-ionomics. Nicolas et al., 2019\n\nDe cette maniÃ¨re, une personne (que ce soit vos collÃ¨gues, quiconque voudra auditer ou Ã©valuer votre code ou vous-mÃªme dans le futur) pourra reproduire le code publiÃ© sur GitHub en installant les versions de R et des modules citÃ©s. Mais cela est fastidieux. Câ€™est pourquoi lâ€™Ã©quipe de RStudio (oui, encore ceux-lÃ ) ont dÃ©veloppÃ© le module renv, qui permet dâ€™installer les modules Ã  mÃªme votre dossier de projet (le dossier contenant le fichier .Rproj).\nPour lâ€™utiliser Ã  tout moment en cours de projet, il suffit de lancer la commande renv::init(). Cette commande configure lâ€™infrastructure du projet, installe les libraries utilisÃ©es dans le dossier renv Ã  lâ€™intÃ©rieur du dossier de projet et bloque leurs versions actuelles dans le fichier renv.lock, crÃ©e un fichier .Rprofile qui traquera les installations futures de librairies supplÃ©mentaires, puis redÃ©marre la session R. Ouf!\n\n\n\n\nFigureÂ 5.22: Lâ€™outil renv de RStudio\n\n\n\nDans le dossier renv, le .gitignore contient tous les documents et les types de documents qui sont ignorÃ©s par git. Lâ€™option par dÃ©faut est dâ€™ignorer le dossier library, qui contient les modules installÃ©s, mais de garder les fichiers activate.R et settings.json, qui contiennent le script dâ€™installation des modules non installÃ©s (qui devront Ãªtre installÃ©s par les autres personnes utilisant votre projet) ainsi que les paramÃ¨tres du projet. Mieux vaut garder les options par dÃ©faut. Initialiser renv revient Ã  scanner vos documents de projet pour trouver les modules utilisÃ©s et crÃ©er un paquet contenant tout cela Ã  mÃªme votre projet, dans un dossier renv.\n\nğŸ“‚ rstats\n|-ğŸ“ data\n|-ğŸ“ docs\n|-ğŸ“ images\n|-ğŸ“ lib\n|-ğŸ“ renv\n|-ğŸ“ tables\nğŸ“„ _quarto.yml\nğŸ“„ sentier-d-or.Rproj\nğŸ“„ stats.qmd\nğŸ“„ README.md\nğŸ“„ renv.lock\nğŸ“„ .Rprofile\nğŸ“„ .gitignore\nCe dossier contiendra tout ce quâ€™il faut pour utiliser les modules du projet dâ€™une personne que lâ€™on nommera Leto. Lorsquâ€™une autre personne, appellons-la Ghanima, utilisera le projet de Leto, RStudio vÃ©rifiera si le module renv est bien installÃ©, et lâ€™installera sâ€™il ne lâ€™est pas. Pour utiliser les modules du projet et non pas les modules de son ordinateur, Ghanima lancera la fonction renv::restore(), qui installera les versions dÃ©crites dans le lockfile (renv.lock). Si Leto dÃ©cide de mettre Ã  jour ses modules en cours de projet, il lancera la fonction renv::install() pour installer de nouveaux modules, renv::update() pour faire la mise Ã  jour de tous les modules utilisÃ©s, puis renv::snapshot() pour que ces nouveaux modules soit intÃ©grÃ©s Ã  son projet dans le lockfile. Lorsque Leto commettra (commit) ses changements dans git et les publiera (push) sur GitHub, puis lorsque Ghanima mettra Ã  jour (fetch) son dÃ©pÃ´t local git liÃ© au dÃ©pÃ´t GitHub, elle devra Ã  nouveau lancer renv::restore() pour que les modules soient bel et bien ceux utilisÃ©s par Leto.\nNotez quâ€™avec renv, vous nâ€™installez pas rÃ©ellement les modules complets Ã  chaque fois : renv garde en mÃ©moire cache lâ€™installation des modules sur votre ordinateur, si bien que lorsque vous installez plusieurs la mÃªme version dâ€™un mÃªme module, vous ne compilez pas Ã  chaque fois et vous nâ€™utilisez pas plus dâ€™espace sur votre disque dur quâ€™il nâ€™en faut."
  },
  {
    "objectID": "05-github.html#pour-terminer-le-reprex",
    "href": "05-github.html#pour-terminer-le-reprex",
    "title": "5Â  Science ouverte et reproductibilitÃ©",
    "section": "\n5.4 Pour terminer, le reprex\n",
    "text": "5.4 Pour terminer, le reprex\n\nLorsque jâ€™ai dÃ©couvert un bogue dans le module weathercan, jâ€™ai ouvert une issue sur GitHub en indiquant le message dâ€™erreur obtenu, en espÃ©rant que lâ€™origine du bogue puisse Ãªtre facilement dÃ©duit. Un dÃ©veloppeur de weathercan mâ€™a demandÃ© un reprex. Jâ€™ai Ã©tÃ© dÃ©Ã§u lorsque jâ€™ai compris que le reprex nâ€™Ã©tait pas une espÃ¨ce de dinosaure, mais plutÃ´t un exemple reproductible (reproducible example).\n\nğŸ“— Reprex: Un exemple reproductible.\n\n\nJâ€™ai essayÃ© dâ€™isoler le problÃ¨me pour reproduire lâ€™erreur avec le minimum de code possible. Ã€ partir dâ€™un code de plus de 7000 lignes (les prÃ©sentes notes de cours), jâ€™en suis arrivÃ© Ã  ceci:\n\nstations &lt;- data.frame(A = 1)\n\nlibrary(\"weathercan\")\nmont_bellevue &lt;- weather_dl(\n  station_ids = c(5397, 48371),\n  start = \"2019-02-01\",\n  end = \"2019-02-07\",\n  interval = \"hour\",\n  verbose = TRUE\n)\n\n, qui me retournait lâ€™erreur\nGetting station: 5397\nFormatting station data: 5397\nError in strptime(xx, f, tz = tz) : valeur 'tz' incorrecte\nLe bogue: la fonction weather_dl() utilisait Ã  lâ€™interne un objet nommÃ© stations, qui entrait en conflit avec un objet stations sâ€™il Ã©tait dÃ©fini hors de la fonction.\nSynthÃ©tiser une question nâ€™est pas facile (crÃ©er cet exemple reprductible mâ€™a pris prÃ¨s de 2 heures). Mais rÃ©pondre Ã  une question non synthÃ©tisÃ©e, câ€™est encore plus difficile. Câ€™est pourquoi on (moi y compris) vous demandera systÃ©matiquement un reprex lorsque vous poserez une question liÃ©e Ã  une erreur systÃ©matique, le plus souvent en programmation.\n\nUn exemple reproductible permet Ã  quelquâ€™un de recrÃ©er lâ€™erreur que vous avez obtenue simplement en copiant-collant votre code. - Hadley Wickham\n\nSelon Hadley Wickham (gourou de R), un reprex devrait comprendre quatre Ã©lÃ©ments (je joue Ã  lâ€™hÃ©rÃ©tique en me permettant dâ€™adapter le document du gourou):\n\nLes modules devraient Ãªtre chargÃ©s en dÃ©but de code.\nPuis vous chargez des donnÃ©es, qui peuvent Ãªtre des donnÃ©es dâ€™exemple ou des donnÃ©es incluses Ã  mÃªme le code R (comme des donnÃ©es gÃ©nÃ©rÃ©es au hasard).\nAssurez-vous que votre code est un exemple minimal (retirer le superflu) et quâ€™il soit facilement lisible.\nIncluez la sortie de la fonction sessionInfo(), qui indique la plateforme matÃ©rielle et logicielle sur laquelle vous avez gÃ©nÃ©rÃ© lâ€™erreur. Ceci est important en particulier sâ€™il sâ€™agit dâ€™un bogue.\n\nLorsque vous pensez avoir gÃ©nÃ©rÃ© votre reprex, redÃ©marrez R (Session &gt; Restart R dans RStudio), puis lancez votre code pour vous assurer que lâ€™erreur puisse Ãªtre gÃ©nÃ©rÃ©e dans un nouvel environnement tout propre.\nLa librarie reprex de tidyverse vous aide Ã  gÃ©nÃ©rer et Ã  tester des exemples reproductibles en vous fournissant un bloc de code au format markdown que vous pouvez directement copier-coller sur GitHub, StackOverflow ou Discourse (les lieux frÃ©quents oÃ¹ vous poserez vos questions)."
  },
  {
    "objectID": "06-python.html#quest-ce-que-python",
    "href": "06-python.html#quest-ce-que-python",
    "title": "6Â  Introduction Ã  Python",
    "section": "6.1 Quâ€™est-ce que Python ?",
    "text": "6.1 Quâ€™est-ce que Python ?\nPython est un langage de programmation de haut niveau (comme R). Ce langage est apparu en fÃ©vrier 1991 et a Ã©tÃ© crÃ©Ã© par Guido van Rossum. Un des objectifs principaux de Van Rossum Ã©tait de crÃ©er un langage libre, simple et intuitif, mais puissant comme dâ€™autres langages dÃ©jÃ  existants. Python a Ã©tÃ© amplement adoptÃ© partout dans le monde et est devenu un des langages de programmation les plus populaires dâ€™aprÃ¨s diffÃ©rents rangs comme les indices TIOBE et PYPL et les tendances des questions dans Stack Overflow.\nDans les derniÃ¨res annÃ©es Python est devenu lâ€™un des outils les plus utilisÃ©s pour le calcul scientifique et pour lâ€™analyse de donnÃ©es, mÃªme si ce langage nâ€™Ã©tait pas conÃ§u spÃ©cifiquement pour ces tÃ¢ches. Lâ€™utilisation de Python dans la science de donnÃ©es a Ã©tÃ© poussÃ© par le dÃ©veloppement de diffÃ©rents modules qui permettent la manipulation et lâ€™analyse de donnÃ©es, certains des modules les plus populaires pour lâ€™analyse de donnÃ©es en Python Ã©tant :\n\nnumpy : Ce module permet de manipuler et de stocker de faÃ§on efficiente les donnÃ©es dans des objets connus comme tableaux (en anglais, array).\npandas : Permet de travailler avec des donnÃ©es tabulaires avec des Ã©tiquettes de file et de colonne, lâ€™objet primaire de ce module est le DataFrame.\nmatplotlib : Câ€™est le module le plus utilisÃ© pour la visualisation de donnÃ©es sur Python, il peut Ãªtre considÃ©rÃ© comme le module de base pour la visualisation en Python.\nSciPy : Ce module a diffÃ©rentes fonctions pour la computation scientifique comme le calcul numÃ©rique, le traitement de signaux et dâ€™images, et certains statistiques.\nscikit-learn : Câ€™est une des modules les plus utilisÃ©s pour lâ€™apprentissage automatique. scikit-learn a des innombrables algorithmes dâ€™apprentissage supervisÃ© et non-supervisÃ© utilisÃ©s pour la classification ou la rÃ©gression."
  },
  {
    "objectID": "06-python.html#installation",
    "href": "06-python.html#installation",
    "title": "6Â  Introduction Ã  Python",
    "section": "6.2 Installation",
    "text": "6.2 Installation\nIl y a diffÃ©rentes faÃ§ons dâ€™installer Python dans un ordinateur. Dans le cas de Mac et Linux Python vient par dÃ©faut avec ces systÃ¨mes dâ€™exploitation. Dans le cas de windows il faut le tÃ©lÃ©charger et lâ€™installer. Une faÃ§on dâ€™installer Python sur windows est de tÃ©lÃ©charger le fichier dâ€™installation directement du site web de Python, une fois le fichier tÃ©lÃ©chargÃ©, lâ€™exÃ©cuter et suivre les instructions pour lâ€™installation.\nUne autre alternative pour tÃ©lÃ©charger Python est Ã  partir de Anaconda. Anaconda câ€™est une distribution de Python, dont lâ€™objectif est de simplifier la gestion et le dÃ©ploiement des modules. Le gestionnaire de modules de Anaconda sâ€™appelle conda. Il y a deux options pour installer Anaconda sur un ordinateur :\n\nTÃ©lÃ©charger la version complÃ¨te qui vient avec Python, conda et 1 500 modules pre-installÃ©s.\nTÃ©lÃ©charger une version minimale appellÃ© Miniconda qui ne vient quâ€™avec Python et conda.\n\nLes deux diffÃ©rences principales entre Anaconda et Miniconda sont (1) lâ€™espace rÃ©quis pour lâ€™installation qui est de 3 GO et 400 MO, respectivement, et (2) le temps dâ€™installation puisque Ã§a prends moins de temps Ã  installer Miniconda que Anaconda. Lâ€™utilisateur peut choisir Anaconda si nâ€™a pas dâ€™expÃ©rience et sâ€™il ne veut pas se prÃ©occuper Ã  installer des modules. Lâ€™installation de Miniconda est rÃ©commandÃ© pour des utilisateurs qui sont plus experimentÃ©s et qui savent dÃ©jÃ  quels modules ils vont utiliser. Enfin, si lâ€™ordinateur nâ€™a pas beaucoup dâ€™espace, il est recommandÃ© dâ€™installer Miniconda. Dans ce manuel, les exemples dâ€™Ã©xecution de Python et lâ€™installation des modules se fera Ã  partir dâ€™un terminal de commande nommÃ© Â« Anaconda prompt Â»."
  },
  {
    "objectID": "06-python.html#chapitre-en-construction",
    "href": "06-python.html#chapitre-en-construction",
    "title": "6Â  Introduction Ã  Python",
    "section": "6.3 Chapitre en construction",
    "text": "6.3 Chapitre en construction"
  },
  {
    "objectID": "06-python.html#anaconda-prompt",
    "href": "06-python.html#anaconda-prompt",
    "title": "6Â  Introduction Ã  Python",
    "section": "6.4 Anaconda prompt",
    "text": "6.4 Anaconda prompt\n\nconda bash\nâ€œBonjour mondeâ€\nRstudio et Python\nVS Code\nOperations Python\nTypes dâ€™objets\nBoucles\nFonctions\nLes gestionnaires de modules\nNumpy, Pandas, Visualisation\nFin"
  },
  {
    "objectID": "07a-biostats.html#populations-et-Ã©chantillons",
    "href": "07a-biostats.html#populations-et-Ã©chantillons",
    "title": "7Â  Biostatistiques",
    "section": "\n7.1 Populations et Ã©chantillons",
    "text": "7.1 Populations et Ã©chantillons\nLe principe dâ€™infÃ©rence consiste Ã  gÃ©nÃ©raliser des conclusions Ã  lâ€™Ã©chelle dâ€™une population Ã  partir dâ€™Ã©chantillons issus de cette population. Alors quâ€™une population contient tous les Ã©lÃ©ments Ã©tudiÃ©s, un Ã©chantillon dâ€™une population est une observation unique. Une expÃ©rience bien conÃ§ue fera en sorte que les Ã©chantillons soient reprÃ©sentatifs de la population qui, la plupart du temps, ne peut Ãªtre observÃ©e entiÃ¨rement pour des raisons pratiques.\nLes principes dâ€™expÃ©rimentation servant de base Ã  la conception dâ€™une bonne mÃ©thodologie sont prÃ©sentÃ©s dans le cours Dispositifs expÃ©rimentaux (BVG-7002). Ã‰galement, je recommande le livre Principes dâ€™expÃ©rimentation: planification des expÃ©riences et analyse de leurs rÃ©sultats de Pierre Dagnelie (2012), disponible en ligne en format PDF. Un bon aperÃ§u des dispositifs expÃ©rimentaux est aussi prÃ©sentÃ© dans Introductory Statistics with R, de Peter Dalgaard (2008), que vous pouvez tÃ©lÃ©charger du site de la bibliothÃ¨que de lâ€™UniversitÃ© Laval vous avez un identifiant autorisÃ©.\nUne population est Ã©chantillonnÃ©e pour induire des paramÃ¨tres: un rendement typique dans des conditions mÃ©tÃ©orologiques, Ã©daphiques et managÃ©riales donnÃ©es, la masse typique des faucons pÃ¨lerins, mÃ¢les et femelles, le microbiome typique dâ€™un sol agricole ou forestier, etc. Une statistique est une estimation dâ€™un paramÃ¨tre calculÃ©e Ã  partir des donnÃ©es, par exemple une moyenne et un Ã©cart-type, ou une ordonnÃ©e Ã  lâ€™origine (intercept) et une pente.\nPar exemple, la moyenne (\\(\\mu\\)) et lâ€™Ã©cart-type (\\(\\sigma\\)) dâ€™une population sont estimÃ©s par les moyennes (\\(\\bar{x}\\)) et Ã©carts-types (\\(s\\)) calculÃ©s sur les donnÃ©es issues de lâ€™Ã©chantillonnage.\nChaque paramÃ¨tre est liÃ©e Ã  une perspective que lâ€™on dÃ©sire connaÃ®tre chez une population. Ces angles dâ€™observations sont les variables."
  },
  {
    "objectID": "07a-biostats.html#les-variables",
    "href": "07a-biostats.html#les-variables",
    "title": "7Â  Biostatistiques",
    "section": "\n7.2 Les variables",
    "text": "7.2 Les variables\nNous avons abordÃ© au chapitreÂ 3 la notion de variable par lâ€™intermÃ©diaire dâ€™une donnÃ©e. Une variable est lâ€™observation dâ€™une caractÃ©ristique dÃ©crivant un Ã©chantillon. Si la charactÃ©ristique varie dâ€™un Ã©chantillon Ã  un autre sans que vous en expliquiez la raison (i.e.Â si identifier la source de la variabilitÃ© ne fait pas partie de votre expÃ©rience), on parlera de variable alÃ©atoire. MÃªme le hasard est rÃ©gi par certaines lois: ce qui est alÃ©atoire dans une variable peut Ãªtre dÃ©crit par des lois de probabilitÃ©, que nous verrons plus bas.\nMais restons aux variables pour lâ€™instant. Par convention, on peut attribuer aux variables un symbole mathÃ©matique. Par exemple, on peut donner Ã  la masse volumique dâ€™un sol (qui est le rÃ©sultat dâ€™une mÃ©thodologie prÃ©cise) le symbole \\(\\rho\\). Lorsque lâ€™on attribue une valeur Ã  \\(\\rho\\), on parle dâ€™une donnÃ©e. Chaque donnÃ©e dâ€™une observation a un indice qui lui est propre, que lâ€™on dÃ©signe souvent par \\(i\\), que lâ€™on place en indice \\(\\rho_i\\). Pour la premiÃ¨re donnÃ©e, on a \\(i=1\\), donc \\(\\rho_1\\). Pour un nombre \\(n\\) dâ€™Ã©chantillons, on aura \\(\\rho_1\\), \\(\\rho_2\\), \\(\\rho_3\\), â€¦, \\(\\rho_n\\), formant le vecteur \\(\\rho = \\left[\\rho_1, \\rho_2, \\rho_3, ..., \\rho_n \\right]\\).\nEn R, une variable est associÃ©e Ã  un vecteur ou une colonne dâ€™un tableau.\n\nrho &lt;- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D\ndata &lt;- data.frame(rho = rho) # tableau\ndata\n\n   rho\n1 1.34\n2 1.52\n3 1.26\n4 1.43\n5 1.39\n\n\nIl existe plusieurs types de variables, qui se regroupent en deux grandes catÃ©gories: les variables quantitatives et les variables qualitatives.\n\n7.2.1 Variables quantitatives\nCes variables peuvent Ãªtre continues dans un espace Ã©chantillonnal rÃ©el ou discrÃ¨tes dans un espace Ã©chantillonnal ne considÃ©rant que des valeurs fixes. Notons que la notion de nombre rÃ©el est toujours une approximation en sciences expÃ©rimentales comme en calcul numÃ©rique, Ã©tant donnÃ©e que lâ€™on est limitÃ© par la prÃ©cision des appareils comme par le nombre dâ€™octets Ã  utiliser. Bien que les valeurs fixes des distributions discrÃ¨tes ne soient pas toujours des valeurs entiÃ¨res, câ€™est bien souvent le cas en biostatistiques comme en dÃ©mographie, oÃ¹ les dÃ©comptes dâ€™individus sont souvent prÃ©sents (et oÃ¹ la notion de fraction dâ€™individus nâ€™est pas acceptÃ©e).\n\n7.2.2 Variables qualitatives\nOn exprime parfois quâ€™une variable qualitative est une variable impossible Ã  mesurer numÃ©riquement: une couleur, lâ€™appartenance Ã  une espÃ¨ce ou Ã  une sÃ©rie de sol. Pourtant, dans bien des cas, les variables qualitatives peuvent Ãªtre encodÃ©es en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile Ã  un loam sableux, qui autrement est dÃ©crit par la classe texturale dâ€™un sol. Pour une couleur, on peut lui associer une longueur dâ€™onde ou des pourcentages de rouge, vert et bleu, ainsi quâ€™un ton. En ce qui a trait aux variables ordonnÃ©es, il est possible de supposer un Ã©talement. Par exemple, une variable dâ€™intensitÃ© faible-moyenne-forte peut Ãªtre transformÃ©e linÃ©airement en valeurs quantitatives -1, 0 et 1. Attention toutefois, lâ€™Ã©talement peut parfois Ãªtre quadratique ou logarithmique. Les sÃ©ries de sol peuvent Ãªtre encodÃ©es par la proportion de gleyfication (Parent et al., 2017). Quant aux catÃ©gories difficilement transformables en quantitÃ©s, on pourra passer par lâ€™encodage catÃ©goriel, souvent appelÃ© dummyfication, que nous verrons plus loin. Lâ€™analyse qualitative consiste en lâ€™analyse de verbatims, essentiellement utile en sciences sociales: nous nâ€™en nâ€™aurons pas besoin ici. Nous considÃ©rerons les variables qualitatives comme des variables quantitatives qui nâ€™ont pas subi de prÃ©traitement."
  },
  {
    "objectID": "07a-biostats.html#les-probabilitÃ©s",
    "href": "07a-biostats.html#les-probabilitÃ©s",
    "title": "7Â  Biostatistiques",
    "section": "\n7.3 Les probabilitÃ©s",
    "text": "7.3 Les probabilitÃ©s\n\nÂ« Nous sommes si Ã©loignÃ©s de connaÃ®tre tous les agens de la nature, et leurs divers modes dâ€™action ; quâ€™il ne serait pas philosophique de nier les phÃ©nomÃ¨nes, uniquement parce quâ€™ils sont inexplicables dans lâ€™Ã©tat actuel de nos connaissances. Seulement, nous devons les examiner avec une attention dâ€™autant plus scrupuleuse, quâ€™il paraÃ®t plus difficile de les admettre ; et câ€™est ici que le calcul des probabilitÃ©s devient indispensable, pour dÃ©terminer jusquâ€™Ã  quel point il faut multiplier les observations ou les expÃ©riences, afin dâ€™obtenir en faveur des agens quâ€™elles indiquent, une probabilitÃ© supÃ©rieure aux raisons que lâ€™on peut avoir dâ€™ailleurs, de ne pas les admettre. Â» â€” Pierre-Simon de Laplace\n\nUne probabilitÃ© est la vraisemblance quâ€™un Ã©vÃ¨nement se rÃ©alise chez un Ã©chantillon. Les probabilitÃ©s forment le cadre des systÃ¨mes stochastiques, câ€™est-Ã -dire des systÃ¨mes trop complexes pour en connaÃ®tre exactement les aboutissants, auxquels on attribue une part de hasard. Ces systÃ¨mes sont prÃ©dominants dans les processus vivants.\nOn peut dÃ©gager deux perspectives sur les probabilitÃ©s: lâ€™une passe par une interprÃ©tation frÃ©quentielle, lâ€™autre bayÃ©sienne.\n\nLâ€™interprÃ©tation frÃ©quentielle reprÃ©sente la frÃ©quence des occurrences aprÃ¨s un nombre infini dâ€™Ã©vÃ¨nements. Par exemple, si vous jouez Ã  pile ou face un grand nombre de fois, le nombre de pile sera Ã©gal Ã  la moitiÃ© du nombre de lancers. Lâ€™approche frÃ©quentielle teste si les donnÃ©es concordent avec un modÃ¨le du rÃ©el. Il sâ€™agit de lâ€™interprÃ©tation communÃ©ment utilisÃ©e.\nLâ€™interprÃ©tation bayÃ©sienne vise Ã  quantifier lâ€™incertitude des phÃ©nomÃ¨nes. Dans cette perspective, plus lâ€™information sâ€™accumule, plus lâ€™incertitude diminue. Cette approche gagne en notoriÃ©tÃ© notamment parce quâ€™elle permet de dÃ©crire des phÃ©nomÃ¨nes qui, intrinsÃ¨quement, ne peuvent Ãªtre rÃ©pÃ©tÃ©s infiniment (absence dâ€™asymptote), comme ceux qui sont bien dÃ©finis dans le temps ou sur des populations limitÃ©es. Lâ€™approche bayÃ©sienne Ã©value la probabilitÃ© que le modÃ¨le soit rÃ©el.\n\nUne erreur courante consiste Ã  aborder des statistiques frÃ©quentielles comme des statistiques bayÃ©siennes. Par exemple, si lâ€™on dÃ©sire Ã©valuer la probabilitÃ© de lâ€™existence de vie sur Mars, on devra passer par le bayÃ©sien, car avec les stats frÃ©quentielles, on devra plutÃ´t conclure si les donnÃ©es sont conformes ou non avec lâ€™hypothÃ¨se de la vie sur Mars (exemple tirÃ©e du blogue Dynamic Ecology).\nDes rivalitÃ©s factices sâ€™installent entre les tenants des diffÃ©rentes approches, dont chacune, en rÃ©alitÃ©, rÃ©pond Ã  des questions diffÃ©rentes dont il convient rÃ©flÃ©chir sur les limitations. Bien que les statistiques bayÃ©siennes soient de plus en plus utilisÃ©es, nous ne couvrirons dans ce chapitre que lâ€™approche frÃ©quentielle. Lâ€™approche bayÃ©sienne est nÃ©anmoins traitÃ©e dans le chapitreÂ 8, qui est facultatif au cours."
  },
  {
    "objectID": "07a-biostats.html#les-distributions",
    "href": "07a-biostats.html#les-distributions",
    "title": "7Â  Biostatistiques",
    "section": "\n7.4 Les distributions",
    "text": "7.4 Les distributions\nUne variable alÃ©atoire peut prendre des valeurs selon des modÃ¨les de distribution des probabilitÃ©s. Une distribution est une fonction mathÃ©matique dÃ©crivant la probabilitÃ© dâ€™observer une sÃ©rie dâ€™Ã©vÃ¨nements. Ces Ã©vÃ¨nements peuvent Ãªtre des valeurs continues, des nombres entiers, des catÃ©gories, des valeurs boolÃ©ennes (Vrai/Faux), etc. DÃ©pendemment du type de valeur et des observations obtenues, on peut associer des variables Ã  diffÃ©rentes lois de probabilitÃ©. Toujours, lâ€™aire sous la courbe dâ€™une distribution de probabilitÃ© est Ã©gale Ã  1.\nEn statistiques infÃ©rentielles, les distributions sont les modÃ¨les, comprenant certains paramÃ¨tres comme la moyenne et la variance pour les distributions normales, Ã  partir desquelles les donnÃ©es sont gÃ©nÃ©rÃ©es.\nIl existe deux grandes familles de distribution: discrÃ¨tes et continues. Les distributions discrÃ¨tes sont contraintes Ã  des valeurs prÃ©dÃ©finies (finies ou infinies), alors que les distributions continues prennent nÃ©cessairement un nombre infini de valeurs, dont la probabilitÃ© ne peut pas Ãªtre Ã©valuÃ©e ponctuellement, mais sur un intervalle.\nLâ€™espÃ©rance mathÃ©matique est une fonction de tendance centrale, souvent dÃ©crite par un paramÃ¨tre. Il sâ€™agit de la moyenne dâ€™une population pour une distribution normale. La variance, quant Ã  elle, dÃ©crit la variabilitÃ© dâ€™une population, i.e.Â son Ã©talement autour de lâ€™espÃ©rance. Pour une distribution normale, la variance dâ€™une population est aussi appelÃ©e variance, souvent prÃ©sentÃ©e par lâ€™Ã©cart-type (Ã©gal Ã  la racine carrÃ©e de la variance).\n\n7.4.1 Distribution binomiale\nEn tant que scÃ©nario Ã  deux issues possibles, des tirages Ã  pile ou face suivent une loi binomiale, comme toute variable boolÃ©enne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la prÃ©sence/absence dâ€™une espÃ¨ce, dâ€™une maladie, dâ€™un trait phylogÃ©nÃ©tique, ainsi que les catÃ©gories encodÃ©es. Lorsque lâ€™opÃ©ration ne comprend quâ€™un seul Ã©chantillon (i.e.Â un seul tirage Ã  pile ou face), il sâ€™agit dâ€™un cas particulier dâ€™une loi binomiale que lâ€™on nomme une loi de Bernouilli.\nPour 25 tirages Ã  pile ou face indÃ©pendants (i.e.Â dont lâ€™ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilitÃ©s est de 1. La fonction dbinom est une fonction de distribution de probabilitÃ©s. Les fonctions de distribution de probabilitÃ©s discrÃ¨tes sont appelÃ©es des fonctions de masse.\n\nlibrary(\"tidyverse\")\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.4.4     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.0\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nx &lt;- 0:25\ny &lt;- dbinom(x = x, size = 25, prob = 0.5)\nprint(paste('La somme des probabilitÃ©s est de', sum(y)))\n\n[1] \"La somme des probabilitÃ©s est de 1\"\n\nggplot(data = tibble(x, y), mapping = aes(x, y)) +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = \"grey50\") +\n  geom_point()\n\n\n\n\n\n7.4.2 Distribution de Poisson\nLa loi de Poisson (avec un P majuscule, introduite par le mathÃ©maticien franÃ§ais SimÃ©on Denis Poisson et non pas lâ€™animal) dÃ©crit des distributions discrÃ¨tes de probabilitÃ© dâ€™un nombre dâ€™Ã©vÃ¨nements se produisant dans lâ€™espace ou dans le temps. Les distributions de Poisson dÃ©crivent ce qui tient du dÃ©compte. Il peut sâ€™agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants dâ€™asclÃ©piades se trouvant sur une terre cultivÃ©e, ou du nombre dâ€™Ã©vÃ¨nements de prÃ©cipitation au mois de juin, etc. La distribution de Poisson nâ€™a quâ€™un seul paramÃ¨tre, \\(\\lambda\\), qui dÃ©crit la moyenne des dÃ©comptes.\nPar exemple, en un mois de 30 jours, et une moyenne de 8 Ã©vÃ¨nements de prÃ©cipitation pour ce mois, on obtient la distribution suivante.\n\nx &lt;- 1:30\ny &lt;- dpois(x, lambda = 8)\nprint(paste('La somme des probabilitÃ©s est de', sum(y)))\n\n[1] \"La somme des probabilitÃ©s est de 0.999664536835124\"\n\nggplot(data = data.frame(x, y), mapping = aes(x, y)) +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = \"grey50\") +\n  geom_point()\n\n\n\n\n\n7.4.3 Distribution uniforme\nLa distribution la plus simple est probablement la distribution uniforme. Si la variable est discrÃ¨te, chaque catÃ©gorie est associÃ©e Ã  une probabilitÃ© Ã©gale. Si la variable est continue, la probabilitÃ© est directement proportionnelle Ã  la largeur de lâ€™intervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour dÃ©crire des a priori vagues pour lâ€™analyse bayÃ©sienne (ce sujet est traitÃ© dans le chapitreÂ 8). Nous utilisons la fonction dunif. Ã€ la diffÃ©rence des distributions discrÃ¨tes, les fonctions de distribution de probabilitÃ©s continues sont appelÃ©es des fonctions de densitÃ© dâ€™une loi de probabilitÃ© (probability density function).\n\nincrement &lt;- 0.01\nx &lt;- seq(-4, 4, by = increment)\ny1 &lt;- dunif(x, min = -3, max = 3)\ny2 &lt;- dunif(x, min = -2, max = 2)\ny3 &lt;- dunif(x, min = -1, max = 1)\n\nprint(paste('La somme des probabilitÃ©s est de', sum(y3 * increment)))\n\n[1] \"La somme des probabilitÃ©s est de 1.005\"\n\ngg_unif &lt;- data.frame(x, y1, y2, y3) |&gt;  \n  pivot_longer(-x, names_to = \"variable\", values_to = \"value\")\n\nggplot(data = gg_unif, mapping = aes(x = x, y = value)) +\n  geom_line(aes(colour = variable))\n\n\n\n\n\n7.4.4 Distribution normale\nLa plus rÃ©pandue de ces lois est probablement la loi normale, parfois nommÃ©e loi gaussienne et plus rarement loi laplacienne. Il sâ€™agit de la distribution classique en forme de cloche.\nLa loi normale est dÃ©crite par une moyenne, qui dÃ©signe la tendance centrale, et une variance, qui dÃ©signe lâ€™Ã©talement des probabilitÃ©s autour de la moyenne. La racine carrÃ©e de la variance est lâ€™Ã©cart-type.\nLes distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approximÃ©es par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne dâ€™une loi log-normale est la moyenne gÃ©omÃ©trique.\n\nincrement &lt;- 0.01\nx &lt;- seq(-10, 10, by = increment)\ny1 &lt;- dnorm(x, mean = 0, sd = 1)\ny2 &lt;- dnorm(x, mean = 0, sd = 2)\ny3 &lt;- dnorm(x, mean = 0, sd = 3)\n\nprint(paste('La somme des probabilitÃ©s est de', sum(y3 * increment)))\n\n[1] \"La somme des probabilitÃ©s est de 0.999147010743368\"\n\ngg_norm &lt;- data.frame(x, y1, y2, y3) |&gt;  \n  pivot_longer(-x, names_to = \"variable\", values_to = \"value\")\n\nggplot(data = gg_norm, mapping = aes(x = x, y = value)) +\n  geom_line(aes(colour = variable))\n\n\n\n\nQuelle est la probabilitÃ© dâ€™obtenir le nombre 0 chez une observation continue distribuÃ©e normalement dont la moyenne est 0 et lâ€™Ã©cart-type est de 1? RÃ©ponse: 0. La loi normale Ã©tant une distribution continue, les probabilitÃ©s non-nulles ne peuvent Ãªtre calculÃ©es que sur des intervalles. Par exemple, la probabilitÃ© de retrouver une valeur dans lâ€™intervalle entre -1 et 2 est calculÃ©e en soustrayant la probabilitÃ© cumulÃ©e Ã  -1 de la probabilitÃ© cumulÃ©e Ã  2.\n\nincrement &lt;- 0.01\nx &lt;- seq(-5, 5, by = increment)\ny &lt;- dnorm(x, mean = 0, sd = 1)\n\nprob_between &lt;- c(-1, 2)\n\ngg_norm &lt;- data.frame(x, y)\ngg_auc &lt;- gg_norm |&gt; \n  filter(x &gt; prob_between[1], x &lt; prob_between[2]) |&gt; \n  rbind(c(prob_between[2], 0)) |&gt; \n  rbind(c(prob_between[1], 0))\n\nggplot(data.frame(x, y), aes(x, y)) +\n  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadÃ©cimal\n  geom_line()\n\n\n\nprob_norm_between &lt;- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1)\nprint(paste(\"La probabilitÃ© d'obtenir un nombre entre\",\n            prob_between[1], \"et\",\n            prob_between[2], \"est d'environ\",\n            round(prob_norm_between, 2) * 100, \"%\"))\n\n[1] \"La probabilitÃ© d'obtenir un nombre entre -1 et 2 est d'environ 82 %\"\n\n\nLa courbe normale peut Ãªtre utile pour Ã©valuer la distribution dâ€™une population. Par exemple, on peut calculer les limites de rÃ©gion sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et dâ€™autre de la moyenne. Il sâ€™agit ainsi de lâ€™intervalle de confiance sur la dÃ©viation de la distribution.\n\nincrement &lt;- 0.01\nx &lt;- seq(-5, 5, by = increment)\ny &lt;- dnorm(x, mean = 0, sd = 1)\n\nalpha &lt;- 0.05\nprob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1),\n                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1))\n\ngg_norm &lt;- data.frame(x, y)\ngg_auc &lt;- gg_norm |&gt; \n  filter(x &gt; prob_between[1], x &lt; prob_between[2]) |&gt; \n  rbind(c(prob_between[2], 0)) |&gt; \n  rbind(c(prob_between[1], 0))\n\nggplot(data = data.frame(x, y), mapping = aes(x, y)) +\n  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadÃ©cimal\n  geom_line() +\n  geom_text(data = data.frame(x = prob_between,\n                              y = c(0, 0),\n                              labels = round(prob_between, 2)),\n            mapping = aes(label = labels))\n\n\n\n\nOn pourrait aussi Ãªtre intÃ©ressÃ© Ã  lâ€™intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont lâ€™Ã©cart-type est notÃ© erreur standard. On calcule cette erreur en divisant la variance par le nombre dâ€™observation, ou en divisant lâ€™Ã©cart-type par la racine carrÃ©e du nombre dâ€™observations. Ainsi, pour 10 Ã©chantillons:\n\nincrement &lt;- 0.01\nx &lt;- seq(-5, 5, by = increment)\ny &lt;- dnorm(x, mean = 0, sd = 1)\n\nalpha &lt;- 0.05\nprob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10),\n                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10))\n\ngg_norm &lt;- data.frame(x, y)\ngg_auc &lt;- gg_norm |&gt; \n  filter(x &gt; prob_between[1], x &lt; prob_between[2]) |&gt; \n  rbind(c(prob_between[2], 0)) |&gt; \n  rbind(c(prob_between[1], 0))\n\nggplot(data = data.frame(x, y), mapping = aes(x, y)) +\n  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadÃ©cimal\n  geom_line() +\n  geom_text(data = data.frame(x = prob_between,\n                              y = c(0, 0),\n                              labels = round(prob_between, 2)),\n            mapping = aes(label = labels))"
  },
  {
    "objectID": "07a-biostats.html#statistiques-descriptives",
    "href": "07a-biostats.html#statistiques-descriptives",
    "title": "7Â  Biostatistiques",
    "section": "\n7.5 Statistiques descriptives",
    "text": "7.5 Statistiques descriptives\nOn a vu comment gÃ©nÃ©rer des statistiques sommaires en R avec la fonction summary(). Reprenons les donnÃ©es dâ€™iris.\n\ndata(\"iris\")\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nPour prÃ©cisÃ©ment effectuer une moyenne et un Ã©cart-type sur un vecteur, passons par les fonctions mean() et sd().\n\nmean(iris$Sepal.Length)\n\n[1] 5.843333\n\nsd(iris$Sepal.Length)\n\n[1] 0.8280661\n\n\nPour effectuer un sommaire de tableau pilotÃ© par une fonction, nous passons par la gamme de fonctions summarise(), de dplyr. Dans ce cas, avec group_by(), nous fragmentons le tableau par espÃ¨ce pour effectuer un sommaire sur toutes les variables.\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise_all(mean)\n\n# A tibble: 3 Ã— 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 versicolor         5.94        2.77         4.26       1.33 \n3 virginica          6.59        2.97         5.55       2.03 \n\n\nVous pourriez Ãªtre intÃ©ressÃ© par les quartiles Ã  25, 50 et 75%. Mais la fonction summarise() nâ€™autorise que les fonctions dont la sortie est dâ€™un seul objet, alors faisons sorte que lâ€™objet soit une liste - lorsque lâ€™on imbrique une fonction funs, le tableau Ã  insÃ©rer dans la fonction est indiquÃ© par un ..\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise_all(list(q25 = ~ quantile(., probs = 0.25),\n                     q50 = ~ quantile(., probs = 0.50),\n                     q75 = ~ quantile(., probs = 0.75)))\n\n# A tibble: 3 Ã— 13\n  Species    Sepal.Length_q25 Sepal.Width_q25 Petal.Length_q25 Petal.Width_q25\n  &lt;fct&gt;                 &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n1 setosa                 4.8             3.2               1.4             0.2\n2 versicolor             5.6             2.52              4               1.2\n3 virginica              6.22            2.8               5.1             1.8\n# â„¹ 8 more variables: Sepal.Length_q50 &lt;dbl&gt;, Sepal.Width_q50 &lt;dbl&gt;,\n#   Petal.Length_q50 &lt;dbl&gt;, Petal.Width_q50 &lt;dbl&gt;, Sepal.Length_q75 &lt;dbl&gt;,\n#   Sepal.Width_q75 &lt;dbl&gt;, Petal.Length_q75 &lt;dbl&gt;, Petal.Width_q75 &lt;dbl&gt;\n\n\nEn mode programmation classique de R, on pourra gÃ©nÃ©rer les quartiles Ã  la piÃ¨ce.\n\nquantile(iris$Sepal.Length[iris$Species == 'setosa'])\n\n  0%  25%  50%  75% 100% \n 4.3  4.8  5.0  5.2  5.8 \n\nquantile(iris$Sepal.Length[iris$Species == 'versicolor'])\n\n  0%  25%  50%  75% 100% \n 4.9  5.6  5.9  6.3  7.0 \n\nquantile(iris$Sepal.Length[iris$Species == 'virginica'])\n\n   0%   25%   50%   75%  100% \n4.900 6.225 6.500 6.900 7.900 \n\n\nLa fonction table() permettra dâ€™obtenir des dÃ©comptes par catÃ©gorie, ici par plages de longueurs de sÃ©pales. Pour obtenir les proportions du nombre total, il sâ€™agit dâ€™encapsuler le tableau croisÃ© dans la fonction prop.table().\n\ntableau_croise &lt;- table(iris$Species,\n                        cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length)))\ntableau_croise\n\n            \n             (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9]\n  setosa            35        14         0         0\n  versicolor         4        20        17         9\n  virginica          1         5        18        26\n\n\n\nprop.table(tableau_croise)\n\n            \n               (4.3,5.1]   (5.1,5.8]   (5.8,6.4]   (6.4,7.9]\n  setosa     0.234899329 0.093959732 0.000000000 0.000000000\n  versicolor 0.026845638 0.134228188 0.114093960 0.060402685\n  virginica  0.006711409 0.033557047 0.120805369 0.174496644"
  },
  {
    "objectID": "07a-biostats.html#tests-dhypothÃ¨ses-Ã -un-et-deux-Ã©chantillons",
    "href": "07a-biostats.html#tests-dhypothÃ¨ses-Ã -un-et-deux-Ã©chantillons",
    "title": "7Â  Biostatistiques",
    "section": "\n7.6 Tests dâ€™hypothÃ¨ses Ã  un et deux Ã©chantillons",
    "text": "7.6 Tests dâ€™hypothÃ¨ses Ã  un et deux Ã©chantillons\nUn test dâ€™hypothÃ¨se permet de dÃ©cider si une hypothÃ¨se est confirmÃ©e ou rejetÃ©e Ã  un seuil de probabilitÃ© prÃ©dÃ©terminÃ©.\nCette section est inspirÃ©e du chapitre 5 de Dalgaard, 2008.\n\nInformation: lâ€™hypothÃ¨se nulle. Les tests dâ€™hypothÃ¨se Ã©valuent des effets statistiques (qui ne sont pas nÃ©cessairement des effets de causalitÃ©). Lâ€™effet Ã  Ã©valuer peut Ãªtre celui dâ€™un traitement, dâ€™indicateurs mÃ©tÃ©orologiques (e.g.Â prÃ©cipitations totales, degrÃ©-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est menÃ©e pour Ã©valuer lâ€™hypothÃ¨se que lâ€™on retrouve des diffÃ©rences entre des unitÃ©s expÃ©rimentales. Par convention, lâ€™hypothÃ¨se nulle (Ã©crite \\(H_0\\)) est lâ€™hypothÃ¨se quâ€™il nâ€™y ait pas dâ€™effet (câ€™est lâ€™hypothÃ¨se de lâ€™avocat du diable ğŸ˜ˆ) Ã  lâ€™Ã©chelle de la population (et non pas Ã  lâ€™Ã©chelle de lâ€™Ã©chantillon). Ã€ lâ€™inverse, lâ€™hypothÃ¨se alternative (Ã©crite \\(H_1\\)) est lâ€™hypothÃ¨se quâ€™il y ait un effet Ã  lâ€™Ã©chelle de la population.\n\nÃ€ titre dâ€™exercice en stats, on dÃ©bute souvent en testant si deux vecteurs de valeurs continues proviennent de populations Ã  moyennes diffÃ©rentes ou si un vecteur de valeurs a Ã©tÃ© gÃ©nÃ©rÃ© Ã  partir dâ€™une population ayant une moyenne donnÃ©e. Dans cette section, nous utiliserons la fonction t.test() pour les tests de t et la fonction wilcox.test() pour les tests de Wilcoxon (aussi appelÃ© de Mann-Whitney).\n\n7.6.1 Test de t Ã  un seul Ã©chantillon\nNous devons assumer, pour ce test, que lâ€™Ã©chantillon est recueillit dâ€™une population dont la distribution est normale, \\(\\mathcal{N} \\sim \\left( \\mu, \\sigma^2 \\right)\\), et que chaque Ã©chantillon est indÃ©pendant lâ€™un de lâ€™autre. Lâ€™hypothÃ¨se nulle est souvent celle de lâ€™avocat du diable, que la moyenne soit Ã©gale Ã  une valeur donnÃ©e (donc la diffÃ©rence entre la moyenne de la population et une moyenne donnÃ©e est de zÃ©ro): ici, que \\(\\mu = \\bar{x}\\). Lâ€™erreur standard sur la moyenne (ESM) de lâ€™Ã©chantillon, \\(\\bar{x}\\) est calculÃ©e comme suit.\n\\[ESM = \\frac{s}{\\sqrt{n}}\\]\noÃ¹ \\(s\\) est lâ€™Ã©cart-type de lâ€™Ã©chantillon et \\(n\\) est le nombre dâ€™Ã©chantillons.\nPour tester lâ€™intervalle de confiance de lâ€™Ã©chantillon, on multiplie lâ€™ESM par lâ€™aire sous la courbe de densitÃ© couvrant une certaine proportion de part et dâ€™autre de lâ€™Ã©chantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et dâ€™autre.\n\nset.seed(33746)\nx &lt;- rnorm(20, 16, 4)\n\nlevel &lt;-  0.95\nalpha &lt;- 1-level\n\nx_bar &lt;- mean(x)\ns &lt;- sd(x)\nn &lt;- length(x)\n\nerror &lt;- qnorm(1 - alpha/2) * s / sqrt(n)\nerror\n\n[1] 1.483253\n\n\nLâ€™intervalle de confiance est lâ€™erreur de part et dâ€™autre de la moyenne.\n\nc(x_bar - error, x_bar + error)\n\n[1] 14.35630 17.32281\n\n\nSi la moyenne de la population est de 16, un nombre qui se situe dans lâ€™intervalle de confiance on accepte lâ€™hypothÃ¨se nulle au seuil 0.05. Si le nombre dâ€™Ã©chantillon est rÃ©duit (gÃ©nÃ©ralement &lt; 30), on passera plutÃ´t par une distribution de t, avec \\(n-1\\) degrÃ©s de libertÃ©.\n\nerror &lt;- qt(1 - alpha/2, n-1) * s / sqrt(n)\nc(x_bar - error, x_bar + error)\n\n[1] 14.25561 17.42351\n\n\nPlus simplement, on pourra utiliser la fonction t.test() en spÃ©cifiant la moyenne de la population. Nous avons gÃ©nÃ©rÃ© 20 donnÃ©es avec une moyenne de 16 et un Ã©cart-type de 4. Nous savons donc que la vraie moyenne de lâ€™Ã©chantillon est de 16. Mais disons que nous testons lâ€™hypothÃ¨se que ces donnÃ©es sont tirÃ©es dâ€™une population dont la moyenne est 18 (et implicitement que son Ã©cart-type est de 4).\n\nt.test(x, mu = 18)\n\n\n    One Sample t-test\n\ndata:  x\nt = -2.8548, df = 19, p-value = 0.01014\nalternative hypothesis: true mean is not equal to 18\n95 percent confidence interval:\n 14.25561 17.42351\nsample estimates:\nmean of x \n 15.83956 \n\n\nLa fonction retourne la valeur de t (t-value), le nombre de degrÃ©s de libertÃ© (\\(n-1 = 19\\)), une description de lâ€™hypothÃ¨se alternative (alternative hypothesis: true mean is not equal to 18), ainsi que lâ€™intervalle de confiance au niveau de 95%. Le test contient aussi la p-value.\n\n\n7.6.1.1 Information: la p-value\n\nLa p-value, ou valeur-p ou p-valeur, est utilisÃ©e pour trancher si, oui ou non, un rÃ©sultat est significatif. En langage scientifique, le mot significatif ne devrait Ãªtre utilisÃ© que lorsque lâ€™on rÃ©fÃ¨re Ã  un test dâ€™hypothÃ¨se statistique. Vous retrouverez des p-values partout en stats. Les p-values indiquent la probabilitÃ© que les donnÃ©es ait Ã©tÃ© Ã©chantillonnÃ©es dâ€™une population oÃ¹ un effet est observable selon le modÃ¨le statistique utilisÃ©.\n\nLa p-value est la probabilitÃ© que les donnÃ©es aient Ã©tÃ© gÃ©nÃ©rÃ©es pour obtenir un effet Ã©quivalent ou plus prononcÃ© si lâ€™hypothÃ¨se nulle est vraie.\n\nUne p-value Ã©levÃ©e indique que le modÃ¨le appliquÃ© Ã  vos donnÃ©es concorde avec la conclusion que lâ€™hypothÃ¨se nulle est vraie, et inversement si la p-value est faible. Le seuil arbitraire utilisÃ©e en Ã©cologie et en agriculture, comme dans plusieurs domaines, est de 0.05. Lâ€™utilisation dâ€™un seuil est toutefois contestÃ©e avec raison. Une enquÃªte menÃ©e dans la littÃ©rature scientifiques a rÃ©vÃ©lÃ© que 49% des 791 articles Ã©tudiÃ©s interprÃ©taient un effet non significatif comme un effet nul (Amrhein et al., 2019). En effet, une catÃ©gorisation de la p-value avec un seuil de significativitÃ© brouille le jugement sur lâ€™importance des effets et de leur incertitude. Les six principes de lâ€™American Statistical Association guident lâ€™interprÃ©tation des p-values. [ma traduction]\n\nLes p-values indiquent lâ€™ampleur de lâ€™incompatibilitÃ© des donnÃ©es avec le modÃ¨le statistique\nLes p-values ne mesurent pas la probabilitÃ© que lâ€™hypothÃ¨se Ã©tudiÃ©e soit vraie, ni la probabilitÃ© que les donnÃ©es ont Ã©tÃ© gÃ©nÃ©rÃ©es uniquement par la chance.\nLes conclusions scientifiques et dÃ©cisions dâ€™affaire ou politiques ne devraient pas Ãªtre basÃ©es sur lâ€™atteinte dâ€™une p-value Ã  un seuil spÃ©cifique.\nUne infÃ©rence appropriÃ©e demande un rapport complet et transparent.\nUne p-value, ou une signification statistique, ne mesure pas lâ€™ampleur dâ€™un effet ou lâ€™importance dâ€™un rÃ©sultat.\nEn tant que tel, une p-value nâ€™offre pas une bonne mesure des Ã©vidences dâ€™un modÃ¨le ou dâ€™une hypothÃ¨se.\n\n\nDans le cas prÃ©cÃ©dent, la p-value Ã©tait de 0.01014. Pour aider notre interprÃ©tation, prenons lâ€™hypothÃ¨se alternative: true mean is not equal to 18. Lâ€™hypothÃ¨se nulle Ã©tait bien que la vraie moyenne est Ã©gale Ã  18. InsÃ©rons la p-value dans la dÃ©finition: la probabilitÃ© que les donnÃ©es aient Ã©tÃ© gÃ©nÃ©rÃ©es pour obtenir un effet Ã©quivalent ou plus prononcÃ© si lâ€™hypothÃ¨se nulle est vraie est de 0.01014. Il est donc trÃ¨s peu probable que les donnÃ©es soient tirÃ©es dâ€™un Ã©chantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette lâ€™hypothÃ¨se nulle et lâ€™on conclut quâ€™Ã  ce seuil de confiance, lâ€™Ã©chantillon ne provient pas dâ€™une population ayant une moyenne de 18.\n\n7.6.2 Attention: mauvaises interprÃ©tations des p-values\n\n\nâ€œLa p-value nâ€™a jamais Ã©tÃ© conÃ§ue comme substitut au raisonnement scientifiqueâ€ Ron Wasserstein, directeur de lâ€™American Statistical Association [ma traduction].\n\nUn rÃ©sultat montrant une p-value plus Ã©levÃ©e que 0.05 est-il pertinent?\nLors dâ€™une confÃ©rence, Dr Evil ne prÃ©sente que les rÃ©sultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importantsâ€¦ En Ã©cartant ces rÃ©sultats, Dr Evil commet 3 erreurs:\n\nLa p-value nâ€™est pas un bon indicateur de lâ€™importance dâ€™un test statistique. Lâ€™importance dâ€™une variable dans un modÃ¨le devrait Ãªtre Ã©valuÃ©e par la valeur de son coefficient. Son incertitude devrait Ãªtre Ã©valuÃ©e par sa variance. Une maniÃ¨re plus intuitive dâ€™Ã©valuer la variance est lâ€™Ã©cart-type ou lâ€™intervalle de confiance. Ã€ un certain seuil dâ€™intervalle de confiance, la p-value traduira la probabilitÃ© quâ€™un coefficient rÃ©ellement nul ait pu gÃ©nÃ©rer des donnÃ©es dÃ©montrant un coefficient Ã©gal ou supÃ©rieur.\nIl est tout aussi important de savoir que le traitement fonctionne que de savoir quâ€™il ne fonctionne pas. Les rÃ©sultats dÃ©montrant des effets sont malheureusement davantage soumis aux journaux et davantage publiÃ©s que ceux ne dÃ©montrant pas dâ€™effets (Decullier et al., 2005).\nLe seuil de 0.05 est arbitraire.\n\n\n\n7.6.2.1 Attention au p-hacking\n\nLe p-hacking (ou data dredging) consiste Ã  manipuler les donnÃ©es et les modÃ¨les pour faire en sorte dâ€™obtenir des p-values favorables Ã  lâ€™hypothÃ¨se testÃ©e et, Ã©ventuellement, aux conclusions recherchÃ©es. Ã€ Ã©viter dans tous les cas. Toujours. Toujours. Toujours.\n\n\n\n\nUn sketch humoristique de John Oliver sur le p-hacking, Last week tonight, 2016 (en anglais)\n\n\n\n\n7.6.3 Test de Wilcoxon Ã  un seul Ã©chantillon\nLe test de t suppose que la distribution des donnÃ©es est normaleâ€¦ ce qui est rarement le cas, surtout lorsque les Ã©chantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: câ€™est un test non-paramÃ©trique basÃ© sur le tri des valeurs.\n\nwilcox.test(x, mu = 18)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x\nV = 39, p-value = 0.01208\nalternative hypothesis: true location is not equal to 18\n\n\nLe V est la somme des rangs positifs. Dans ce cas, la p-value est semblable Ã  celle du test de t, et les mÃªmes conclusions sâ€™appliquent.\n\n7.6.4 Tests de t Ã  deux Ã©chantillons\nLes tests Ã  un Ã©chantillon servent plutÃ´t Ã  sâ€™exercer: rarement en aura-t-on besoin en recherche, oÃ¹ plus souvent, on voudra comparer les moyennes de deux unitÃ©s expÃ©rimentales. Lâ€™expÃ©rience comprend donc deux sÃ©ries de donnÃ©es continues, \\(x_1\\) et \\(x_2\\), issues de lois de distribution normale \\(\\mathcal{N} \\left( \\mu_1, \\sigma_1^2 \\right)\\) et \\(\\mathcal{N} \\left( \\mu_2, \\sigma_2^2 \\right)\\), et nous testons lâ€™hypothÃ¨se nulle que \\(\\mu_1 = \\mu_2\\). La statistique t est calculÃ©e comme suit.\n\\[t = \\frac{\\bar{x_1} - \\bar{x_2}}{ESDM}\\]\nLâ€™ESDM est lâ€™erreur standard de la diffÃ©rence des moyennes:\n\\[ESDM = \\sqrt{ESM_1^2 + ESM_2^2}\\]\nSi vous supposez que les variances sont identiques, lâ€™erreur standard (\\(s\\)) est calculÃ©e pour les Ã©chantillons des deux groupes, puis insÃ©rÃ©e dans le calcul des ESM. La statistique t sera alors Ã©valuÃ©e Ã  \\(n_1 + n_2 - 2\\) degrÃ©s de libertÃ©. Si vous supposez que la variance est diffÃ©rente (procÃ©dure de Welch), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degrÃ©s de libertÃ© calculÃ© Ã  partir des erreurs standards et du nombre dâ€™Ã©chantillon dans les groupes: cette procÃ©dure est considÃ©rÃ©e comme plus prudente (Dalgaard, 2008, page 101).\nPrenons les donnÃ©es dâ€™iris pour lâ€™exemple en excluant lâ€™iris setosa Ã©tant donnÃ©e que les tests de t se restreignent Ã  deux groupes. Nous allons tester la longueur des pÃ©tales.\n\niris_pl &lt;- iris |&gt; \n    filter(Species != \"setosa\") |&gt; \n    select(Species, Petal.Length)\nslice_sample(iris_pl, n = 5)\n\n     Species Petal.Length\n1  virginica          5.1\n2 versicolor          4.0\n3  virginica          5.0\n4 versicolor          4.6\n5 versicolor          4.1\n\n\nDans la prochaine cellule de code, nous introduisons lâ€™interface-formule de R, oÃ¹ lâ€™on retrouve typiquement le ~, entre les variables de sortie Ã  gauche et les variables dâ€™entrÃ©e Ã  droite. Dans notre cas, la variable de sortie est la variable testÃ©e, Petal.Length, qui varie en fonction du groupe Species, qui est la variable dâ€™entrÃ©e (variable explicative) - nous verrons les types de variables plus en dÃ©tails dans la section sur les modÃ¨les statistiques (chapitreÂ 7.8).\n\nt.test(formula = Petal.Length ~ Species,\n       data = iris_pl, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Length by Species\nt = -12.604, df = 95.57, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\nNous obtenons une sortie similaire aux prÃ©cÃ©dentes. Lâ€™intervalle de confiance Ã  95% exclut le zÃ©ro, ce qui est cohÃ©rent avec la p-value trÃ¨s faible, qui nous indique le rejet de lâ€™hypothÃ¨se nulle au seuil 0.05. Les donnÃ©es montrent que les groupes ont des moyennes de longueur de pÃ©tales diffÃ©rentes.\n\n\n7.6.4.1 Enregistrer les rÃ©sultats dâ€™un test\nIl est possible dâ€™enregistrer un test dans un objet.\n\ntt_pl &lt;- t.test(formula = Petal.Length ~ Species,\n                data = iris_pl, var.equal = FALSE)\nsummary(tt_pl)\n\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \np.value     1      -none- numeric  \nconf.int    2      -none- numeric  \nestimate    2      -none- numeric  \nnull.value  1      -none- numeric  \nstderr      1      -none- numeric  \nalternative 1      -none- character\nmethod      1      -none- character\ndata.name   1      -none- character\n\nstr(tt_pl)\n\nList of 10\n $ statistic  : Named num -12.6\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 95.6\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 4.9e-22\n $ conf.int   : num [1:2] -1.5 -1.09\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num [1:2] 4.26 5.55\n  ..- attr(*, \"names\")= chr [1:2] \"mean in group versicolor\" \"mean in group virginica\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"difference in means between group versicolor and group virginica\"\n $ stderr     : num 0.103\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Welch Two Sample t-test\"\n $ data.name  : chr \"Petal.Length by Species\"\n - attr(*, \"class\")= chr \"htest\"\n\n\n\n7.6.5 Comparaison des variances\nPour comparer les variances, on a recours au test de F (F pour Fisher).\n\nvar.test(formula = Petal.Length ~ Species,\n         data = iris_pl)\n\n\n    F test to compare two variances\n\ndata:  Petal.Length by Species\nF = 0.72497, num df = 49, denom df = 49, p-value = 0.2637\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.411402 1.277530\nsample estimates:\nratio of variances \n         0.7249678 \n\n\nIl semble que lâ€™on pourrait relancer le test de t sans la procÃ©dure Welch, avec var.equal = TRUE.\n\n7.6.6 Tests de Wilcoxon Ã  deux Ã©chantillons\nCela ressemble au test de t!\n\nwilcox.test(formula = Petal.Length ~ Species,\n       data = iris_pl, var.equal = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Petal.Length by Species\nW = 44.5, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n7.6.7 Les tests pairÃ©s\nLes tests pairÃ©s sont utilisÃ©s lorsque deux Ã©chantillons proviennent dâ€™une mÃªme unitÃ© expÃ©rimentale: il sâ€™agit en fait de tests sur la diffÃ©rence entre deux observations.\n\nset.seed(2555)\n\nn &lt;- 20\navant &lt;- rnorm(n, 16, 4)\napres &lt;- rnorm(n, 18, 3)\n\nIl est important de spÃ©cifier que le test est pairÃ©, la valeur par dÃ©faut de paired Ã©tant FALSE.\n\nt.test(avant, apres, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  avant and apres\nt = -1.5168, df = 19, p-value = 0.1458\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.5804586  0.7311427\nsample estimates:\nmean difference \n      -1.924658 \n\n\nLâ€™hypothÃ¨se nulle quâ€™il nâ€™y ait pas de diffÃ©rence entre lâ€™avant et lâ€™aprÃ¨s traitement est acceptÃ©e au seuil 0.05.\nExercice. Effectuer un test de Wilcoxon pairÃ©."
  },
  {
    "objectID": "07a-biostats.html#lanalyse-de-variance",
    "href": "07a-biostats.html#lanalyse-de-variance",
    "title": "7Â  Biostatistiques",
    "section": "\n7.7 Lâ€™analyse de variance",
    "text": "7.7 Lâ€™analyse de variance\nLâ€™analyse de variance consiste Ã  comparer des moyennes de plusieurs groupes distribuÃ©s normalement et de mÃªme variance. Cette section sera Ã©laborÃ©e prochainement plus en profondeur. ConsidÃ©rons-la pour le moment comme une rÃ©gression sur une variable catÃ©gorielle.\n\npl_aov &lt;- aov(Petal.Length ~ Species, iris)\nsummary(pl_aov)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  437.1  218.55    1180 &lt;2e-16 ***\nResiduals   147   27.2    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa prochaine section, justement, est vouÃ©e aux modÃ¨les statistiques explicatifs, qui incluent la rÃ©gression."
  },
  {
    "objectID": "07a-biostats.html#sec-bios-models",
    "href": "07a-biostats.html#sec-bios-models",
    "title": "7Â  Biostatistiques",
    "section": "\n7.8 Les modÃ¨les statistiques",
    "text": "7.8 Les modÃ¨les statistiques\nLa modÃ©lisation statistique consiste Ã  lier de maniÃ¨re explicite des variables de sortie \\(y\\) (ou variables-rÃ©ponse / variables dÃ©pendantes) Ã  des variables explicatives \\(x\\) (ou variables prÃ©dictives / indÃ©pendantes / covariables). Les variables-rÃ©ponse sont modÃ©lisÃ©es par une fonction des variables explicatives ou prÃ©dictives.\nPourquoi garder les termes explicatives et prÃ©dictives? Parce que les modÃ¨les statistiques (basÃ©s sur des donnÃ©es et non pas sur des mÃ©canismes) sont de deux ordres. Dâ€™abord, les modÃ¨les prÃ©dictifs sont conÃ§us pour prÃ©dire de maniÃ¨re fiable une ou plusieurs variables-rÃ©ponse Ã  partir des informations contenues dans les variables qui sont, dans ce cas, prÃ©dictives (par exemple : Les sÃ©ries temporelles au ?sec-temps). Lorsque lâ€™on dÃ©sire tester des hypothÃ¨ses pour Ã©valuer quelles variables expliquent la rÃ©ponse, on parlera de modÃ©lisation (et de variables) explicative. En infÃ©rence statistique, on Ã©valuera les corrÃ©lations entre les variables explicatives et les variables-rÃ©ponse. Un lien de corrÃ©lation nâ€™est pas un lien de causalitÃ©. Lâ€™infÃ©rence causale peut en revanche Ãªtre Ã©valuÃ©e par des modÃ¨les dâ€™Ã©quations structurelles.\nCette section couvre la modÃ©lisation explicative. Les variables qui contribuent Ã  crÃ©er les modÃ¨les peuvent Ãªtre de diffÃ©rentes natures et distribuÃ©es selon diffÃ©rentes lois de probabilitÃ©. Alors que les modÃ¨les linÃ©aires simples (lm) impliquent une variable-rÃ©ponse distribuÃ©e de maniÃ¨re continue, les modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s peuvent aussi expliquer des variables de sorties discrÃ¨tes.\nDans les deux cas, on distinguera les variables fixes et les variables alÃ©atoires. Les variables fixes sont les variables testÃ©es lors de lâ€™expÃ©rience: dose du traitement, espÃ¨ce/cultivar, mÃ©tÃ©o, etc. Les variables alÃ©atoires sont les sources de variation qui gÃ©nÃ¨rent du bruit dans le modÃ¨le: les unitÃ©s expÃ©rimentales ou le temps lors de mesures rÃ©pÃ©tÃ©es. Les modÃ¨les incluant des effets fixes seulement sont des modÃ¨les Ã  effets fixes. GÃ©nÃ©ralement, les modÃ¨les incluant des variables alÃ©atoires incluent aussi des variables fixes: on parlera alors de modÃ¨les mixtes. Nous couvrirons ces deux types de modÃ¨les.\n\n7.8.1 ModÃ¨les Ã  effets fixes\nLes tests de t et de Wilcoxon, explorÃ©s prÃ©cÃ©demment, sont des modÃ¨les statistiques Ã  une seule variable. Nous avons vu dans lâ€™interface-formule quâ€™une variable-rÃ©ponse peut Ãªtre liÃ©e Ã  une variable explicative avec le tilde ~. En particulier, le test de t est une rÃ©gression linÃ©aire univariÃ©e (Ã  une seule variable explicative) dont la variable explicative comprend deux catÃ©gories. De mÃªme, lâ€™anova est une rÃ©gression linÃ©aire univariÃ©e dont la variable explicative comprend plusieurs catÃ©gories. Or lâ€™interface-formule peut Ãªtre utilisÃ© dans plusieurs circonstances, notamment pour ajouter plusieurs variables de diffÃ©rents types: on parlera de rÃ©gression multivariÃ©e.\nLa plupart des modÃ¨les statistiques peuvent Ãªtre approximÃ©s comme une combinaison linÃ©aire de variables: ce sont des modÃ¨les linÃ©aires. Les modÃ¨les non-linÃ©aires impliquent des stratÃ©gies computationnelles complexes qui rendent leur utilisation plus difficile Ã  manÅ“uvrer.\nUn modÃ¨le linÃ©aire univariÃ© prendra la forme \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), oÃ¹ \\(\\beta_0\\) est lâ€™intercept et \\(\\beta_1\\) est la pente et \\(\\epsilon\\) est lâ€™erreur.\nVous verrez parfois la notation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\). La notation avec le chapeau \\(\\hat{y}\\) exprime quâ€™il sâ€™agit des valeurs gÃ©nÃ©rÃ©es par le modÃ¨le. En fait, \\(y = \\hat{y} - \\epsilon\\).\n\n7.8.1.1 ModÃ¨le linÃ©aire univariÃ© avec variable continue\nPrenons les donnÃ©es lasrosas.corn incluses dans le module agridat, oÃ¹ lâ€™on retrouve le rendement dâ€™une production de maÃ¯s Ã  dose dâ€™azote variable, en Argentine.\n\nlibrary(\"agridat\")\ndata(\"lasrosas.corn\")\nslice_sample(lasrosas.corn, n = 10)\n\n   year       lat      long yield nitro topo     bv rep nf\n1  1999 -33.05207 -63.84230 69.57   0.0   LO 185.67  R1 N0\n2  1999 -33.05137 -63.84383 67.41  53.0    E 175.12  R2 N2\n3  1999 -33.05104 -63.84323 68.33  29.0   LO 168.70  R3 N1\n4  1999 -33.05162 -63.84456 68.06  53.0    E 171.71  R1 N2\n5  1999 -33.05180 -63.84386 63.99   0.0   LO 172.46  R1 N0\n6  2001 -33.05065 -63.84578 35.85  50.6   HT 194.85  R1 N2\n7  1999 -33.05170 -63.84553 58.89 131.5   HT 187.98  R1 N5\n8  2001 -33.05077 -63.84502 50.95 124.6   HT 184.66  R2 N5\n9  1999 -33.05181 -63.84202 78.75 106.0   LO 169.25  R2 N4\n10 1999 -33.05154 -63.84468 68.58  29.0    E 169.35  R1 N1\n\n\nCes donnÃ©es comprennent plusieurs variables. Prenons le rendement (yield) comme variable de sortie et, pour le moment, ne retenons que la dose dâ€™azote (nitro) comme variable explicative: il sâ€™agit dâ€™une rÃ©gression univariÃ©e. Les deux variables sont continues. Explorons dâ€™abord le nuage de points de lâ€™une et lâ€™autre.\n\nggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) +\n    geom_point()\n\n\n\n\nLâ€™hypothÃ¨se nulle est que la dose dâ€™azote nâ€™affecte pas le rendement, câ€™est Ã  dire que le coefficient de pente et nul. Une autre hypothÃ¨se est que lâ€™intercept est nul, câ€™est Ã  dire quâ€™Ã  une dose de 0, le rendement est de 0. Un modÃ¨le linÃ©aire Ã  variable de sortie continue est crÃ©Ã© avec la fonction lm(), pour linear model.\n\nmodlin_1 &lt;- lm(yield ~ nitro, data = lasrosas.corn)\nsummary(modlin_1)\n\n\nCall:\nlm(formula = yield ~ nitro, data = lasrosas.corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.843213   0.608573 108.193  &lt; 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,   Adjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n\n\nLe diagnostic du modÃ¨le comprend plusieurs informations. Dâ€™abord, la formule utilisÃ©e est affichÃ©e pour la traÃ§abilitÃ©. Vient ensuite un aperÃ§u de la distribution des rÃ©sidus. La mÃ©diane devrait sâ€™approcher de la moyenne des rÃ©sidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en considÃ©ration lâ€™Ã©chelle de y, et ce -3.079 est exprimÃ© en terme de rendement, ici en quintaux (i.e.Â 100 kg) par hectare. La distribution des rÃ©sidus mÃ©rite dâ€™Ãªtre davantage investiguÃ©e. Nous verrons cela un peu plus tard.\nLes coefficients apparaissent ensuite. Les estimÃ©s sont les valeurs des effets. R fournit aussi lâ€™erreur standard associÃ©e, la valeur de t ainsi que la p-value (la probabilitÃ© dâ€™obtenir cet effet ou un effet plus extrÃªme si en rÃ©alitÃ© il y avait absence dâ€™effet). Lâ€™intercept est bien sÃ»r plus Ã©levÃ© que 0 (Ã  dose nulle, on obtient un rendement de 65.8 quintaux par hectare en moyenne). La pente de la variable nitro est de ~0.06: pour chaque augmentation dâ€™un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de maÃ¯s. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que lâ€™intercept. Soulignons que lâ€™ampleur du coefficient est trÃ¨s important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait quâ€™elle est infÃ©rieure Ã  0.05 (ce qui arrive souvent dans la littÃ©rature), serait trÃ¨s insuffisant pour lâ€™interprÃ©tation des statistiques. La p-value nous indique nÃ©anmoins quâ€™il serait trÃ¨s improbable quâ€™une telle pente ait Ã©tÃ© gÃ©nÃ©rÃ©e alors que celle-ci est nulle en rÃ©alitÃ©. Les Ã©toiles Ã  cÃ´tÃ© des p-values indiquent lâ€™ampleur selon lâ€™Ã©chelle Signif. codes indiquÃ©e en-dessous du tableau des coefficients.\nSous ce tableau, R offre dâ€™autres statistiques. En outre, les RÂ² et RÂ² ajustÃ©s indiquent si la rÃ©gression passe effectivement par les points. Le RÂ² prend un maximum de 1 lorsque la droite passe exactement sur les points.\nEnfin, le test de F gÃ©nÃ¨re une p-value indiquant la probabilitÃ© que les coefficients de pente ait Ã©tÃ© gÃ©nÃ©rÃ©s si les vrais coefficients Ã©taient nuls. Dans le cas dâ€™une rÃ©gression univariÃ©e, cela rÃ©pÃ¨te lâ€™information sur lâ€™unique coefficient.\nOn pourra Ã©galement obtenir les intervalles de confiance avec la fonction confint().\n\nconfint(modlin_1, level = 0.95)\n\n                  2.5 %      97.5 %\n(Intercept) 64.65001137 67.03641474\nnitro        0.04629164  0.07714271\n\n\nOu soutirer lâ€™information de diffÃ©rentes maniÃ¨res, comme avec la fonction coefficients().\n\ncoefficients(modlin_1)\n\n(Intercept)       nitro \n65.84321305  0.06171718 \n\n\nÃ‰galement, on pourra exÃ©cuter le modÃ¨le sur les donnÃ©es qui ont servi Ã  le gÃ©nÃ©rer:\n\npredict(modlin_1)[1:5]\n\n       1        2        3        4        5 \n73.95902 73.95902 73.95902 73.95902 73.95902 \n\n\nOu sur des donnÃ©es externes.\n\nnouvelles_donnees &lt;- data.frame(nitro = seq(from = 0, to = 100, by = 5))\npredict(modlin_1, newdata = nouvelles_donnees)[1:5]\n\n       1        2        3        4        5 \n65.84321 66.15180 66.46038 66.76897 67.07756 \n\n\n\n7.8.1.2 Analyse des rÃ©sidus\nLes rÃ©sidus sont les erreurs du modÃ¨le. Câ€™est le vecteur \\(\\epsilon\\), qui est un dÃ©calage entre les donnÃ©es et le modÃ¨le. Le RÂ² est un indicateur de lâ€™ampleur du dÃ©calage, mais une rÃ©gression linÃ©aire explicative en bonne et due forme devrait Ãªtre accompagnÃ©e dâ€™une analyse des rÃ©sidus. On peut les calculer par \\(\\epsilon = y - \\hat{y}\\), ou alors simplement utiliser la fonction residuals().\n\nres_df &lt;- data.frame(nitro = lasrosas.corn$nitro,\n                     residus_lm = residuals(modlin_1),\n                     residus_calcul = lasrosas.corn$yield - predict(modlin_1))\nslice_sample(res_df, n = 10)\n\n     nitro residus_lm residus_calcul\n2931 124.6  24.666827      24.666827\n1793 124.6  11.126827      11.126827\n2006  99.8  25.417413      25.417413\n116   66.0 -11.636547     -11.636547\n1235 131.5  11.460978      11.460978\n2426  75.4 -18.686688     -18.686688\n1132  29.0  -1.763011      -1.763011\n15   131.5 -11.289022     -11.289022\n1691 131.5  -5.639022      -5.639022\n38   131.5 -13.129022     -13.129022\n\n\nDans une bonne rÃ©gression linÃ©aire, on ne retrouvera pas de structure identifiable dans les rÃ©sidus, câ€™est-Ã -dire que les rÃ©sidus sont bien distribuÃ©s de part et dâ€™autre du modÃ¨le de rÃ©gression.\n\nggplot(res_df, aes(x = nitro, y = residus_lm)) +\n  geom_point() +\n  labs(x = \"Dose N\", y = \"RÃ©sidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\n\n\n\n\nBien que le jugement soit subjectif, on peut dire avec confiance quâ€™il nâ€™y a pas structure particuliÃ¨re. En revanche, on pourrait gÃ©nÃ©rer un \\(y\\) qui varie de maniÃ¨re quadratique avec \\(x\\), un modÃ¨le linÃ©aire montrera une structure Ã©vidente.\n\nset.seed(36164)\nx &lt;- 0:100\ny &lt;- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50)\nmodlin_2 &lt;- lm(y ~ x)\nggplot(data.frame(y, residus = residuals(modlin_2)),\n       aes(x = x, y = residus)) +\n  geom_point() +\n  labs(x = \"x\", y = \"RÃ©sidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\n\n\n\nÃ‰galement, les rÃ©sidus ne devraient pas croÃ®tre avec \\(x\\).\n\nset.seed(3984)\nx &lt;- 0:100\ny &lt;-  10 + x + x * rnorm(length(x), 0, 2)\nmodlin_3 &lt;- lm(y ~ x)\nggplot(data.frame(x, residus = residuals(modlin_3)),\n       aes(x = x, y = residus)) +\n  geom_point() +\n  labs(x = \"x\", y = \"RÃ©sidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\n\n\n\nOn pourra aussi inspecter les rÃ©sidus avec un graphique de leur distribution. Reprenons notre modÃ¨le de rendement du maÃ¯s.\n\nggplot(res_df, aes(x = residus_lm)) +\n  geom_histogram(binwidth = 2, color = \"white\") +\n  labs(x = \"Residual\")\n\n\n\n\nLâ€™histogramme devrait prÃ©senter une distribution normale. Les tests de normalitÃ© comme le test de Shapiro-Wilk peuvent aider, mais ils sont gÃ©nÃ©ralement trÃ¨s sÃ©vÃ¨res.\n\nshapiro.test(res_df$residus_lm)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res_df$residus_lm\nW = 0.94868, p-value &lt; 2.2e-16\n\n\nLâ€™hypothÃ¨se nulle que la distribution est normale est rejetÃ©e au seuil 0.05. Dans notre cas, il est Ã©vident que la sÃ©vÃ©ritÃ© du test nâ€™est pas en cause car les rÃ©sidus semblent gÃ©nÃ©rer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilitÃ© de la variable-rÃ©ponse.\n\n7.8.1.3 RÃ©gression multiple\nComme câ€™est le cas pour bien des phÃ©nomÃ¨nes en Ã©cologie, le rendement dâ€™une culture nâ€™est certainement pas expliquÃ© seulement par la dose dâ€™azote.\nLorsque lâ€™on combine plusieurs variables explicatives, on crÃ©e un modÃ¨le de rÃ©gression multivariÃ©e, ou une rÃ©gression multiple. Bien que les tendances puissent sembler non-linÃ©aires, lâ€™ajout de variables et le calcul des coefficients associÃ©s reste un problÃ¨me dâ€™algÃ¨bre linÃ©aire.\nOn pourra en effet gÃ©nÃ©raliser les modÃ¨les linÃ©aires, univariÃ©s et multivariÃ©s, de la maniÃ¨re suivante.\n\\[ y = X \\beta + \\epsilon \\]\noÃ¹:\n\\(X\\) est la matrice du modÃ¨le Ã  \\(n\\) observations et \\(p\\) variables.\n\\[ X = \\left( \\begin{matrix}\n1 & x_{11} & \\cdots & x_{1p}  \\\\\n1 & x_{21} & \\cdots & x_{2p}  \\\\\n\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n1 & x_{n1} & \\cdots & x_{np}\n\\end{matrix} \\right) \\]\n\\(\\beta\\) est la matrice des \\(p\\) coefficients, \\(\\beta_0\\) Ã©tant lâ€™intercept qui multiplie la premiÃ¨re colonne de la matrice \\(X\\).\n\\[ \\beta = \\left( \\begin{matrix}\n\\beta_0  \\\\\n\\beta_1  \\\\\n\\vdots \\\\\n\\beta_p\n\\end{matrix} \\right) \\]\n\\(\\epsilon\\) est lâ€™erreur de chaque observation.\n\\[ \\epsilon = \\left( \\begin{matrix}\n\\epsilon_0  \\\\\n\\epsilon_1  \\\\\n\\vdots \\\\\n\\epsilon_n\n\\end{matrix} \\right) \\]\n\n7.8.1.4 ModÃ¨les linÃ©aires univariÃ©s avec variable catÃ©gorielle nominale\n\nUne variable catÃ©gorielle nominale (non ordonnÃ©e) utilisÃ©e Ã  elle seule dans un modÃ¨le comme variable explicative, est un cas particulier de rÃ©gression multiple. En effet, lâ€™encodage catÃ©goriel (ou dummyfication) transforme une variable catÃ©gorielle nominale en une matrice de modÃ¨le comprenant une colonne dÃ©signant lâ€™intercept (une sÃ©rie de 1) dÃ©signant la catÃ©gorie de rÃ©fÃ©rence, ainsi que des colonnes pour chacune des autres catÃ©gories dÃ©signant lâ€™appartenance (1) ou la non appartenance (0) de la catÃ©gorie dÃ©signÃ©e par la colonne.\n\n7.8.1.4.1 Lâ€™encodage catÃ©goriel\nUne variable Ã  \\(C\\) catÃ©gories pourra Ãªtre dÃ©clinÃ©e en \\(C\\) variables dont chaque colonne dÃ©signe par un 1 lâ€™appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour lâ€™exemple, crÃ©ons un vecteur dÃ©signant le cultivar de pomme de terre.\n\ndata &lt;- data.frame(cultivar = factor(c('Superior', 'Superior', 'Superior', 'Russet', 'Kenebec', 'Russet')))\nmodel.matrix(~cultivar, data)\n\n  (Intercept) cultivarRusset cultivarSuperior\n1           1              0                1\n2           1              0                1\n3           1              0                1\n4           1              1                0\n5           1              0                0\n6           1              1                0\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$cultivar\n[1] \"contr.treatment\"\n\n\nNous avons trois catÃ©gories, encodÃ©es en trois colonnes. La premiÃ¨re colonne est un intercept et les deux autres dÃ©crivent lâ€™absence (0) ou la prÃ©sence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que lâ€™appartenance Ã  une catÃ©gorie est mutuellement exclusive, câ€™est-Ã -dire quâ€™un Ã©chantillon ne peut Ãªtre assignÃ© quâ€™Ã  une seule catÃ©gorie, on peut dÃ©duire une catÃ©gorie Ã  partir de lâ€™information sur toutes les autres. Par exemple, si cultivar_Russet et cultivar_Superior sont toutes deux Ã©gales Ã  \\(0\\), on conclura que cultivar_Kenebec est nÃ©cessairement Ã©gal Ã  \\(1\\). Et si lâ€™un dâ€™entre cultivar_Russet et cultivar_Superior est Ã©gal Ã  \\(1\\), cultivar_Kenebec est nÃ©cessairement Ã©gal Ã  \\(0\\). Lâ€™information contenue dans un nombre \\(C\\) de catÃ©gories peut Ãªtre encodÃ©e dans un nombre \\(C-1\\) de colonnes. Câ€™est pourquoi, dans une analyse statistique, on dÃ©signera une catÃ©gorie comme une rÃ©fÃ©rence, que lâ€™on dÃ©tecte lorsque toutes les autres catÃ©gories sont encodÃ©es avec des \\(0\\): cette rÃ©fÃ©rence sera incluse dans lâ€™intercept. La catÃ©gorie de rÃ©fÃ©rence par dÃ©faut en R est la premiÃ¨re catÃ©gorie dans lâ€™ordre alphabÃ©tique. On pourra modifier cette rÃ©fÃ©rence avec la fonction relevel().\n\ndata$cultivar &lt;- relevel(data$cultivar, ref = \"Superior\")\nmodel.matrix(~cultivar, data)\n\n  (Intercept) cultivarKenebec cultivarRusset\n1           1               0              0\n2           1               0              0\n3           1               0              0\n4           1               0              1\n5           1               1              0\n6           1               0              1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$cultivar\n[1] \"contr.treatment\"\n\n\nPour certains modÃ¨les, vous devrez vous assurer vous-mÃªme de lâ€™encodage catÃ©goriel. Pour dâ€™autre, en particulier avec lâ€™interface par formule de R, ce sera fait automatiquement.\n\n7.8.1.4.2 Exemple dâ€™application\nPrenons la topographie du terrain, qui peut prendre plusieurs niveaux.\n\nlevels(lasrosas.corn$topo)\n\n[1] \"E\"  \"HT\" \"LO\" \"W\" \n\n\nExplorons le rendement selon la topographie.\n\nggplot(lasrosas.corn, aes(x = topo, y = yield)) +\n    geom_boxplot()\n\n\n\n\nLes diffÃ©rences sont Ã©videntes, et la modÃ©lisation devrait montrer des effets diffÃ©rents.\nLâ€™encodage catÃ©goriel peut Ãªtre visualisÃ© en gÃ©nÃ©rant la matrice de modÃ¨le avec la fonction model.matrix() et lâ€™interface-formule - sans la variable-rÃ©ponse.\n\nmodel.matrix(~ topo, data = lasrosas.corn) |&gt; \n    as_tibble() |&gt;  # as_tibble pour transformer la matrice en tableau\n    slice_sample(n = 10)\n\n# A tibble: 10 Ã— 4\n   `(Intercept)` topoHT topoLO topoW\n           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1             1      0      0     0\n 2             1      0      1     0\n 3             1      0      0     1\n 4             1      0      0     1\n 5             1      0      1     0\n 6             1      0      0     1\n 7             1      1      0     0\n 8             1      0      1     0\n 9             1      0      0     1\n10             1      0      0     0\n\n\nDans le cas dâ€™un modÃ¨le avec une variable catÃ©gorielle nominale seule, lâ€™intercept reprÃ©sente la catÃ©gorie de rÃ©fÃ©rence, ici E. Les autres colonnes spÃ©cifient lâ€™appartenance (1) ou la non-appartenance (0) de la catÃ©gorie pour chaque observation.\nCette matrice de modÃ¨le utilisÃ©e pour la rÃ©gression donnera un intercept, qui indiquera lâ€™effet de la catÃ©gorie de rÃ©fÃ©rence, puis les diffÃ©rences entre les catÃ©gories subsÃ©quentes et la catÃ©gorie de rÃ©fÃ©rence.\n\nmodlin_4 &lt;- lm(yield ~ topo, data = lasrosas.corn)\nsummary(modlin_4)\n\n\nCall:\nlm(formula = yield ~ topo, data = lasrosas.corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.371 -11.933  -1.593  11.080  44.119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  78.6653     0.5399 145.707   &lt;2e-16 ***\ntopoHT      -30.0526     0.7500 -40.069   &lt;2e-16 ***\ntopoLO        6.2832     0.7293   8.615   &lt;2e-16 ***\ntopoW       -11.8841     0.7039 -16.883   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.59 on 3439 degrees of freedom\nMultiple R-squared:  0.4596,    Adjusted R-squared:  0.4591 \nF-statistic:   975 on 3 and 3439 DF,  p-value: &lt; 2.2e-16\n\n\nLe modÃ¨le linÃ©aire est Ã©quivalent Ã  lâ€™anova, mais les rÃ©sultats de lm sont plus Ã©laborÃ©s.\n\nsummary(aov(yield ~ topo, data = lasrosas.corn))\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \ntopo           3 622351  207450     975 &lt;2e-16 ***\nResiduals   3439 731746     213                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLâ€™analyse de rÃ©sidus peut Ãªtre effectuÃ©e de la mÃªme maniÃ¨re.\n\n7.8.1.5 ModÃ¨les linÃ©aires univariÃ©s avec variable catÃ©gorielle ordinale\n\nBien que jâ€™introduise la rÃ©gression sur variable catÃ©gorielle ordinale Ã  la suite de la section sur les variables nominales, nous revenons dans ce cas Ã  une rÃ©gression simple, univariÃ©e. Voyons un cas Ã  5 niveaux.\n\nstatut &lt;- c(\"Totalement en dÃ©saccord\",\n            \"En dÃ©saccord\",\n            \"Ni en accord, ni en dÃ©saccord\",\n            \"En accord\",\n            \"Totalement en accord\")\nstatut_o &lt;- factor(statut, levels = statut, ordered=TRUE)\nmodel.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) oÃ¹ 5 est le nombre de niveaux\n\n  (Intercept)    statut_o.L statut_o.Q    statut_o.C statut_o^4\n1           1 -6.324555e-01  0.5345225 -3.162278e-01  0.1195229\n2           1 -3.162278e-01 -0.2672612  6.324555e-01 -0.4780914\n3           1 -3.510833e-17 -0.5345225  1.755417e-16  0.7171372\n4           1  3.162278e-01 -0.2672612 -6.324555e-01 -0.4780914\n5           1  6.324555e-01  0.5345225  3.162278e-01  0.1195229\nattr(,\"assign\")\n[1] 0 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$statut_o\n[1] \"contr.poly\"\n\n\nLa matrice de modÃ¨le a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres dÃ©signant diffÃ©rentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils linÃ©airement? De maniÃ¨re quadratique, cubique ou plus loin dans des distributions polynomiales?\n\nmodmat_tidy &lt;- data.frame(statut, model.matrix(~statut_o)[, -1]) |&gt; \n    pivot_longer(-statut, names_to = \"variable\", values_to = \"valeur\")\nmodmat_tidy$statut &lt;- factor(modmat_tidy$statut,\n                             levels = statut,\n                             ordered=TRUE)\nggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) +\n    facet_wrap(. ~ variable) +\n    geom_point() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nRÃ¨gle gÃ©nÃ©rale, pour les variables ordinales, on prÃ©fÃ©rera une distribution linÃ©aire, et câ€™est lâ€™option par dÃ©faut de la fonction lm(). Lâ€™utilisation dâ€™une autre distribution peut Ãªtre effectuÃ©e Ã  la mitaine en utilisant dans le modÃ¨le la colonne dÃ©sirÃ©e de la sortie de la fonction model.matrix().\n\n7.8.1.6 RÃ©gression multiple Ã  plusieurs variables\nReprenons le tableau de donnÃ©es du rendement de maÃ¯s.\n\nhead(lasrosas.corn)\n\n  year       lat      long yield nitro topo     bv rep nf\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5\n\n\nPour ajouter des variables au modÃ¨le dans lâ€™interface-formule, on additionne les noms de colonne. La variable lat dÃ©signe la latitude, la variable long dÃ©signe la longitude et la variable bv (brightness value) dÃ©signe la teneur en matiÃ¨re organique du sol (plus bv est Ã©levÃ©e, plus faible est la teneur en matiÃ¨re organique).\n\nmodlin_5 &lt;- lm(yield ~ lat + long + nitro + topo + bv,\n               data = lasrosas.corn)\nsummary(modlin_5)\n\n\nCall:\nlm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.405 -11.071  -1.251  10.592  40.078 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.946e+05  3.309e+04   5.882 4.45e-09 ***\nlat          5.541e+03  4.555e+02  12.163  &lt; 2e-16 ***\nlong         1.776e+02  4.491e+02   0.395    0.693    \nnitro        6.867e-02  5.451e-03  12.597  &lt; 2e-16 ***\ntopoHT      -2.665e+01  1.087e+00 -24.520  &lt; 2e-16 ***\ntopoLO       5.565e+00  1.035e+00   5.378 8.03e-08 ***\ntopoW       -1.465e+01  1.655e+00  -8.849  &lt; 2e-16 ***\nbv          -5.089e-01  3.069e-02 -16.578  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.47 on 3435 degrees of freedom\nMultiple R-squared:  0.5397,    Adjusted R-squared:  0.5387 \nF-statistic: 575.3 on 7 and 3435 DF,  p-value: &lt; 2.2e-16\n\n\nLâ€™ampleur des coefficients est relatif Ã  lâ€™Ã©chelle de la variable. En effet, un coefficient de 5541 sur la variable lat nâ€™est pas comparable au coefficient de la variable bv, de -0.5089, Ã©tant donnÃ© que les variables ne sont pas exprimÃ©es avec la mÃªme Ã©chelle. Pour les comparer sur une mÃªme base, on peut centrer (soustraire la moyenne) et rÃ©duire (diviser par lâ€™Ã©cart-type).\n\nlasrosas.corn_sc &lt;- lasrosas.corn |&gt; \n  mutate_at(c(\"lat\", \"long\", \"nitro\", \"bv\"), scale)\n\nmodlin_5_sc &lt;- lm(yield ~ lat + long + nitro + topo + bv,\n               data = lasrosas.corn_sc)\nsummary(modlin_5_sc)\n\n\nCall:\nlm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.405 -11.071  -1.251  10.592  40.078 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  78.9114     0.6666 118.376  &lt; 2e-16 ***\nlat           3.9201     0.3223  12.163  &lt; 2e-16 ***\nlong          0.3479     0.8796   0.395    0.693    \nnitro         2.9252     0.2322  12.597  &lt; 2e-16 ***\ntopoHT      -26.6487     1.0868 -24.520  &lt; 2e-16 ***\ntopoLO        5.5647     1.0347   5.378 8.03e-08 ***\ntopoW       -14.6487     1.6555  -8.849  &lt; 2e-16 ***\nbv           -4.9253     0.2971 -16.578  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.47 on 3435 degrees of freedom\nMultiple R-squared:  0.5397,    Adjusted R-squared:  0.5387 \nF-statistic: 575.3 on 7 and 3435 DF,  p-value: &lt; 2.2e-16\n\n\nTypiquement, les variables catÃ©gorielles, qui ne sont pas mises Ã  lâ€™Ã©chelle, donneront des coefficients plus Ã©levÃ©es, et devrons Ãªtre Ã©valuÃ©es entre elles et non comparativement aux variables mises Ã  lâ€™Ã©chelle. Une maniÃ¨re conviviale de reprÃ©senter des coefficients consiste Ã  utiliser la fonction tidy du module broom, qui gÃ©nÃ¨re un tableau contenant les coefficients ainsi que leurs intervalles de confiance, que nous pourrons ensuite porter graphiquement.\n\nlibrary(\"broom\") # ou bien charger le mÃ©ta-module tidymodels\nintervals &lt;- tidy(modlin_5_sc, conf.int = TRUE, conf.level = 0.95)\nintervals\n\n# A tibble: 8 Ã— 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   78.9       0.667   118.    0            77.6      80.2 \n2 lat            3.92      0.322    12.2   2.34e- 33     3.29      4.55\n3 long           0.348     0.880     0.395 6.93e-  1    -1.38      2.07\n4 nitro          2.93      0.232    12.6   1.33e- 35     2.47      3.38\n5 topoHT       -26.6       1.09    -24.5   1.74e-122   -28.8     -24.5 \n6 topoLO         5.56      1.03      5.38  8.03e-  8     3.54      7.59\n7 topoW        -14.6       1.66     -8.85  1.39e- 18   -17.9     -11.4 \n8 bv            -4.93      0.297   -16.6   1.92e- 59    -5.51     -4.34\n\n\nLa valeur par dÃ©faut de lâ€™argument conf.level est de 0.95, mais je vous suggÃ¨re de toujours lâ€™Ã©crire de maniÃ¨re explicite, ne serait-ce que pour rappeler Ã  vous-mÃªme ainsi quâ€™Ã  vos collÃ¨gues que cette valeur est arbitraire: il sâ€™agit dâ€™une dÃ©cision dâ€™analyse, non pas dâ€™une valeur Ã  utiliser par convention.\nPour le graphique, on aura avantage Ã  sÃ©parer les effets catÃ©goriels aux effets numÃ©riques pour mieux interprÃ©ter leurs effets entre eux. Jâ€™utilise la fonction dplyr::case_when() pour crÃ©er une nouvelle colonne qui catÃ©gorisera les termes de lâ€™Ã©quation. Cette catÃ©gorie me permettra dâ€™effectuer un facet_wrap().\n\nintervals |&gt; \n  mutate(type = case_when(\n    term %in% c(\"topoHT\", \"topoLO\", \"topoW\") ~ \"CatÃ©gorie\", # condition ~ rÃ©sultat\n    term == \"(Intercept)\" ~ \"Intercept\",  # condition ~ rÃ©sultat\n    TRUE ~ \"numÃ©rique\" # pour toute autre condition (TRUE) ~ rÃ©sultat\n  )) |&gt; \n  ggplot(mapping = aes(x = estimate, y = term)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  geom_segment(mapping = aes(x = conf.low, xend = conf.high, yend = term)) +\n  geom_point() +\n  labs(x = \"Coefficient standardisÃ©\", y = \"\") +\n  facet_wrap(~type, scales = \"free\", ncol = 1, strip.position = \"right\")\n\n\n\n\nOn y voit quâ€™Ã  lâ€™exception de la variable long, tous les coefficients sont Ã©loignÃ©s de 0. Le coefficient bv est nÃ©gatif, indiquant que plus la valeur de bv est Ã©levÃ© (donc plus le sol est pauvre en matiÃ¨re organique), plus le rendement est faible. Plus la latitude est Ã©levÃ©e (plus on se dirige vers le Nord de lâ€™Argentine), plus le rendement est Ã©levÃ©. La dose dâ€™azote a aussi un effet statistique positif sur le rendement.\nQuant aux catÃ©gories topographiques, elles sont toutes Ã©loignÃ©es de la catÃ©gorie E, placÃ©e Ã  zÃ©ro. De plus, les intervalles de confiance Ã  0.95 ne se chevauchant pas, on peut conclure que la variabilitÃ© du phÃ©nomÃ¨ne Ã©chantillonnÃ© nâ€™est pas suffisante pour expliquer les diffÃ©rences importantes entre elles.\nOn pourra retrouver des cas oÃ¹ lâ€™effet combinÃ© de plusieurs variables diffÃ¨re de lâ€™effet des deux variables prises sÃ©parÃ©ment. Par exemple, on pourrait Ã©valuer lâ€™effet de lâ€™azote et celui de la topographie dans un mÃªme modÃ¨le, puis y ajouter une interaction entre lâ€™azote et la topographie, qui dÃ©finira des effets supplÃ©mentaires de lâ€™azote selon chaque catÃ©gorie topographique. Câ€™est ce que lâ€™on appelle une interaction.\nDans lâ€™interface-formule, lâ€™interaction entre lâ€™azote et la topographie est notÃ©e nitro:topo. Pour ajouter cette interaction, la formule deviendra yield ~ nitro + topo + nitro:topo. Une approche Ã©quivalente est dâ€™utiliser le raccourci yield ~ nitro * topo.\n\nmodlin_5_sc &lt;- lm(yield ~ nitro * topo,\n               data = lasrosas.corn_sc)\nsummary(modlin_5_sc)\n\n\nCall:\nlm(formula = yield ~ nitro * topo, data = lasrosas.corn_sc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.984 -11.985  -1.388  10.339  40.636 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   78.6999     0.5322 147.870  &lt; 2e-16 ***\nnitro          1.8131     0.5351   3.388 0.000711 ***\ntopoHT       -30.0052     0.7394 -40.578  &lt; 2e-16 ***\ntopoLO         6.2026     0.7190   8.627  &lt; 2e-16 ***\ntopoW        -11.9628     0.6939 -17.240  &lt; 2e-16 ***\nnitro:topoHT   1.2553     0.7461   1.682 0.092565 .  \nnitro:topoLO   0.5695     0.7186   0.792 0.428141    \nnitro:topoW    0.7702     0.6944   1.109 0.267460    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.38 on 3435 degrees of freedom\nMultiple R-squared:  0.4756,    Adjusted R-squared:  0.4746 \nF-statistic: 445.1 on 7 and 3435 DF,  p-value: &lt; 2.2e-16\n\n\nLes rÃ©sultats montrent des effets de lâ€™azote et des catÃ©gories topographiques, mais il y a davantage dâ€™incertitude sur les interactions, indiquant que lâ€™effet statistique de lâ€™azote est sensiblement le mÃªme indÃ©pendamment des niveaux topographiques.\nDans le cas des rÃ©gressions multiples, les rÃ©sidus ne peuvent pas Ãªtre prÃ©sentÃ©s selon une variable explicative \\(x\\), puisquâ€™il y en a plusieurs. On fera lâ€™analyse des rÃ©sidus selon la variable rÃ©ponse \\(y\\).\n\ntibble(\n  y = lasrosas.corn_sc$yield,\n  residus = residuals(modlin_5_sc)\n) |&gt; \n  ggplot(aes(x = y, y = residus)) +\n  geom_point() +\n  labs(x = \"y\", y = \"RÃ©sidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\n\n\n\nDans ce modÃ¨le, il y a clairement une structure qui nous Ã©chappe! Lâ€™ajout dâ€™autres variables nous permettrait Ã©ventuellement dâ€™obtenir une distribution qui sâ€™approche dâ€™un bruit.\n\n7.8.1.7 Les interactions\nUne interaction est un effet supplÃ©mentaire qui est investiguÃ© pour des combinaisons de variables. Lâ€™interaction entre lâ€™azote et la topographie est une nouvelle variable crÃ©Ã©e par la multiplication de lâ€™azote, une variable numÃ©rique, et de la topographie, qui ici est une variable catÃ©gorielle.\n\nmodel.matrix(~ nitro * topo, data = lasrosas.corn_sc) |&gt; head()\n\n  (Intercept)    nitro topoHT topoLO topoW nitro:topoHT nitro:topoLO\n1           1 1.571194      0      0     1            0            0\n2           1 1.571194      0      0     1            0            0\n3           1 1.571194      0      0     1            0            0\n4           1 1.571194      0      0     1            0            0\n5           1 1.571194      0      0     1            0            0\n6           1 1.571194      0      0     1            0            0\n  nitro:topoW\n1    1.571194\n2    1.571194\n3    1.571194\n4    1.571194\n5    1.571194\n6    1.571194\n\n\nLâ€™entÃªte de la matrice modÃ¨le montre que lâ€™interaction est lâ€™addition de trois variables, qui sont nulles si la catÃ©gorie topographique est absente, mais qui prend la dose dâ€™azote pour la catÃ©gorie prÃ©sente seulement.\nLâ€™interprÃ©tation dâ€™une interaction est spÃ©cifique au modÃ¨le utilisÃ©. Une maniÃ¨re de lâ€™interprÃ©ter est de se demander dans quelles unitÃ©s elle est exprimÃ©e. Dans notre exemple, il sâ€™agit de kg/ha standardisÃ©s.\nPrenons un autre exemple, cette fois-ci avec des donnÃ©es fictives. Une enquÃªte a Ã©tÃ© menÃ©e oÃ¹ des personnes Ã©valuaient le karma (Ã©chelle 0 Ã  10) de pieds nus, en bas (chaussettes) et/ou en sandales.\n\nkarma_df &lt;- read_csv(\"data/karma_df.csv\")\n\nRows: 600 Columns: 4\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (4): ID, sandales, bas, karma\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNous dÃ©sirons savoir quelle est lâ€™effet des bas et des sandales sur le karma, donc ğŸ’– ~ ğŸ‘¡ + ğŸ§¦.\n\ntidy(lm(karma ~ sandales + bas, karma_df))\n\n# A tibble: 3 Ã— 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.67     0.147      25.0 1.11e-94\n2 sandales        2.29     0.166      13.8 7.99e-38\n3 bas             1.77     0.168      10.5 6.31e-24\n\n\nÃ€ partir du scÃ©nario Ã  pieds nus dâ€™un karma de 3.67, les sandales ajoutent 2.29 de points de karma, alors que les bas en ajoutent 1.8. Mais ce modÃ¨le est incomplet, cas on nâ€™Ã©value pas lâ€™effet des bas ET des sandales, donc ğŸ’– ~ ğŸ‘¡ * ğŸ§¦.\n\ntidy(lm(karma ~ sandales * bas, karma_df))\n\n# A tibble: 4 Ã— 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      2.68     0.134      20.0 2.22e-68\n2 sandales         3.68     0.159      23.2 4.07e-85\n3 bas              3.20     0.162      19.8 1.40e-67\n4 sandales:bas    -5.25     0.309     -17.0 3.79e-53\n\n\nLe modÃ¨le est plus clair. Sans interaction, les effets sur le karma des bas et des sandales Ã©taient nÃ©gativement affectÃ©s par lâ€™effet dâ€™interaction sandales:bas, le karma Ã©tant poussÃ© Ã  la baisse par le bas blanc dans vos sandales.\nIl est possible dâ€™ajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a dâ€™interactions, plus votre modÃ¨le comprendra de variables et vos tests dâ€™hypothÃ¨se perdront en puissance statistique.\n\n7.8.1.8 Les modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s\nDans un modÃ¨le linÃ©aire ordinaire, un changement constant dans les variables explicatives rÃ©sulte en un changement constant de la variable-rÃ©ponse. Cette supposition ne serait pas adÃ©quate si la variable-rÃ©ponse Ã©tait un dÃ©compte, si elle est boolÃ©enne ou si, de maniÃ¨re gÃ©nÃ©rale, la variable-rÃ©ponse ne suivait pas une distribution continue. De maniÃ¨re plus spÃ©cifique, elle ne sâ€™applique pas aux cas oÃ¹ il nâ€™y a pas moyen de retrouver une distribution normale des rÃ©sidus. On pourra bien sÃ»r transformer les variables. Mais il pourrait sâ€™avÃ©rer impossible ou tout simplement non souhaitable de transformer les variables. Le modÃ¨le linÃ©aire gÃ©nÃ©ralisÃ© (MLG, ou generalized linear model - GLM) est une gÃ©nÃ©ralisation du modÃ¨le linÃ©aire ordinaire chez qui la variable-rÃ©ponse peut Ãªtre caractÃ©risÃ© par une distribution de Poisson, de Bernouilli, etc.\nPrenons dâ€™abord le cas dâ€™un dÃ©compte de vers fil-de-fer (worms) retrouvÃ©s dans des parcelles sous diffÃ©rents traitements (trt). Les dÃ©comptes sont typiquement distribuÃ©s selon une loi de Poisson.\n\ncochran.wireworms |&gt; ggplot(aes(x = worms)) + geom_histogram(bins = 10)\n\n\n\n\nExplorons les dÃ©comptes selon les traitements.\n\ncochran.wireworms |&gt; ggplot(aes(x = trt, y = worms)) + geom_boxplot()\n\n\n\n\nLes traitements semblent Ã  premiÃ¨re vue avoir un effet comparativement au contrÃ´le. LanÃ§ons un MLG avec la fonction glm(), et spÃ©cifions que la sortie est une distribution de Poisson. Bien que la fonction de lien (link = \"log\") soit explictement imposÃ©e, le log est la valeur par dÃ©faut pour les distributions de Poisson. Ainsi, les coefficients du modÃ¨les devront Ãªtre interprÃ©tÃ©s selon un modÃ¨le \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\).\n\nmodglm_1 &lt;- glm(worms ~ trt, cochran.wireworms, family = stats::poisson(link=\"log\"))\nsummary(modglm_1)\n\n\nCall:\nglm(formula = worms ~ trt, family = stats::poisson(link = \"log\"), \n    data = cochran.wireworms)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.1823     0.4082   0.447 0.655160    \ntrtM          1.6422     0.4460   3.682 0.000231 ***\ntrtN          1.7636     0.4418   3.991 6.57e-05 ***\ntrtO          1.5755     0.4485   3.513 0.000443 ***\ntrtP          1.3437     0.4584   2.931 0.003375 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 64.555  on 24  degrees of freedom\nResidual deviance: 38.026  on 20  degrees of freedom\nAIC: 125.64\n\nNumber of Fisher Scoring iterations: 5\n\n\nLâ€™interprÃ©tation spÃ©cifique des coefficients dâ€™une rÃ©gression de Poisson doit passer par la fonction de lien \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). Le traitement de rÃ©fÃ©rence (K), qui correspond Ã  lâ€™intercept, sera accompagnÃ© dâ€™un nombre de vers de \\(exp \\left(0.1823\\right) = 1.20\\) vers, et le traitement M, Ã  \\(exp \\left(1.6422\\right) = 5.17\\) vers. Cela correspond Ã  ce que lâ€™on observe sur les boxplots plus haut.\nIl est trÃ¨s probable (p-value de ~0.66) quâ€™un intercept (traitement K) de 0.18 ayant une erreur standard de 0.4082 ait Ã©tÃ© gÃ©nÃ©rÃ© depuis une population dont lâ€™intercept est nul. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils Ãªtre considÃ©rÃ©s comme Ã©quivalents?\n\nintervals &lt;- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l'intercept\n                    LL = confint(modglm_1)[, 1], # [-1, ] enlever la premiÃ¨re ligne, celle de l'intercept\n                    UL = confint(modglm_1)[, 2],\n                    variable = names(coefficients(modglm_1)))\n\nWaiting for profiling to be done...\nWaiting for profiling to be done...\n\nintervals\n\n# A tibble: 5 Ã— 4\n  Estimate     LL    UL variable   \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n1    0.182 -0.740 0.888 (Intercept)\n2    1.64   0.840 2.62  trtM       \n3    1.76   0.972 2.74  trtN       \n4    1.58   0.766 2.56  trtO       \n5    1.34   0.509 2.34  trtP       \n\n\n\nggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) +\n    geom_vline(xintercept = 0, lty = 2) +\n    geom_segment(mapping = aes(x = LL, xend = UL,\n                               y = variable, yend = variable)) +\n    geom_point() +\n    labs(x = \"Coefficient\", y = \"\")\n\n\n\n\nLes intervalles de confiance se superposant, on ne peut pas conclure quâ€™un traitement est liÃ© Ã  une rÃ©duction plus importante de vers quâ€™un autre, au seuil 0.05.\nMaintenant, Ã  dÃ©faut de trouver un tableau de donnÃ©es plus appropriÃ©, prenons le tableau mtcars, qui rassemble des donnÃ©es sur des modÃ¨les de voitures. La colonne vs, pour v-shaped, inscrit 0 si les pistons sont droits et 1 sâ€™ils sont placÃ©s en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du vÃ©hicule (wt)?\n\nmtcars |&gt; slice_sample(n = 6)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat 128          32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nVolvo 142E        21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nChrysler Imperial 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n\n\n\nmtcars |&gt; \n    ggplot(aes(x = wt, y = vs)) + geom_point()\n\n\n\n\nIl semble y avoir une tendance: les vÃ©hicules plus lourds ont plutÃ´t des pistons droits (vs = 0). VÃ©rifions cela.\n\nmodglm_2 &lt;- glm(vs ~ wt, data = mtcars, family = stats::binomial())\nsummary(modglm_2)\n\n\nCall:\nglm(formula = vs ~ wt, family = stats::binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   5.7147     2.3014   2.483  0.01302 * \nwt           -1.9105     0.7279  -2.625  0.00867 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.860  on 31  degrees of freedom\nResidual deviance: 31.367  on 30  degrees of freedom\nAIC: 35.367\n\nNumber of Fisher Scoring iterations: 5\n\n\nExercice. Analyser les rÃ©sultats.\n\n7.8.1.9 Les modÃ¨les non-linÃ©aires\nLa hauteur dâ€™un arbre en fonction du temps nâ€™est typiquement pas linÃ©aire. Elle tend Ã  croÃ®tre de plus en plus lentement jusquâ€™Ã  un plateau. De mÃªme, le rendement dâ€™une culture traitÃ©e avec des doses croissantes de fertilisants tend Ã  atteindre un maximum, puis Ã  se stabiliser.\nCes phÃ©nomÃ¨nes ne peuvent pas Ãªtre approximÃ©s par des modÃ¨les linÃ©aires. Examinons les donnÃ©es du tableau engelstad.nitro.\n\nengelstad.nitro |&gt; slice_sample(n = 10)\n\n         loc year nitro yield\n1  Knoxville 1966     0  63.0\n2  Knoxville 1965   335  61.2\n3    Jackson 1965   335  73.0\n4    Jackson 1966   201  61.3\n5    Jackson 1966   335  59.8\n6  Knoxville 1964     0  60.9\n7  Knoxville 1964    67  75.9\n8    Jackson 1966    67  45.2\n9    Jackson 1962   201  73.1\n10   Jackson 1964   335  67.8\n\n\n\nengelstad.nitro |&gt; \n    ggplot(aes(x = nitro, y = yield)) +\n        facet_grid(year ~ loc) +\n        geom_line() +\n        geom_point()\n\n\n\n\nLe modÃ¨le de Mitscherlich pourrait Ãªtre utilisÃ©.\n\\[ y = A \\left( 1 - e^{-R \\left( E + x \\right)} \\right) \\]\noÃ¹ \\(y\\) est le rendement, \\(x\\) est la dose, \\(A\\) est lâ€™asymptote vers laquelle la courbe converge Ã  dose croissante, \\(E\\) est lâ€™Ã©quivalent de dose fourni par lâ€™environnement et \\(R\\) est le taux de rÃ©ponse.\nExplorons la fonction.\n\nmitscherlich_f &lt;- function(x, A, E, R) {\n    A * (1 - exp(-R*(E + x)))\n}\n\nx &lt;- seq(0, 350, by = 5)\ny &lt;- mitscherlich_f(x, A = 75, E = 30, R = 0.02)\n\nggplot(tibble(x, y), aes(x, y)) +\n    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +\n    geom_line() + ylim(c(0, 100))\n\n\n\n\nExercice. Changez les paramÃ¨tres pour visualiser comment la courbe rÃ©agit.\nNous pouvons dÃ©crire le modÃ¨le grÃ¢ce Ã  lâ€™interface formule dans la fonction nls(). Notez que les modÃ¨les non-linÃ©aires demandent des stratÃ©gies de calcul diffÃ©rentes de celles des modÃ¨les linÃ©aires. En tout temps, nous devons identifier des valeurs de dÃ©part raisonnables pour les paramÃ¨tres dans lâ€™argument start. Vous rÃ©ussirez rarement Ã  obtenir une convergence du premier coup avec vos paramÃ¨tres de dÃ©part. Le dÃ©fi est dâ€™en trouver qui permettront au modÃ¨le de converger. Parfois, le modÃ¨le ne convergera jamais. Dâ€™autres fois, il convergera vers des solutions diffÃ©rentes selon les variables de dÃ©part choisies.\nmodnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))),\n                data = engelstad.nitro,\n                start = list(A = 50, E = 10, R = 0.2))\nLe modÃ¨le ne converge pas (le bloc de calcul est dÃ©sactivÃ©). Essayons les valeurs prises plus haut, lors de la crÃ©ation du graphique, qui semblent bien sâ€™ajuster.\n\nmodnl_1 &lt;-  nls(yield ~ A * (1 - exp(-R*(E + nitro))),\n                data = engelstad.nitro,\n                start = list(A = 75, E = 30, R = 0.02))\n\nBingo! Voyons maintenant le sommaire.\n\nsummary(modnl_1)\n\n\nFormula: yield ~ A * (1 - exp(-R * (E + nitro)))\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nA 75.023427   3.331860  22.517   &lt;2e-16 ***\nE 66.164111  27.251591   2.428   0.0184 *  \nR  0.012565   0.004881   2.574   0.0127 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.34 on 57 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 8.058e-06\n\n\nLes paramÃ¨tres sont diffÃ©rents de zÃ©ro, et donnent la courbe suivante.\n\nx &lt;- seq(0, 350, by = 5)\ny &lt;- mitscherlich_f(x,\n                    A = coefficients(modnl_1)[1],\n                    E = coefficients(modnl_1)[2],\n                    R = coefficients(modnl_1)[3])\n\nggplot(tibble(x, y), aes(x, y)) +\n    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +\n    geom_line() + ylim(c(0, 100))\n\n\n\n\nEt les rÃ©sidusâ€¦\n\ntibble(res = residuals(modnl_1)) |&gt; \n    ggplot(aes(x = res)) + geom_histogram(bins = 20)\n\n\n\n\n\ntibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) |&gt; \n    ggplot(aes(x = nitro, y = res)) +\n        geom_point() +\n        geom_hline(yintercept = 0, colour = \"red\")\n\n\n\n\nLes rÃ©sidus ne sont pas distribuÃ©s normalement, mais semblent bien partagÃ©s de part et dâ€™autre de la courbe.\n\n7.8.2 ModÃ¨les Ã  effets mixtes\nLorsque lâ€™on combine des variables fixes (testÃ©es lors de lâ€™expÃ©rience) et des variables alÃ©atoire (variation des unitÃ©s expÃ©rimentales), on obtient un modÃ¨le mixte. Les modÃ¨les mixtes peuvent Ãªtre univariÃ©s, multivariÃ©s, linÃ©aires (ordinaires ou gÃ©nÃ©ralisÃ©s) ou non linÃ©aires.\nÃ€ la diffÃ©rence dâ€™un effet fixe, un effet alÃ©atoire sera toujours distribuÃ© normalement avec une moyenne de 0 et une certaine variance. Dans un modÃ¨le linÃ©aire oÃ¹ lâ€™effet alÃ©atoire est un dÃ©calage dâ€™intercept, cet effet sâ€™additionne aux effets fixes:\n\\[ y = X \\beta + Z b + \\epsilon \\]\noÃ¹:\n\\(Z\\) est la matrice du modÃ¨le Ã  \\(n\\) observations et \\(p\\) variables alÃ©atoires. Les variables alÃ©atoires sont souvent des variables nominales qui subissent un encodage catÃ©goriel.\n\\[ Z = \\left( \\begin{matrix}\nz_{11} & \\cdots & z_{1p}  \\\\\nz_{21} & \\cdots & z_{2p}  \\\\\n\\vdots & \\ddots & \\vdots  \\\\\nz_{n1} & \\cdots & z_{np}\n\\end{matrix} \\right) \\]\n\\(b\\) est la matrice des \\(p\\) coefficients alÃ©atoires.\n\\[ b = \\left( \\begin{matrix}\nb_0  \\\\\nb_1  \\\\\n\\vdots \\\\\nb_p\n\\end{matrix} \\right) \\]\nLe tableau lasrosas.corn, utilisÃ© prÃ©cÃ©demment, contenait trois rÃ©pÃ©titions effectuÃ©es au cours de deux annÃ©es, 1999 et 2001. Ã‰tant donnÃ© que la rÃ©pÃ©tition R1 de 1999 nâ€™a rien Ã  voir avec la rÃ©pÃ©tition R1 de 2001, on dit quâ€™elle est emboÃ®tÃ©e dans lâ€™annÃ©e.\nLe module nlme nous aidera Ã  monter notre modÃ¨le mixte.\n\nlibrary(\"nlme\")\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nmmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv,\n                 random = ~ 1|year/rep,\n                 data = lasrosas.corn,\n                 control = lmeControl(opt = \"optim\"))\n\nÃ€ ce stade vous devriez commencer Ã  Ãªtre familier avec lâ€™interface formule et vous devriez saisir lâ€™argument fixed, qui dÃ©signe lâ€™effet fixe. Lâ€™effet alÃ©atoire est random, suit un tilde ~. Ã€ gauche de la barre verticale |, on place les variables dÃ©signant les effets alÃ©atoire sur la pente. Nous nâ€™avons pas couvert cet aspect, alors nous le laissons Ã  1. Ã€ droite, on retrouve un structure dâ€™emboÃ®tement dÃ©signant lâ€™effet alÃ©atoire: le premier niveau est lâ€™annÃ©e, dans laquelle est emboÃ®tÃ©e la rÃ©pÃ©tition.\n\nsummary(mmodlin_1)\n\nLinear mixed-effects model fit by REML\n  Data: lasrosas.corn \n       AIC      BIC    logLik\n  26535.37 26602.93 -13256.69\n\nRandom effects:\n Formula: ~1 | year\n        (Intercept)\nStdDev:    20.35426\n\n Formula: ~1 | rep %in% year\n        (Intercept) Residual\nStdDev:    11.17447 11.35617\n\nFixed effects:  yield ~ lat + long + nitro + topo + bv \n                 Value Std.Error   DF    t-value p-value\n(Intercept) -1379436.9  55894.55 3430 -24.679273   0.000\nlat           -25453.0   1016.53 3430 -25.039084   0.000\nlong           -8432.3    466.05 3430 -18.092988   0.000\nnitro              0.0      0.00 3430   1.739757   0.082\ntopoHT           -27.7      0.92 3430 -30.122438   0.000\ntopoLO             6.8      0.88 3430   7.804733   0.000\ntopoW            -16.7      1.40 3430 -11.944793   0.000\nbv                -0.5      0.03 3430 -19.242424   0.000\n Correlation: \n       (Intr) lat    long   nitro  topoHT topoLO topoW \nlat     0.897                                          \nlong    0.866  0.555                                   \nnitro   0.366  0.391  0.247                            \ntopoHT  0.300 -0.017  0.582  0.024                     \ntopoLO -0.334 -0.006 -0.621 -0.038 -0.358              \ntopoW   0.403 -0.004  0.762  0.027  0.802 -0.545       \nbv     -0.121 -0.012 -0.214 -0.023 -0.467  0.346 -0.266\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.32360269 -0.66781575 -0.07450856  0.61587533  3.96434001 \n\nNumber of Observations: 3443\nNumber of Groups: \n         year rep %in% year \n            2             6 \n\n\nLa sortie est semblable Ã  celle de la fonction lm().\n\n7.8.2.1 ModÃ¨les mixtes non-linÃ©aires\nLe modÃ¨le non linÃ©aire crÃ©Ã© plus haut liait le rendement Ã  la dose dâ€™azote. Toutefois, les unitÃ©s expÃ©rimentales (le site loc et lâ€™annÃ©e year) nâ€™Ã©taient pas pris en considÃ©ration. Nous allons maintenant les considÃ©rer.\nNous devons dÃ©cider la structure de lâ€™effet alÃ©atoire, et sur quelles variables il doit Ãªtre appliquÃ© - la dÃ©cision appartient Ã  lâ€™analyste. Il me semble plus convenable de supposer que le site et lâ€™annÃ©e affectera le rendement maximum plutÃ´t que lâ€™environnement et le taux: les effets alÃ©atoires seront donc affectÃ©s Ã  la variable A. Les effets alÃ©atoires nâ€™ont pas de structure dâ€™emboÃ®tement. Lâ€™effet de lâ€™annÃ©e sur A sera celui dâ€™une pente et lâ€™effet de site sera celui de lâ€™intercept. La fonction que nous utiliserons est nlme().\n\nmm &lt;- nlme(yield ~ A * (1 - exp(-R*(E + nitro))),\n           data = engelstad.nitro,\n           start = c(A = 75, E = 30, R = 0.02),\n           fixed = list(A ~ 1, E ~ 1, R ~ 1),\n           random = A ~ year | loc)\nsummary(mm)\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: yield ~ A * (1 - exp(-R * (E + nitro))) \n  Data: engelstad.nitro \n       AIC     BIC    logLik\n  477.2286 491.889 -231.6143\n\nRandom effects:\n Formula: A ~ year | loc\n Structure: General positive-definite, Log-Cholesky parametrization\n              StdDev       Corr  \nA.(Intercept)  2.608588499 A.(In)\nA.year         0.003066584 -0.556\nResidual      11.152757993       \n\nFixed effects:  list(A ~ 1, E ~ 1, R ~ 1) \n                 Value Std.Error DF   t-value p-value\nA.(Intercept) 74.58222  4.722715 56 15.792235  0.0000\nE             65.56721 25.533994 56  2.567840  0.0129\nR              0.01308  0.004808 56  2.720215  0.0087\n Correlation: \n  A.(In) E     \nE  0.379       \nR -0.483 -0.934\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.83373140 -0.89293039  0.07418166  0.68353578  1.82434344 \n\nNumber of Observations: 60\nNumber of Groups: 2 \n\n\nEt sur graphique:\n\nengelstad.nitro |&gt; \n  ggplot(aes(x = nitro, y = yield)) +\n  facet_grid(year ~ loc) +\n  geom_line(data = tibble(nitro = engelstad.nitro$nitro,\n                          yield = predict(mm, level = 0)),\n            colour = \"grey35\") +\n  geom_point() +\n  ylim(c(0, 95))\n\n\n\n\nLes modÃ¨les mixtes non linÃ©aires peuvent devenir trÃ¨s complexes lorsque les paramÃ¨tres, par exemple A, E et R, sont eux-mÃªme affectÃ©s linÃ©airement par des variables (par exemple A ~ topo). Pour aller plus loin, consultez Parent et al.Â (2017) ainsi que les calculs associÃ©s Ã  lâ€™article. Ou Ã©crivez-moi un courriel pour en discuter!\nNote. Lâ€™interprÃ©tation de p-values sur les modÃ¨les mixtes est controversÃ©e. Ã€ ce sujet, Douglas Bates a Ã©crit une longue lettre Ã  la communautÃ© de dÃ©veloppement du module lme4, une alternative Ã  nlme, qui remet en cause lâ€™utilisation des p-values, ici. De plus en plus, pour les modÃ¨les mixtes, on se tourne vers les statistiques bayÃ©siennes, couvertes dans le chapitreÂ 8 avec le module greta. Mais en ce qui a trait aux modÃ¨les mixtes, le module brms automatise bien des aspects de lâ€™approche bayÃ©sienne.\n\n7.8.3 Aller plus loin\n\n7.8.3.1 Statistiques gÃ©nÃ©rales:\n\nThe analysis of biological data\n\n7.8.3.2 Statistiques avec R\n\nDisponibles en version Ã©lectronique Ã  la bibliothÃ¨que de lâ€™UniversitÃ© Laval:\n\nIntroduction aux statistiques avec R: Introductory statistics with R\n\nApprofondir les statistiques avec R: The R Book, Third edition\n\nApprofondir les modÃ¨les Ã  effets mixtes avec R: Mixed Effects Models and Extensions in Ecology with R\n\n\n\n\nModernDive, un livre en ligne offrant une approche moderne avec le package moderndive."
  },
  {
    "objectID": "07b-bayes.html",
    "href": "07b-bayes.html",
    "title": "8Â  Introduction Ã  lâ€™analyse bayÃ©sienne en Ã©cologie",
    "section": "",
    "text": "Objectifs spÃ©cifiques:\nCe chapitre est un extra. Il ne fait pas partie des objectifs du cours. Il ne sera pas Ã©valuÃ©.\nÃ€ la fin de ce chapitre, vous\n\nserez en mesure de dÃ©finir ce que sont les statistiques bayÃ©siennes\nserez en mesure de calculer des statistiques descriptives de base en mode bayÃ©sien avec le module greta.\n\n\nÃ€ venir!"
  },
  {
    "objectID": "08-explorer.html#r-sur-le-web",
    "href": "08-explorer.html#r-sur-le-web",
    "title": "9Â  Explorer R",
    "section": "\n9.1 R sur le web",
    "text": "9.1 R sur le web\nDans un environnement de travail en Ã©volution rapide et constante, il est difficile de considÃ©rer que ses compÃ©tences sont abouties. Rester informÃ© sur le dÃ©veloppement de R vous permettra de trouver et de rÃ©soudre des problÃ¨mes persistants de maniÃ¨re plus efficace ou par de nouvelles avenues, et vous offrira mÃªme lâ€™occasion de dÃ©nicher des problÃ¨mes dont vous ne soupÃ§onniez pas lâ€™existence. Plusieurs sources dâ€™information vous permettront de vous tenir Ã  jour sur le dÃ©veloppement de R, de ses environnements de travail (RStudio, Jupyter, etc.) et des nouveaux modules qui sâ€™y greffent. Plus largement, vous gagnerez Ã  vous informer sur les derniÃ¨res tendances en calcul scientifique sur dâ€™autres plate-forme que R (Python, Javascript, Julia, etc.). Ã‰videmment, nos tÃ¢ches quotidiennes ne nous permettent pas de tout suivre. MÃªme si vous pouviez nâ€™attrapper que 1% du dÃ©filement, ce serait dÃ©jÃ  1% de plus que rien du tout.\n\nÃ‰videmment, rester au courant aide parce que vous en apprenez davantage sur les outils et leurs applications. Mais Ã§a aide aussi parce que Ã§a vous permet de connaÃ®tre des gens et des organisations! Il est trÃ¨s utile de savoir qui travaille sur quoi et oÃ¹ se dÃ©roulent les dÃ©veloppements sur un sujet donnÃ©, car si vous cherchez consciemment quelque chose plus tard, Ã§a vous aidera Ã  trouver votre chemin plus facilement. - MaÃ«lle Salmon, Keeping up to date with R news (ma traduction)\n\nJe vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez Ã  lâ€™aventure!\n\n\n\n\nTirÃ© du film The Hobbit: An Unexpected Journey, de Peter Jackson (2012).\n\n\n\n\n9.1.1 GitHub\nNous avons vu chapitreÂ 5 lâ€™importance dâ€™utilser des outils dâ€™archivage et de suivi de version, comme git, dans le dÃ©ploiement de la science ouverte. En effet, GitHub est une plateforme git sur Internet, acquise par Microsoft, qui est devenue un rÃ©seau social de dÃ©veloppement informatique. De nombreux modules de R y sont dÃ©veloppÃ©s. Au chapitreÂ 5, vous avez appris Ã  y ouvrir un compte et Ã  y archiver du contenu. Vous pourrez alors suivre (dans le mÃªme sens que sur dâ€™autres rÃ©seaux sociaux) le dÃ©veloppement de projets et suivre les travaux des personnes qui vous semblent dâ€™intÃ©rÃªt.\n\n9.1.2 X (anciennement Twitter)\nLe hashtag #rstats rassemble (ou plutÃ´t rassemblait) sur Twitter ce qui se publie sur le sujet. Depuis le rachat du rÃ©seau social par Elon Musk et les divers changements qui y ont Ã©tÃ© effectuÃ©s, plusieurs sâ€™en sont dÃ©tournÃ©s pour privilÃ©gier des plateformes opensource, par exemple les rÃ©seaux sociaux dÃ©centralisÃ©s de [Mastodon](https://joinmastodon.org/fr.\nOn y retrouve les comptes de R-bloggers, RStudio et rOpenSci. Certaines communautÃ©s y sont aussi actives, comme R4DS online learning community, qui partage des nouvelles sur R, et R-Ladies Global, qui vise Ã  amener davantage de diversitÃ© Ã  la communautÃ© de R. Des comptes thÃ©matiques comme Daily R Cheatsheets, R for the Rest of Us et One R Package a Day permettent de dÃ©couvrir quotidiennement de nouvelles possibilitÃ©s. Enfin, plusieurs personnes contribuent positivement Ã  la communautÃ© R. Hadley Wickham brille parmi les Ã©toiles de R. Les comptes de Mara Averick, Claus Wilke et David Robinson sont aussi intÃ©ressants.\n\n9.1.3 Les serveurs Mastodon\nMastodon est un rÃ©seau social opensource dÃ©centralisÃ©, gratuit et interopÃ©rable qui fait partie de lâ€™Univers Fediverse. Il est composÃ© de plusieurs serveurs indÃ©pendants pouvant communiquer entre eux, et est assez similaire Ã  Twitter. Depuis quelques annÃ©es, la communautÃ© de programmation R sâ€™est largement tournÃ©e vers cette option, et vous trouverez la plupart des personnes ou des groupes mentionnÃ©s Ã  la section prÃ©cÃ©dente sur ce site :\n\nPosit\nrOpenSci\nR4DS online learning community\nR-Ladies Global\nOne R Package a Day\nHadley Wickham\n\nMara Averick,\nMaÃ«lle Salmon\n\nEt pour les hashtags :\n\n#rstats\n#tidytuesday\n\nPeu importe votre choix de serveur, vous pourrez ensuite effectuer une recherche en utilisant le @ pour trouver des comptes et le # pour chercher des mots-clÃ©s. Bref, nâ€™hÃ©sitez pas Ã  fouiller et Ã  suivre ce qui vous intÃ©resse!\n\n9.1.4 Nouvelles\nLe site dâ€™aggrÃ©gation R-bloggers, mis Ã  jour quotidiennement, republie des articles en anglais tirÃ©s dâ€™un peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux dÃ©veloppements. Deux fois par mois, lâ€™organisation rOpenSci offre un portrait de lâ€™univ-R (#dadjoke), ce que R Weekly offre de maniÃ¨re hebdomadaire (lâ€™information sera probablement redondante). Le tidyverse a quant Ã  lui son propre blogue.\n\n9.1.5 Des questions?\nBien que davantage vouÃ©s Ã  la rÃ©solution de problÃ¨me quâ€™ Ã  lâ€™exploration de nouvelles opportunitÃ©s, Stackoverflow et Cross Validated sont des plateformes prisÃ©es. De plus, la liste de courriels r-sig-ecology permet des Ã©changes entre professionnels et novices en analyse de donnÃ©es Ã©cologiques avec R.\n\n9.1.6 Participer\nR est un logiciel basÃ© sur une communautÃ© de dÃ©veloppement, dâ€™utilisation et de vulgarisation. Des personnes offrent gÃ©nÃ©reusement du temps de support. Si vous vous sentez Ã  lâ€™aise, offrez aussi le vÃ´tre!\n\n9.1.7 Mise en garde\nLes modules de R sont dÃ©veloppÃ©s par quiconque le veut bien: leur qualitÃ© nâ€™est pas nÃ©cessairement auditÃ©e. Souvent, ils ne sont vÃ©rifiÃ©s que par une vigilance communautaire: dans ce cas, vous Ãªtes les cobailles. Ce qui nâ€™est pas nÃ©cessairement une mauvaise chose, mais cela nÃ©cessite de prendre ses prÃ©cautions. Dans sa confÃ©rence How to be a resilient R user, MaÃ«lle Salmon propose quelques guides pour juger de la qualitÃ© dâ€™un module.\n1. Le module est-il activement dÃ©veloppÃ©?\nBien!\n\nAttention!\n\n2. Le module est-il bien testÃ©?\nVÃ©rifiez si le module a fait lâ€™objet dâ€™une publication scientifique, sâ€™il a Ã©tÃ© utilisÃ© avec succÃ¨s dans la littÃ©rature ou dans des documents crÃ©dibles.\n3. Le module est-il bien documentÃ©?\nUn site internet dÃ©diÃ© est-il utilisÃ© pour documenter lâ€™utilisation du module? Les fichiers dâ€™aide sont-ils complets, et sont-ils de bonne qualitÃ©?\n4. Le module est-il largement utilisÃ©?\nUn module peu populaire nâ€™est pas nÃ©cessessairement de mauvaise qualitÃ©: peut-Ãªtre est-il seulement destinÃ© Ã  des applications de niche. Sâ€™il nâ€™est pas un indicateur Ã  lui seul de la soliditÃ© ou la validitÃ© dâ€™un module, une masse critique indique que le module a passÃ© sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peut Ãªtre Ã©valuÃ© par le nombre dâ€™Ã©toiles attribuÃ© au module (Ã©quivalent Ã  un Jâ€™aime).\n\n5. Le module est-il dÃ©veloppÃ© par une personne ou une organisation crÃ©dible?\nOn peut affirmer sans trop se compromettre que lâ€™Ã©quipe de RStudio (Posit) dÃ©veloppe des modules de confiance. Tout comme il faudrait se mÃ©fier dâ€™un module dÃ©veloppÃ© par une personne anonyme.\nLe module packagemetrics permet dâ€™Ã©valuer ces critÃ¨res.\n\n# devtools::install_github(\"sfirke/packagemetrics\")\nlibrary(\"packagemetrics\")\npm &lt;- package_list_metrics(c(\"dplyr\", \"ggplot2\", \"vegan\", \"greta\"))\nmetrics_table(pm)\n\n\n9.1.8 Prendre tout Ã§a en note\nUn logiciel de prise de notes (il en existe plein, mais je vous suggÃ¨re dâ€™opter pour les options encryptÃ©es) pourrait vous Ãªtre utile pour retrouver lâ€™information soutirÃ©e de vos flux dâ€™information. Mais certaines personnes consignent simplement leurs informations dans un carnet ou un document de traitement de texte."
  },
  {
    "objectID": "08-explorer.html#r-en-chaire-et-en-os",
    "href": "08-explorer.html#r-en-chaire-et-en-os",
    "title": "9Â  Explorer R",
    "section": "\n9.2 R en chaire et en os",
    "text": "9.2 R en chaire et en os\nLâ€™UniversitÃ© Laval (institution auprÃ¨s de laquelle ce manuel est dÃ©veloppÃ©) est hÃ´te Ã  tous les 2 ans de la confÃ©rence R Ã  QuÃ©bec. La prochaine confÃ©rence aura lieu en 2021."
  },
  {
    "objectID": "08-explorer.html#quelques-outils-en-Ã©cologie-mathÃ©matique-avec-r",
    "href": "08-explorer.html#quelques-outils-en-Ã©cologie-mathÃ©matique-avec-r",
    "title": "9Â  Explorer R",
    "section": "\n9.2 Quelques outils en Ã©cologie mathÃ©matique avec R",
    "text": "9.2 Quelques outils en Ã©cologie mathÃ©matique avec R\n\n9.2.1 PrÃ©traitement des donnÃ©es\nIl arrive souvent ques les donnÃ©es brutes ne soient pas exprimÃ©es de maniÃ¨re appropriÃ©e ou optimale pour lâ€™analyse statistique ou la modÃ©lisation. Vous devrez alors effectuer un prÃ©traitement sur ces donnÃ©es. Lors du chapitreÂ 7, nous avons abordÃ© la mise Ã  lâ€™Ã©chelle, oÃ¹ des variables numÃ©riques Ã©taient transformÃ©es pour avoir une moyenne de zÃ©ro et un Ã©cart-type de 1. Cette opÃ©ration permettait dâ€™apprÃ©cier les coefficients et leur incertitude sur une mÃªme Ã©chelle. Lâ€™encodage catÃ©gorielle a quant Ã  lui permi dâ€™utiliser des mÃ©thodes quantitatives sur des donnÃ©es qualitatives. Dans les deux cas, nous nâ€™avons pas utilisÃ© le terme, mais il sâ€™agissait dâ€™un prÃ©traitement, câ€™est-Ã -dire une transformation des donnÃ©es prÃ©alable Ã  lâ€™analyse ou la modÃ©lisation.\nUn prÃ©traitement peut consister simplement en une transformation logarithmique ou exponentielle. Nous verrons les transformations les plus communes comme la standardisation, la mise Ã  lâ€™Ã©chelle sur une Ã©tendue et la normalisation. Puis nous verrons comment ces opÃ©rations de prÃ©traitement sont offertes dans le module recipes.\nrecipes nâ€™est pas en mesure dâ€™effectuer toutes les transformations imaginables. Pour des opÃ©rations plus spÃ©cialisÃ©es, si vos donnÃ©es forment une partie dâ€™un tout (exprimÃ©es en pourcentages ou fractions), vous devriez probablement utiliser un prÃ©traitement grÃ¢ce aux outils de lâ€™analyse compositionnelle. Avant de les aborder, nous allons traiter des transformations de base.\n\n9.2.1.1 Standardisation\nLa standardisation consiste Ã  centrer vos donnÃ©es Ã  une moyenne de 0 et Ã  les Ã©chelonner Ã  une variance de 1, câ€™est-Ã -dire\n\\[x_{standard} = \\frac{x - \\bar{x}}{\\sigma}\\]\noÃ¹ \\(\\bar{x}\\) est la moyenne du vecteur \\(x\\) et oÃ¹ \\(\\sigma\\) est son Ã©cart-type.\nCe prÃ©traitement des donnÃ©es peut sâ€™avÃ©rÃ©r utile lorsque la modÃ©lisation tient compte de lâ€™Ã©chelle de vos mesures (par exemple, les paramÃ¨tres de rÃ©gression vus au chapitreÂ 7 ou les distances que nous verrons au chapitreÂ 10). En effet, les pentes dâ€™une rÃ©gression linÃ©aire multiple ne pourront Ãªtre comparÃ©es entre elles que si elles sont une mÃªme Ã©chelle. Par exemple, on veut modÃ©liser la consommation en miles au gallon (mpg) de voitures en fonction de leur puissance (hp), le temps en secondes pour parcourir un quart de mile (qsec) et le nombre de cylindre.\n\ndata(\"mtcars\")\nmodl &lt;- lm(mpg ~ hp + qsec + cyl, mtcars)\nsummary(modl)\n\n\nCall:\nlm(formula = mpg ~ hp + qsec + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3223 -1.9483 -0.5656  1.5452  7.7773 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 55.30540    9.03697   6.120 1.33e-06 ***\nhp          -0.03552    0.01622  -2.190  0.03700 *  \nqsec        -0.89424    0.42755  -2.092  0.04567 *  \ncyl         -2.26960    0.54505  -4.164  0.00027 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.003 on 28 degrees of freedom\nMultiple R-squared:  0.7757,    Adjusted R-squared:  0.7517 \nF-statistic: 32.29 on 3 and 28 DF,  p-value: 3.135e-09\n\n\nLes pentes signifient que la distance parcourue par gallon dâ€™essence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. Lâ€™interprÃ©tation est conviviale Ã  cette Ã©chelle. Mais lequel de ces effets est le plus important? L t value indique que ce seraient les cylindres. Mais pour juger lâ€™importance en terme de pente, il vaudrait mieux standardiser.\n\nlibrary(\"tidyverse\")\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.4.4     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.0\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nstandardise &lt;- function(x) (x-mean(x))/sd(x)\nmtcars_sc &lt;- mtcars %&gt;%\n  mutate_if(is.numeric, standardise) # ou bien scale(mtcars, center = TRUE, scale = TRUE)\nmodl_sc &lt;- lm(mpg ~ hp + qsec + cyl, mtcars_sc)\nsummary(modl_sc)\n\n\nCall:\nlm(formula = mpg ~ hp + qsec + cyl, data = mtcars_sc)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.71716 -0.32326 -0.09384  0.25639  1.29042 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.061e-16  8.808e-02   0.000  1.00000    \nhp          -4.041e-01  1.845e-01  -2.190  0.03700 *  \nqsec        -2.651e-01  1.268e-01  -2.092  0.04567 *  \ncyl         -6.725e-01  1.615e-01  -4.164  0.00027 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4983 on 28 degrees of freedom\nMultiple R-squared:  0.7757,    Adjusted R-squared:  0.7517 \nF-statistic: 32.29 on 3 and 28 DF,  p-value: 3.135e-09\n\n\nLes valeurs des pentes ne peuvent plus Ãªtre interprÃ©tÃ©es directement, mais peuvent maintenant Ãªtre comparÃ©es entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile.\nLes algorithmes basÃ©s sur des distances auront, de mÃªme, avantage Ã  Ãªtre standardisÃ©s.\n\n9.2.1.2 Ã€ lâ€™Ã©chelle de la plage\nSi vous dÃ©sirez prÃ©server le zÃ©ro dans le cas de donnÃ©es positives ou plus gÃ©nÃ©ralement vous voulez que vos donnÃ©es prÃ©traitÃ©es soient positives, vous pouvez les transformer Ã  lâ€™Ã©chelle de la plage, câ€™est-Ã -dire les forcer Ã  sâ€™Ã©taler de 0 Ã  1:\n\\[ x_{range01} = \\frac{x - x_{min}}{x_{max} - x_{min}}  \\]\nCette transformation est sensible aux valeurs aberrantes, et une fois le vecteur transformÃ© les valeurs aberrantes seront toutefois plus difficiles Ã  dÃ©tecter.\n\nrange_01 &lt;- function(x) (x-min(x))/(max(x) - min(x))\nmtcars %&gt;%\n  mutate_if(is.numeric, range_01) %&gt;% # en fait, toutes les colonnes sont numÃ©riques, alors mutate_all aurait pu Ãªtre utilisÃ© au lieu de mutate_if\n  sample_n(4)\n\n                    mpg cyl       disp        hp      drat         wt      qsec\nMerc 230      0.5276596 0.0 0.17385882 0.1519435 0.5345622 0.41856303 1.0000000\nMazda RX4 Wag 0.4510638 0.5 0.22175106 0.2049470 0.5253456 0.34824853 0.3000000\nVolvo 142E    0.4680851 0.0 0.12446994 0.2014134 0.6221198 0.32395807 0.4880952\nHonda Civic   0.8510638 0.0 0.01147418 0.0000000 1.0000000 0.02608029 0.4785714\n              vs am gear      carb\nMerc 230       1  0  0.5 0.1428571\nMazda RX4 Wag  0  1  0.5 0.4285714\nVolvo 142E     1  1  0.5 0.1428571\nHonda Civic    1  1  0.5 0.1428571\n\n\n\n9.2.1.3 Normaliser\nLe terme normaliser est associÃ© Ã  des opÃ©rations diffÃ©rentes dans la littÃ©rature. Nous prendrons la nomenclature de scikit-learn, pour qui la normalisation consiste Ã  faire en sorte que la longueur du vecteur (sa norme, dâ€™oÃ¹ normaliser) soit unitaire. Cette opÃ©ration est le plus souvent utilisÃ©e par observation (ligne), non pas par variable (colonne). Il existe plusieurs maniÃ¨res de mesurer la distance dâ€™un vecteur, mais la plus commune est la distance euclidienne. La seule fois que jâ€™ai eu Ã  utiliser ce prÃ©traitement Ã©tait en analyse spectrale (Chemometrics with R, Ron Wehrens, 2011, chapitre 3.5). En R,\n\nlibrary(\"pls\")\n\n\nAttaching package: 'pls'\n\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\ndata(\"gasoline\")\nspectro &lt;- gasoline$NIR %&gt;% unclass() %&gt;% as_tibble()\n\nnormalise &lt;- function(x) x/sqrt(sum(x^2))\nspectro_norm &lt;- spectro %&gt;% \n  rowwise() %&gt;% # diffÃ©rentes approches possibles pour les opÃ©rations sur les lignes\n  normalise()\nspectro_norm[1:4, 1:4]\n\n         900 nm        902 nm        904 nm        906 nm\n1 -0.0011224834 -0.0010265446 -0.0009434425 -0.0008314021\n2 -0.0009890637 -0.0008856332 -0.0007977676 -0.0006912734\n3 -0.0010481029 -0.0009227116 -0.0008269742 -0.0007035061\n4 -0.0010444801 -0.0009446277 -0.0008623530 -0.0007718261\n\n\n\n9.2.1.4 Le module recipes\n\nNous avons vu comment standardiser avec notre propre fonction. Certaines personnes prÃ©fÃ¨rent utiliser la fonction scale(). Mais une nouvelle approche est en train de sâ€™installer, avec le module recipes, un module de lâ€™ombrelle tidymodels, un mÃ©ta module en dÃ©veloppement visant Ã  faire de R un outil de modÃ©lisation plus convivial.\nrecipes fonctionne en mode tidyverse, câ€™est-Ã -dire en suites dâ€™opÃ©rations. De nombreuses fonctions sont offertes, dont des fonctions dâ€™imputation, que nous verrons au chapitre @ref(chapitre-outliers). Nous couvrirons ici la standardisation et la mise Ã  lâ€™Ã©chelle, juste pour lâ€™apÃ©ro ğŸ³.\nLe module ne sâ€™appelle pas recette pour rien. Il fonctionne en trois Ã©tapes:\n\nMonter la liste des ingrÃ©dients: spÃ©cifier ce quâ€™il faut faire\nMÃ©langer les ingrÃ©dients: transformer tout ce quâ€™il faut faire en une procÃ©dure\nCuire les ingrÃ©dients: appliquer la procÃ©dure Ã  un tableau.\n\nVoici une petite application sur le tableau lasrosas.corn.\n\nlibrary(\"tidymodels\")\n\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.1.1 â”€â”€\n\n\nâœ” broom        1.0.5      âœ” rsample      1.2.0 \nâœ” dials        1.2.1      âœ” tune         1.1.2 \nâœ” infer        1.0.6      âœ” workflows    1.1.4 \nâœ” modeldata    1.3.0      âœ” workflowsets 1.0.1 \nâœ” parsnip      1.2.0      âœ” yardstick    1.3.0 \nâœ” recipes      1.0.10     \n\n\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nâœ– scales::discard() masks purrr::discard()\nâœ– dplyr::filter()   masks stats::filter()\nâœ– recipes::fixed()  masks stringr::fixed()\nâœ– dplyr::lag()      masks stats::lag()\nâœ– yardstick::spec() masks readr::spec()\nâœ– recipes::step()   masks stats::step()\nâ€¢ Use tidymodels_prefer() to resolve common conflicts.\n\ndata(lasrosas.corn, package = \"agridat\")\nlasrosas.corn %&gt;% \n  head()\n\n  year       lat      long yield nitro topo     bv rep nf\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5\n\n\nDisons que pour mon modÃ¨le statistique, ma variable de sortie est le rendement (yield), que je dÃ©sire lier Ã  la dose dâ€™azote (nitro), Ã  un indicateur de la teneur en matiÃ¨re organique du sol (bv) et Ã  la topographie (topo).\nMais pour rendre le modÃ¨le prÃ©dictif (et non pas seulement descriptif), je dois lâ€™Ã©valuer sur des donnÃ©es qui nâ€™ont pas servies Ã  lisser le modÃ¨le (nous verrons en plus de dÃ©tails Ã§a au chapitreÂ 15). Je vais donc sÃ©parer mon tableau au hasard en un tableau dâ€™entraÃ®nement comprenant 70% des observations et un autre pour tester le modÃ¨le comprenant le 30% restant.\n\ntrain_test_split &lt;- lasrosas.corn %&gt;% \n  select(yield, nitro, bv, topo) %&gt;% \n  initial_split(prop = 0.7)\ntrain_df &lt;- training(train_test_split)\ntest_df &lt;- testing(train_test_split)\n\nVoici ma recette. Je lâ€™expliquerai tout de suite aprÃ¨s.\n\nrecette &lt;- recipe(yield ~ ., data = train_df) %&gt;% \n  step_zv(all_numeric()) %&gt;% \n  step_normalize(all_numeric(), -all_outcomes()) %&gt;% \n  # step_downsample(topo) %&gt;%\n  step_dummy(topo) %&gt;% \n  prep()\n\nLa recette peut se baser sur lâ€™interface formule - souvenez-vous que le . signifie â€œtoutes les autres variables du tableauâ€. Elle se base toujours sur le jeu dâ€™entraÃ®nement. La prochaine Ã©tape (souvenez-vous que %&gt;% signifie â€œpuisâ€ ou â€œensuiteâ€) consiste Ã  retirer les variables dont la variance est non-nulle, ce qui est pratique pour Ã©viter que la standardisation divise par \\(0\\). Cette Ã©tape est appliquÃ©e Ã  toutes les variables numÃ©riques all_numeric(). Ensuite, je standardise avec la fonction step_normalize() - il y a beaucoup de confusion entre la notion de standardisation et de normalisation dans les fonctions comme dans la littÃ©rature. Dans cette fonction, je spÃ©cifie que la standardisation nâ€™est applicable que sur les entrÃ©es numÃ©riques du modÃ¨le (all_numeric(), -all_outcomes()). Lâ€™Ã©tape step_downsample() retire des obsservations pour faire en sorte que les catÃ©gories dâ€™une variable apparaissent toutes en mÃªme nombre. Bien que lâ€™interface-formule de R sâ€™en occupe automatiquement, je spÃ©cifie ensuite que je dÃ©sire que la variable topo subisse un encodage catÃ©goriel. Puis je mÃ©lange mes ingrÃ©dients avec la fonction prep(). Jâ€™obtiens un objet de type recette.\n\nrecette\n\n\n\n\nâ”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n\n\n\n\nâ”€â”€ Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\nâ”€â”€ Training information \n\n\nTraining data contained 2410 data points and no incomplete rows.\n\n\n\n\n\nâ”€â”€ Operations \n\n\nâ€¢ Zero variance filter removed: &lt;none&gt; | Trained\n\n\nâ€¢ Centering and scaling for: nitro and bv | Trained\n\n\nâ€¢ Dummy variables from: topo | Trained\n\n\nLa recette Ã©tant bien mÃ©langÃ©e, on peut en extraire le jus avec la fonction bake(), qui permet de gÃ©nÃ©rer le tableau transformÃ©.\n\ntest_proc &lt;- bake(recette, test_df)\ntest_proc %&gt;% sample_n(5)\n\n# A tibble: 5 Ã— 6\n    nitro     bv yield topo_HT topo_LO topo_W\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 -0.326   0.432  46.1       1       0      0\n2  0.250  -0.698 104         0       1      0\n3  0.0319 -0.203  57.6       0       0      1\n4  0.960   1.24   60.7       1       0      0\n5  1.39    0.555  56.6       0       0      1\n\n\nLa fonction bake() peut aussi Ãªtre appliquÃ©e au donnÃ©es dâ€™entraÃ®nement, mais certaines Ã©tapes de recette doivent passer par des opÃ©rations particuliÃ¨res, comme step_downsample() Il est donc prÃ©fÃ©rable, pour les donnÃ©es dâ€™entrÃ¢inement, dâ€™en extraire le jus avec la fonction juice().\n\ntrain_proc &lt;- bake(recette, train_df)\ntrain_proc %&gt;% sample_n(5)\n\n# A tibble: 5 Ã— 6\n   nitro     bv yield topo_HT topo_LO topo_W\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1  0.250 -0.296  94.6       0       1      0\n2 -0.827  0.600  59.2       0       0      0\n3  0.960 -1.84   72.1       0       1      0\n4  0.960  0.813  74.3       0       1      0\n5 -0.270 -0.275  68.1       0       0      0\n\n\nLe tableau train_proc peut Ãªtre envoyÃ© dans un modÃ¨le de votre choix! Par exemple,\n\nlm(yield ~ ., train_proc) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = yield ~ ., data = train_proc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.888 -11.133  -2.347  10.705  41.205 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  77.8903     0.6199 125.654  &lt; 2e-16 ***\nnitro         2.4698     0.2842   8.689  &lt; 2e-16 ***\nbv           -5.4303     0.3568 -15.221  &lt; 2e-16 ***\ntopo_HT     -23.8887     0.9537 -25.048  &lt; 2e-16 ***\ntopo_LO       3.0660     0.8631   3.553 0.000389 ***\ntopo_W      -10.4725     0.8029 -13.043  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.95 on 2404 degrees of freedom\nMultiple R-squared:  0.5116,    Adjusted R-squared:  0.5106 \nF-statistic: 503.7 on 5 and 2404 DF,  p-value: &lt; 2.2e-16\n\n\n\n9.2.1.5 Analyse compositionnelle en R\nEn 1898, le statisticien Karl Pearson nota que des corrÃ©lations Ã©taient induites lorsque lâ€™on effectuait des ratios par rapport Ã  une variable commune.\n Source Karl Pearson, 1897. Mathematical contributions to the theory of evolution.â€”on a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of London\nFaisons lâ€™exercice! Nous gÃ©nÃ©rons au hasard 1000 donnÃ©es (comme le proposait Pearson) pour trois dimensions: le fÃ©mur, le tibia et lâ€™humÃ©rus. Ces dimensions ne sont pas gÃ©nÃ©rÃ©es par des distributions corrÃ©lÃ©es.\n\nset.seed(3570536)\nn &lt;- 1000\nbones &lt;- tibble(femur = rnorm(n, 10, 3),\n                tibia = rnorm(n, 8, 2),\n                humerus = rnorm(n, 6, 2))\nplot(bones)\n\n\n\ncor(bones)\n\n               femur        tibia      humerus\nfemur    1.000000000 -0.069006171  0.002652292\ntibia   -0.069006171  1.000000000 -0.008994704\nhumerus  0.002652292 -0.008994704  1.000000000\n\n\nPourtant, si jâ€™utilise des ratios allomÃ©triques avec lâ€™humÃ©rus comme base,\n\nbones_r &lt;- bones %&gt;% \n  transmute(fh = femur/humerus,\n            th = tibia/humerus)\nplot(bones_r)\ntext(30, 20, paste(\"corrÃ©lation =\", round(cor(bones_r$fh, bones_r$th), 2)), col = \"blue\")\n\n\n\n\nNous avons induit ce que Pearson appelait une fausse corrÃ©lation (spurious correlation). En 1960, Chayes proposa que de telles fausses corrÃ©lations sont induites non seulement sur des ratios de valeurs absolues, mais aussi sur des ratios dâ€™une somme totale. Par exemple, dans une composition simple de deux types dâ€™utilisation du territoire, si une proportion augmente, lâ€™autre doit nÃ©cessairement diminuer.\n\nn &lt;- 100\ntibble(A = runif(n, 0, 1)) %&gt;% \n  mutate(B = 1 - A) %&gt;% \n  ggplot(aes(x=A, y=B)) +\n  geom_point()\n\n\n\n\nLes variables exprimÃ©es relativement Ã  une somme totale sont dites compositionnelles. Elles possÃ¨dent les caractÃ©ristiques suivantes.\n\n\nRedondance dâ€™information. Un systÃ¨me de deux proportions ne contient quâ€™une seule variable du fait que lâ€™on puisse dÃ©duire lâ€™une en soutrayant lâ€™autre de la somme totale. Un vecteur compositionnel contient de lâ€™information redondante. Pourtant, effectuer des statistiques sur lâ€™une plutÃ´t que sur lâ€™autre donnera des rÃ©sultats diffÃ©rents.\n\nDÃ©pendance dâ€™Ã©chelle. Les statistiques devraient Ãªtre indÃ©pendantes de la somme totale utilisÃ©e. Pourtant, elles diffÃ©reront sur lâ€™on utilise par exemple, une proportion des mÃ¢les dâ€™une part et des femelles dâ€™autre part, ou la proportion de la somme des deux, de mÃªme que les rÃ©sultats dâ€™un test sanguin diffÃ©rera si lâ€™on utilise une base sÃ¨che ou une base humide.\n\nDistribution thÃ©orique des donnÃ©es. Ã‰tant donnÃ©e que les proportions sont confinÃ©es entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui sâ€™Ã©tend de -âˆ Ã  +âˆ) nâ€™est souvent pas appropriÃ©e. On pourra utiliser la distribution de Dirichlet ou la distribution logitique-normale, mais dâ€™autres approches sont souvent plus pratiques.\n\nPour illustrer lâ€™effet de la distribution, voyons un diagramme ternaire incluant le sable, le limon et lâ€™argile. En utilisant des Ã©cart-types univariÃ©s, nous obtenons lâ€™ellipse en rouge, qui non seulement reprÃ©sente peu lâ€™Ã©talement des donnÃ©es, mais elle dÃ©passe les bornes du triangle, admettant ainsi des proportions nÃ©gatives. En bleu, la distribution logistique normale (issue des mÃ©thodes prÃ©sentÃ©es plus loin dans cette section) convient davantage.\n\nLes consÃ©quences dâ€™effectuer des statistiques linÃ©aires sur des donnÃ©es compositionnelles brutes peuvent Ãªtre majeures. En outre, Pawlowksy-Glahn et Egozcue (2006), sâ€™appuyant en outre sur Rock (1988), note les problÃ¨mes suivants (exprimÃ©s en mes mots).\n\nles rÃ©gressions, les regroupements et les analyses en composantes principales peuvent avoir peu ou pas de signification\nles propriÃ©tÃ©s des distributions peuvent Ãªtre gÃ©nÃ©rÃ©es par lâ€™opÃ©ration de fermeture de la composition (sâ€™assurer que le total des proportions donne 100%)\nles rÃ©sultats dâ€™analyses discriminantes linÃ©aires sont propices Ã  Ãªtre illusoires\ntous les coefficients de corrÃ©lation seront affectÃ©s Ã  des degrÃ©s inconnus\nles rÃ©sultats des tests dâ€™hypothÃ¨ses seront intrinsÃ¨quement faussÃ©s\n\nPour contourner ces problÃ¨mes, il faut dâ€™abord aborder les donnÃ©es compositionnelles pour ce quâ€™elles sont: des donnÃ©es intrinsÃ¨quement multivariÃ©es. Elles sont un nuage de point, et non pas une collection de variables individuelles. Ceci qui nâ€™empÃªche pas dâ€™effectuer des analyses consciencieusement sous des angles particuliers.\nEn R, on pourra aisÃ©ment rapporter une composition en somme unitaire grÃ¢ce Ã  la fonction apply. Mais auparavant, chargeons le module compositions (nâ€™oubliez pas de lâ€™installer au prÃ©alable) pour accÃ©der Ã  des donnÃ©es fictives de proportions de sable, limon et argile dans des sÃ©diments.\n\nlibrary(\"compositions\")\ndata(\"ArcticLake\")\nArcticLake &lt;- ArcticLake %&gt;% as_tibble()\nhead(ArcticLake)\n\n# A tibble: 6 Ã— 4\n   sand  silt  clay depth\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  77.5  19.5   3    10.4\n2  71.9  24.9   3.2  11.7\n3  50.7  36.1  13.2  12.8\n4  52.2  40.9   6.6  13  \n5  70    26.5   3.5  15.7\n6  66.5  32.2   1.3  16.3\n\n\n\ncomp &lt;- ArcticLake %&gt;%\n  select(-depth) %&gt;%\n  apply(., 1, function(x) x/sum(x)) %&gt;% \n  t()\ncomp[1:5, ]\n\n          sand      silt      clay\n[1,] 0.7750000 0.1950000 0.0300000\n[2,] 0.7190000 0.2490000 0.0320000\n[3,] 0.5070000 0.3610000 0.1320000\n[4,] 0.5235707 0.4102307 0.0661986\n[5,] 0.7000000 0.2650000 0.0350000\n\n\nOn pourra aussi utiliser la fonction acomp (pour Aitchison-composition) pour fermer la composition Ã  une somme de 1.\n\ncomp &lt;- ArcticLake %&gt;%\n  select(-depth) %&gt;%\n  acomp(.)\ncomp[1:5, ]\n\n     sand        silt        clay       \n[1,] \"0.7750000\" \"0.1950000\" \"0.0300000\"\n[2,] \"0.7190000\" \"0.2490000\" \"0.0320000\"\n[3,] \"0.5070000\" \"0.3610000\" \"0.1320000\"\n[4,] \"0.5235707\" \"0.4102307\" \"0.0661986\"\n[5,] \"0.7000000\" \"0.2650000\" \"0.0350000\"\nattr(,\"class\")\n[1] \"acomp\"\n\n\nCette stratÃ©gie a pour avantage dâ€™attribuer Ã  la variable comp la classe acomp, qui automatise les opÃ©rations dans lâ€™espace compositionnel (que lâ€™on nomme aussi le simplex). La reprÃ©sentation ternaire est souvent utilisÃ©e pour prÃ©senter des compositions. Toutefois, il est difficile dâ€™interprÃ©ter les compositions de plus de trois parties. La classe acomp automatise aussi la reprÃ©sentation teranaire.\n\nplot(comp)\n\n\n\n\nAfin de transposer cet espace clÃ´t en un espace ouvert, on pourra diviser chaque proportion par une proportion de rÃ©fÃ©rence choisie parmi nâ€™importe quelle proportion. Du coup, on retire une dimension redondante! Dans ce ratio, on choisit dâ€™utiliser la proportion de rÃ©fÃ©rence au dÃ©nominateur, ce qui est arbitraire. En utilisant le log du ratio, lâ€™inverse du ratio ne sera quâ€™un changement de signe, ce qui est pratique en statistiques linÃ©aries. Cette solution, proposÃ©e par Aitchison (1986), sâ€™applique non seulement sur les compositions Ã  deux composantes, mais sur toute composition. Il sâ€™agit alors dâ€™utiliser une composition de rÃ©fÃ©rence pour effecteur les ratios. Pour une composition de \\(A\\), \\(B\\), \\(C\\), \\(D\\) et \\(E\\):\n\\[alr_A = log \\left( \\frac{A}{E} \\right), alr_B = log \\left( \\frac{B}{E} \\right), alr_C = log \\left( \\frac{C}{E} \\right), alr_D = log \\left( \\frac{D}{E} \\right)\\]\nDans R, la colonne de rÃ©fÃ©rence est par dÃ©faut la derniÃ¨re colonne de la matrice des compositions.\n\nadd_lr &lt;- alr(comp)\n\nCette derniÃ¨re stratÃ©gie se nomme les log-ratios aditifs (\\(alr\\) pour additive log-ratio). Bien que valide pour effectuer des tests statistiques, cette stratÃ©gie a le dÃ©savantage de dÃ©pendre de la dÃ©cision arbitraire de la composante Ã  utiliser au numÃ©rateur. DeuxiÃ¨me restriction des alr: les axes de lâ€™espace des alr nâ€™Ã©tant pas orthogonaux, ils ne peuvent pas Ãªtre utilisÃ©s pour effectuer des statistiques basÃ©es sur les distances (que nous couvrirons au chapitreÂ 10).\nLâ€™autre stratÃ©gie proposÃ©e par Aitchison Ã©tait dâ€™effectuer un log-ratio entre chaque composante et la moyenne gÃ©omÃ©trique de toutes les composantes. Cette transformation se nomme le log-ratio centrÃ© (\\(clr\\), pour centered log-ratio)\n\\[clr_i = log \\left( \\frac{x_i}{g \\left( x \\right)} \\right)\\]\nEn R,\n\ncen_lr &lt;- clr(comp)\n\nAvec des CLRs, les distances sont valides. Maisâ€¦ nous restons avec le problÃ¨me de la redondance dâ€™information. En fait, la somme de chacunes des lignes dâ€™une matrice de clr est de 0. Pas trÃ¨s pratique lorsque lâ€™on effectue des statistiques incluant une inversion de la matrice de covariance (distance de Mahalanobis, gÃ©ostatistiques, etc.)\ncen_lr %&gt;% \n  cov() %&gt;% \n  solve()\n Error in solve.default(.) : le systÃ¨me est numÃ©riquement singulier : conditionnement de la rÃ©ciproque = 4.44407e-17\nEnfin, une autre mÃ©thode de transformation dÃ©veloppÃ©e par Egoscue et al.Â (2003), les log-ratios isomÃ©triques (ou isometric log-ratios, ilr) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonormÃ©es. Ces dimensions doivent doivent Ãªtre prÃ©alablement Ã©tablie dans un dendrogramme de bifurcation, oÃ¹ chaque composante ou groupe de composante est successivement divisÃ© en deux embranchement. La maniÃ¨re dâ€™arranger ces balances importe peu, mais on aura avantage Ã  crÃ©er des balances interprÃ©tables.\nLe diagramme de balances peut Ãªtre encodÃ© dans une partition binaire sÃ©quentielle (ou sequential bianry partition, sbp). Une sbp est une matrice de contraste ou chaque ligne reprÃ©sente une partition entre deux variables ou groupes de variables. Une composante Ã©tiquettÃ©e +1 correspondra au groupe du numÃ©rateur, une composante Ã©tiquettÃ©e -1 au dÃ©nominateur et une composante Ã©tiquettÃ©e 0 sera exclue de la partition (Parent et al., 2013). Jâ€™ai reformulÃ© la fonction CoDaDendrogram pour que lâ€™on puisse ajouter des informations intÃ©ressantes sur les balants horizontaux. Cette fonction est disponible sur github.\n\nsource(\"https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R\")\n\nsbp &lt;- matrix(c(1, 1,-1,\n                1,-1, 0),\n              byrow = TRUE,\n              ncol = 3)\n\nCoDaDendrogram2(comp, V = gsi.buildilrBase(t(sbp)), ylim = c(0, 1),\n                equal.height = TRUE)\n\n\n\n\nSi la SBP est plus imposante, il pourrait Ãªtre plus aisÃ© de monter dans un chiffrier, puis de lâ€™importer dans R via un fichier csv.\nLe calcul des ILRs est effectuÃ© comme suit.\n\\[ilr_j = \\sqrt{\\frac{n_j^+ n_j^-}{n_j^+ + n_j^-}} log \\left( \\frac{g \\left( c_j^+ \\right)}{g \\left( c_j^+ \\right)} \\right)\\]\nou, Ã  la ligne \\(j\\) de la SBP, \\(n_j^+\\) et \\(n_j^-\\) sont respectivement le nombre de composantes au numÃ©rateur et au dÃ©nominateur, \\(g \\left( c_j^+ \\right)\\) est la moyenne gÃ©omÃ©trique des composantes au numÃ©rateur et \\(g \\left( c_j^- \\right)\\) est la moyenne gÃ©omÃ©trique des composantes au dÃ©nominateur.\nLes balances sont conventionnellement notÃ©es [A,B | C,D], ou les composantes A et B au dÃ©nominateur sont balancÃ©es avec les composantes C and D au numÃ©rateur. Une balance positive signifie que la moyenne gÃ©omÃ©trique des concentrations au numÃ©rateur est supÃ©rieur Ã  celle au dÃ©nominateur, et inversement, alors quâ€™une balance nulle signifie que les moyennes gÃ©omÃ©triques sont Ã©gales (Ã©quilibre). Ainsi, en modÃ©lisation linÃ©aire, un coefficient positif sur [A,B | C,D] signifie que lâ€™augmentation de lâ€™importance de C et D comparativement Ã  A et B est associÃ© Ã  une augmentation de la variable rÃ©ponse du modÃ¨le.\nEn R,\n\niso_lr &lt;- ilr(comp, V = gsi.buildilrBase(t(sbp)))\n\nNotez la forme gsi.buildilrBase(t(sbp)) est une opÃ©ration pour obtenir la matrice dâ€™orthonormalitÃ© Ã  partir de la SBP.\nLes ILRs sont des balances multivariÃ©es sur lesquelles on pourra effectuer des statistiques linÃ©aries. Bien que lâ€™interprÃ©tation des rÃ©sultats comme collection dâ€™interprÃ©tations sur des balances univariÃ©es pourra Ãªtre affectÃ©e par la structure de la SBP, ni les statistiques linÃ©aires multivariÃ©es, ni la distance entre les points ne seront affectÃ©s. En effet, chaque variante de la SBP est une rotation (dâ€™un facteur de 60Â°) par rapport Ã  lâ€™origine:\n\nsource(\"lib/ilr-rotation-sbp.R\")\n\n\nPour les transformations inverses, vous pourrez utiliser les fonctions alrInv, clrInv et ilrInv. Dans tous les cas, si vous tenez Ã  garder la trace de vos donnÃ©es dans leur format original, vous aurez avantage Ã  ajouter Ã  votre vecteur compositionnel la valeur de remplissage, constituÃ© dâ€™un amalgame des composantes non mesurÃ©es. Par exemple,\n\npourc &lt;- c(N = 0.03, P = 0.001, K = 0.01)\nacomp(pourc) # vous perdez la trace des proportions originales\n\n           N            P            K \n\"0.73170732\" \"0.02439024\" \"0.24390244\" \nattr(,\"class\")\n[1] \"acomp\"\n\n\n\npourc &lt;- c(N = 0.03, P = 0.001, K = 0.01)\nFv &lt;- 1 - sum(pourc)\ncomp &lt;- acomp(c(pourc, Fv = Fv))\ncomp\n\n      N       P       K      Fv \n\"0.030\" \"0.001\" \"0.010\" \"0.959\" \nattr(,\"class\")\n[1] \"acomp\"\n\n\n\niso_lr &lt;- ilr(comp) # avec une sbp par dÃ©faut\nilrInv(iso_lr)\n\n      N       P       K      Fv \n\"0.030\" \"0.001\" \"0.010\" \"0.959\" \nattr(,\"class\")\n[1] \"acomp\"\n\n\nSi vos donnÃ©es font partie dâ€™un tout, je vous recommande chaudement dâ€™utiliser des mÃ©thodes compositionnelles autant pour lâ€™analyse que la modÃ©lisation. Pour en savoir davantage, le livre Compositional data analysis with R, de van den Boogart et Tolosana-Delgado, est disponible en format Ã©lectronique Ã  la bibliothÃ¨que de lâ€™UniversitÃ© Laval.\nPour aller plus loin, jâ€™ai Ã©cri un billet Ã  ce sujet (auquel Ã  ce jour il manque toujours un cas dâ€™Ã©tude): We should use balances and machine learning to diagnose ionomes.\n\n9.2.2 AcquÃ©rir des donnÃ©es mÃ©tÃ©o\nUne tÃ¢che commune en Ã©cologie est de lier des observations Ã  la mÃ©tÃ©oâ€¦ qui sont rarement collectÃ©s lors dâ€™expÃ©riences. Environnement Canada possÃ¨de son rÃ©seau de stations. Les donnÃ©es sont disponibles sur internet en libre accÃ¨s. Vous pouvez chercher des stations, effectuer des requÃªtes et tÃ©lÃ©charger des fichiers csv. Pour un petit tableau, la tÃ¢che est plutÃ´t triviale. Mais Ã§a devient rapidement laborieux Ã  mesure que lâ€™on doit rechercher de nombreuses donnÃ©es.\nLe module weathercan, dÃ©veloppÃ© par Steffi LaZerte, permet dâ€™effectuer des requÃªtes rapidement Ã  partir des coordonnÃ©es de votre site expÃ©rimental. Par exemple, si je cherche une station mÃ©tÃ©o fournissant des donnÃ©es horaires situÃ©e Ã  moins de 20 km du sommet du Mont-Bellevue, Ã  Sherbrooke, aux coordonnÃ©es [latitude 45.35, longitude -71.90] :\n\nlibrary(\"weathercan\")\nstation_site &lt;- stations_search(coords = c(45.35, -71.90), dist = 20, interval = \"hour\")\n\nThe stations data frame hasn't been updated in over 4 weeks. Consider running `stations_dl()` to check for updates and make sure you have the most recent stations list available\nThe stations data frame hasn't been updated in over 4 weeks. Consider running `stations_dl()` to check for updates and make sure you have the most recent stations list available\n\nstation_site\n\n# A tibble: 4 Ã— 17\n  prov  station_name station_id climate_id WMO_id TC_id   lat   lon  elev tz    \n  &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n1 QC    LENNOXVILLE        5397 7024280     71611 WQH    45.4 -71.8  181  Etc/Gâ€¦\n2 QC    SHERBROOKE        48371 7028123     71610 YSC    45.4 -71.7  241. Etc/Gâ€¦\n3 QC    SHERBROOKE A       5530 7028124     71610 YSC    45.4 -71.7  241. Etc/Gâ€¦\n4 QC    SHERBROOKE A      30171 7028126        NA GSC    45.4 -71.7  241. Etc/Gâ€¦\n# â„¹ 7 more variables: interval &lt;chr&gt;, start &lt;dbl&gt;, end &lt;dbl&gt;, normals &lt;lgl&gt;,\n#   normals_1981_2010 &lt;lgl&gt;, normals_1971_2000 &lt;lgl&gt;, distance &lt;dbl&gt;\n\n\nJe prends en note lâ€™identifiant de la station dÃ©sirÃ©e (ou des stations, disons 5397 et 48371), puis je lance une requÃªte pour obtenir la mÃ©tÃ©o horaire entre les dates dÃ©sirÃ©es.\n\nmont_bellevue &lt;- weather_dl(station_ids = c(5397, 48371),\n                            start = \"2024-02-16\",\n                            end = \"2024-02-23\",\n                            interval = \"hour\",\n                            verbose = TRUE, \n                            time_disp = \"local\")\n\nThe stations data frame hasn't been updated in over 4 weeks. Consider running `stations_dl()` to check for updates and make sure you have the most recent stations list available\n\n\nGetting station: 5397\n\n\nFormatting station data: 5397\n\n\nAdding header data: 5397\n\n\nGetting station: 48371\n\n\nFormatting station data: 48371\n\n\nAdding header data: 48371\n\n\nTrimming missing values before and after\n\n\nAs of weathercan v0.3.0 time display is either local time or UTC\nSee Details under ?weather_dl for more information.\nThis message is shown once per session\n\nmont_bellevue |&gt; head(5)\n\n# A tibble: 5 Ã— 37\n  station_name station_id station_operator prov    lat   lon  elev climate_id\n  &lt;chr&gt;             &lt;dbl&gt; &lt;lgl&gt;            &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n1 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n2 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n3 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n4 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n5 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n# â„¹ 29 more variables: WMO_id &lt;chr&gt;, TC_id &lt;chr&gt;, date &lt;date&gt;, time &lt;dttm&gt;,\n#   year &lt;chr&gt;, month &lt;chr&gt;, day &lt;chr&gt;, hour &lt;chr&gt;, weather &lt;chr&gt;, hmdx &lt;dbl&gt;,\n#   hmdx_flag &lt;chr&gt;, precip_amt &lt;dbl&gt;, precip_amt_flag &lt;chr&gt;, pressure &lt;dbl&gt;,\n#   pressure_flag &lt;chr&gt;, rel_hum &lt;dbl&gt;, rel_hum_flag &lt;chr&gt;, temp &lt;dbl&gt;,\n#   temp_dew &lt;dbl&gt;, temp_dew_flag &lt;chr&gt;, temp_flag &lt;chr&gt;, visib &lt;dbl&gt;,\n#   visib_flag &lt;chr&gt;, wind_chill &lt;dbl&gt;, wind_chill_flag &lt;chr&gt;, wind_dir &lt;dbl&gt;,\n#   wind_dir_flag &lt;chr&gt;, wind_spd &lt;dbl&gt;, wind_spd_flag &lt;chr&gt;\n\n\nEt voilÃ .\n\nmont_bellevue |&gt;  \n  ggplot(aes(x = time, y = temp)) +\n  geom_line(aes(colour = station_name))\n\n\n\n\n\n9.2.3 PÃ©domÃ©trie avec R\nCette section a Ã©tÃ© Ã©crite par Michael Leblanc. Je nâ€™y ai appliquÃ© que quelques retouches esthÃ©tiques.\nPlusieurs fonctionnalitÃ©s ont Ã©tÃ© dÃ©veloppÃ©es sur R afin dâ€™aider les pÃ©domÃ©triciens Ã  visualiser, explorer et traiter les donnÃ©es numÃ©riques en science des sols. Voici quelques exemples.\n\n9.2.3.1 Texture du sol\nLa texture du sol est dÃ©finie par sa composition granulomÃ©trique, habituellement reprÃ©sentÃ©e par trois fractions (sable, limon, argile), laquelle peut Ãªtre gÃ©nÃ©ralisÃ©e en classe texturale. La dÃ©finition des classes texturales diffÃ¨re dâ€™un systÃ¨me ou dâ€™un pays Ã  lâ€™autre comme en tÃ©moigne lâ€™article Perdus dans le triangle des textures (Richer de Forges et al.Â 2008). La dÃ©finition des fractions granulomÃ©triques peut Ã©galement diffÃ©rer selon le domaine dâ€™Ã©tude (ingÃ©nierie, pÃ©dologie) ou le pays. Par exemple, le diamÃ¨tre du limon est de 0,002 mm Ã  0,05 mm dans le systÃ¨me canadien, amÃ©ricain et franÃ§ais alors quâ€™il est de 0,002 mm Ã  0,02 mm dans le systÃ¨me australien et de 0,002 mm Ã  0,063 mm dans le systÃ¨me allemand. Il est donc important de vÃ©rifier la mÃ©thodologie et le systÃ¨me de classification utilisÃ©s pour interprÃ©ter les donnÃ©es de texture du sol. Le module soilTexture propose des fonctions permettant dâ€™aborder ces multiples dÃ©finitions.\n\nlibrary(\"soiltexture\")\n\n\n9.2.3.1.1 Les triangles texturaux\nAvec la fonction TT.plot, vous pouvez prÃ©senter vos donnÃ©es granulomÃ©triques dans un triangle textural tel que dÃ©fini par les diffÃ©rents systÃ¨mes nationaux. Auparavant, crÃ©ons un objet comprenant des textures alÃ©atoires.\n\nset.seed(848341) # random.org\nrand_text &lt;- TT.dataset(n=100, seed.val=29)\nhead(rand_text)\n\n       CLAY     SILT      SAND          Z\n1 54.650857 40.37101  4.978129 13.2477582\n2 44.745954 40.81782 14.436221 20.8433109\n3 18.192509 48.26752 33.539970  7.1814626\n4 17.750492 40.14405 42.105458 -0.2077358\n5 65.518360 23.36110 11.120538 10.8656027\n6  6.610293 22.45353 70.936173  3.7108567\n\n\nAvec le module soiltexture, les tableaux de texture doivent inclure les intitullÃ©s exactes CLAY, SILT et SAND (notez les majuscules). Les points des textures gÃ©nÃ©rÃ©es peuvent Ãªtre portÃ©s dans des diagrammes ternaires texturaux de diffÃ©rents systÃ¨mes de classification, par exemple le systÃ¨me canadioen et le systÃ¨me USDA.\n\npar(mfrow=c(1, 2))\n\nTT.plot(class.sys = \"CA.FR.TT\", \n        tri.data = rand_text,\n        col = \"blue\")\nTT.plot(class.sys = \"USDA.TT\", \n        tri.data = rand_text,\n        col = \"blue\")\n\n\n\n\nLes paramÃ¨tres de la figure (titres, polices, style de la grille, etc.) peuvent Ãªtre personnalisÃ©s avec les arguments TT.plot.\n\n9.2.3.1.2 Les classes texturales\nLa fonction TT.points.in.classes est utile pour dÃ©signer la classe texturale Ã  partir des donnÃ©es granulomÃ©triques, en spÃ©cifiant bien le systÃ¨me de classification dÃ©sirÃ©.\n\nTT.points.in.classes(\n  tri.data = rand_text[1:10, ], # \n  class.sys = \"CA.FR.TT\",\n  PiC.type = \"t\"\n)\n\n [1] \"ALi\" \"ALi\" \"L\"   \"L\"   \"ALo\" \"LS\"  \"ALo\" \"A\"   \"LLi\" \"LSA\"\n\n\nPlusieurs autres fonctions sont proposÃ©es par soiltexture afin de visualiser, classifier et transformer les donnÃ©es de texture du sol : Functions in soiltexture. Julien Moeys (2018) propose Ã©galement le tutoriel The soil texture wizard: a tutorial.\n\n9.2.3.2 Profils de sols\nLe profil de sols est une entitÃ© dÃ©crite par une sÃ©quence de couches ou dâ€™horizons avec diffÃ©rentes caractÃ©ristiques morphologiques. Le module AQP, pour Algorithms for Quantitative Pedology, propose des fonctions de visualisation, dâ€™agrÃ©gation et de classification permettant dâ€™aborder la complexitÃ© inhÃ©rente aux informations pÃ©dologiques.\n\n9.2.3.2.1 La visualisation de profils\nVous devez dâ€™abord structurer vos donnÃ©es dans un tableau (data.frame) incluant minimalement ces trois colonnes :\n\nIdentifiant unique du profil (groupes dâ€™horizons) (id)\nLimites supÃ©rieures de lâ€™horizon (top)\nLimites infÃ©rieures de lâ€™horizon (down)\n\nVos donnÃ©es morphologiques, physico-chimiques, etc., sont incluses dans les autres colonnes. Chargeons un fichier pÃ©dologique Ã  titre dâ€™exemple.\n\nprofils &lt;- read_csv(\"data/08_pedometric-profile.csv\")\n\nRows: 44 Columns: 9\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (2): horizon, hue\ndbl (7): id, top, bottom, value, chroma, pH.CaCl2, C.CNS.pc\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(profils)\n\n# A tibble: 6 Ã— 9\n     id horizon   top bottom hue   value chroma pH.CaCl2 C.CNS.pc\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     1 Ap1         0     23 10YR      2      3     4.78     2.71\n2     1 Ap2        23     34 10YR      2      2     4.74     2.2 \n3     1 Bfcj       34     46 7.5YR     4      5     4.79     2.4 \n4     1 BC         46     83 2.5Y      4      5     4.93     0.22\n5     1 C          83    100 2.5Y      5      4     4.82     0.18\n6     2 Ap          0     29 10YR      2      2     4.6      4.22\n\n\nLa fonction munsell2rgb permet de convertir le code de couleur Munsell en format RGB.\n\nlibrary(\"aqp\")\n\nThis is aqp 2.0.2\n\n\n\nAttaching package: 'aqp'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, slice\n\nprofils$soil_color &lt;- with(profils, munsell2rgb(hue, value, chroma))\n\nPrÃ©alablement Ã  la visualisation, le tableau est transformÃ© en objet SoilProfileCollection par la fonction depths. Pour ce faire, le tableau doit Ãªtre un pur data.frame, non pas un tibble.\n\nprofils &lt;- profils %&gt;% as.data.frame()\ndepths(profils) &lt;- id ~ top + bottom\n\nLa fonction plot dÃ©tectera le type dâ€™objet et appellera la fonction de visualisation en consÃ©quence.\n\npar(mfrow = c(1, 3))\nplot(profils, name=\"horizon\")\ntitle('Couleur des horizons', cex.main=1)\nplot(profils, name=\"horizon\", color='C.CNS.pc', col.label='C total (%)')\nplot(profils, name=\"horizon\", color='pH.CaCl2', col.label='pH CaCl2')\n\n\n\n\nDe multiples figures thÃ©matiques peuvent Ãªtre gÃ©nÃ©rÃ©es afin de reprÃ©senter les particuliaritÃ©s des profils. Pour aller plus loin, consultez les guides Introduction to SoilProfileCollection Objects et Generating Sketches from SPC Objects.\n\n9.2.3.2.2 Les plans verticaux (depth functions)\nLes plans verticaux sont des diagrammes qui permettent dâ€™interprÃ©ter les donnÃ©es en fonction de la profondeur. La fonction slab permet le calcul de statistiques descriptives par intervalles de profondeur rÃ©guliers, lesquelles permettent de visualiser la variabilitÃ© verticale des propriÃ©tÃ©s des sols.\n\nagg &lt;- slab(profils, fm = ~ C.CNS.pc + pH.CaCl2)\n\nLa visualisation est gÃ©nÃ©rÃ©e par le module graphique ggplot2\n\nagg %&gt;%\n  ggplot(mapping = aes(x = -top, y = p.q50)) +\n  facet_grid(. ~ variable, scale = \"free\") +\n  geom_ribbon(aes(ymin =  p.q25, ymax = p.q75), fill = \"grey75\", alpha = 0.5) +\n  geom_path() +\n  labs(x = \"Profondeur (cm)\",\n       y = \"MÃ©diane bordÃ©e des 25e and 75e percentiles\") +\n  coord_flip()\n\n\n\n\n\n9.2.3.2.3 Le regroupement de profils\nLe calcul des distances de dissimilaritÃ© entre les profils avec profile_compare permet la construction de dendrogramme et le regroupement des profils. Notez que nous survolerons au chapitreÂ 10 les concepts de dissimilaritÃ© et de partitionnement.\n\nlibrary(\"cluster\")\nlibrary(\"mvtnorm\")\nlibrary(\"sharpshootR\") \n\nRegistered S3 method overwritten by 'vegan':\n  method          from   \n  print.nullmodel parsnip\n\nlibrary(\"igraph\")\n\n\nAttaching package: 'igraph'\n\n\nThe following object is masked from 'package:compositions':\n\n    normalize\n\n\nThe following objects are masked from 'package:dials':\n\n    degree, neighbors\n\n\nThe following objects are masked from 'package:lubridate':\n\n    %--%, union\n\n\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n\n\nThe following objects are masked from 'package:purrr':\n\n    compose, simplify\n\n\nThe following object is masked from 'package:tidyr':\n\n    crossing\n\n\nThe following object is masked from 'package:tibble':\n\n    as_data_frame\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\n# remotes::install_github(\"ncss-tech/sharpshootR\")\nd &lt;- profile_compare(profils, vars=c('C.CNS.pc', 'pH.CaCl2'), k=0, max_d=40)\n\nWarning: 'profile_compare' is deprecated.\nUse 'NCSP' instead.\nSee help(\"Deprecated\")\n\n\nWarning: profile_compare() has been deprecated, please use NCSP()\n\n\nComputing dissimilarity matrices from 10 profiles\n\n\n [0.08 Mb]\n\n\nWarning: SPC / distance matrix IDs out of order, soon to be fixed (#7)\n\nd_diana &lt;- diana(d)\nplotProfileDendrogram(profils, name=\"horizon\", d_diana,\n                      scaling.factor = 0.3, y.offset = 5,\n                      color='pH.CaCl2',  col.label='pH CaCl2')\n\nprofile IDs and clustering IDs are not in the same order\n\n\n\n\n\n\n9.2.3.2.4 Diagramme de relations entre les horizons\nIl est possible de visualiser les transitions dâ€™horizon les plus probables dans un groupe de profils de sols.\n\ntp &lt;- hzTransitionProbabilities(profils, name=\"horizon\")\n\nWarning: ties in transition probability matrix\n\npar(mar = c(0, 0, 0, 0), mfcol = c(1, 2))\nplot(profils, name=\"horizon\")\nplotSoilRelationGraph(tp, graph.mode = \"directed\", edge.arrow.size = 0.5, edge.scaling.factor = 2, vertex.label.cex = 0.75, \n                      vertex.label.family = \"sans\")\n\n\n\n\nConsultez AQP project pour des prÃ©sentations, des tutoriels et des exemples de figures qui montrent les nombreuses possibilitÃ©s du package AQP.\n\n9.2.4 MÃ©ta-analyses en R\nJe conseille les livres Introduction to Meta-Analysis, Meta-analysis with R et Handbook of Meta-analysis in Ecology and Evolution pour les mÃ©ta-analyses sur des Ã©cosystÃ¨mes. Le module metafor est un incournable pour effectuer des mÃ©taanalyses en R. On ne passe pas tout Ã  fait Ã  cÃ´tÃ© si lâ€™on utilise le module meta, lui-mÃªme basÃ© en partie sur metafor. Le module meta a touttefois lâ€™avantage dâ€™Ãªtre simple dâ€™utilisation. Par exemple, pour une mÃ©ta-analyse dâ€™une rÃ©ponse continue,\n\nlibrary(\"meta\")\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 7.0-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\n\n\nAttaching package: 'meta'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    ci\n\ndata(Fleiss1993cont) |&gt; head(5)\n\n[1] \"Fleiss1993cont\"\n\nmeta_analyse &lt;- metacont(n.e = n.psyc, mean.e = mean.psyc, sd.e = sd.psyc, \n                        n.c = n.cont, mean.c = mean.cont, sd.c = sd.cont,\n                        comb.fixed = T, comb.random = T, studlab = study,\n                        data = Fleiss1993cont, sm = \"SMD\")\nmeta_analyse\n\nNumber of studies: k = 5\nNumber of observations: o = 232 (o.e = 106, o.c = 126)\n\n                         SMD             95%-CI     z p-value\nCommon effect model  -0.3434 [-0.6068; -0.0801] -2.56  0.0106\nRandom effects model -0.3434 [-0.6068; -0.0801] -2.56  0.0106\n\nQuantifying heterogeneity:\n tau^2 = 0 [0.0000; 0.7255]; tau = 0 [0.0000; 0.8518]\n I^2 = 0.0% [0.0%; 79.2%]; H = 1.00 [1.00; 2.19]\n\nTest of heterogeneity:\n    Q d.f. p-value\n 3.68    4  0.4514\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hedges' g (bias corrected standardised mean difference; using exact formulae)\n\n\nEt pour effectuer un forest plot,\n\nforest(meta_analyse)\n\n\n\n\n\n9.2.5 CrÃ©er des applications avec R\nRStudio vous permet de dÃ©ployer vos rÃ©sultats sous forme dâ€™applications web grÃ¢ce Ã  son module shiny. Pour ce faire, le seul prÃ©alable est de savoir programmer en R. En agenÃ§ant une interface avec des inputs (listes de sÃ©lection, des boÃ®tes de dialogue, des sÃ©lecteurs, des boutons, etc.) avec des modÃ¨les que vous dÃ©veloppez, vous pourrez crÃ©er des interfaces intÃ©ractives.\nPour crÃ©er une application shiny, vous devez crÃ©er une partie pour lâ€™interface (ui) et une autre pour le calcul (server). Je nâ€™irai pas dans les dÃ©tails, Ã©tant donnÃ©e quâ€™il sâ€™agit dâ€™un sujet Ã  part entiÃ¨re. Pour aller plus loin, visitez le site du projet shiny.\nlibrary(\"shiny\")\n\nui &lt;- basicPage(\n  sliderInput(\"A\", \"Asymptote:\", min = 0, max = 100, value = 50),\n  sliderInput(\"E\", \"Environnement:\", min = -10, max = 100, value = 20),\n  sliderInput(\"R\", \"Taux:\", min = 0, max = 0.1, value = 0.035),\n  sliderInput(\"prix_dose\", \"Prix dose:\", min = 0, max = 5, value = 1),\n  sliderInput(\"prix_vente\", \"Prix vente:\", min = 0, max = 200, value = 100),\n  sliderInput(\"dose\", \"Dose:\", min = 0, max = 300, value = c(0, 200)),\n  plotOutput(\"distPlot\")\n)\n\nserver &lt;- function(input, output) {\n  mitsch_f &lt;- reactive({\n    input$A * (1 - exp(-input$R * (seq(input$dose[1], input$dose[2], length = 100) + input$E)))\n  })\n  \n  mitsch_opt &lt;- reactive({\n    (log((input$A * input$R * input$prix_vente) / input$prix_dose - input$E * input$R) / input$R )\n  })\n  \n  \n  output$distPlot &lt;- renderPlot({\n    plot(seq(input$dose[1], input$dose[2], length = 100), mitsch_f(), type = \"l\", ylim = c(0, 100))\n    abline(v = mitsch_opt() )\n    text(mitsch_opt(), 2, paste(\"Dose optimale:\", round(mitsch_opt(), 0)))\n  })\n}\n\nshinyApp(ui, server)\nUne fois lâ€™application crÃ©Ã©e, il est possible de la dÃ©ployer sur le site shninyapps.io. Dâ€™abord crÃ©er une application shiny dans RStudio: File &gt; New File &gt; Shiny Web App. Ã‰crivez votre code dans le fichier app.R (dans ce cas, ce peut Ãªtre un copier-coller), puis cliquez sur Run App en haut Ã  droite de la fenÃªtre dâ€™Ã©dition du code. Lorsque lâ€™application fonctionne, vous pourrez la publier via RStudio en cliquant sur le bouton Publish dans la fenÃªtre Viewer (vous devez au prÃ©alable avoir un comte sur shinyapp.io).\nUne application sera publique et sera ouverte. https://essicolo.shinyapps.io/Mitscherlich/\nPour dÃ©ployer en mode privÃ©, vous devrez dÃ©bourser pour un forfait ou installer votre propre serveur.\n\n9.2.6 Travailler en Python\nLe chapitreÂ 8 a prÃ©sentÃ© un module pour les statistiques bayÃ©siennes nÃ©cessitant un environnement Python. Il sâ€™agissait de faire fonctionner un module en R qui, Ã  lâ€™interne, effectue ses calculs en Python. Rien ne vous empÃªche dâ€™effectuer des calculs directement en Python Ã  mÃªme lâ€™interface de RStudio.\nNote : Nous avons aussi ajoutÃ© une section facultative dâ€™introduction Ã  Python au chapitreÂ 6 si vous souhaitez aller plus loin.\nIl vous faudra dâ€™abord installer Python et les modules de calcul que vous dÃ©sirez. Il existe plusieurs distributions de Python. Parmi elles, Anaconda est probablement la plus intuitive Ã  installer. Choisissez dâ€™abord Anaconda (~500 Mo) ou Miniconda pour une installation minimale (~60 Mo) - si vous installez Miniconda, vous devrez aussi installer les modules nÃ©cessaires pour le calcul. Installez aussi le module reticulate de R, de sorte que vous puissiez communiquer avec Python. Anaconda fonctionne avec des environnements de calcul. Chaque environnement possÃ¨de sa propre version de Python et ses propres modules: cela vous permet dâ€™isoler vos environnements et de contrÃ´ler la version des modules. Vous pouvez connecter R Ã  lâ€™environnement de base crÃ©Ã© lors de lâ€™installation dâ€™Anaconda, ou bien en crÃ©er un autre. Pour en crÃ©er un nouveau, incluant une liste de modules de calcul,\n#{r message=FALSE} library(\"reticulate\") conda_create(envname = \"monprojet\", packages = c(\"python\", \"numpy\", \"scipy\", \"matplotlib\", \"pandas\", \"scikit-learn\"))\nConnectez-vous Ã  votre environnement Python, par exemple jâ€™utilise lâ€™environnement par dÃ©faut anaconda3.\n#{r expl-connecter-python} library(\"reticulate\") conda_list() use_condaenv(\"anaconda3\", required = TRUE)\nSupposons que vous travailliez en R markdown. Pour lancer un bloc de code en Python, indiquez python au lieu de r dans lâ€™entÃªte. Il faudra que les modules numpy, pandas et matplotlib soient installÃ© dans votre environnement Python\n#{python expl-py-matplotlib} import numpy as np import pandas as pd import matplotlib.pyplot as plt a = np.linspace(0, 30, 101) b = np.sin(a) plt.plot(a, b) plt.title(\"Un graphique en Matplotlib dans RStudio\")\nPour rÃ©cupÃ©rer une variable Python en R, prÃ©cÃ©dez la variable de py$.\n#{r expl-importer-var-py} plot(py$a, py$b, type = \"l\", main = \"Un graphique en R avec \\n des variables dÃ©finies en Python\")\nIdem, pour rÃ©cupÃ©rer un objet R en Python, .\n#{python expl-importer-var-r} r.iris.head(6)\nVous aurez ainsi accÃ¨s aux fonctionnalitÃ©s de Python et R dans un mÃªme flux de travail. Python nâ€™est pas si difÃ©rent de R, mais il vous faudra dÃ©ployer des efforts pour approvoiser le serpent. Pour dÃ©buter en Python, je suggÃ¨re Python Data Science Handbook, de Jake VanderPlas. Pour en savoir plus sur le travail en R et en Python dans RSutdio, rÃ©fÃ©rez-vous Ã  la documentation du module reticulate."
  },
  {
    "objectID": "08-explorer.html#bonus-ia-gÃ©nÃ©rative",
    "href": "08-explorer.html#bonus-ia-gÃ©nÃ©rative",
    "title": "9Â  Explorer R",
    "section": "\n9.3 Bonus : IA GÃ©nÃ©rative",
    "text": "9.3 Bonus : IA GÃ©nÃ©rative\nEn 2024, les robots conversationnels font les manchettes rÃ©guliÃ¨rement, et nous promettent des Superpouvoirs. En effet, lâ€™IA gÃ©nÃ©rative a le potentiel dâ€™accÃ©lÃ©rer votre flux de travail et de rÃ©diger pour vous des sections de code. Le fonctionnement repose sur des algorithmes dâ€™autoapprentissage : La puissance de calcul permet dÃ©sormais dâ€™entraÃ®ner les modÃ¨les avec une quantitÃ© phÃ©nomÃ©nale de documents, si bien que les modÃ¨les apprennent de nous. Ils peuvent ensuite gÃ©nÃ©rer du texte, des blocs de code, des images; Ã  condition de leur poser les bonnes questions! En effet, les modÃ¨les ne sont pas capables (pour lâ€™instant!) de crÃ©er de la nouveautÃ©, et ils dÃ©pendent de ce qui existe dÃ©jÃ  dans leur entraÃ®nement. La vitesse de dÃ©veloppement de ces outils est exponentielle, si bien que derniÃ¨rement, toutes les grandes entreprises semblent vouloir crÃ©er leur propre IA.\nToutefois, les risques liÃ©s Ã  cette technologie sont tels que la communautÃ© scientifique appelle Ã  une pause dans son dÃ©veloppement, le temps de sâ€™organiser pour la recevoir adÃ©quatement. Les principes dâ€™Asilomar, premiers principes de gouvernance en IA, fournissent un premier jet de la direction souhaitÃ©e par les experts dans le domaine : Loin de viser des outils non dirigÃ©s et autonomes, lâ€™intelligence artificielle devrait Ãªtre bÃ©nÃ©fique, utile, sÃ©curitaire, transparente et non-subversive, tout en profitant au plus grand nombre de personnes possible et au bien commun (je rÃ©sume ici, mais vous pouvez consulter les principes sur le site mentionnÃ© plus haut).\nIl existe plusieurs modÃ¨les de language, et la plupart dâ€™entre eux sont pour lâ€™instant propriÃ©taires, incluant le fameux ChatGPT dâ€™OpenAI. Ã€ ce sujet, certains scientifiques suggÃ¨rent dâ€™Ã©viter lâ€™attrait des modÃ¨les propriÃ©taires et de travailler au dÃ©veloppement de modÃ¨les opensource, le but Ã©tant notamment de favoriser une meilleure reproductibilitÃ© (les modÃ¨les propriÃ©taires sont appelÃ©s Ã  Ãªtre modifiÃ©s sans prÃ©-avis, ce qui rend difficile la reproduction des rÃ©sultats). De plus, les modÃ¨les propriÃ©taires ont tendance Ã  recueillir vos donnÃ©es de recherche et vos donnÃ©es personnelles pour sâ€™entraÃ®ner, si bien que des questions sur lâ€™Ã©thique et la vie privÃ©e se posent.\n\n9.3.1 Utilisation en rÃ©daction scientifique\nLâ€™utilisation de lâ€™IA gÃ©nÃ©rative demeure controversÃ©e pour une utilisation en rÃ©daction scientifique. En effet, en plus des questionnements Ã©thiques et des erreurs potentielles et frÃ©quentes, les IA gÃ©nÃ©ratives sont incapables de comprendre ou de gÃ©nÃ©rer de nouvelles informations ou dâ€™effectuer une analyse profonde, ce qui pourrait limiter la discussion dans un article scientifique. Si vous souhaitez tout de mÃªme les utiliser, je vous suggÃ¨re de respecter les recommandations suivantes (ma traduction) :\n\n\nReconnaÃ®tre leur utilisation dans la section appropriÃ©e (typiquement Acknowledgments), indiquer les endroits dans le manuscrit oÃ¹ lâ€™IA a Ã©tÃ© utilisÃ©e et fournir les requÃªtes (prompt) et les questions posÃ©es au modÃ¨le dans le matÃ©riel supplÃ©mentaire.\n\nSouvenez-vous (ainsi que vos co-auteurs) que les sorties des modÃ¨les dâ€™IA ne reprÃ©sentent quâ€™un brouillon trÃ¨s prÃ©liminaire, au mieux. La sortie est incomplÃ¨te, peut comporter de lâ€™information incorrecte, et chaque phrase ainsi que chaque affirmation doit Ãªtre considÃ©rÃ©e de faÃ§on critique. VÃ©rifiez, vÃ©rifiez, et vÃ©rifiez encore. Ensuite, vÃ©rifiez une fois de plus.\n\nNâ€™utilisez pas les sorties des modÃ¨les de faÃ§on textuelles. Il ne sâ€™agit pas de vos mots. Le robot pourrait avoir rÃ©utilisÃ© du texte provenant dâ€™autre sources, ce qui mÃ¨nerait Ã  du plagiat involontaire.\n\nToutes les citations recommandÃ©es par une IA gÃ©nÃ©rative doivent Ãªtre vÃ©rifiÃ©es dans le document original puisque les robots conversationnels sont rÃ©putÃ©s pour gÃ©nÃ©rer des citations erronÃ©es.\n\nNâ€™incluez pas le robot conversationnel comme un co-auteur. Il ne peut pas gÃ©nÃ©rer de nouvelles idÃ©es ou rÃ©diger une discussion Ã  partir de nouveaux rÃ©sultats, Ã©lÃ©ments qui demeurent du domaine des humains. Il ne sâ€™agit que dâ€™un outil, comme plusieurs autres programmes, pour aider la formulation et la rÃ©daction de manuscrits (et jâ€™ajouterais de codes).\n\nLâ€™IA gÃ©nÃ©rative ne peut pas Ãªtre tenue responsable de toute affirmation ou manquement Ã  lâ€™Ã©thique. Ce sont tous les auteurs dâ€™un manuscrit qui partagent cette responsabilitÃ©.\n\nEt le plus important, ne laissez pas les robots conversationnels Ã©craser votre crÃ©ativitÃ© et votre rÃ©flexion profonde. Utilisez les pour ouvrir vos horizons et susciter de nouvelles idÃ©es!\n\nCes recommandations sont pour lâ€™utilisation dâ€™un texte, mes elles sâ€™appliquent aussi pour votre code.\n\n9.3.2 Et maintenantâ€¦ Pour lâ€™utiliser sur R?\nSous R, il existe plusieurs options, mais jâ€™ai seulement essayÃ© le module chattr, qui permet dâ€™intÃ©grer les modÃ¨les directement dans votre interface RStudio. Vous pouvez simplement suivre les Ã©tapes dâ€™installation, puis vous crÃ©er un compte et gÃ©nÃ©rer une clÃ© secrÃ¨te dâ€™activation auprÃ¨s dâ€™OpenAI si vous utilisez les modÃ¨les GPTs avec la commande Sys.setenv(\"OPENAI_API_KEY\" = \"####################\").\nJe vous conseille de modifier la version par dÃ©faut de modÃ¨le puisque GPT-4 est payant (personnellement, jâ€™ai utilisÃ© GPT-3.5-turbo avec la commande suivante : chattr::chattr_use(\"gpt35\")).\nUne fois installÃ©, vous pouvez lancer chattr::chattr_app() pour lancer lâ€™application dans lâ€™ongler Viewer de RStudio (alternativement, vous pouvez crÃ©er un raccourci-clavier Ã  partir de lâ€™onglet Outils &gt; Modifier les raccourcis-claviers). Ensuite, vous effectuez vos requÃªtes et GPT vous rÃ©pond, comme une conversation. Vous pouvez alors copier-coller les sorties, ou alors gÃ©nÃ©rer un script Ã  partir de celles-ci.\nAu dÃ©but, pour une utilisation simple et des opÃ©rations faciles, jâ€™ai Ã©tÃ© agrÃ©ablement surpris par les rÃ©ponses rapides, Ã©purÃ©es et bien structurÃ©es de GPT-3.5. Comme dans une conversation, je lui fournissais des requÃªtes (prompts) en lui disant quoi faire, puis le robot me rÃ©pondait avec une explication, un bloc de code et des commentaires sur les lignes de codes. Venaient ensuite quelques ajustements, par exemple utiliser ggplot au lieu de plot pour rÃ©aliser les graphiques, ou alors ajouter des facettes et intÃ©grer toutes les variables sous forme de boxplots.\nJâ€™ai ensuite dÃ©cidÃ© de passer au niveau supÃ©rieur et de faire des requÃªtes un peu plus complexes : Fais un modÃ¨le linÃ©aire sur telles variables, crÃ©e un code pour effectuer tel calcul sur un ensemble de donnÃ©es situÃ©es dans un dossier, etc. Ã€ cette Ã©tape, GPT sâ€™est emballÃ© et a commencÃ© Ã  me fournir des codes en pigeant dans plusieurs modules diffÃ©rents, en mÃ©langeant des mÃ©thodes rÃ©centes avec des mÃ©thodes plus anciennes et parfois dÃ©suettes. Les rÃ©sultats Ã©taient des blocs de codes souvent non fonctionnels, ou alors qui ne faisaient pas du tout ce que jâ€™avais dÃ©crit, mÃªme aprÃ¨s lâ€™ajout de dÃ©tails dans mes requÃªtes. Au bout dâ€™un certain temps, jâ€™avais atteint ma limite gratuite de Tokens (et de patience) pour le modÃ¨le GPT-3.5.\nAu final, Ã  ce stade-ci, jâ€™ai de la difficultÃ© Ã  recommander cet outil. Les algorithmes dâ€™autoapprentissage qui seront prÃ©sentÃ©s au chapitreÂ 13 sont beaucoup plus intÃ©ressants selon moi puisque leur utilisation reste limitÃ©e Ã  ce quâ€™on veut quâ€™ils fassent et que les dÃ©rives scientifiques me semblent moins importantes. Lâ€™IA gÃ©nÃ©rative a certainement sa place, mais nÃ©cessite une bonne connaissance initiale du sujet et des opÃ©rations Ã  effectuer. Son rÃ´le devrait Ãªtre principalement de faciliter le travail de programmeurs ou de scientifiques aguerris, mais ils ne devraient jamais remplacer lâ€™esprit critique, ni faire le travail pour vous."
  },
  {
    "objectID": "09-ordination.html",
    "href": "09-ordination.html",
    "title": "10Â  Association, partitionnement et ordination",
    "section": "",
    "text": "Ã€ venir"
  },
  {
    "objectID": "10-imputation.html",
    "href": "10-imputation.html",
    "title": "11Â  DÃ©tection de valeurs aberrantes et imputation de donnÃ©es manquantes",
    "section": "",
    "text": "Ã€ venir"
  },
  {
    "objectID": "11-series-temporelles.html",
    "href": "11-series-temporelles.html",
    "title": "12Â  Les sÃ©ries temporelles",
    "section": "",
    "text": "Ã€ venir"
  },
  {
    "objectID": "12-autoapprentissage.html",
    "href": "12-autoapprentissage.html",
    "title": "13Â  Introduction Ã  lâ€™autoapprentissage",
    "section": "",
    "text": "Ã€ venir"
  },
  {
    "objectID": "13-donnees-spatiales.html",
    "href": "13-donnees-spatiales.html",
    "title": "14Â  Les donnÃ©es gÃ©ospatiales",
    "section": "",
    "text": "Ã€ venir"
  },
  {
    "objectID": "14-modelisation.html",
    "href": "14-modelisation.html",
    "title": "15Â  ModÃ©lisation de mÃ©canismes Ã©cologiques",
    "section": "",
    "text": "Ã€ venir"
  },
  {
    "objectID": "08-explorer.html",
    "href": "08-explorer.html",
    "title": "9Â  Explorer R",
    "section": "",
    "text": "9.1 R sur le web\nDans un environnement de travail en Ã©volution rapide et constante, il est difficile de considÃ©rer que ses compÃ©tences sont abouties. Rester informÃ© sur le dÃ©veloppement de R vous permettra de trouver et de rÃ©soudre des problÃ¨mes persistants de maniÃ¨re plus efficace ou par de nouvelles avenues, et vous offrira mÃªme lâ€™occasion de dÃ©nicher des problÃ¨mes dont vous ne soupÃ§onniez pas lâ€™existence. Plusieurs sources dâ€™information vous permettront de vous tenir Ã  jour sur le dÃ©veloppement de R, de ses environnements de travail (RStudio, Jupyter, etc.) et des nouveaux modules qui sâ€™y greffent. Plus largement, vous gagnerez Ã  vous informer sur les derniÃ¨res tendances en calcul scientifique sur dâ€™autres plate-forme que R (Python, Javascript, Julia, etc.). Ã‰videmment, nos tÃ¢ches quotidiennes ne nous permettent pas de tout suivre. MÃªme si vous pouviez nâ€™attrapper que 1% du dÃ©filement, ce serait dÃ©jÃ  1% de plus que rien du tout.\nJe vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez Ã  lâ€™aventure!\nTirÃ© du film The Hobbit: An Unexpected Journey, de Peter Jackson (2012).",
    "crumbs": [
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>Explorer R</span>"
    ]
  },
  {
    "objectID": "09-ordination.html#espaces-danalyse",
    "href": "09-ordination.html#espaces-danalyse",
    "title": "10Â  Association, partitionnement et ordination",
    "section": "\n10.1 Espaces dâ€™analyse",
    "text": "10.1 Espaces dâ€™analyse\n\n10.1.1 Abondance et occurrence\nLâ€™abondance est le dÃ©compte dâ€™espÃ¨ces observÃ©es, tandis que lâ€™occurrence est la prÃ©sence ou lâ€™absence dâ€™une espÃ¨ce. Le tableau suivant contient des donnÃ©es dâ€™abondance.\n\nabundance &lt;- tibble('Bruant familier' = c(1, 0, 0, 3),\n                    'Citelle Ã  poitrine rousse' = c(1, 0, 0, 0),\n                    'Colibri Ã  gorge rubis' = c(0, 1, 0, 0),\n                    'Geai bleu' = c(3, 2, 0, 0),\n                    'Bruant chanteur' = c(1, 0, 5, 2),\n                    'Chardonneret' = c(0, 9, 6, 0),\n                    'Bruant Ã  gorge blanche' = c(1, 0, 0, 0),\n                    'MÃ©sange Ã  tÃªte noire' = c(20, 1, 1, 0),\n                    'Jaseur borÃ©al' = c(66, 0, 0, 0))\n\nCe tableau peut Ãªtre rapidement transformÃ© en donnÃ©es dâ€™occurrence, qui ne comprennent que lâ€™information boolÃ©enne de prÃ©sence (notÃ© 1) et dâ€™absence (notÃ© 0).\n\noccurrence &lt;- abundance |&gt;\n  transmute_all(~if_else(. &gt; 0, 1, 0))\n\nLâ€™espace des espÃ¨ces (ou des variables ou descripteurs) est celui oÃ¹ les espÃ¨ces forment les axes et oÃ¹ les sites sont positionnÃ©s dans cet espace. Il sâ€™agit dâ€™une perspective en mode R, qui permet principalement dâ€™identifier quels espÃ¨ces se retrouvent plus couramment ensemble.\n\nabundance |&gt; \n  select(`Bruant chanteur`, Chardonneret, `MÃ©sange Ã  tÃªte noire`)\n\n# A tibble: 4 Ã— 3\n  `Bruant chanteur` Chardonneret `MÃ©sange Ã  tÃªte noire`\n              &lt;dbl&gt;        &lt;dbl&gt;                  &lt;dbl&gt;\n1                 1            0                     20\n2                 0            9                      1\n3                 5            6                      1\n4                 2            0                      0\n\n\nDans lâ€™espace des sites (ou les Ã©chantillons ou objets), on transpose la matrice dâ€™abondance. On passe ici en mode Q, oÃ¹ chaque point est une espÃ¨ce, et oÃ¹ lâ€™on peut observer quels Ã©chantillons sont similaires.\n\nabundance |&gt; t()\n\n                          [,1] [,2] [,3] [,4]\nBruant familier              1    0    0    3\nCitelle Ã  poitrine rousse    1    0    0    0\nColibri Ã  gorge rubis        0    1    0    0\nGeai bleu                    3    2    0    0\nBruant chanteur              1    0    5    2\nChardonneret                 0    9    6    0\nBruant Ã  gorge blanche       1    0    0    0\nMÃ©sange Ã  tÃªte noire        20    1    1    0\nJaseur borÃ©al               66    0    0    0\n\n\n\n10.1.2 Environnement\nLâ€™espace de lâ€™environnement comprend souvent un autre tableau contenant lâ€™information sur lâ€™environnement oÃ¹ se trouve les espÃ¨ces: les coordonnÃ©es et lâ€™Ã©lÃ©vation, la pente, le pH du sol, la pluviomÃ©trie, etc."
  },
  {
    "objectID": "09-ordination.html#analyse-dassociation",
    "href": "09-ordination.html#analyse-dassociation",
    "title": "10Â  Association, partitionnement et ordination",
    "section": "\n10.2 Analyse dâ€™association",
    "text": "10.2 Analyse dâ€™association\nNous utiliserons le terme association comme une mesure pour quantifier la ressemblance ou la diffÃ©rence entre deux objets (Ã©chantillons) ou variables (descripteurs).\nAlors que la corrÃ©lation et la covariance sont des mesures dâ€™association entre des variables (analyse en mode R), la similaritÃ© et la distance sont deux types de une mesure dâ€™association entre des objets (analyse en mode Q). Une distance de 0 est mesurÃ©e chez deux objets identiques. La distance augmente au fur et Ã  mesure que les objets sont dissociÃ©s. Une similaritÃ© ayant une valeur de 0 indique aucune association, tandis quâ€™une valeur de 1 indique une association parfaite. Ã€ lâ€™opposÃ©, la dissimilaritÃ© est Ã©gale Ã  1-similaritÃ©.\nLa distance peut Ãªtre liÃ©e Ã  la similaritÃ© par la relation:\n\\[distance=\\sqrt{1-similaritÃ©}\\]\nou\n\\[distance=\\sqrt{dissimilaritÃ©}\\]\nLa racine carrÃ©e permet, pour certains indices de similaritÃ©, dâ€™obtenir des propriÃ©tÃ©s euclidiennes. Pour plus de dÃ©tails, voyez le tableau 7.2 de Legendre et Legendre (2012).\nLes matrices dâ€™association sont gÃ©nÃ©ralement prÃ©sentÃ©es comme des matrices carrÃ©es, dont les dimensions sont Ã©gales au nombre dâ€™objets (mode Q) ou de variables (mode R) dans le tableau. Chaque Ã©lÃ©ment (â€œcelluleâ€) de la matrice est un indice dâ€™association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilaritÃ©) ou unitaire (similaritÃ©), car elle correspond Ã  lâ€™association entre un objet et lui-mÃªme.\nPuisque lâ€™association entre A et B est la mÃªme quâ€™entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible dâ€™exprimer une matrice dâ€™association en mode â€œcompactâ€, sous forme de vecteur. Le vecteur dâ€™association entre des objets A, B et C contiendra toute lâ€™information nÃ©cessaire en un vecteur de trois chiffres, [AB, AC, BC], plutÃ´t quâ€™une matrice de dimension \\(3 \\times 3\\). Lâ€™impact sur la mÃ©moire vive peut Ãªtre considÃ©rable pour les calculs comprenant de nombreuses dimensions.\nEn R, les calculs de similaritÃ© et de distances peuvent Ãªtre effectuÃ©s avec le module vegan. La fonction vegdist permet de calculer les indices dâ€™association en forme carrÃ©e.\nNous verrons plus tard les mÃ©thodes de mesure de similaritÃ© et de distance plus loin. Pour lâ€™instant, utilisons la mÃ©thode de Jaccard pour une dÃ©monstration sur des donnÃ©es dâ€™occurrence.\n\nlibrary(\"vegan\")\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\n\nThis is vegan 2.6-4\n\nvegdist(occurrence, method = \"jaccard\",\n        diag = TRUE, upper = TRUE)\n\n          1         2         3         4\n1 0.0000000 0.7777778 0.7500000 0.7142857\n2 0.7777778 0.0000000 0.6000000 1.0000000\n3 0.7500000 0.6000000 0.0000000 0.7500000\n4 0.7142857 1.0000000 0.7500000 0.0000000\n\n\nRemarquez que vegdist retourne une matrice dont la diagonale est de 0 (on lâ€™affiche en spÃ©cifiant diag = TRUE). La diagonale est lâ€™association dâ€™un objet avec lui-mÃªme. Or la similaritÃ© dâ€™un objet avec lui-mÃªme devrait Ãªtre de 1! En fait, par convention vegdist retourne des dissimilaritÃ©s, non pas des similaritÃ©s. La matrice de distance serait donc calculÃ©e en extrayant la racine carrÃ©e des Ã©lÃ©ments de la matrice de dissimilaritÃ©:\n\ndissimilarity &lt;- vegdist(occurrence, method = \"jaccard\",\n                         diag = TRUE, upper = TRUE)\ndistance &lt;- sqrt(dissimilarity)\ndistance\n\n          1         2         3         4\n1 0.0000000 0.8819171 0.8660254 0.8451543\n2 0.8819171 0.0000000 0.7745967 1.0000000\n3 0.8660254 0.7745967 0.0000000 0.8660254\n4 0.8451543 1.0000000 0.8660254 0.0000000\n\n\nDans le chapitre sur lâ€™analyse compositionnelle, nous avons abordÃ© les significations diffÃ©rentes que peuvent prendre le zÃ©ro. Lâ€™information fournie par un zÃ©ro peut Ãªtre diffÃ©rente selon les circonstances. Dans le cas dâ€™une variable continue, un zÃ©ro signifie gÃ©nÃ©ralement une mesure sous le seuil de dÃ©tection. Deux tissus dont la concentration en cuivre est nulle ont une affinitÃ© sous la perspective de la concentration en cuivre. Dans le cas de mesures dâ€™abondance (dÃ©compte) ou dâ€™occurrence (prÃ©sence-absence), on pourra dÃ©crire comme similaires deux niches Ã©cologiques oÃ¹ lâ€™on retrouve une espÃ¨ce en particulier. Mais deux sites oÃ¹ lâ€™on de retrouve pas dâ€™ours polaires ne correspondent pas nÃ©cessairement Ã  des niches similaires! En effet, il peut exister de nombreuses raisons Ã©cologiques et mÃ©thodologiques pour lesquelles lâ€™espÃ¨ce ou les espÃ¨ces nâ€™ont pas Ã©tÃ© observÃ©es. Câ€™est le problÃ¨me des double-zÃ©ros (espÃ¨ces non observÃ©es Ã  deux sites), problÃ¨me qui est amplifiÃ© avec les grilles comprenant des espÃ¨ces rares.\nLa ressemblance entre des objets comprenant des donnÃ©es continues devrait Ãªtre calculÃ©e grÃ¢ce Ã  des indicateurs symÃ©triques. Inversement, les affinitÃ©s entre les objets dÃ©crits par des donnÃ©es dâ€™abondance ou dâ€™occurrence susceptibles de gÃ©nÃ©rer des problÃ¨mes de double-zÃ©ros devraient Ãªtre Ã©valuÃ©es grÃ¢ce Ã  des indicateurs asymÃ©triques. Un dÃ©fi supplÃ©mentaire arrive lorsque les donnÃ©es sont de type mixte.\nNous utiliserons la convention de vegan et nous calculerons la dissimilaritÃ©, non pas la similaritÃ©. Les mesures de dissimilaritÃ© sont calculÃ©es sur des donnÃ©es dâ€™abondance ou des donnÃ©es dâ€™occurrence. Notons quâ€™il existe beaucoup de confusion dans la littÃ©rature sur la maniÃ¨re de nommer les dissimilaritÃ©s (ce qui nâ€™est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilaritÃ© avec un \\(d\\) minuscule et la distance avec un \\(D\\) majuscule.\n\n10.2.1 Association entre objets (mode Q)\n\n10.2.1.1 Objets: Abondance\nLa dissimilaritÃ© de Bray-Curtis est asymÃ©trique. Elle est aussi appelÃ©e lâ€™indice de Steinhaus, de Czekanowski ou de SÃ¸rensen. Il est important de sâ€™assurer de bien sâ€™entendre la mÃ©thode Ã  laquelle on fait rÃ©fÃ©rence. Lâ€™Ã©quation enlÃ¨ve toute ambiguÃ¯tÃ©. La dissimilaritÃ© de Bray-Curtis entre les points A et B est calculÃ©e comme suit.\n\\[d_{AB} =  \\frac {\\sum \\left| A_{i} - B_{i} \\right| }{\\sum \\left(A_{i}+B_{i}\\right)}\\]\nUtilisons vegdist pour gÃ©nÃ©rer les matrices dâ€™association. Le format â€œlisteâ€ de R est pratique pour enregistrer la collection dâ€™objets, dont les matrice dâ€™association que nous allons crÃ©er dans cette section.\n\nassociations_abund &lt;- list()\nassociations_abund[['BrayCurtis']] &lt;- vegdist(abundance, method = \"bray\")\nassociations_abund[['BrayCurtis']]\n\n          1         2         3\n2 0.9433962                    \n3 0.9619048 0.4400000          \n4 0.9591837 1.0000000 0.7647059\n\n\nLa dissimilaritÃ© de Bray-Curtis est souvent utilisÃ©e dans la littÃ©rature. Toutefois, la version originale de Bray-Curtis nâ€™est pas tout Ã  fait mÃ©trique (semimÃ©trique). ConsÃ©quemment, la dissimilaritÃ© de Ruzicka (une variante de la dissimilaritÃ© de Jaccard pour les donnÃ©es dâ€™abondance) est mÃ©trique, et devrait probablement Ãªtre prÃ©fÃ©rÃ© Ã  Bray-Curtis (Oksanen, 2006).\n\\[d_{AB, Ruzicka} =  \\frac { 2 \\times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}\\]\n\nassociations_abund[['Ruzicka']] &lt;- associations_abund[['BrayCurtis']] * 2 / (1 + associations_abund[['BrayCurtis']])\n\nLa dissimilaritÃ© de Kulczynski (aussi Ã©crit Kulsinski) est asymÃ©trique et semimÃ©trique, tout comme celle de Bray-Curtis. Elle est calculÃ©e comme suit.\n\\[d_{AB} = 1-\\frac{1}{2} \\times \\left[ \\frac{\\sum min(A_i, B_i)}{\\sum A_i} + \\frac{\\sum min(A_i, B_i)}{\\sum B_i} \\right]\\]\n\nassociations_abund[['Kulczynski']] &lt;- vegdist(abundance, method = \"kulczynski\")\n\nUne approche commune pour mesurer lâ€™association entre sites dÃ©crits par des donnÃ©es dâ€™abondance est la distance de Hellinger. Notez quâ€™il sâ€™agit ici dâ€™une distance, non pas dâ€™une dissimilaritÃ©. Pour lâ€™obtenir, on doit dâ€™abord diviser chaque donnÃ©e dâ€™abondance par lâ€™abondance totale pour chaque site pour obtenir les espÃ¨ces en tant que proportions, puis on extrait la racine carrÃ©e de chaque Ã©lÃ©ment. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la gÃ©nÃ©ralisation en plusieurs dimensions du thÃ©orÃ¨me de Pythagore, \\(c = \\sqrt{a^2 + b^2}\\).\n\\[D_{AB} = \\sqrt {\\sum \\left( \\frac{A_i}{\\sum A_i} - \\frac{B_i}{\\sum B_i} \\right)^2}\\]\n\n\n\n\n\n\nğŸ˜±Â Attention\n\nLa distance dâ€™Hellinger hÃ©rite des biais liÃ©es aux donnÃ©es compositionnelles. Elle peut Ãªtre substituÃ©e par une matrice de distances dâ€™Aitchison.\n\n\n\nassociations_abund[['Hellinger']] &lt;- dist(decostand(abundance, method=\"hellinger\"))\n\nToute comme la distance dâ€™Hellinger, la distance de chord est calculÃ©e par une distance euclidienne sur des donnÃ©es dâ€™abondance transformÃ©es de sorte que chaque ligne ait une longueur (norme) de 1.\n\nassociations_abund[['Chord']] &lt;- dist(decostand(abundance, method=\"normalize\"))\n\nLa mÃ©trique du chi-carrÃ©, ou \\(\\chi\\)-carrÃ©, ou chi-square, donne davantage de poids aux espÃ¨ces rares quâ€™aux espÃ¨ces communes. Son utilisation est recommandÃ©e lorsque les espÃ¨ces rares sont de bons indicateurs de conditions Ã©cologiques particuliÃ¨res (Legendre et Legendre, 2012, p.Â 308).\n\\[  d_{AB} = \\sqrt{\\sum _j \\frac{1}{\\sum y_j} \\left( \\frac{A_j}{\\sum A} - \\frac{B_j}{\\sum B} \\right)^2 }  \\]\nLa mÃ©trique peut Ãªtre transformÃ©e en distance en la multipliant par la racine carrÃ©e de la somme totale des espÃ¨ces dans la matrice dâ€™abondance (\\(X\\)).\n\\[ D_{AB} = \\sqrt{\\sum X} \\times d_{AB} \\]\n\nassociations_abund[['ChiSquare']] &lt;- dist(decostand(abundance, method=\"chi.square\"))\n\nUne maniÃ¨re visuellement plus intÃ©ressante de prÃ©senter une matrice dâ€™association est un graphique de type heatmap.\n\nassociations_abund_df &lt;- list()\n\nfor (i in 1:length(associations_abund)) {\n  associations_abund_df[[i]] &lt;- data.frame(as.matrix(associations_abund[[i]]))\n  colnames(associations_abund_df[[i]]) &lt;- rownames(associations_abund_df[[i]])\n  associations_abund_df[[i]]$row &lt;- rownames(associations_abund_df[[i]])\n  associations_abund_df[[i]] &lt;- associations_abund_df[[i]] |&gt; gather(key=row)\n  associations_abund_df[[i]]$column = rep(1:4, 4)\n  associations_abund_df[[i]]$dist &lt;- names(associations_abund)[i]\n}\nassociations_abund_df &lt;- do.call(rbind, associations_abund_df)\n\nggplot(associations_abund_df, aes(x=row, y=column)) +\n  facet_wrap(. ~ dist, nrow = 2) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = round(value, 2))) +\n  scale_fill_gradient2(low = \"#00ccff\", mid = \"#aad400\", high = \"#ff0066\", midpoint = 2) +\n  labs(x=\"Site\", y=\"Site\")\n\n\n\n\n\n\n\nPeu importe le type dâ€™association utilisÃ©e, les heatmaps montrent les mÃªmes tendances. Les associations de dissimilaritÃ© (Bray-Curtis, Kulczynski et Ruzicka) sâ€™Ã©talent de 0 Ã  1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de zÃ©ro, mais nâ€™ont pas de limite supÃ©rieure. On note les plus grandes diffÃ©rences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures dâ€™association Ã  lâ€™exception de la dissimilaritÃ© de Kulczynski.\n\n10.2.1.2 Objets: Occurrence (prÃ©sence-absence)\nDes indices dâ€™association diffÃ©rents devraient Ãªtre utilisÃ©s lorsque des donnÃ©es sont compilÃ©es sous forme boolÃ©enne. En gÃ©nÃ©ral, les tableaux de donnÃ©es dâ€™occurrence seront compilÃ©s avec des 1 (prÃ©sence) et des 0 (absence).\nLa similaritÃ© de Jaccard entre le site A et le site B est la proportion de double 1 (prÃ©sences de 1 dans A et B) parmi les espÃ¨ces. La dissimilaritÃ© est la proportion complÃ©mentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carrÃ©e de la dissimilaritÃ©.\n\nassociations_occ &lt;- list()\nassociations_occ[['Jaccard']] &lt;- vegdist(occurrence, method = \"jaccard\")\n\nLes distances dâ€™Hellinger, de chord et de chi-carrÃ© sont aussi appropriÃ©es pour les calculs de distances sur des tableaux dâ€™occurrence.\n\nassociations_occ[['Hellinger']] &lt;- dist(decostand(occurrence, method=\"hellinger\"))\nassociations_occ[['Chord']] &lt;- dist(decostand(occurrence, method=\"normalize\"))\nassociations_occ[['ChiSquare']] &lt;- dist(decostand(occurrence, method=\"chi.square\"))\n\nGraphiquement,\n\nassociations_occ_df &lt;- list()\n\nfor (i in 1:length(associations_occ)) {\n  associations_occ_df[[i]] &lt;- data.frame(as.matrix(associations_occ[[i]]))\n  colnames(associations_occ_df[[i]]) &lt;- rownames(associations_occ_df[[i]])\n  associations_occ_df[[i]]$row &lt;- rownames(associations_occ_df[[i]])\n  associations_occ_df[[i]] &lt;- associations_occ_df[[i]] |&gt; gather(key=row)\n  associations_occ_df[[i]]$column = rep(1:4, 4)\n  associations_occ_df[[i]]$dist &lt;- names(associations_occ)[i]\n}\nassociations_occ_df &lt;- do.call(rbind, associations_occ_df)\n\nggplot(associations_occ_df, aes(x=row, y=column)) +\n  facet_wrap(. ~ dist) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = round(value, 2))) +\n  scale_fill_gradient2(low = \"#00ccff\", mid = \"#aad400\", high = \"#ff0066\", midpoint = 1) +\n  labs(x=\"Site\", y=\"Site\")\n\n\n\n\n\n\n\nIl est attendu que les matrices dâ€™association sur lâ€™occurrence sont semblables Ã  celles sur lâ€™abondance. Dans ce cas-ci, la distance dâ€™Hellinger donne des rÃ©sultats semblables Ã  la dissimilaritÃ© de Jaccard.\n\n10.2.1.3 Objets: DonnÃ©es quantitatives\nLes donnÃ©es quantitative en Ã©cologie peuvent dÃ©crire lâ€™Ã©tat de lâ€™environnement: le climat, lâ€™hydrologie, lâ€™hydrogÃ©ochimie, la pÃ©dologie, etc. En rÃ¨gle gÃ©nÃ©rale, les coordonnÃ©es des sites ne sot pas des variables environnementales, Ã  que lâ€™on soupÃ§onne la coordonnÃ©e elle-mÃªme dâ€™Ãªtre responsable dâ€™effets sur notre systÃ¨me: mais il sâ€™agira la plupart du temps dâ€™effets confondants (par exemple, on peut mesurer un effet de la latitude sur le rendement des agrumes, mais il sâ€™agira probablement avant tout dâ€™effets dus aux conditions climatiques, qui elles changent en fonction de la latitude). Dâ€™autre types de donnÃ©es quantitative pouvant Ãªtre apprÃ©hendÃ©es par des distances sont les traits phÃ©nologiques, les ionomes, les gÃ©nomes, etc.\nLa distance euclidienne est la racine carrÃ©e de la somme des carrÃ©s des distances sur tous les axes. Il sâ€™agit dâ€™une application multidimensionnelle du thÃ©orÃ¨me de Pythagore. La distance dâ€™Aitchison, couverte dans le chapitreÂ 9, est une distance euclidienne calculÃ©e sur des donnÃ©es compositionnelles prÃ©alablement transformÃ©es. La distance euclidienne est sensible aux unitÃ©s utilisÃ©s: utiliser des millimÃ¨tres plutÃ´t que des mÃ¨tres enflera la distance euclidienne. Il est recommandÃ© de porter une attention particuliÃ¨re aux unitÃ©s, et de standardiser les donnÃ©es au besoin (par exemple, en centrant la moyenne Ã  zÃ©ro et en fixant lâ€™Ã©cart-type Ã  1).\nOn pourrait, par exemple, mesurer la distance entre des observations des dimensions de diffÃ©rentes espÃ¨ces dâ€™iris. Ce tableau est inclus dans R par dÃ©faut.\n\ndata(iris)\niris |&gt; sample_n(5)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1          5.3         3.7          1.5         0.2    setosa\n2          6.4         3.1          5.5         1.8 virginica\n3          4.9         3.6          1.4         0.1    setosa\n4          7.9         3.8          6.4         2.0 virginica\n5          4.4         2.9          1.4         0.2    setosa\n\n\nLes mesures du tableau sont en centimÃ¨tres. Pour Ã©viter de donner davantage de poids aux longueurs des sÃ©pales et en mÃªme temps de nÃ©gliger la largeur des pÃ©tales, nous allons standardiser le tableau.\n\niris_sc &lt;- iris |&gt;\n  select(-Species) |&gt; \n  scale() |&gt; \n  as_tibble() |&gt; \n  mutate(Species = iris$Species) \niris_sc\n\n# A tibble: 150 Ã— 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1       -0.898      1.02          -1.34       -1.31 setosa \n 2       -1.14      -0.132         -1.34       -1.31 setosa \n 3       -1.38       0.327         -1.39       -1.31 setosa \n 4       -1.50       0.0979        -1.28       -1.31 setosa \n 5       -1.02       1.25          -1.34       -1.31 setosa \n 6       -0.535      1.93          -1.17       -1.05 setosa \n 7       -1.50       0.786         -1.34       -1.18 setosa \n 8       -1.02       0.786         -1.28       -1.31 setosa \n 9       -1.74      -0.361         -1.34       -1.31 setosa \n10       -1.14       0.0979        -1.28       -1.44 setosa \n# â„¹ 140 more rows\n\n\nPour les comparaisons des dimensions, prenons la moyenne des dimensions (mises Ã  lâ€™Ã©chelle) par espÃ¨ce.\n\niris_means &lt;- iris_sc |&gt;\n  group_by(Species) |&gt;\n  summarise_all(mean) |&gt;\n  select(-Species)\niris_means\n\n# A tibble: 3 Ã— 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1       -1.01        0.850       -1.30       -1.25 \n2        0.112      -0.659        0.284       0.166\n3        0.899      -0.191        1.02        1.08 \n\n\nNous pouvons utiliser la distance euclidienne, commune en gÃ©omÃ©trie, pour comparer les espÃ¨ces. La distance euclidienne est calculÃ©e comme suit.\n\\[ \\mathcal{E} = \\sqrt{\\Sigma_i \\left( A_i - B_i \\right) ^2 } \\]\n\nassociations_cont = list()\nassociations_cont[['Euclidean']] &lt;- dist(iris_sc |&gt; select(-Species), method=\"euclidean\")\n\nLa distance de Mahalanobis est semblable Ã  la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut Ãªtre utilisÃ©e pour dÃ©crire la structure dâ€™un nuage de points. La distance de Mahalanobis se calcule comme suit.\n\\[\\mathcal{M} = \\sqrt{(A - B)^T S^{-1} (A-B)}\\]\nNotez quâ€™il sâ€™agit dâ€™une gÃ©nÃ©ralisation de la distance euclidienne, qui Ã©quivaut Ã  une distance de Mahalanobis dont la matrice de covariance est une matrice identitÃ©.\nLa distance de Mahalanobis permet de reprÃ©senter des distances dans un espace fortement corrÃ©lÃ©. Elle est couramment utilisÃ©e pour dÃ©tecter les valeurs aberrantes selon des critÃ¨res de distance Ã  partir du centre dâ€™un jeu de donnÃ©es multivariÃ©es.\n\nassociations_cont[['Mahalanobis']] &lt;- vegdist(iris_sc |&gt; select(-Species), 'mahalanobis')\n\nLa distance de Manhattan porte aussi le nom de distance de cityblock ou de taxi. Câ€™est la distance que vous devrez parcourir pour vous rendre du point A au point B Ã  Manhattan, câ€™est-Ã -dire selon une sÃ©quence de tronÃ§ons perpendiculaires.\n\\[ D_{AB} = \\sum _i \\left| A_i - B_i \\right| \\]\nLa distance de Manhattan est appropriÃ©e lorsque les gradients (changements dâ€™un Ã©tat Ã  lâ€™autre ou dâ€™une rÃ©gion Ã  lâ€™autre) ne permettent pas des changements simultanÃ©s. Mieux vaut standardiser les variables pour Ã©viter quâ€™une dimension soit prÃ©pondÃ©rante.\n\nassociations_cont[['Manhattan']] &lt;- vegdist(iris_sc |&gt; select(-Species), 'manhattan')\n\nAvant de prÃ©senter les rÃ©sultats des espÃ¨ces dâ€™iris, voici une reprÃ©sentation des distances euclidiennes (rouge), de Mahalanobis (bleu) et de Manhattan (vert), chacune de 1 et 2 unitÃ©s Ã  partir du centre et, pour ce qui est de la distance de Mahalanobis, selon la covariance.\n\nlibrary(\"car\")\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(\"MASS\")\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nselect &lt;- dplyr::select # Ã©viter les conflits de fonctions entre MASS et dplyr\nfilter &lt;- dplyr::filter\n\nsigma &lt;- matrix(c(1, 0.6, 0.6, 1), ncol = 2) # matrice de covariance\nmu &lt;- c(0, 0) # centre\ndata &lt;- mvrnorm(n = 100, mu, sigma) # gÃ©nÃ©rer des donnÃ©es\n\nplot(data, ylim = c(-2, 2), xlim = c(-2, 2), asp = 1)\n\n## cercles\nt &lt;- seq(0,2*pi,length=100)\nc1 &lt;- t(rbind(mu[2] + sin(t)*1, mu[1] + cos(t)*1))\nc2 &lt;- t(rbind(mu[2] + sin(t)*2, mu[1] + cos(t)*2))\nlines(c1, lwd = 2, col = \"red\")\nlines(c2, lwd = 2, col = \"red\")\n\n\n## ellipses\ne1 &lt;- ellipse(mu, sigma, radius=1, add=TRUE)\ne2 &lt;- ellipse(mu, sigma, radius=2, add=TRUE)\n\n## carrÃ©s\nlines(c(1, 0, -1, 0, 1), c(0, 1, 0, -1, 0), lwd = 2, col = \"green\")\nlines(c(2, 0, -2, 0, 2), c(0, 2, 0, -2, 0), lwd = 2, col = \"green\")\n\n\n\n\n\n\n\nEt, graphiquement, les rÃ©sultats des distances des iris.\n\nassociations_cont_df &lt;- list()\n\nfor (i in 1:length(associations_cont)) {\n  associations_cont_df[[i]] &lt;- data.frame(as.matrix(associations_cont[[i]]))\n  colnames(associations_cont_df[[i]]) &lt;- rownames(associations_cont_df[[i]])\n  associations_cont_df[[i]]$row &lt;- rownames(associations_cont_df[[i]])\n  associations_cont_df[[i]] &lt;- associations_cont_df[[i]] |&gt; gather(key=row)\n  associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris))\n  associations_cont_df[[i]]$dist &lt;- names(associations_cont)[i]\n}\nassociations_cont_df &lt;- do.call(rbind, associations_cont_df)\n\nggplot(associations_cont_df, aes(x=row, y=column)) +\n  facet_wrap(. ~ dist) +\n  geom_tile(aes(fill = value), colour = NA) +\n  #geom_text(aes(label = round(value, 2))) +\n  scale_fill_gradient2(low = \"#00ccff\", mid = \"#aad400\", high = \"#ff0066\", midpoint = 5) +\n  labs(x=\"Site\", y=\"Site\")\n\n\n\n\n\n\n\nLe tableau iris est ordonnÃ© par espÃ¨ce. Les distances euclidienne et de Manhattan permettent aisÃ©ment de distinguer les espÃ¨ces selon les dimensions des pÃ©tales et des sÃ©pales. Toutefois, lâ€™utilisation de la covariance avec la distance de Mahalanobis crÃ©e des distinction moins tranchÃ©es.\n\n10.2.1.4 Objets: DonnÃ©es mixtes\nLes donnÃ©es catÃ©gorielles ordinales peuvent Ãªtre transformÃ©es en donnÃ©es continues par gradations linÃ©aires ou quadratiques. Les donnÃ©es catÃ©gorielles nominales, quant Ã  elles, peuvent Ãªtre encodÃ©es (encodage catÃ©goriel) en donnÃ©es similaires Ã  des occurrences. Attention toutefois: contrairement Ã  la rÃ©gression linÃ©aire qui demande dâ€™exclure une catÃ©gorie, lâ€™encodage catÃ©goriel doit inclure toutes les catÃ©gories. Le comportement par dÃ©faut de la fonction model.matrix est dâ€™exclure la catÃ©gorie de rÃ©fÃ©rence: on doit spÃ©cifier que lâ€™intercept est de zÃ©ro, câ€™est-Ã -dire model.matrix(~ + categorie).\nLa similaritÃ© de Gower a Ã©tÃ© dÃ©veloppÃ©e pour mesurer des associations entre des objets dont les donnÃ©es sont mixtes: boolÃ©ennes, catÃ©gorielles et continues. La similaritÃ© de Gower est calculÃ©e en additionnant les distances calculÃ©es par colonne, individuellement. Si la colonne est boolÃ©enne, on utilise les distances de Jaccard (qui exclue les double-zÃ©ro) de maniÃ¨re univariÃ©e: une variable Ã  la fois. Pour les variables continues, on utilise la distance de Manhattan divisÃ©e par la plage de valeurs de la variable (pour fin de standardisation). Puisquâ€™elle hÃ©rite de la particularitÃ© de la distance de Manhattan et de la similaritÃ© de Jaccard univariÃ©e, la similaritÃ© de Gower reste une combinaison linÃ©aire de distances univariÃ©es.\n\nX &lt;- tibble(ID = 1:8,\n            age = c(21, 21, 19, 30, 21, 21, 19, 30),\n            gender = c('M','M','N','M','F','F','F','F'),\n            civil_status = c('MARRIED','SINGLE','SINGLE','SINGLE','MARRIED','SINGLE','WIDOW','DIVORCED'),\n            salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0),\n            children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE),\n            available_credit = c(2200,100,22000,1100,2000,100,6000,2200))\nX\n\n# A tibble: 8 Ã— 7\n     ID   age gender civil_status salary children available_credit\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt; &lt;lgl&gt;               &lt;dbl&gt;\n1     1    21 M      MARRIED        3000 TRUE                 2200\n2     2    21 M      SINGLE         1200 FALSE                 100\n3     3    19 N      SINGLE        32000 TRUE                22000\n4     4    30 M      SINGLE         1800 TRUE                 1100\n5     5    21 F      MARRIED        2900 TRUE                 2000\n6     6    21 F      SINGLE         1100 TRUE                  100\n7     7    19 F      WIDOW         10000 FALSE                6000\n8     8    30 F      DIVORCED       1500 TRUE                 2200\n\n\nIl faut prÃ©alablement procÃ©der Ã  lâ€™encodage catÃ©goriel pour les variables catÃ©gorielles nominales.\n\nX_dum &lt;- model.matrix(~ 0 + ., X[, -1])\nX_dum\n\n  age genderF genderM genderN civil_statusMARRIED civil_statusSINGLE\n1  21       0       1       0                   1                  0\n2  21       0       1       0                   0                  1\n3  19       0       0       1                   0                  1\n4  30       0       1       0                   0                  1\n5  21       1       0       0                   1                  0\n6  21       1       0       0                   0                  1\n7  19       1       0       0                   0                  0\n8  30       1       0       0                   0                  0\n  civil_statusWIDOW salary childrenTRUE available_credit\n1                 0   3000            1             2200\n2                 0   1200            0              100\n3                 0  32000            1            22000\n4                 0   1800            1             1100\n5                 0   2900            1             2000\n6                 0   1100            1              100\n7                 1  10000            0             6000\n8                 0   1500            1             2200\nattr(,\"assign\")\n [1] 1 2 2 2 3 3 3 4 5 6\nattr(,\"contrasts\")\nattr(,\"contrasts\")$gender\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$civil_status\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$children\n[1] \"contr.treatment\"\n\n\nCalculons la dissimilaritÃ© de Gower (cette fois le graphique est fait avec pheatmap).\n\nlibrary(\"pheatmap\")\nd_gow &lt;- as.matrix(vegdist(X_dum, 'gower'))\ncolnames(d_gow) &lt;- rownames(d_gow) &lt;- X$ID\npheatmap(d_gow)\n\n\n\n\nLes dendrogrammes apparaissant sur les axes du graphique sont issus dâ€™un processus de partitionnement basÃ© sur la distance, que nous verrons plus loin dans ce chapitre. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diffÃ¨rent le plus. Les profils 3 et 4 sont nÃ©anmoins plutÃ´t diffÃ©rents.\n\n10.2.2 Associations entre variables (mode R)\nIl existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corrÃ©lation. Mais les donnÃ©es dâ€™abondance et dâ€™occurrence demandent des approches diffÃ©rentes.\n\n10.2.2.1 Variables: Abondance\nLa distance du chi-carrÃ© est suggÃ©rÃ©e par Borcard et al.Â (2011).\n\nabundance_r &lt;- t(abundance)\nD_chisq_R &lt;- as.matrix(dist(decostand(abundance_r, method=\"chi.square\")))\npheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2))\n\n\n\n\nDes coabondances sont notables pour la mÃ©sange Ã  tÃªte noire, le jaseur borÃ©al, la citelle Ã  poitrine rousse et le bruant Ã  gorge blanche (tache bleu au centre).\n\n10.2.2.2 Variables: occurrence\nLa dissimilaritÃ© de Jaccard peut Ãªtre utilisÃ©e.\n\noccurrence_r &lt;- t(occurrence)\nD_jacc_R &lt;- as.matrix(vegdist(occurrence_r, method = \"jaccard\"))\npheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2))\n\n\n\n\nDes cooccurrences sont notables pour le jaseur borÃ©al, la citelle Ã  poitrine rousse et le bruant Ã  gorge blanche (tache bleu au centre).\n\n10.2.2.3 Variables: QuantitÃ©s\nLa matrice des corrÃ©lations de Pearson peut Ãªtre utilisÃ©e pour les donnÃ©es continues. Quant aux variables ordinales, elles devraient idÃ©alement Ãªtre liÃ©es linÃ©airement ou quadratiquement. Si ce nâ€™est pas le cas, câ€™est-Ã -dire que les catÃ©gories sont ordonnÃ©es par rang seulement, vous pourrez avoir recours aux coefficients de corrÃ©lation de Spearman ou de Kendall.\n\niris_cor &lt;- iris |&gt;\n  select(-Species) |&gt;\n  cor()\npheatmap(cor(iris[, -5]), cluster_rows = FALSE, cluster_cols = FALSE,\n         display_numbers = round(iris_cor, 2))\n\n\n\n\n\n10.2.3 Conclusion sur les associations\nIl nâ€™existe pas de rÃ¨gle claire pour dÃ©terminer quelle technique dâ€™association utiliser. Cela dÃ©pend en premier lieu de vos donnÃ©es. Vous sÃ©lectionnerez votre mÃ©thode dâ€™association selon le type de donnÃ©es que vous abordez, la question Ã  laquelle vous dÃ©sirez rÃ©pondre ainsi lâ€™expÃ©rience dans la littÃ©rature comme celle de vos collÃ¨gues scientifiques. Sâ€™il nâ€™existe pas de rÃ¨gle clair, câ€™est quâ€™il existe des dizaines de mÃ©thodes diffÃ©rentes, et la plupart dâ€™entre elles vous donneront une perspective juste et valide. Il faut nÃ©anmoins faire attention pour Ã©viter de sÃ©lectionner les mÃ©thodes qui ne sont pas appropriÃ©es."
  },
  {
    "objectID": "09-ordination.html#partitionnement",
    "href": "09-ordination.html#partitionnement",
    "title": "10Â  Association, partitionnement et ordination",
    "section": "\n10.3 Partitionnement",
    "text": "10.3 Partitionnement\nLes donnÃ©es suivantes ont Ã©tÃ© gÃ©nÃ©rÃ©es par Leland McInnes (Tutte institute of mathematics, Ottawa). ÃŠtes-vous en mesure dâ€™identifier des groupes? Combien en trouvez-vous?\n\ndf_mcinnes &lt;- read_csv(\"data/clusterable_data.csv\", col_names = c(\"x\", \"y\"), skip = 1)\n\nRows: 2309 Columns: 2\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (2): x, y\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed()\n\n\n\n\nEn 2D, lâ€™oeil humain peut facilement dÃ©tecter les groupes. En 3D, câ€™est toujours possible, mais au-delÃ  de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors dâ€™une aide prÃ©cieuse. Mais ils transportent en pratique tout un bagage de limitations. Quel est le critÃ¨re dâ€™association entre les groupes? Combien de groupe devrions-nous crÃ©er? Comment distinguer une donnÃ©e trop bruitÃ©e pour Ãªtre classifiÃ©e?\nLe partitionnement de donnÃ©es (clustering en anglais), et inversement leur regroupement, permet de crÃ©er des ensembles selon des critÃ¨res dâ€™association. On suppose donc que Le partitionnement permet de crÃ©er des groupes selon lâ€™information que lâ€™on fait Ã©merger des donnÃ©es. Il est consÃ©quemment entendu que les donnÃ©es ne sont pas catÃ©gorisÃ©es Ã  priori: il ne sâ€™agit pas de prÃ©dire la catÃ©gorie dâ€™un objet, mais bien de crÃ©er des catÃ©gories Ã  partir des objets par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs gÃ¨nes, etc.\nPlusieurs mÃ©thodes sont aujourdâ€™hui offertes aux analystes pour partitionner leurs donnÃ©es. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes.\n\nMÃ©thodes hiÃ©rarchique et non hiÃ©rarchiques. Dans un partitionnement hiÃ©rarchique, lâ€™ensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment lâ€™ultime partitionnement. On pourra alors identifier comment se dÃ©cline un partitionnement. Ã€ lâ€™inverse, un partitionnement non-hiÃ©rarchique des algorithmes permettent de crÃ©er les groupes non hiÃ©rarchisÃ©s les plus diffÃ©rents que possible.\nMembership exclusif ou flou. Certaines techniques attribuent Ã  chaque objet une classe unique: lâ€™appartenance sera indiquÃ©e par un 1 et la non appartenance par un 0. Dâ€™autres techniques vont attribuer un membership flou oÃ¹ le degrÃ© dâ€™appartenance est une variable continue de 0 Ã  1. Parmi les mÃ©thodes floues, on retrouve les mÃ©thodes probabilistes.\n\n\n10.3.1 Ã‰valuation dâ€™un partitionnement\nLe choix dâ€™une technique de partitionnement parmi de nombreuses disponibles, ainsi que le choix des paramÃ¨tres gouvernant chacune dâ€™entre elles, est avant tout basÃ© sur ce que lâ€™on dÃ©sire dÃ©finir comme Ã©tant un groupe, ainsi que la maniÃ¨re dâ€™interprÃ©ter les groupes. En outre, le nombre de groupe Ã  dÃ©partager est toujours une dÃ©cision de lâ€™analyste. NÃ©anmoins, on peut se fier des indicateurs de performance de partitionnement. Parmis ceux-ci, retenons le score silhouette ainsi que lâ€™indice de Calinski-Harabaz.\n\n10.3.1.1 Score silhouette\nEn anglais, le h dans silhouette se trouve aprÃ¨s le l: on parle donc de silhouette coefficient pour dÃ©signer le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le sÃ©pare des autres points de son groupe (\\(a\\)) ainsi que la distance moyenne qui le sÃ©pare des points du groupe le plus rapprochÃ©.\n\\[s = \\frac{b-a}{max \\left(a, b \\right)}\\]\nUn coefficient de -1 indique le pire classement, tandis quâ€™un coefficient de 1 indique le meilleur classement. La moyenne des coefficients silhouette est le score silhouette.\n\n10.3.1.2 Indice de Calinski-Harabaz\nLâ€™indice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus lâ€™indice est Ã©levÃ©, mieux les groupes sont dÃ©finis. La mathÃ©matique est dÃ©crite dans la documentation de scikit-learn, un module dâ€™analyse et autoapprentissage sur Python.\nNote. Les coefficients silhouette et lâ€™indice de Calinski-Harabaz sont plus appropriÃ©s pour les formes de groupes convexes (cercles, sphÃ¨res, hypersphÃ¨res) que pour les formes irrÃ©guliÃ¨res (notamment celles obtenues par la DBSCAN, discutÃ©e ci-dessous).\n\n10.3.2 Partitionnement non hiÃ©rarchique\nIl peut arriver que vous nâ€™ayez pas besoin de comprendre la structure dâ€™agglomÃ©ration des objets (ou variables). Plusieurs techniques de partitionnement non hiÃ©rarchique sont disponibles sur R. On sâ€™intÃ©ressera en particulier aux k-means et au dbscan.\n\n10.3.2.1 Kmeans\nLâ€™objectif des kmeans est de minimiser la distance euclidienne entre un nombre prÃ©dÃ©fini de k groupes exclusifs.\n\nLâ€™algorithme commence par placer une nombre k de centroides au hasard dans lâ€™espace dâ€™un nombre p de variables (vous devez fixer k, et p est le nombre de colonnes de vos donnÃ©es).\nEnsuite, chaque objet est Ã©tiquetÃ© comme appartenant au groupe du centroÃ¯de le plus prÃ¨s.\nLa position du centroÃ¯de est dÃ©placÃ©e Ã  la moyenne de chaque groupe.\nRecommencer Ã  partir de lâ€™Ã©tape 2 jusquâ€™Ã  ce que lâ€™assignation des objets aux groupes ne change plus.\n\n\nSource: David Sheehan\n\nLa technique des kmeans suppose que les groupes ont des distributions multinormales - reprÃ©sentÃ©es par des cercles en 2D, des sphÃ¨res en 3D, des hypersphÃ¨res en plus de 3D. Cette limitation est problÃ©matique lorsque les groupes se prÃ©sentent sous des formes irrÃ©guliÃ¨res, comme celles du nuage de points de Leland McInnes, prÃ©sentÃ© plus haut. De plus, la technique classique des kmeans est basÃ©e sur des distances euclidiennes: lâ€™utilisation des kmeans nâ€™est appropriÃ©e pour les donnÃ©es comprenant beaucoup de zÃ©ros, comme les donnÃ©es dâ€™abondance, qui devraient prÃ©alablement Ãªtre transformÃ©es en variables centrÃ©es et rÃ©duites (Legendre et Legendre, 2012). La technique des mixtures gaussiennes (gaussian mixtures) est une gÃ©nÃ©ralisation des kmeans permettant dâ€™intÃ©grer la covariance des groupes. Les groupes ne sont plus des hyper-sphÃ¨res, mais des hyper-ellipsoÃ¯des.\n\n10.3.2.1.1 Application\nNous pouvons utilisÃ© la fonction kmeans de R. Toutefois, puisque lâ€™on dÃ©sire ici effectuer des tests de partitionnement pour plusieurs nombres de groupes, nous utiliserons cascadeKM, du module vegan. Notez que de nombreux paramÃ¨tres par dÃ©faut sont utilisÃ©s dans les exÃ©cutions ci-dessous. Ces notes de cours ne forment pas un travail de recherche scientifique. Lors de travaux de recherche, lâ€™utilsation dâ€™un argument ou dâ€™un autre dans une fonction doit Ãªtre justifiÃ©: quâ€™un paramÃ¨tre soit utilisÃ© par dÃ©faut dans une fonction nâ€™est a priori pas une justification convaincante.\nPour les kmeans, on doit fixer le nombre de groupes. Le graphique des donnÃ©es de Leland McInnes montrent 6 groupes. Toutefois, il est rare que lâ€™on puisse visualiser des dÃ©marcations aussi tranchÃ©es que celles de lâ€™exemple, qui plus est dans des cas oÃ¹ lâ€™on doit traiter de plus de deux dimensions. Je vais donc lancer le partitionnement en boucle pour plusieurs nombres de groupes, de 3 Ã  10 et pour chaque groupe, Ã©valuer le score silhouette et de Calinski-Habaraz. Jâ€™utilise un argument random_state pour mâ€™assurer que les groupes seront les mÃªmes Ã  chaque fois que la cellule sera lancÃ©e.\n\nlibrary(\"vegan\")\nmcinnes_kmeans &lt;- cascadeKM(df_mcinnes, inf.gr = 3, sup.gr = 10, criterion = \"calinski\")\nstr(mcinnes_kmeans)\n\nList of 4\n $ partition: int [1:2309, 1:8] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2309] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:8] \"3 groups\" \"4 groups\" \"5 groups\" \"6 groups\" ...\n $ results  : num [1:2, 1:8] 85.1 2164.5 61.4 2294.6 51.4 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"SSE\" \"calinski\"\n  .. ..$ : chr [1:8] \"3 groups\" \"4 groups\" \"5 groups\" \"6 groups\" ...\n $ criterion: chr \"calinski\"\n $ size     : int [1:10, 1:8] 1243 561 505 NA NA NA NA NA NA NA ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:10] \"Group 1\" \"Group 2\" \"Group 3\" \"Group 4\" ...\n  .. ..$ : chr [1:8] \"3 groups\" \"4 groups\" \"5 groups\" \"6 groups\" ...\n - attr(*, \"class\")= chr \"cascadeKM\"\n\n\nLâ€™objet mcinnes_kmeans, de type cascadeKM, peut Ãªtre visualisÃ© directement avec la fonction plot.\n\nplot(mcinnes_kmeans)\n\n\n\n\nOn obtient un maximum de Calinski Ã  4 groupes, qui correspond Ã  la deuxiÃ¨me simulation effectuÃ©e de 3 Ã  10.\nExaminons les scores silhouette (module: cluster).\n\nlibrary(\"cluster\")\nasw &lt;- c()\nfor (i in 1:ncol(mcinnes_kmeans$partition)) {\n  mcinnes_kmeans_silhouette &lt;- silhouette(mcinnes_kmeans$partition[, i], dist = vegdist(df_mcinnes, method = \"euclidean\"))\n  asw[i] &lt;- summary(mcinnes_kmeans_silhouette)$avg.width\n}\nplot(3:10, asw, type = 'b')\n\n\n\n\n\n\n\nLe score silhouette maximum est Ã  3 groupes. La forme des groupes nâ€™Ã©tant pas convexe, il fallait sâ€™attendre Ã  ce que indicateurs maximaux pour les deux indicateurs soient diffÃ©rents. Câ€™est dâ€™ailleurs souvent le cas. Cet exemple supporte que le choix du nombre de groupe Ã  dÃ©partager repose sur lâ€™analyste, non pas uniquement sur les indicateurs de performance. Choisissons 6 groupes, puisque que câ€™est visuellement ce que lâ€™on devrait chercher pour ce cas dâ€™Ã©tude.\n\nkmeans_group &lt;- mcinnes_kmeans$partition[, 4]\nmcinnes_kmeans$partition |&gt; head(3)\n\n  3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups 10 groups\n1        1        3        1        5        2        3        1         6\n2        1        3        5        4        2        6        1         6\n3        1        4        1        5        4        3        6        10\n\ndf_mcinnes |&gt; \n  mutate(kmeans_group = kmeans_group) |&gt; # ajouter une colonne de regoupement\n  ggplot(aes(x=x, y=y)) +\n  geom_point(aes(colour = factor(kmeans_group))) +\n  coord_fixed()\n\n\n\n\n\n\n\nLâ€™algorithme kmeans est loin dâ€™Ãªtre statisfaisant. Cela est attendu, puisque les kmeans recherchent des distribution gaussiennes sur des groupes vraisemblablement non-gaussiens.\nNous pouvons crÃ©er un graphique silhouette pour nos 6 groupes. Notez quâ€™Ã  cause dâ€™un bogue, il nâ€™est pas possible de prÃ©senter les donnÃ©es clairement lorsquâ€™elles sont nombreuses.\n\nsil &lt;- silhouette(mcinnes_kmeans$partition[, 6],\n                  dist = vegdist(df_mcinnes[, ], method = \"euclidean\"))\nsil &lt;- sortSilhouette(sil)\nplot(sil, col = 'black')\n\n\n\n\n\n\n\n\n10.3.2.2 DBSCAN\nLa technique DBSCAN (* Density-Based Spatial Clustering of Applications with Noise) sousentend que les groupes sont composÃ©s de zones oÃ¹ lâ€™on retrouve plus de points (zones denses) sÃ©parÃ©es par des zones de faible densitÃ©. Pour lancer lâ€™algorithme, nous devons spÃ©cifier une mesure dâ€™association critique (distance ou dissimilaritÃ©) d* ainsi quâ€™un nombre de point critique k dans le voisinage de cette distance.\n\nLâ€™algorithme commence par Ã©tiqueter chaque point selon lâ€™une de ces catÃ©gories:\n\n\n\nNoyau: le point a au moins k points dans son voisinage, câ€™est-Ã -dire Ã  une distance infÃ©rieure ou Ã©gale Ã  d.\n\nBordure: le point a moins de k points dans son voisinage, mais lâ€™un de des points voisins est un noyau.\n\nBruit: le cas Ã©chÃ©ant. Ces points sont considÃ©rÃ©s comme des outliers.\n\n\n\n\n\n\nLes noyaux distancÃ©s de d ou moins sont connectÃ©s entre eux en englobant les bordures.\n\n\n\n\n\nLe nombre de groupes est prescrit par lâ€™algorithme DBSCAN, qui permet du coup de dÃ©tecter des donnÃ©es trop bruitÃ©es pour Ãªtre classÃ©es.\nDamiani et al.Â (2014) a dÃ©veloppÃ© une approche utilisant la technique DBSCAN pour partitionner des zones dâ€™escale pour les flux de populations migratoires.\n\n10.3.2.2.1 Application\nLa technique DBSCAN nâ€™est pas basÃ©e sur le nombre de groupe, mais sur la densitÃ© des points. Lâ€™argument x ne constitue pas les donnÃ©es, mais une matrice dâ€™association. Lâ€™argument minPts spÃ©cifie le nombre minimal de points qui lâ€™on doit retrouver Ã  une distance critique d* pour la formation des *noyaux et la propagation des groupes, spÃ©cifiÃ©e dans lâ€™argument eps. La distance d peut Ãªtre estimÃ©e en prenant une fraction de la moyenne, mais on aura volontiers recours Ã  sont bon jugement.\n\nlibrary(\"dbscan\")\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nmcinnes_dbscan &lt;- dbscan(x = vegdist(df_mcinnes[, ], method = \"euclidean\"),\n                         eps = 0.03, minPts = 10)\ndbscan_group &lt;- mcinnes_dbscan$cluster\nunique(dbscan_group)\n\n[1] 1 0 2 6 3 4 5\n\n\nLes paramÃ¨tres spÃ©cifiÃ©s donnent 5 groupes (1, 2, ..., 5) et des points trop bruitÃ©s pour Ãªtre classifiÃ©s (Ã©tiquetÃ©s 0). Voyons comment les groupes ont Ã©tÃ© formÃ©s.\n\ndf_mcinnes |&gt; \n  mutate(dbscan_group = dbscan_group) |&gt; # ajouter une colonne de regoupement\n  ggplot(aes(x=x, y=y)) +\n  geom_point(aes(colour = factor(dbscan_group))) +\n  coord_fixed()\n\n\n\n\n\n\n\nLe partitionnement semble plus conforme Ã  ce que lâ€™on recherche. NÃ©anmoins, DBSCAN crÃ© quelques petits groupes indÃ©sirables (groupe 6, en rose) ainsi quâ€™un grand groupe (violet) qui auraient lieu dâ€™Ãªtre partitionnÃ©. Ces dÃ©faut pourraient Ãªtre rÃ©glÃ©s en jouant sur les paramÃ¨tres eps et minPts.\n\n10.3.3 Partitionnement hiÃ©rarchique\nLes techniques de partitionnement hiÃ©rarchique sont basÃ©es sur les matrices dâ€™association. La technique pour mesurer lâ€™association (entre objets ou variables) dÃ©terminera en grande partie le partitionnement des donnÃ©es. Les partitionnements hiÃ©rarchiques ont lâ€™avantage de pouvoir Ãªtre reprÃ©sentÃ©s sous forme de dendrogramme (ou arbre) de partition. Un tel dendrogramme prÃ©sente des sous-groupes qui se joignent en groupes jusquâ€™Ã  former un seul ensemble.\nLe partitionnement hiÃ©rarchique est abondamment utilisÃ© en phylogÃ©nie, pour Ã©tudier les relations de parentÃ© entre organismes vivants, populations dâ€™organismes et espÃ¨ces. La phÃ©nÃ©tique, branche empirique de la phylogÃ©nÃ¨se interspÃ©cifique, fait usage du partitionnement hiÃ©rarchique Ã  partir dâ€™associations gÃ©nÃ©tiques entre unitÃ©s taxonomiques. On retrouve de nombreuses ressources acadÃ©miques en phylogÃ©nÃ©tique ainsi que des outils pour R et Python. Toutefois, la phylogÃ©nÃ©tique en particulier ne fait pas partie de la prÃ©sente itÃ©ration de ce manuel.\n\n10.3.3.1 Techniques de partitionnement hiÃ©rarchique\nLe partitionnement hiÃ©rarchique est typiquement effectuÃ© avec une des quatre mÃ©thodes suivantes, dont chacune possÃ¨de ses particularitÃ©s, mais sont toutes agglomÃ©ratives: Ã  chaque Ã©tape dâ€™agglomÃ©ration, on fusionne les deux groupes ayant le plus dâ€™affinitÃ© sur la base des deux sous-groupes les plus rapprochÃ©s.\nSingle link (single). Les groupes sont agglomÃ©rÃ©s sur la base des deux points parmi les groupes, qui sont les plus proches.\nComplete link (complete). Ã€ la diffÃ©rence de la mÃ©thode single, on considÃ¨re comme critÃ¨re dâ€™agglomÃ©ration les Ã©lÃ©ments les plus Ã©loignÃ©s de chaque groupe.\nAgglomÃ©ration centrale. Il sâ€™agit dâ€™une famille de mÃ©thodes basÃ©es sur les diffÃ©rences entre les tendances centrales des objets ou des groupes.\n\n\nAverage (average). AppelÃ©e UPGMA (Unweighted Pair-Group Method unsing Average), les groupes sont agglomÃ©rÃ©s selon un centre calculÃ©s par la moyenne et le nombre dâ€™objet pondÃ¨re lâ€™agglomÃ©ration (le poids des groupes est retirÃ©). Cette technique est historiquement utilisÃ©e en bioinformatique pour partitionner des groupes phylogÃ©nÃ©tiques (Sneath et Sokal, 1973).\n\nWeighted (weighted). La version de average, mais non pondÃ©rÃ©e (WPGMA).\n\nCentroid (centroid). Tout comme average, mais le centroÃ¯de (centre gÃ©omÃ©trique) est utilisÃ© au lieu de la moyenne. Accronyme: UPGMC.\n\nMedian (median). AppelÃ©e WPGMC. Devinez! ;)\n\nWard (ward). Lâ€™optimisation vise Ã  minimiser les sommes des carrÃ©s par regroupement.\n\n10.3.3.2 Quel outil de partitionnement hiÃ©rarchique utiliser?\nAlors que le choix de la matrice dâ€™association dÃ©pend des donnÃ©es et de leur contexte, la technique de partitionnement hiÃ©rarchique peut, quant Ã  elle, Ãªtre basÃ©e sur un critÃ¨re numÃ©rique. Il en existe plusieurs, mais le critÃ¨re recommandÃ© pour le choix dâ€™une technique de partitionnement hiÃ©rarchique est la corrÃ©lation cophÃ©nÃ©tique. La distance cophÃ©nÃ©tique est la distance Ã  laquelle deux objets ou deux sous-groupes deviennent membres dâ€™un mÃªme groupe. La corrÃ©lation cophÃ©nÃ©tique est la corrÃ©lation de Pearson entre le vecteur dâ€™association des objets et le vecteur de distances cophÃ©nÃ©tiques.\n\n10.3.3.3 Application\nLes techniques de partitionnement hiÃ©rarchique prÃ©sentÃ©es ci-dessus sont disponibles dans le module stats de R, qui est chargÃ© automatiquement lors de lâ€™ouverture de R. Nous allons classifier les dimensions des iris grÃ¢ce Ã  la distance de Manhattan.\n\nmcinnes_hclust_distmat &lt;- vegdist(df_mcinnes, method = \"manhattan\")\n\nclustering_methods &lt;- c('single', 'complete', 'average', 'centroid', 'ward')\n\nclust_l &lt;- list()\ncoph_corr_l &lt;- c()\n\nfor (i in seq_along(clustering_methods)) {\n  clust_l[[i]] &lt;- hclust(mcinnes_hclust_distmat, method = clustering_methods[i])\n  coph_corr_l[i] &lt;- cor(mcinnes_hclust_distmat, cophenetic(clust_l[[i]]))\n}\n\nThe \"ward\" method has been renamed to \"ward.D\"; note new \"ward.D2\"\n\ntibble(clustering_methods, coph_corr = coph_corr_l) |&gt; \n  ggplot(aes(x = fct_reorder(clustering_methods, -coph_corr), y = coph_corr)) +\n  geom_col() +\n  labs(x = \"MÃ©thode de partitionnement\", y = \"CorrÃ©lation cophÃ©nÃ©tique\")\n\n\n\n\n\n\n\nLa mÃ©thode average retourne la corrÃ©lation la plus Ã©levÃ©e. Pour plus de flexibilitÃ©, enchÃ¢ssons le nom de la mÃ©thode dans une variable. Ainsi, en chageant le nom de cette variable, le reste du code sera consÃ©quent.\n\nnames(clust_l) &lt;- clustering_methods\nbest_method &lt;- \"average\"\n\nLe partitionnement hiÃ©rarchique peut Ãªtre visualisÃ© par un dendrogramme.\n\nplot(clust_l[[best_method]])\n\n\n\n\n\n\n\n\n10.3.3.4 Combien de groupes utiliser?\nLa longueur des lignes verticales est la distance sÃ©parant les groupes enfants. Bien que la sÃ©lection du nombre de groupe soit avant tout basÃ©e sur les besoins du problÃ¨me, nous pouvons nous appuyer sur certains outils. La hauteur totale peut servir de critÃ¨re pour dÃ©finir un nombre de groupes adÃ©quat. On pourra sÃ©lectionner le nombre de groupe oÃ¹ la hauteur se stabilise en fonction du nombre de groupe. On pourra aussi utiliser le graphique silhouette, comprenant une collection de largeurs de silhouette, reprÃ©sentant le degrÃ© dâ€™appartenance Ã  son groupe. La fonction sklearn.metrics.silhouette_score, du module scikit-learn, sâ€™en occupe.\n\nasw &lt;- c()\nnum_groups &lt;- 3:10\nfor(i in seq_along(num_groups)) {\n  sil &lt;- silhouette(cutree(clust_l[[best_method]], k = num_groups[i]), mcinnes_hclust_distmat)\n  asw[i] &lt;- summary(sil)$avg.width\n}\nplot(num_groups, asw, type = \"b\")\n\n\n\n\n\n\n\nLe nombre optimal de groupes serait de 5. Coupons le dendrorgamme Ã  la hauteur correspondant Ã  5 groupes avec la fonction cutree.\n\nk_opt &lt;- num_groups[which.max(asw)]\nhclust_group &lt;- cutree(clust_l[[best_method]], k = k_opt)\nplot(clust_l[[best_method]])\nrect.hclust(clust_l[[best_method]], k = k_opt)\n\n\n\n\n\n\n\nLa classification hiÃ©rarchique, uniquement basÃ©e sur la distance, peut Ãªtre inappropriÃ©e pour dÃ©finir des formes complexes.\n\ndf_mcinnes |&gt; \n  mutate(hclust_group = hclust_group) |&gt; # ajouter une colonne de regoupement\n  ggplot(aes(x=x, y=y)) +\n  geom_point(aes(colour = factor(hclust_group))) +\n  coord_fixed()\n\n\n\n\n\n\n\n\n10.3.4 Partitionnement hiÃ©rarchique basÃ©e sur la densitÃ© des points\nLa tecchinque HDBSCAN, dont lâ€™algorithme est relativement rÃ©cent (Campello et al., 2013), permet une partitionnement hiÃ©rarchique sur le mÃªme principe des zones de densitÃ© de la technique DBSCAN. Le HDBSCAN a Ã©tÃ© utilisÃ©e pour partitionner les lieux dâ€™escale dâ€™oiseaux migrateurs en Chine (Xu et al., 2013).\nAvec DBSCAN, un rayon est fixÃ© dans une mÃ©trique appropriÃ©e. Pour chaque point, on compte le nombre de point voisins, câ€™est Ã  dire le nombre de point se situant Ã  une distance (ou une dissimilaritÃ©) Ã©gale ou infÃ©rieure au rayon fixÃ©. Avec HDBSCAN, on spÃ©cifie le nombre de points devant Ãªtre recouverts et on calcule le rayon nÃ©cessaire pour les recouvrir. Ainsi, chaque point est associÃ© Ã  un rayon critique que lâ€™on nommera \\(d_{noyau}\\). La mÃ©trique initiale est ensuite altÃ©rÃ©e: on remplace les associations entre deux objets A et B par la valeur maximale entre cette association, le rayon critique de A et le rayon critique de B. Cette nouvelle distance est appelÃ©e la distance dâ€™atteinte mutuelle: elle accentue les distances pour les points se trouvant dans des zones peu denses. On applique par la suite un algorithme semblable Ã  la partition hiÃ©rarchique single link: En sâ€™Ã©largissant, les rayons se superposent, chaque superposition de rayon forment graduellement des groupes qui sâ€™agglomÃ¨rent ainsi de maniÃ¨re hiÃ©rarchique. Au lieu dâ€™effectuer une tranche Ã  une hauteur donnÃ©e dans un dendrogramme de partitionnement, la technique HDBSCAN se base sur un dendrogramme condensÃ© qui discarte les sous-groupes comprenant moins de n objets (\\(n_{gr min}\\)). Dans nouveau dendrogramme, on recherche des groupes qui occupent bien lâ€™espace dâ€™analyse. Pour ce faire, on utilise lâ€™inverse de la distance pour crÃ©er un indicateur de persistance (semblable Ã  la similaritÃ©), \\(\\lambda\\). Pour chaque groupe hiÃ©rarchique dans le dendrogramme condensÃ©, on peut calculer la persistance oÃ¹ le groupe prend naissance. De plus, pour chaque objet dâ€™un groupe, on peut aussi calculer une distance Ã  laquelle il quitte le groupe. La stabilitÃ© dâ€™un groupe est la somme des diffÃ©rences de persistance entre la persistance Ã  la naissance et les persistances des objets. On descend dans le dendrogramme. Si la somme des stabilitÃ© des groupes enfants est plus grande que la stabilitÃ© du groupe parent, on accepte la division. Sinon, le parent forme le groupe. La documentation du module hdbscan pour Python offre une description intuitive et plus exhaustive des principes et algorithme de HDBSCAN.\n\n10.3.4.1 ParamÃ¨tres\nOutre la mÃ©trique dâ€™association dont nous avons discutÃ©, HDBSCAN demande dâ€™Ãªtre nourri avec quelques paramÃ¨tres importants. En particulier, le nombre minimum dâ€™objets par groupe, \\(n_{gr min}\\) dÃ©pend de la quantitÃ© de donnÃ©es que vous avez Ã  votre disposition, ainsi que de la quantitÃ© dâ€™objets que vous jugez suffisante pour crÃ©er des groupes. Nous utiliserons lâ€™implÃ©mentation de HDBSCAN du module dbscan. Si vous dÃ©sirez davantage dâ€™options, vous prÃ©fÃ©rerez probablement lâ€™implÃ©mentation du module largeVis.\n\nmcinnes_hdbscan &lt;- hdbscan(x = vegdist(df_mcinnes, method = \"euclidean\"),\n                           minPts = 20,\n                           gen_hdbscan_tree = TRUE,\n                           gen_simplified_tree = FALSE)\nhdbscan_group &lt;- mcinnes_hdbscan$cluster\nunique(hdbscan_group)\n\n[1] 6 0 4 3 5 1 2\n\n\nNous avons 6 groupes, numÃ©rotÃ©s de 1 Ã  6, ainsi que des Ã©tiquettes identifiant des objets dÃ©signÃ©s comme Ã©tant du bruit de fond, numÃ©rotÃ© 0. Le dendrogramme non condensÃ© peu Ãªtre produit.\n\nplot(mcinnes_hdbscan$hdbscan_tree)\n\n\n\n\n\n\n\nDifficile dâ€™y voir clair avec autant dâ€™objets. Lâ€™objet mcinnes_hdbscan a un nombre minimum dâ€™objets par groupe de 20. Ce qui permet de prÃ©senter le dendrogramme de maniÃ¨re condensÃ©e.\n\nplot(mcinnes_hdbscan)\n\n\n\n\n\n\n\nEnfin, un aperÃ§u des stratÃ©gies de partitionnement utilisÃ©s jusquâ€™ici.\n\nclustering_group &lt;- df_mcinnes |&gt; \n  mutate(kmeans_group,\n         hclust_group,\n         dbscan_group,\n         hdbscan_group) |&gt; \n  gather(-x, -y, key = \"method\", value = \"cluster\")\n\nWarning: attributes are not identical across measure variables; they will be\ndropped\n\nclustering_group$cluster &lt;- factor(clustering_group$cluster)\nclustering_group |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point(aes(colour = cluster)) +\n  facet_wrap(~method, ncol = 2) +\n  coord_equal() +\n  theme_bw()\n\n\n\n\n\n\n\nClairement, le partitionnement avec HDBSCAN donne les meilleurs rÃ©sultats.\n\n10.3.5 Conclusion sur le partitionnement\nAu chapitreÂ 4, nous avons vu avec le jeu de donnÃ©es â€œdatasaurusâ€ que la visualisation peut permettre de dÃ©tecter des structures en segmentant les donnÃ©es selon des groupes.\n\n\n\n\n\n\n\n\nOr, si les donnÃ©es nâ€™Ã©taient pas Ã©tiquetÃ©es, leur structure serait indÃ©tectable avec les algorithmes disponibles actuellement. Le partitionnement permet dâ€™explorer des donnÃ©es, de dÃ©tecter des tendances et de dÃ©gager des groupes permettant la prise de dÃ©cision.\nPlusieurs techniques de partitionnement ont Ã©tÃ© prÃ©sentÃ©es. Le choix de la technique sera dÃ©terminante sur la maniÃ¨re dont les groupes seront partitionnÃ©s. La dÃ©finition dâ€™un groupe variant dâ€™un cas Ã  lâ€™autre, il nâ€™existe pas de rÃ¨gle pour prescrire une mÃ©thode ou une autre. La partitionnement hiÃ©rarchique a lâ€™avantage de permettre de visualiser comment les groupes sâ€™agglomÃ¨rent. Parmi les mÃ©thodes de partitionnement hiÃ©rarchique disponibles, les mÃ©thodes basÃ©es sur la densitÃ© permettent une grande flexibilitÃ©, ainsi quâ€™une dÃ©tection dâ€™observations ne faisant partie dâ€™aucun groupe."
  },
  {
    "objectID": "09-ordination.html#ordination",
    "href": "09-ordination.html#ordination",
    "title": "10Â  Association, partitionnement et ordination",
    "section": "\n10.4 Ordination",
    "text": "10.4 Ordination\nEn Ã©cologie, biologie, agronomie comme en foresterie, la plupart des tableaux de donnÃ©es comprennent de nombreuses variables: pH, nutriments, climat, espÃ¨ces ou cultivars, etc. Lâ€™ordination vise Ã  mettre de lâ€™ordre dans des donnÃ©es dont le nombre Ã©levÃ© de variables peut amener Ã  des difficultÃ©s dâ€™apprÃ©ciation et dâ€™interprÃ©tation (Legendre et Legendre, 2012). Plus prÃ©cisÃ©ment, le terme ordination est utilisÃ© en Ã©cologie pour dÃ©signer les techniques de rÃ©duction dâ€™axe. Lâ€™analyse en composante principale est probablement la plus connue de ces techniques. Mais de nombreuses techniques dâ€™ordination ont Ã©tÃ© dÃ©veloppÃ©es au cours des derniÃ¨res annÃ©es, chacune ayant ses domaines dâ€™application.\nLes techniques de rÃ©duction dâ€™axe permettent de dÃ©gager lâ€™information la plus importante en projetant une synthÃ¨se des relations entre les observations et entre les variables. Les techniques ne supposant aucune structure a priori sont dites non-contraignantes: elles ne comprennent pas de tests statistiques. Ã€ lâ€™inverse, les ordinations contraignantes lient des variables descriptives avec une ou plusieurs variables prÃ©dictives.\nLa rÃ©fÃ©rence en la matiÃ¨re est indiscutablement (Legendre et Legendre, 2012). Cette section en couvrira quelques unes et vous guidera vers la technique la plus appropriÃ©e pour vos donnÃ©es.\n\n10.4.1 Ordination non contraignante\nCette section couvrira lâ€™analyse en composantes principales (ACP), lâ€™analyse de correspondance (AC), lâ€™analyse factorielle (AF) ainsi que lâ€™analyse en coordonnÃ©es principales (ACoP).\n\n\n\n\n\n\n\nMÃ©thode\nDistance prÃ©servÃ©e\nVariables\n\n\n\nAnalyse en composantes principales (ACP)\nDistance euclidienne\nDonnÃ©es quantitatives, relations linÃ©aires (attention aux double-zÃ©ros)\n\n\nAnalyse de correspondance (AC)\nDistance de \\(\\chi^2\\)\n\nDonnÃ©es non-nÃ©gatives, dimentionnellement homogÃ¨nes ou binaires, abondance ou occurrence\n\n\nPositionnement multidimensionnel (PoMd)\nToute mesure de dissimilaritÃ©\nDonnÃ©es quantitatives, qualitatives nominales/ordinales ou mixtes\n\n\n\nSource: AdaptÃ© de (Legendre et Legendre, 2012, chapitre 9)\n\n10.4.1.1 Analyse en composantes principales\nLâ€™objectif dâ€™une ACP est de reprÃ©senter les donnÃ©es dans un nombre rÃ©duit de dimensions reprÃ©sentant le plus possible la variation dâ€™un tableau de donnÃ©es: elle permet de projeter les donnÃ©es dans un espace oÃ¹ les variables sont combinÃ©es en axes orthogonaux dont le premier axe capte le maximum de variance. Lâ€™ACP peut par exemple Ãªtre utilisÃ©e pour analyser des corrÃ©lations entre variables ou dÃ©gager lâ€™information la plus pertinente dâ€™un tableau de donnÃ©es mÃ©tÃ©o ou de signal en un nombre plus retreint de variables.\nLâ€™ACP effectue une rotation des axes Ã  partir du centre (moyenne) du nuage de points effectuÃ©e de maniÃ¨re Ã  ce que le premier axe dÃ©finisse la direction oÃ¹ lâ€™on retrouve la variance maximale. Ce premier axe est une combinaison linÃ©aire des variables et forme la premiÃ¨re composante principale. Une fois cet axe dÃ©finit, on trouve de deuxiÃ¨me axe, orthogonal au premier, oÃ¹ lâ€™on retrouve la variance maximale - cet axe forme la deuxiÃ¨me composante principale, et ainsi de suite jusquâ€™Ã  ce que le nombre dâ€™axe corresponde au nombre de variables. Les projections des observations sur ces axes principaux sont appelÃ©s les scores. Les projections des variables sur les axes principaux sont les vecteurs propres (eigenvectors, ou loadings). La variance des composantes principales diminue de la premiÃ¨re Ã  la derniÃ¨re, et peut Ãªtre calculÃ©e comme une proportion de la variance totale: câ€™est le pourcentage dâ€™inertie. Par convention, on utilise les valeurs propres (eigenvalues) pour mesurer lâ€™importance des axes. Si la premiÃ¨re composante principale a une inertie de 50% et la deuxiÃ¨me a une inertie de 30%, la reprÃ©sentation en 2D des projection reprÃ©sentera 80% de la variance du nuage de points.\nLâ€™hÃ©tÃ©rogÃ©nÃ©itÃ© des Ã©chelles de mesure peut avoir une grande importance sur les rÃ©sultats dâ€™une ACP (les donnÃ©es doivent Ãªtre dimensionnellement homogÃ¨nes). En effet, la hauteur dâ€™un ceriser aura une variance plus grande que le diamÃ¨tre dâ€™une cerise exprimÃ© dans les mÃªmes unitÃ©s, et cette derniÃ¨re aura plus de variance que la teneur en cuivre dâ€™une feuille. Il est consÃ©quemment avisÃ© de mettre les donnÃ©es Ã  lâ€™Ã©chelle en centrant la moyenne Ã  zÃ©ro et lâ€™Ã©cart-type Ã  1 avant de procÃ©der Ã  une ACP.\nLâ€™ACP a Ã©tÃ© conÃ§ue pour projeter en un nombre moindre de dimensions des observations dont les distributions sont multinormales. Bien que lâ€™ACP soit une technique robuste, il est prÃ©fÃ©rable de transformer prÃ©alablement les variables dont la distribution est particuliÃ¨rement asymÃ©triques (Legendre et Legendre, 2012, p.Â 450). Le cas Ã©chÃ©ant, les valeurs extrÃªmes pourraient faire dÃ©vier les vecteurs propres et biaiser lâ€™analyse. En particulier, les donnÃ©es ACP menÃ©es sur des donnÃ©es compositionnelles sont rÃ©putÃ©es pour gÃ©nÃ©rer des analyses biaisÃ©es (Pawlowsky-Glahn and Egozcue, 2006). Le test de Mardia (Korkmaz, 2014) peut Ãªtre utilisÃ© pour tester la multinormalitÃ©. Une distribution multinormale devrait gÃ©nÃ©rer des scores en forme dâ€™hypersphÃ¨re (en forme de cercle sur un biplot: voir plus loin).\n\n10.4.1.1.1 Vecteurs propres et valeurs propres\nUne matrice carrÃ©e (comme une matrice de covariance \\(\\Sigma\\)) multipliÃ©e par un vecteur propre \\(e\\) est Ã©gale aux valeurs propres \\(\\lambda\\) multipliÃ©es par les vecteurs propres \\(e\\).\n\\[ \\Sigma e = \\lambda e \\]\nDe maniÃ¨re intuitive, les vecteurs propres indiquent lâ€™orientation de la covariance, et les valeurs propres indique la longueur associÃ©e Ã  cette direction. Lâ€™ACP est basÃ©e sur le calcul des vecteurs propres et des valeurs propres de la matrice de covariance des variables. Pour dâ€™abord obtenir les valeurs propres \\(\\lambda\\), il faut rÃ©soudre lâ€™Ã©quation\n\\[ det(cov(X) - \\lambda I) = 0 \\],\noÃ¹ \\(det\\) est lâ€™opÃ©ration permettant de calculer le dÃ©terminant, \\(cov\\) est lâ€™opÃ©ration pour calculer la covariance, \\(X\\) est la matrice de donnÃ©es, \\(\\lambda\\) sont les valeurs propres et \\(I\\) est une matrice dâ€™identitÃ©.\nPour \\(p\\) variables dans votre tableau \\(X\\), vous obtiendrex \\(p\\) valeurs propres. Ensuite, on trouve les vecteurs propres en rÃ©solvant lâ€™Ã©quation $ e = e $.\nBien quâ€™il soit possible dâ€™effectuer cette opÃ©ration Ã  la main pour des cas trÃ¨s simples, vous aurez avantage Ã  utiliser un langage de programmation.\nChargeons les donnÃ©es dâ€™iris, puis isolons seulement les deux dimensions des sÃ©pales lâ€™espÃ¨ce setosa.\n\ndata(\"iris\")\nsetosa_sepal &lt;- iris |&gt; \n  filter(Species == \"setosa\") |&gt; \n  select(starts_with(\"Sepal\"))\nsetosa_sepal\n\n   Sepal.Length Sepal.Width\n1           5.1         3.5\n2           4.9         3.0\n3           4.7         3.2\n4           4.6         3.1\n5           5.0         3.6\n6           5.4         3.9\n7           4.6         3.4\n8           5.0         3.4\n9           4.4         2.9\n10          4.9         3.1\n11          5.4         3.7\n12          4.8         3.4\n13          4.8         3.0\n14          4.3         3.0\n15          5.8         4.0\n16          5.7         4.4\n17          5.4         3.9\n18          5.1         3.5\n19          5.7         3.8\n20          5.1         3.8\n21          5.4         3.4\n22          5.1         3.7\n23          4.6         3.6\n24          5.1         3.3\n25          4.8         3.4\n26          5.0         3.0\n27          5.0         3.4\n28          5.2         3.5\n29          5.2         3.4\n30          4.7         3.2\n31          4.8         3.1\n32          5.4         3.4\n33          5.2         4.1\n34          5.5         4.2\n35          4.9         3.1\n36          5.0         3.2\n37          5.5         3.5\n38          4.9         3.6\n39          4.4         3.0\n40          5.1         3.4\n41          5.0         3.5\n42          4.5         2.3\n43          4.4         3.2\n44          5.0         3.5\n45          5.1         3.8\n46          4.8         3.0\n47          5.1         3.8\n48          4.6         3.2\n49          5.3         3.7\n50          5.0         3.3\n\n\n\nlibrary(\"MVN\")\nsetosa_sepal_mvn &lt;- mvn(setosa_sepal, mvnTest = \"mardia\")\nsetosa_sepal_mvn$multivariateNormality\n\n             Test          Statistic           p value Result\n1 Mardia Skewness  0.759503524380438 0.943793240544741    YES\n2 Mardia Kurtosis 0.0934600553610254 0.925538081956867    YES\n3             MVN               &lt;NA&gt;              &lt;NA&gt;    YES\n\n\nPour considÃ©rer la distribution comme multinormale, la p-value de la distortion (Mardia Skewness) et la statistique de Kurtosis (Mardia Kurtosis) doit Ãªtre Ã©gale ou plus Ã©levÃ©e que 0.05 (Kormaz, 2019, fiche dâ€™aide de la fonction mvn de R). Câ€™est bien le cas pour les donnÃ©es du tableau setosa_sepal.\nRetirons de la matrice de covariance les valeurs et vecteurs propres avec la fonction eigen.\n\nsetosa_eigen &lt;- eigen(cov(setosa_sepal))\nsetosa_eigenval &lt;- setosa_eigen$values\nsetosa_eigenvec &lt;- setosa_eigen$vectors\n\nLe premier vecteur propre correspond Ã  la premiÃ¨re colonne, et le second Ã  la deuxiÃ¨me. Les coordonnÃ©es x et y sont les premiÃ¨res et deuxiÃ¨mes lignes. Les vecteurs propres ont une longueur unitaire (norme de 1). Ils peuvent Ãªtre mis Ã  lâ€™Ã©chelles Ã  la racine carrÃ©e des valeurs propres.\n\nsetosa_eigenvec_sc &lt;- setosa_eigenvec %*% diag(sqrt(setosa_eigen$values))\n\nPour effectuer une translation des vecteurs propres au centre du nuage de point, nous avons besoin du centroÃ¯de.\n\ncentroid &lt;- setosa_sepal %&gt;% apply(., 2, mean)\n\n\nplot(setosa_sepal, asp = 1)\n\n# vecteurs propres brutes\nlines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]),\n      y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), col = \"green\", lwd = 3) # vecteur propre 1\nlines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]),\n      y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), col = \"green\", lwd = 3) # vecteur propre 1\n\n# vecteurs propres Ã  l'Ã©chelle\nlines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]),\n      y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), col = \"red\", lwd = 4) # vecteur propre 1\nlines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]),\n      y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), col = \"red\", lwd = 4) # vecteur propre 1\n\npoints(x=centroid[1], y=centroid[2], pch = 16, cex = 2, col  =\"blue\") # centroid\n\n\n\n\n\n\n\nOn peut observer que, comme je lâ€™ai mentionnÃ© plus haut, les vecteurs propres indiquent lâ€™orientation de la covariance, et les valeurs propres indique la longueur associÃ©e Ã  cette direction.\n\n10.4.1.1.2 Biplot\nImaginez un nuage de points en 3D, axes y compris. Vous tournez votre nuage de points pour trouver la perspective en 2D qui fera en sorte que vos donnÃ©es soient les plus dispersÃ©es possibles. Avec une lampe de poche, vous illuminez votre nuage de points dans lâ€™axe de cette perspective: vous venez dâ€™effectuer une analyse en composantes principales, et lâ€™ombre des points et des axes sur le mur formera votre biplot.\nPour crÃ©er un biplot, on juxtapose les descripteurs (variables) en tant que vecteurs propres, reprÃ©sentÃ©s par des flÃ¨ches, et les objets (observations) en tant que scores, reprÃ©sentÃ©s par des points. Les rÃ©sultats dâ€™une ordination peuvent Ãªtre prÃ©sentÃ©s selon deux types de biplots (Legendre et Legendre, 2012).\n\n\n\n\n\nBiplot de corrÃ©lation permettant de visualiser les corrÃ©lations entre des variables mÃ©tÃ©orologiques.\n\nDeux types de projection sont couramment utilisÃ©s.\nBiplot de distance. Ce type de projection permet de visualiser la position des objets entre eux et par rapport aux descripteurs et dâ€™apprÃ©cier la contribution des descripteurs pour crÃ©er les composantes principales. Pour crÃ©er un biplot de distance, on projette directement les vecteurs propres (\\(U\\)) en guise de descripteurs. Pour ce qui est des objets, on utilise les scores de lâ€™ACP (\\(F\\)). De cette maniÃ¨re,\n\nles distances euclidiennes entre les scores sont des approximations des distances euclidiennes dans lâ€™espace multidimensionnel,\nla projection dâ€™un objet sur un descripteur perpendiculairement Ã  ce dernier est une approximation de la position de lâ€™objet sur le descripteur et\nla projection dâ€™un descripteur sur un axe principal est proportionnelle Ã  sa contribution pour gÃ©nÃ©rer lâ€™axe.\n\nBiplot de corrÃ©lation. Cette projection permet dâ€™apprÃ©cier les corrÃ©lations entre les descripteurs. Pour ce faire, les objets et les valeurs propres doivent Ãªtre transformÃ©s. Pour gÃ©nÃ©rer les descripteurs, les vecteurs propres (\\(U\\)) doivent Ãªtre multipliÃ©s par la matrice diagonalisÃ©e de la racine carrÃ©e des valeurs propres (\\(\\Lambda\\)), câ€™est-Ã -dire \\(U \\Lambda ^{\\frac{1}{2}}\\). En ce qui a trait aux objets, on multiplie les scores par (\\(F\\)) par la racine carrÃ©e nÃ©gative des valeurs propres diagonalisÃ©es, câ€™est-Ã -dire \\(F \\Lambda ^{- \\frac{1}{2}}\\). De cette maniÃ¨re,\n\ntout comme câ€™est le cas pour le biplot de distance, la projection dâ€™un objet sur un descripteur perpendiculairement Ã  ce dernier est une approximation de la position de lâ€™objet sur le descripteur,\nla projection dâ€™un descripteur sur un axe principal est proportionnelle Ã  son Ã©cart-type et\nles angles entre les descripteurs sont proportionnelles Ã  leur corrÃ©lation (et non pas leur proximitÃ©).\n\nEn dâ€™autres mots, le bilot de distances devrait Ãªtre utilisÃ© pour apprÃ©cier la distance entre les objets et le biplot de corrÃ©lation devrait Ãªtre utilisÃ© pour apprÃ©cier les corrÃ©lations entre les descripteurs. Mais dans tous les cas, le type de biplot utilisÃ© doit Ãªtre indiquÃ©.\nLe triplot est une forme apparentÃ©e au biplot, auquel on ajoute des variables prÃ©dictives. Le triplot est utile pour reprÃ©senter les rÃ©sultats des ordinations contraignantes comme les analyses de redondance et les analyse de correspondance canoniques.\n\n10.4.1.1.3 Application\nBien que lâ€™ACP puisse Ãªtre effectuÃ©e grÃ¢ce Ã  des modules de base de R, nous utiliserons le module vegan. Le tableau varechem comprend des donnÃ©es issues dâ€™analyse de sols identifiÃ©s par leur composition chimique, leur pH, leur profondeur totale et la profondeur de lâ€™humus publiÃ©es dans VÃ¤re et al.Â (1995) et exportÃ©es du module vegan.\n\nlibrary(\"vegan\")\ndata(\"varechem\")\nvarechem |&gt; \n  sample_n(5)\n\n      N    P     K    Ca    Mg    S    Al   Fe    Mn   Zn  Mo Baresoil Humdepth\n27 20.6 60.8 233.7 834.0 127.2 40.7  15.4  4.4 132.0 10.7 0.2     18.7      2.9\n28 29.8 73.5 260.0 748.6 105.3 42.5  17.9  2.4 106.6  9.3 0.3     17.6      3.0\n18 19.8 42.1 139.9 519.4  90.0 32.3  39.0 40.9  58.1  4.5 0.3     43.9      2.2\n5  33.1 22.7  43.6 240.3  25.7 14.9  39.0  8.4  26.8  8.4 0.2      8.1      1.0\n6  19.1 26.4  61.1 259.1  37.0 21.4 155.1 81.4  20.6  4.0 0.6      5.8      1.9\n    pH\n27 2.8\n28 2.8\n18 2.7\n5  3.1\n6  3.0\n\n\nComme nous lâ€™avons vu prÃ©cedemment, les donnÃ©es de concentration sont de type compositionnelles. Les donnÃ©es compositionnelles du tableau varechem mÃ©riteraient dâ€™Ãªtre transformÃ©es (Aitchison et Greenacre, 2002). Utilisons les log-ratios centrÃ©s (clr).\n\nlibrary(\"compositions\")\n\nWelcome to compositions, a package for compositional data analysis.\nFind an intro with \"? compositions\"\n\n\n\nAttaching package: 'compositions'\n\n\nThe following objects are masked from 'package:stats':\n\n    anova, cor, cov, dist, var\n\n\nThe following object is masked from 'package:graphics':\n\n    segments\n\n\nThe following objects are masked from 'package:base':\n\n    %*%, norm, scale, scale.default\n\nvarecomp &lt;- varechem |&gt;\n  select(-Baresoil, -Humdepth, -pH) %&gt;%\n  mutate(Fv = apply(., 1, function(x) 1e6 - sum(x)))\nvareclr &lt;- varecomp |&gt;\n  acomp() |&gt;\n  clr() |&gt; \n  as_tibble() |&gt; \n  bind_cols(varechem |&gt;\n              select(Baresoil, Humdepth, pH))\nvareclr |&gt; \n  sample_n(5)\n\n# A tibble: 5 Ã— 15\n      N      P     K    Ca      Mg      S    Al     Fe     Mn    Zn    Mo    Fv\n* &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -1.88 -0.600 0.641  1.48 -0.684  -0.805 1.30  -0.156 -1.14  -2.52 -4.68  9.04\n2 -1.69 -0.624 0.830  1.59 -0.0312 -0.729 0.189 -0.626 -0.331 -2.60 -5.49  9.53\n3 -1.76 -0.553 0.565  2.11  0.575  -0.706 0.159 -1.20  -1.30  -1.95 -4.99  9.05\n4 -1.78 -0.613 0.354  1.72 -0.174  -0.645 0.932 -1.02  -0.935 -2.31 -4.85  9.32\n5 -1.34 -0.760 0.673  2.07  0.549  -0.762 0.181 -1.60  -0.696 -2.26 -5.39  9.34\n# â„¹ 3 more variables: Baresoil &lt;dbl&gt;, Humdepth &lt;dbl&gt;, pH &lt;dbl&gt;\n\n\nEffectuons lâ€™ACP. Pour cet exemple, nous standardiserons les donnÃ©es Ã©tant donnÃ©es que les colonnes Baresoil, Humedepth et pH ne sont pas Ã  la mÃªme Ã©chelle que les colonnes des clr.\n\nvareclr_sc &lt;- scale(vareclr)\nvare_pca &lt;- rda(vareclr_sc) # ou bien rda(vareclr, scale = TRUE, mais la mise Ã  l'Ã©chelle prÃ©alable est plus explicite)\n\nLâ€™objet vareclr_pca contient lâ€™information nÃ©cessaire pour mener notre ACP.\n\nsummary(vare_pca, scaling = 2) # scaling = 2 pour obtenir les infos pour les biplots de corrÃ©lation\n\n\nCall:\nrda(X = vareclr_sc) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal              15          1\nUnconstrained      15          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                         PC1    PC2    PC3     PC4     PC5     PC6     PC7\nEigenvalue            7.1523 2.4763 2.1122 0.93015 0.57977 0.48786 0.36646\nProportion Explained  0.4768 0.1651 0.1408 0.06201 0.03865 0.03252 0.02443\nCumulative Proportion 0.4768 0.6419 0.7827 0.84473 0.88338 0.91590 0.94034\n                          PC8     PC9    PC10     PC11     PC12     PC13\nEigenvalue            0.29432 0.19686 0.15434 0.107357 0.095635 0.042245\nProportion Explained  0.01962 0.01312 0.01029 0.007157 0.006376 0.002816\nCumulative Proportion 0.95996 0.97308 0.98337 0.990527 0.996902 0.999719\n                           PC14\nEigenvalue            0.0042200\nProportion Explained  0.0002813\nCumulative Proportion 1.0000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  4.309777 \n\n\nSpecies scores\n\n             PC1     PC2     PC3      PC4        PC5       PC6\nN         0.1437  0.7606 -0.6792  0.19837  0.1128526 -0.050149\nP         0.8670 -0.3214 -0.2950 -0.22940  0.1437960 -0.042884\nK         0.9122 -0.3857  0.2357  0.03469  0.2737020  0.075717\nCa        0.9649 -0.3362 -0.2147  0.17757 -0.2188717  0.008051\nMg        0.8263 -0.2723  0.1035  0.52135 -0.1495399 -0.342214\nS         0.8825 -0.3169  0.3539 -0.21216  0.1176279 -0.191386\nAl       -1.0105 -0.2442  0.2146  0.02674 -0.1005560 -0.043569\nFe       -1.0338 -0.2464  0.1492  0.13162  0.1512218  0.081571\nMn        0.9556  0.1041 -0.1256 -0.21300  0.2565831  0.275275\nZn        0.7763 -0.1031 -0.3123 -0.36493 -0.5665691  0.153089\nMo       -0.2152  0.8717  0.4065 -0.33643 -0.2134335 -0.167725\nFv        0.2360  0.5776 -0.8112  0.12736  0.1280097 -0.109737\nBaresoil  0.5147  0.4210  0.4472  0.54980 -0.1438570  0.463148\nHumdepth  0.7455  0.4379  0.5194  0.16493  0.0004757 -0.273056\npH       -0.5754 -0.5864 -0.5957  0.23408 -0.1517661 -0.056641\n\n\nSite scores (weighted sums of species scores)\n\n        PC1       PC2      PC3      PC4      PC5      PC6\n18  0.16862  0.423777  0.46731  0.91175  1.10380  1.06421\n15 -0.09705 -0.097482  0.61143 -0.29049  1.14916  0.40622\n24  0.02831 -0.795737  0.74176 -0.19097 -2.43337 -0.81762\n27  1.39081 -0.354376 -0.19377 -0.45160  0.46020 -0.31446\n23  1.30346  0.357866  0.29887  0.76856  0.20913 -0.64145\n19  0.43636  0.495037  1.21722  1.18128 -0.98242 -0.74474\n22  1.07306  0.467575 -0.32245  0.03717  0.13956 -0.64972\n16  0.02545  0.659714 -0.28861 -0.01424  0.47105  0.45173\n28  1.42005  0.007356 -0.29000 -0.78474  0.97592 -0.80263\n13 -0.50638 -0.220909  1.52981  0.26289  0.42135  0.94054\n14  0.45392  0.649297  0.44573 -0.26620 -0.74522 -0.53228\n20  0.18623  0.259640  0.89112  0.21096 -0.51393  2.24361\n25  1.26264  0.225744 -0.96668 -0.69334  0.61990  0.43312\n7  -1.48685  0.739545 -0.20926  1.09256  0.61856 -0.87999\n5  -0.50622  1.108685 -2.61287 -1.00433 -1.35383  1.21964\n6  -1.28653  0.898663 -0.38778 -0.47556 -0.02449 -0.29419\n3  -1.72773  0.476962 -0.48878  0.71156  1.06398 -1.33473\n4  -0.82844 -0.296515  1.20315 -1.49821 -0.18330  1.05231\n2  -1.00247 -0.609253  0.25185 -0.85420  0.71031  0.14854\n9  -0.43405 -0.338912  0.55348 -1.35776 -0.81986 -1.02468\n12 -0.05083  0.122645 -0.04611 -0.56047 -0.26151 -0.98053\n10  0.17891 -2.315489 -0.69084 -0.19547  0.80628  0.04291\n11 -0.46443 -2.592018 -1.21615  1.56359 -0.62334  0.28748\n21  0.46316  0.728185 -0.49843  1.89726 -0.80791  0.72671\n\n\nLa deuxiÃ¨me ligne de Importance of components, Proportion Explained, indique la proportion de la variance totale captÃ©e successivement par les axes principaux. Le premier axe principal comporte 47.68% de la variance. Le deuxiÃ¨me axe principal ajoutant une proportion de 16,51%, une reprÃ©sentation en deux axes principaux prÃ©sentent 64.19 % de la variance.\n\nprop_expl &lt;- vare_pca$CA$eig / sum(vare_pca$CA$eig)\nprop_expl\n\n         PC1          PC2          PC3          PC4          PC5          PC6 \n0.4768180610 0.1650859388 0.1408156459 0.0620101490 0.0386511040 0.0325238535 \n         PC7          PC8          PC9         PC10         PC11         PC12 \n0.0244303815 0.0196215021 0.0131238464 0.0102890284 0.0071571089 0.0063756951 \n        PC13         PC14 \n0.0028163495 0.0002813359 \n\n\nLa dÃ©cision du nombre dâ€™axes principaux Ã  retenir est arbitraire. Elle peut dÃ©pendre dâ€™un nombre maximal de paramÃ¨tre Ã  retenir pour Ã©viter de surdimensionner un modÃ¨le (curse of dimensionality, section 11) ou dâ€™un seuil de pourcentage de variance minimal Ã  retenir, par exemple 75%. Ou bien, vous retiendrez deux composantes principales si vous dÃ©sirez prÃ©senter un seul biplot.\nLâ€™approche de Kaiser-Guttmann (Borcard et al., 2011) consiste Ã  sÃ©lectionner les composantes principales dont la valeur propre est supÃ©rieure Ã  leur moyenne.\n\nplot(x = 1:length(vare_pca$CA$eig),\n     y = vare_pca$CA$eig,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\")\nabline(h = mean(vare_pca$CA$eig), col = \"red\", lty = 2)\n\n\n\n\n\n\n\nLâ€™approche du broken stick consiste Ã  couper un bÃ¢ton dâ€™une longueur de 1 en n tranches. La premiÃ¨re tranche est de longueur \\(\\frac{1}{n}\\). La tranche suivante est dâ€™une longueur de la tranche prÃ©cÃ©dente Ã  laquelle on aditionne \\(\\frac{1}{longueur~restante}\\). Puis on place les longueurs en ordre dÃ©croissant. On retient les composantes principales dont les valeurs propres cumulÃ©es sont plus grandes que le broken stick.\n\nbroken_stick &lt;- function(x) {\n  bsm &lt;- vector(\"numeric\", length = x)\n  bsm[1] &lt;- 1/x\n  for (i in 2:x) {\n    bsm[i] &lt;- bsm[i-1] + 1/(x+1-i)\n  }\n  bsm &lt;- rev(bsm/x)\n  return(bsm)\n}\n\nLe graphique du broken stick:\n\nplot(x = 1:length(vare_pca$CA$eig),\n     y = prop_expl,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\")\nlines(x = 1:length(vare_pca$CA$eig),\n      y = broken_stick(length(vare_pca$CA$eig)),\n      col = \"red\",\n      lty = 2)\n\n\n\n\n\n\n\nLes approches Kaiser-Guttmann et broken stick suggÃ¨rent que les trois premiÃ¨res composantes sont suffisantes pour dÃ©crire la dispersion des donnÃ©es.\nExaminons les loadings (vecteurs propres) plus en particulier. Dans le langage du module vegan, les vecteurs propres sont les espÃ¨ces (species) et les scores sont les sites.\n\nvare_eigenvec &lt;- vegan::scores(vare_pca, scaling = 2, display = \"species\", choices = 1:(ncol(vareclr)-1))\nvare_eigenvec\n\n                PC1        PC2        PC3        PC4           PC5          PC6\nN         0.1437343  0.7606006 -0.6792046  0.1983670  0.1128526122 -0.050148980\nP         0.8669892 -0.3213683 -0.2949864 -0.2294036  0.1437959857 -0.042883754\nK         0.9122089 -0.3857245  0.2356904  0.0346904  0.2737019601  0.075717162\nCa        0.9648855 -0.3361651 -0.2147486  0.1775746 -0.2188716732  0.008050762\nMg        0.8263327 -0.2723055  0.1035276  0.5213484 -0.1495399242 -0.342213793\nS         0.8824519 -0.3169039  0.3538854 -0.2121562  0.1176278503 -0.191386377\nAl       -1.0105173 -0.2441785  0.2145614  0.0267422 -0.1005559874 -0.043569364\nFe       -1.0337676 -0.2463987  0.1491865  0.1316173  0.1512218115  0.081571443\nMn        0.9555632  0.1041030 -0.1256178 -0.2130047  0.2565830557  0.275275174\nZn        0.7763480 -0.1030878 -0.3122919 -0.3649341 -0.5665691228  0.153089144\nMo       -0.2152399  0.8717229  0.4064967 -0.3364279 -0.2134335302 -0.167725160\nFv        0.2360040  0.5775863 -0.8111953  0.1273582  0.1280096553 -0.109737235\nBaresoil  0.5147445  0.4209983  0.4472351  0.5497950 -0.1438569673  0.463148072\nHumdepth  0.7455213  0.4379436  0.5193895  0.1649306  0.0004756685 -0.273056212\npH       -0.5753858 -0.5863743 -0.5957495  0.2340826 -0.1517660977 -0.056640816\n                 PC7         PC8          PC9        PC10         PC11\nN        -0.09111164 -0.06122008  0.315645453  0.08090232  0.019251478\nP         0.26894062  0.34111276  0.021124287  0.08756299  0.045741546\nK        -0.21662612 -0.01641260  0.143099440 -0.08737113 -0.183005607\nCa        0.03630015  0.04775616 -0.073609828 -0.10601799 -0.161460554\nMg        0.04617838 -0.12098602 -0.051599273  0.18373857  0.009862571\nS        -0.26825994  0.15822845  0.038378858  0.05100717  0.138785063\nAl       -0.22737412  0.10598673  0.040586196 -0.14473132  0.089462074\nFe        0.10553041 -0.09254655 -0.079426433  0.09908706  0.006376211\nMn        0.20224538 -0.19347804 -0.038859808 -0.07637994  0.083300112\nZn       -0.12332232 -0.14862229  0.024026151  0.02643462  0.064973307\nMo        0.13788948  0.17165900  0.032981311  0.01419924 -0.128814989\nFv       -0.20911147  0.11289753 -0.281443886 -0.08391004  0.012456867\nBaresoil -0.02103009  0.23028292  0.004554036  0.02604286  0.061147847\nHumdepth  0.17061078 -0.11310394  0.027515405 -0.23068827  0.102189307\npH        0.19890884  0.12152266  0.150118818 -0.15240317  0.037691048\n                 PC12        PC13         PC14\nN         0.045420621 -0.05020956  0.002340519\nP         0.145128883  0.03337551 -0.010109130\nK         0.002260341  0.10566808  0.001169065\nCa        0.041210064 -0.14341793  0.007419161\nMg       -0.063493608  0.03782662 -0.023575986\nS        -0.117144869 -0.06075094  0.025874035\nAl        0.058212507 -0.01983102 -0.037901576\nFe        0.049837173  0.01169516  0.036048221\nMn       -0.133353213 -0.02679781 -0.021373612\nZn        0.051057277  0.06538348  0.010896560\nMo       -0.114803631  0.01989539 -0.001335923\nFv       -0.020157331  0.05448619  0.005707928\nBaresoil -0.019696758  0.01640490  0.003823725\nHumdepth  0.109293684  0.02485030  0.016559206\npH       -0.153813168  0.04523353  0.014193061\nattr(,\"const\")\n[1] 4.309777\n\n\nLâ€™ordre dâ€™importance des vecteurs propres est Ã©tabli en ordre croissant des Ã©lÃ©ment des vecteurs propres associÃ©es. Un vecteur propre est une combinaison linÃ©aire des variables. Par exemple, le premier vecteur propre pointe surtout dans la direction du Fe (-1.497) et de lâ€™Al (-1.463). Le deuxiÃ¨me pointe surtout vers le Mo (2.145). Les vecteurs (loadings) dâ€™un biplot de distance prÃ©sentant les des deux premiÃ¨res composantes principales prendront les coordonnÃ©es des deux premiÃ¨res colonnes. Le vecteur Al aura la coordonnÃ©e [-1.463 ; -0.601], le vecteur de Fe sera placÃ© Ã  [-1.497 ; -0.606] et le vecteur Mo Ã  [-0.312 ; 2.145]. Il existe diffÃ©rentes fonctions dâ€™affichage des biplots. Notez que leur longueur peut Ãªtre magnifiÃ©e pour amÃ©liorer la visualisation.\nLanÃ§ons la fonction biplot pour crÃ©er un biplot de distance et un autre de corrÃ©lation.\n\npar(mfrow = c(1, 2))\nbiplot(vare_pca, scaling = 1, main = \"Biplot de distance\")\nbiplot(vare_pca, scaling = 2, main = \"Biplot de corrÃ©lation\")\n\n\n\n\n\n\n\nLe biplot de distance permet de dÃ©gager les variables qui expliquent davantage la variabilitÃ© dans notre tableau: les clr du Fe et de lâ€™Al forment en grande partie le premier axe principal, alors que le clr du Mo forme en grande partie le second axe. Le biplot de corrÃ©lation montre que les clr du Fe et du Al sont corrÃ©lÃ©s dans le mÃªme sens, mais das le sens contraire du clr du Mn. Lâ€™information sur la teneur en Fe et celle de lâ€™Al est en grande partie redondante. Toutefois, le clr du Mo est presque indÃ©pendant du clr du Fe, ceux-ci Ã©tant Ã  angle presque droit (~90Â°). Ces relations peuvent Ãªtre explorÃ©es directement.\n\npar(mfrow = c(1, 2))\nplot(vareclr$Al, vareclr$Fe)\nplot(vareclr$Mo, vareclr$Fe)\n\n\n\n\n\n\n\nNous avons mentionnÃ© que lâ€™ACP est une rotation. Prenons un second exemple pour bien en saisir les tenants et aboutissants. Le tableau de donnÃ©es que nous chargerons provient dâ€™un infographie dâ€™un dauphin, intitullÃ©e Bottlenose Dolphin, conÃ§u par lâ€™artiste Tarnyloo. Les points correspondent Ã  la surface dâ€™un dauphin. Jâ€™ai ajoutÃ© une colonne anatomy, qui indique Ã  quelle partie anatomique le point appartient.\n\ndolphin &lt;- read_csv(\"data/07_dolphin.csv\")\n\nRows: 12969 Columns: 4\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (1): anatomy\ndbl (3): x, y, z\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndolphin |&gt; sample_n(5)\n\n# A tibble: 5 Ã— 4\n        x      y       z anatomy   \n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     \n1  0.0762 -0.521 -0.0803 Head      \n2  0.472   1.00   1.28   Caudal fin\n3 -0.172  -0.361  0.0223 Head      \n4  0.0461 -0.571 -0.0907 Head      \n5  0.0152 -0.710 -0.125  Head      \n\n\nVoici en vue isomÃ©trique ce en quoi consiste ce nuage de points.\n\nlibrary(\"scatterplot3d\")\nscatterplot3d(x = dolphin$x, y = dolphin$y, z = dolphin$z, pch = 16, cex.symbols = 0.2)\n\n\n\n\n\n\n\nEffectuons lâ€™ACP sur le dauphin.\n\ndolph_pca &lt;- rda(dolphin |&gt; select(x, y, z), scale = FALSE)\nbiplot(dolph_pca, scaling = 2)\n\n\n\n\n\n\n\nOn nâ€™y voit pas grand chose, mais si lâ€™on extrait les scores et que lâ€™on raccourcit les vecteurs:\n\ndolph_scores &lt;- vegan::scores(dolph_pca, display = \"sites\")\ndolph_loads &lt;- vegan::scores(dolph_pca, display = \"species\")\ndolph_loads\n\n          PC1         PC2\nx -0.02990131  0.01608095\ny -7.13731672 -1.43221776\nz -4.56612084  2.23859843\nattr(,\"const\")\n[1] 9.089026\n\nplot(dolph_scores, pch = 16, cex = 0.24, asp = 1, col = factor(dolphin$anatomy))\nsegments(x0 = rep(0, 3), y0 = rep(0, 3),\n         x1 = dolph_loads[, 1]/50,\n         y1 = dolph_loads[, 2]/50,\n         col = \"chocolate\", lwd = 4)\n\n\n\n\n\n\n\nLa meilleure reprÃ©sentation du dauphin en 2D, selon la variance, est son profil - en effet, il est plus long et haut que large.\n\nNote. Une ACP effectue seulement une rotation des points. Les distances euclidiennes entre les points sont maintenues.\n\n\nNote. Lâ€™ACP a Ã©tÃ© conÃ§ue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales (ce nâ€™est Ã©videmment pas le cas du dauphin).\n\n\nNote. Les axes principaux dâ€™une ACP sont des variables alÃ©atoires. Elles peuvent Ãªtre assujetties Ã  des tests ststistiques, des modÃ¨les, du partitionnement de donnÃ©es, etc.\n\n\nExcercice. Effectuez maintenant une ACP avec les donnÃ©es dâ€™iris.\n\n\n10.4.1.2 Analyse de correspondance (AC)\nLâ€™analyse de correspondance (AC) est particuliÃ¨rement appropriÃ©e pour traiter des donnÃ©es dâ€™abondance et dâ€™occurrence. Tout comme lâ€™analyse en composantes principales, les donnÃ©es apportÃ©s vers une AC doivent Ãªtre dimensionnellement homogÃ¨nes, câ€™est-Ã -dire que chaque variable doit Ãªtre de mÃªme mÃ©trique: pour des donnÃ©es dâ€™abondance, cela signifie que les dÃ©comptes rÃ©fÃ¨rent tous au mÃªme concept: individus, colonies, surfaces occupÃ©es, etc. Alors que la distance euclidienne est prÃ©servÃ©e avec lâ€™ACP, lâ€™AC prÃ©serve la distance du \\(\\chi^2\\), qui est insensible aux double-zÃ©ros.\nLâ€™AC produit \\(min(n,p)-1\\) axes principaux orthogonaux qui captent non pas le maximum de variance, mais la proportion de mesures aux carrÃ© par rapport Ã  la somme des carrÃ©s de la matrice. Le biplot obtenu peut Ãªtre prÃ©sentÃ© sous forme de biplot de site (scaling 1), oÃ¹ la distance du \\(\\chi^2\\) est prÃ©servÃ©e entre les sites ou biplot dâ€™espÃ¨ces (scaling 2), ou la distance du \\(\\chi^2\\) est prÃ©servÃ©e entre les espÃ¨ces. Lâ€™AC hÃ©rite du coup une propriÃ©tÃ© importate de la distance du \\(\\chi^2\\), qui accorde davantage de distance entre un compte de 0 et de 1 quâ€™entre 1 et 2, et davantage entre 1 et 2 quâ€™entre 2 et 3.\nPar exemple, sur ces trois sites, on a comptÃ© un individu A de moins que dâ€™individu B.\n\nabundance_0123 = tibble(Site = c(\"Site 1\", \"Site 2\", \"Site 3\"),\n                        A = c(0, 1, 9),\n                        B = c(1, 2, 10))\nabundance_0123\n\n# A tibble: 3 Ã— 3\n  Site       A     B\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Site 1     0     1\n2 Site 2     1     2\n3 Site 3     9    10\n\n\nPourtant, la distance du \\(\\chi^2\\) est plus Ã©levÃ©e entre le site 1 et le site 2 quâ€™entre le site 2 et le site 3.\n\ndist(decostand(abundance_0123 |&gt; select(-Site), method=\"chi.square\"))\n\n          1         2\n2 0.6724111          \n3 0.9555316 0.2831205\n\n\nLa distance du \\(\\chi^2\\) donne davantage dâ€™importance aux espÃ¨ces rares, ce dont une analyse doit tenir compte. Il pourrait Ãªtre envisageable de retirer dâ€™un tableau des espÃ¨ces rare, ou bien prÃ©transformer des donnÃ©es dâ€™abondance par une transformation de chord ou de Hellinger (tel que discutÃ© au chapitre 6), puis procÃ©der Ã  une ACP sur ces donnÃ©es (Legendre et Gallagher, 2001).\n\n10.4.1.2.1 Application\nLe tableau varespec comprend des donnÃ©es de surface de couverture de 44 espÃ¨ces de plantes en lien avec les donnÃ©es environnementales du tableau varechem. Ces donnÃ©es ont Ã©tÃ© publiÃ©es dans VÃ¤re et al.Â (1995) et exportÃ©es du module vegan.\n\ndata(\"varespec\")\nvarespec |&gt;sample_n(5)\n\n   Callvulg Empenigr Rhodtome Vaccmyrt Vaccviti Pinusylv Descflex Betupube\n27     0.00    15.13     2.42     5.92    15.97     0.00     3.70        0\n6      0.30     5.75     0.00     0.00    10.50     0.10     0.00        0\n7      0.00     5.30     0.00     0.00     8.20     0.00     0.05        0\n5      0.00     0.13     0.00     0.00     2.75     0.03     0.00        0\n2      0.05     9.30     0.00     0.00     8.50     0.03     0.00        0\n   Vacculig Diphcomp Dicrsp Dicrfusc Dicrpoly Hylosple Pleuschr Polypili\n27     1.12     0.00      0     3.63     0.00      6.7    58.07     0.00\n6      0.00     0.00      0     0.85     0.00      0.0     0.05     0.03\n7      8.10     0.28      0     0.45     0.03      0.0     0.10     0.00\n5      0.00     0.00      0     0.25     0.03      0.0     0.03     0.18\n2      0.00     0.00      0     0.03     0.00      0.0     0.75     0.00\n   Polyjuni Polycomm Pohlnuta Ptilcili Barbhatc Cladarbu Cladrang Cladstel\n27     0.00     0.13     0.02     0.08     0.08     1.42     7.63     2.55\n6      0.08     0.00     0.00     0.08     0.00    39.00    37.50    11.30\n7      0.25     0.00     0.03     0.00     0.00    35.00    42.50     0.28\n5      0.65     0.00     0.00     0.00     0.00    18.50    59.00     0.98\n2      0.03     0.00     0.00     0.03     0.00     0.48    24.50    75.00\n   Cladunci Cladcocc Cladcorn Cladgrac Cladfimb Cladcris Cladchlo Cladbotr\n27     0.15     0.00     0.38     0.12     0.10     0.03     0.00     0.02\n6      3.45     0.18     0.20     0.25     0.25     0.23     0.03     0.00\n7      0.35     0.08     0.20     0.25     0.18     0.13     0.08     0.00\n5      0.28     0.23     0.23     0.23     0.10     0.05     0.00     0.00\n2      0.20     0.00     0.03     0.03     0.05     0.03     0.03     0.00\n   Cladamau Cladsp Cetreric Cetrisla Flavniva Nepharct Stersp Peltapht Icmaeric\n27     0.00   0.02     0.00        0     0.00      0.0   0.00     0.07     0.00\n6      0.00   0.03     0.35        0     0.08      0.0   0.03     0.00     0.00\n7      0.00   0.00     0.05        0     0.23      0.2   0.93     0.00     0.03\n5      0.03   0.00     0.18        0     0.28      0.0  10.28     0.00     0.10\n2      0.00   0.00     0.00        0     0.00      0.0   0.00     0.00     0.00\n   Cladcerv Claddefo Cladphyl\n27     0.00     0.15        0\n6      0.00     0.28        0\n7      0.00     0.10        0\n5      0.00     0.25        0\n2      0.03     0.03        0\n\n\nPour effectuer lâ€™AC, nous utiliserons, comme pour lâ€™ACP, le module vegan mais cette fois-ci avec la fonction cca. Lâ€™AC en scaling 1 est effectuÃ©e sur le tableau des abondances avec les espÃ¨ces comme colonnes et les sites comme lignes. Les matrices dâ€™abondance transposÃ©es indique les sites oÃ¹ chaque espÃ¨ce ont Ã©tÃ© dÃ©nombrÃ©es: pour une analyse en scaling 2, on effectue une analyse de correspondance sur la matrice dâ€™abondance (ou dâ€™occurrence) transposÃ©e.\nPour chacune des AC, je filtre pour mâ€™assurer que toutes les lignes contiennent au moins une observation. Ce nâ€™est pas nÃ©cessaire dans notre cas, mais je le laisse pour lâ€™exemple.\n\nvare_cca &lt;- cca(varespec %&gt;% filter(rowSums(.) &gt; 0))\nsummary(vare_cca, scaling = 1)\n\n\nCall:\ncca(X = varespec %&gt;% filter(rowSums(.) &gt; 0)) \n\nPartitioning of scaled Chi-square:\n              Inertia Proportion\nTotal           2.083          1\nUnconstrained   2.083          1\n\nEigenvalues, and their contribution to the scaled Chi-square \n\nImportance of components:\n                         CA1    CA2    CA3     CA4     CA5     CA6     CA7\nEigenvalue            0.5249 0.3568 0.2344 0.19546 0.17762 0.12156 0.11549\nProportion Explained  0.2520 0.1713 0.1125 0.09383 0.08526 0.05835 0.05544\nCumulative Proportion 0.2520 0.4233 0.5358 0.62962 0.71489 0.77324 0.82868\n                          CA8     CA9    CA10    CA11    CA12    CA13     CA14\nEigenvalue            0.08894 0.07318 0.05752 0.04434 0.02546 0.01710 0.014896\nProportion Explained  0.04269 0.03513 0.02761 0.02129 0.01222 0.00821 0.007151\nCumulative Proportion 0.87137 0.90650 0.93411 0.95539 0.96762 0.97583 0.982978\n                          CA15     CA16     CA17     CA18     CA19      CA20\nEigenvalue            0.010160 0.007830 0.006032 0.004008 0.002865 0.0019275\nProportion Explained  0.004877 0.003759 0.002896 0.001924 0.001375 0.0009253\nCumulative Proportion 0.987855 0.991614 0.994510 0.996434 0.997809 0.9987341\n                           CA21      CA22      CA23\nEigenvalue            0.0018074 0.0005864 0.0002434\nProportion Explained  0.0008676 0.0002815 0.0001168\nCumulative Proportion 0.9996017 0.9998832 1.0000000\n\nScaling 1 for species and site scores\n* Sites are scaled proportional to eigenvalues\n* Species are unscaled: weighted dispersion equal on all dimensions\n\n\nSpecies scores\n\n                CA1       CA2      CA3       CA4        CA5       CA6\nCallvulg  0.0303167 -1.597460  0.11455 -2.894569  0.1376073  2.291129\nEmpenigr  0.0751030  0.379305  0.39303  0.023675  0.8568729 -0.400964\nRhodtome  1.1052309  1.499299  3.04284  0.120106  3.2324306 -0.283510\nVaccmyrt  1.4614812  1.622935  2.72375  0.231688  0.4604556  0.712538\nVaccviti  0.1468014  0.313436  0.14696  0.243505  0.6868371 -0.147815\nPinusylv -0.4820096  0.588517 -0.36020 -0.127094  0.4064754  0.386604\nDescflex  1.5348239  1.218806  1.87562 -0.001340 -1.3136979 -0.070731\nBetupube  0.6694503  1.951826  3.84017  1.389423  7.5959115 -0.244478\nVacculig -0.0830789 -1.629259  1.05063  0.802648 -0.3058811 -1.625341\nDiphcomp -0.5446464 -1.037570  0.52282  0.940275  0.3682126 -1.082929\nDicrsp    1.8120408  0.360290 -4.92082  3.088562  1.3867372  0.157815\nDicrfusc  1.2704743 -0.562978 -0.39718 -2.929542  0.3848272 -2.408710\nDicrpoly  0.7248118  1.409347  0.80341  1.915549  4.5674148  1.295447\nHylosple  2.0062408  1.743883  2.27549  0.928884 -3.7648428  2.254851\nPleuschr  1.3102086  0.583036 -0.01004  0.137298 -1.1216144  0.200422\nPolypili -0.3805097 -1.243904  0.54593  1.477188 -0.7276341 -0.387641\nPolyjuni  1.0133795  0.099043 -2.24697  1.510641  0.7729714 -3.062378\nPolycomm  0.8468241  1.321773  1.13585  1.140723  2.6836594 -0.605038\nPohlnuta -0.0136453  0.589290 -0.35542  0.135481  0.9369707  0.397246\nPtilcili  0.4223631  1.598584  3.43474  1.400065  6.3209491  0.198935\nBarbhatc  0.5018348  2.119334  4.57303  1.693188  8.1101807  0.645995\nCladarbu -0.1531729 -1.483884  0.20024  0.193680  0.0734141  0.358926\nCladrang -0.5502561 -1.084008  0.40552  0.724060 -0.3357992 -0.335924\nCladstel -1.4373146  1.077753 -0.44397 -0.375926 -0.2421525  0.004212\nCladunci  0.8151727 -1.006186 -1.82587 -1.389523  1.6046713  3.675908\nCladcocc -0.2133215 -0.584429 -0.21434 -0.567886 -0.0003788 -0.145303\nCladcorn  0.2631227 -0.177858 -0.44464  0.272422  0.3992282 -0.306738\nCladgrac  0.1956947 -0.311167 -0.23894  0.379013  0.4933026  0.037581\nCladfimb  0.0009213 -0.161418  0.18463 -0.435908  0.4831233 -0.143751\nCladcris  0.3373031 -0.470369 -0.05093 -0.823855  0.7182250  0.636140\nCladchlo -0.6200021  1.207278  0.21889  0.426447  1.9506082  0.120722\nCladbotr  0.5647242  1.047333  2.65330  0.907734  4.4946805  1.201655\nCladamau -0.6598144 -1.512880  0.83251  1.577699 -0.0407227 -1.419139\nCladsp   -0.8209003  0.476164 -0.49752 -0.998241 -0.2393208  0.390785\nCetreric  0.2458192 -0.689228 -1.68427 -0.131681  0.7439412  2.374535\nCetrisla -0.3465221  1.362693  0.85897  0.396752  2.7526968  0.396591\nFlavniva -1.4391907 -0.833589 -0.12919  0.007071 -1.4841375  2.956977\nNepharct  1.6813309  0.199484 -4.33509  2.229917  0.9561223 -5.472858\nStersp   -0.5172793 -2.280900  0.99775  2.377013 -0.8892757 -1.441228\nPeltapht  0.4035858 -0.043265  0.04538  0.711040  0.1824679 -0.841227\nIcmaeric  0.0378754 -2.419595  0.72135  0.361302 -0.3736424 -2.092136\nCladcerv -0.9232858 -0.005233 -1.22058  0.305290 -0.8142627  0.414135\nCladdefo  0.5190399 -0.496632 -0.15271 -0.695927  0.9042143  0.909191\nCladphyl -1.2836161  1.155872 -0.79912 -0.741170 -0.1608002  0.490526\n\n\nSite scores (weighted averages of species scores)\n\n         CA1      CA2       CA3      CA4        CA5      CA6\n18 -0.108122 -0.53705  0.229574  0.24412  0.1405624 -0.14253\n15  0.697118 -0.14441 -0.031788 -0.21743 -0.2738522 -0.08146\n24  0.987603  0.15042 -1.348447  0.80472  0.3095168  0.46773\n27  0.851765  0.49901  0.443559  0.12277 -0.4814871  0.07589\n23  0.359881 -0.05608  0.145813  0.15087  0.2405263 -0.17770\n19  0.003545  0.37017  0.027760  0.06168 -0.1158930 -0.03413\n22  0.860732 -0.11504  0.110869 -1.02169  0.0772348 -0.60530\n16  0.636936 -0.33250  0.001120 -0.79797  0.0130769 -0.54049\n28  1.279352  0.81557  0.670053  0.23137 -0.8929976  0.41783\n13 -0.195009 -0.80564  0.117686 -0.58286 -0.0007212  0.53071\n14  0.528532 -0.70420 -0.517771 -0.86836  0.5713441  0.91671\n20  0.382866 -0.18686 -0.004789  0.10156  0.0458125  0.21087\n25  0.990715  0.11967 -1.110040  0.44929  0.1885902 -0.70694\n7  -0.264704 -1.06013  0.334900  0.45973 -0.0326631 -0.19945\n5  -0.428410 -1.20765  0.374344  0.74970 -0.2596294 -0.30467\n6  -0.330534 -0.77498  0.130760  0.22391  0.0632686  0.09060\n3  -0.899601  0.12075 -0.075742  0.03842 -0.1489585 -0.12031\n4  -0.770294 -0.35351 -0.033779 -0.01795 -0.3007839  0.44303\n2  -0.992193  0.50319 -0.157505 -0.07070 -0.1065172 -0.09928\n9  -0.937173  0.78688 -0.258119 -0.19377 -0.0343535 -0.01259\n12 -0.726413  0.49163 -0.157235 -0.08698 -0.0105774 -0.02801\n10 -1.002083  0.71239 -0.236526 -0.18643 -0.0231666 -0.04928\n11 -0.322647 -0.03871 -0.001297  0.09029 -0.1481448  0.06934\n21  0.259527  0.80746  1.124258  0.36083  1.5437866  0.07051\n\n\n\nvarespec_eigenval &lt;- eigenvals(vare_cca, scaling = 1)\n\nprop_expl &lt;- varespec_eigenval / sum(varespec_eigenval)\n\npar(mfrow = c(1, 2))\nplot(x = 1:length(varespec_eigenval),\n     y = vare_cca$CA$eig,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\")\nabline(h = mean(varespec_eigenval), col = \"red\", lty = 2)\n\nplot(x = 1:length(varespec_eigenval),\n     y = prop_expl,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\")\nlines(x = 1:length(varespec_eigenval),\n      y = broken_stick(length(varespec_eigenval)),\n      col = \"red\",\n      lty = 2)\n\n\n\n\n\n\n\nCrÃ©ons les biplots.\n\npar(mfrow = c(1, 2))\nplot(vare_cca, scaling = 1, main = \"Biplot des espÃ¨ces\")\nplot(vare_cca, scaling = 2, main = \"Biplot des sites\")\n\n\n\n\n\n\n\nLe biplot des espÃ¨ces, Ã  gauche (scaling = 1), montre la distribution des sites selon les espÃ¨ces. Les emplacements des scores (en noir) montrent les contrastes entre sites selon les espÃ¨ces qui les recouvrent. Les sites 14 et 15, par exemple, contrastent les sites 19, 20, 21 et 22 selon le 2iÃ¨me axe principal. Par ailleurs, les axes principaux sont formÃ© de plusieurs espÃ¨ces dont aucune ne domine clairement.\nLe biplot des sites, Ã  droite (scaling = 2), montre la distribution des recouvrements dâ€™espÃ¨ces selon les sites. Par exemple, les espÃ¨ces Betupube (Betula pubescens) et Barbhatc (Barbilophozia hatcheri ) se recouvrent en particulier le site 24. Le site 1 est difficile Ã  identifier, car il est couvert par plusieurs noms dâ€™espÃ¨ces, au bas au centre. Les sites 3 et 13 se confondent avec Dicrsp (une espÃ¨ce de Dicranum) qui le recouvre amplement.\nPour les deux types de biplot, les sites oÃ¹ les espÃ¨ces situÃ©s prÃ¨s de lâ€™origine, car ils peuvent Ãªtre soit prÃ¨s de la moyenne, soit distribuÃ©s uniformÃ©ment.\nLe nombre de composantes Ã  retenir peut Ãªtre Ã©valuÃ© par les approches Kaiser-Guttmann et broken-stick.\n\nscaling &lt;- 1\nvarespec_eigenval &lt;- eigenvals(vare_cca, scaling = scaling) # peut Ãªtre effectuÃ© sur les deux types de scaling\n\nprop_expl &lt;- varespec_eigenval / sum(varespec_eigenval)\n\npar(mfrow = c(1, 2))\nplot(x = 1:length(varespec_eigenval),\n     y = vare_cca$CA$eig,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\",\n     main = paste(\"Eigenvalue - Kaiser-Guttmann, scaling =\", scaling))\nabline(h = mean(varespec_eigenval), col = \"red\", lty = 2)\n\nplot(x = 1:length(varespec_eigenval),\n     y = prop_expl,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\",\n     main = paste(\"Proportion - broken stick, scaling =\", scaling))\nlines(x = 1:length(varespec_eigenval),\n      y = broken_stick(length(varespec_eigenval)),\n      col = \"red\",\n      lty = 2)\n\n\n\n\n\n\n\nPour les deux scalings, lâ€™approche Kaiser-Guttmann propose 7 axes, tandis que lâ€™approche broken-stick en propose 5.\nLes reprÃ©sentations biplot dâ€™analyse de correspondance peuvent prendre la forme dâ€™un boomerang, en particulier celles qui sont basÃ©es sur des donnÃ©es dâ€™occurrence. Le tableau suivant initialement de Chessel et al.Â (1987) et est distribuÃ© dans le module ade4.\n\nlibrary(\"ade4\")\ndata(\"doubs\")\nfish &lt;- doubs$fish\ndoubs_cca &lt;- cca(fish %&gt;% filter(rowSums(.) &gt; 0))\nplot(doubs_cca, scaling = 2)\n\n\n\n\n\n\n\nLes numÃ©ros de sites correspondent Ã  la position dans une riviÃ¨re, 1 Ã©tant en amont et 30 en aval. Le premier axe discrimine lâ€™amont et lâ€™aval, tandis que le deuxiÃ¨me montre deux niches en amont. Bien que lâ€™on observe une discontinuitÃ© dans le cours dâ€™eau, il y a une continuitÃ© dans les abondances. Cet effet peut Ãªtre corrigÃ© en retirant la tendance de lâ€™analyse de correspondance par une detrended correspondance analysis. Pour cela, il faudra utiliser la fonction decorana, ce qui ne sera pas couvert ici.\nLâ€™analyse des correspondances multiples (ACM) est utile pour lâ€™ordination des donnÃ©es catÃ©gorielles. Le module ade4 est en mesure dâ€™effectuer des AMC, mais nâ€™est pas couvert dans ce manuel.\n\nExcercice. Effectuez et analysez une AC avec les donnÃ©es de recouvrement varespec.\n\n\n10.4.1.3 Positionnement multidimensionnel (PoMd)\nLe positionnement multidimensionnel (PoMd), ou manifold analysis, se base sur les associations entre les objets (mode Q) ou les variables (mode R) pour en rÃ©duire les dimensions. Alors que lâ€™analyse en composantes principales conserve la distance euclidienne et que lâ€™analyse de correspondance conserve la distance du \\(\\chi^2\\), le PoMd conserve lâ€™association que vous sÃ©lectionnerez Ã  votre convenance. Le PoMd vise Ã  reprÃ©senter en un nombre limitÃ© de dimensions (souvent 2) la distance (ou dissimilaritÃ©) quâ€™ont les objets (ou des variables) les uns par rapport aux autres dans lâ€™espace multidimensionnel.\nIl existe deux types dâ€™AEM. Le PoMd-mÃ©trique (metric multidimentional scaling MMDS, parfois le metric est retirÃ©, MDS, et parfois lâ€™on parle de classic MDS) vise Ã  reprÃ©senter fidÃ¨lement la distance entre les objets ou les variables. Le PoMd-mÃ©trique ne devrait Ãªtre utilisÃ©e que lorsque la mÃ©trique nâ€™est ni euclidienne, ni de \\(\\chi^2\\) et que lâ€™on dÃ©sire prÃ©server les distances entre les objets. Lâ€™PoMd-mÃ©trique aussi appelÃ©e analyse en coordonnÃ©es principales (ACoP ou de lâ€™anglais PCoA) .\nLe PoMd-non-mÃ©trique (nonmetric multidimentional scaling, NMDS) vise quant Ã  lui Ã  reprÃ©senter lâ€™ordre des distances entre les objets ou les variables. Câ€™est une approche par rang: le PoMd-non-mÃ©trique vise reprÃ©senter les objets sont plus proches ou plus Ã©loignÃ©es les uns des autres plutÃ´t que de reprÃ©senter leur similaritÃ© dans lâ€™espace multidimentionnelle.\nLâ€™IsoMap, pour isometric feature mapping, est une extension du PoMd qui reconstruit les distances selon les points retrouvÃ©s dans le voisinage. Les isomaps sont en mesure dâ€™aplatir des donnÃ©es ayant des formes complexes.\nNous ne traitons pour lâ€™instant que de lâ€™PoMd-mÃ©trique (fonction vegan::cmdscale) et des PoMd-non-mÃ©trique (fonction vegan::metaMDS).\n\n10.4.1.3.1 Application\nUtilisons les donnÃ©es dâ€™abondance que nous avions au tout dÃ©but de ce chapitre. La matrice dâ€™association de Bray-Curtis sera utilisÃ©e.\n\nassoc_mat &lt;- vegdist(abundance, method = \"bray\")\npheatmap(assoc_mat |&gt; as.matrix(), cluster_rows = FALSE, cluster_cols = FALSE,\n         display_numbers = round(assoc_mat |&gt; as.matrix(), 2))\n\n\n\n\n\n\n\nLes sites 2 et 3 devraient Ãªtre plus prÃ¨s lâ€™un et lâ€™autre, puis les sites 3 et 4. Les autres associations sont Ã©loignÃ©s dâ€™environ la mÃªme distance. LanÃ§ons le calcul de la PoMd-mÃ©trique.\n\npcoa &lt;- cmdscale(assoc_mat, k = nrow(abundance)-1, eig = TRUE)\nspec_scores &lt;- wascores(pcoa$points, abundance)\nordiplot(vegan::scores(pcoa), type = 't', cex = 1.5)\n\nspecies scores not available\n\ntext(spec_scores, row.names(spec_scores), col = \"red\", cex = 0.75)\n\n\n\n\n\n\n\nOn observe en effet que les sites 2 et 3 sont les plus prÃ¨s. Les sites 3 et 4sont plus Ã©loignÃ©s. Les sites 1, 2 et 4 font Ã  peu prÃ¨s un triangle Ã©quilatÃ©ral, ce qui correspond Ã  ce Ã  quoi on devrait sâ€™attendre. Les wa-scores permettent de juxtaposer les espÃ¨ces sur les sites, pour rÃ©fÃ©rence. Le colibri nâ€™est prÃ©sent que sur le site 2. Le site 1 est populÃ© par des jaseurs et des mÃ©sanges, et câ€™est le seul site oÃ¹ lâ€™on a observÃ© une citelle. On a observÃ© des chardonnerets sur les sites 2 et 3. Sur le site 4, on nâ€™a observÃ© que des bruants, que lâ€™on a aussi observÃ© ailleurs, sauf au site 2.\nLe PoMd-non-mÃ©trique (non metric dimensional scaling, NMDS) fonctionne de la mÃªme maniÃ¨re que la PoMd-mÃ©trique, Ã  la diffÃ©rence que la distance est basÃ©e sur les rangs. Ã€ cet Ã©gard, le site 4 Ã  une distance de 0.76 du site 3, mais plutÃ´t le deuxiÃ¨me plus loin, aprÃ¨s le site 2 et avant le site 1. Utilisons la fonction metaMDS.\n\nnmds &lt;- metaMDS(assoc_mat, k = nrow(abundance)-1, eig = TRUE)\n\nRun 0 stress 0 \nRun 1 stress 0 \n... Procrustes: rmse 0.1216366  max resid 0.1634103 \nRun 2 stress 0 \n... Procrustes: rmse 0.1403208  max resid 0.1888214 \nRun 3 stress 0 \n... Procrustes: rmse 0.115492  max resid 0.1523862 \nRun 4 stress 0 \n... Procrustes: rmse 0.1551062  max resid 0.202592 \nRun 5 stress 0 \n... Procrustes: rmse 0.09908794  max resid 0.1384734 \nRun 6 stress 0 \n... Procrustes: rmse 0.0987008  max resid 0.1454209 \nRun 7 stress 0 \n... Procrustes: rmse 0.1164685  max resid 0.1501751 \nRun 8 stress 0 \n... Procrustes: rmse 0.0831961  max resid 0.09943451 \nRun 9 stress 0 \n... Procrustes: rmse 0.1429865  max resid 0.1921223 \nRun 10 stress 0 \n... Procrustes: rmse 0.09337227  max resid 0.1214715 \nRun 11 stress 0 \n... Procrustes: rmse 0.06970139  max resid 0.1001732 \nRun 12 stress 0 \n... Procrustes: rmse 0.08842632  max resid 0.1314237 \nRun 13 stress 0 \n... Procrustes: rmse 0.08200456  max resid 0.1163193 \nRun 14 stress 0 \n... Procrustes: rmse 0.09483784  max resid 0.1239188 \nRun 15 stress 0 \n... Procrustes: rmse 0.1513612  max resid 0.1946446 \nRun 16 stress 0 \n... Procrustes: rmse 0.08664967  max resid 0.1141698 \nRun 17 stress 0 \n... Procrustes: rmse 0.1179912  max resid 0.1610405 \nRun 18 stress 0 \n... Procrustes: rmse 0.1099474  max resid 0.1455303 \nRun 19 stress 0 \n... Procrustes: rmse 0.1063117  max resid 0.1398709 \nRun 20 stress 0 \n... Procrustes: rmse 0.1009788  max resid 0.1310601 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n    20: stress &lt; smin\n\n\nWarning in metaMDS(assoc_mat, k = nrow(abundance) - 1, eig = TRUE): stress is\n(nearly) zero: you may have insufficient data\n\n\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n\nspec_scores &lt;- wascores(nmds$points, abundance)\nordiplot(vegan::scores(nmds), type = 't', cex = 1.5)\n\nspecies scores not available\n\ntext(spec_scores, row.names(spec_scores), col = \"red\", cex = 0.75)\n\n\n\n\n\n\n\nDans ce cas, entre PoMd-mÃ©trique et non-mÃ©trique, les rÃ©sultats peuvent Ãªtre interprÃ©tÃ©s de maniÃ¨re similaire.\nEn ce qui a trait au dauphin,\n\n\n\n\n\n\n\n\nPour plus de dÃ©tails, je vous invite Ã  vous rÃ©fÃ©rer Ã  Borcard et al.Â (2011)) ou de consulter lâ€™excellent site GUSTA ME.\n\n10.4.1.4 Conclusion sur lâ€™ordination non contraignante\nLorsque les donnÃ©es sont euclidiennes, lâ€™analyse en composantes principales (ACP) devrait Ãªtre utilisÃ©e. Lorsque la mÃ©trique est celle du \\(\\chi^2\\), on prÃ©fÃ©rera lâ€™analyse de correspondance (AC). Si la mÃ©trique est autre, le positionnement multidimensionel (PoMd) est prÃ©fÃ©rable. Dans ce dernier cas, si lâ€™on recherche une reprÃ©sentation simplifiÃ©e de la distance entre les objets ou variables, on utilisera un PoMd-mÃ©trique. Ã€ lâ€™inverse, si lâ€™on dÃ©sire une reprÃ©sentation plus fidÃ¨le au rang des distances, on prÃ©fÃ©rera lâ€™PoMd-non-mÃ©trique.\n\n10.4.2 Ordination contraignante\nAlors que lâ€™ordination non contraignante vous permet de dresser un portrait de vos variables, lâ€™ordination contraignante (ou canonique) permet de tester statistiquement ainsi que de reprÃ©senter la relation entre plusieurs variables explicatives (par exemple, des conditions environnementales) et une ou plusieurs variables rÃ©ponses (par exemple, les espÃ¨ces observÃ©es).\n\nLâ€™analyse discriminante nâ€™a fondamentalement quâ€™une seulement variable rÃ©ponse, et celle-ci doit dÃ©crire lâ€™appartenance Ã  une catÃ©gorie.\nLâ€™analyse de redondance sera prÃ©fÃ©rÃ©e lorsque le nombre de variable est plus restreint (variables ionomiques et indicateurs de performance des cultures). Les dÃ©tails, ainsi que les tenants et aboutissants de ces mÃ©thodes, sont prÃ©sentÃ©s dans Numerical Ecology (Legendre et Legendre, 2012).\nLâ€™analyse canonique des corrÃ©lations sera prÃ©fÃ©rÃ©e lorsque les variables sont parsemÃ©es (beaucoup de colonnes avec beaucoup de zÃ©ros, comme les variables dâ€™abondance).\n\n\n10.4.2.1 Analyse discriminante\nAlors que lâ€™analyse en composante principale vise Ã  prÃ©senter la perspective (les axes) selon laquelle les points sont les plus Ã©clatÃ©es, lâ€™analyse discriminante, le plus souvent utilisÃ© dans sa forme linÃ©aire (ADL) et quadratique (ADQ), vise Ã  prÃ©senter la perspective selon laquelle les groupes sont les plus Ã©clatÃ©s, les groupes formant la variable contraignante. Ces groupes peuvent Ãªtre connus (e.g.Â cultivar, rÃ©gion gÃ©ographique) ou attribuÃ©s (exemple: par partitionnement). Lâ€™ADL est parfois nommÃ©e analyse canonique de la variance.\nLâ€™AD vise Ã  reprÃ©senter des diffÃ©rences entre des groupes aux moyens de combinaisons linÃ©aires (ADL) ou quadratique (ADQ) de variables mesurÃ©es. Sa reprÃ©sentation sous forme de biplot permet dâ€™apprÃ©cier les diffÃ©rences entre les groupes dâ€™identifier les variables qui sont responsables de la discrimination.\n\n\n\n\n\nBiplot de distance de lâ€™analyse discriminante des ionomes dâ€™espÃ¨ces de plantes Ã  fruits cultivÃ©es sauvages et domestiquÃ©es, Source: Parent et al.Â (2013)\n\nLâ€™ADL a Ã©tÃ© dÃ©veloppÃ©e par Fisher (1936), qui Ã  titre dâ€™exemple dâ€™application a utilisÃ© un jeu de donnÃ©es de dimensions dâ€™iris collectÃ©es par Edgar Anderson, du Jardin botanique du Missouri, sur 150 spÃ©cimens dâ€™iris collectÃ©s en GaspÃ©sie (Est du QuÃ©bec), ma rÃ©gion natale (suis-je assez chauvin?). Ce jeu de donnÃ©es est amplement utilisÃ© Ã  titre dâ€™exemple en analyse multivariÃ©e.\nWilliams (1983) a prÃ©sentÃ© les tenants et aboutissants de lâ€™ADL en Ã©cologie. Tout comme les donnÃ©es passant pas une ACP doivent suivre une distribution multinormale pour Ãªtre statistiquement valide, les distributions des groupes dans une ADL doivent Ãªtre multinormales et les variances des points par groupe doivent Ãªtre homogÃ¨nesâ€¦ ce qui est rarement le cas en science. NÃ©anmoins:\n\nHeureusement, il y a des Ã©vidences dans la littÃ©rature que certaines dâ€™entre [ces rÃ¨gles] peuvent Ãªtre transgressÃ©es modÃ©rÃ©ment sans de grands changement dans les taux de classification. Cette conclusion dÃ©pends, toutefois, de la sÃ©vÃ©ritÃ© des transgressions, et de facteurs structueaux comme la position relative des moyennes des populations et de la nature des dispersions. - Williams (1983)\n\nLâ€™ADL peut servir autant dâ€™outil dâ€™interprÃ©tation que dâ€™outil de classification, câ€™est Ã  dire de prÃ©dire une catÃ©gorie selon les variables (chapitreÂ 13). Dans les deux cas, lorsque le nombre de variables approchent le nombre dâ€™observation, les rÃ©sultats dâ€™une ADL risque dâ€™Ãªtre difficilement interprÃ©tables. Le test appropriÃ© pour Ã©valuer lâ€™homodÃ©nÃ©itÃ© de la covariance est le M-test de Box. Ce test est peu documentÃ© dans la littÃ©rature, est rarement utilisÃ© mais a la rÃ©putation dâ€™Ãªtre particuliÃ¨rement sÃ©vÃ¨re.\nIl est rare que des donnÃ©es Ã©cologiques aient des dispersions (covariances) homogÃ¨nes. Contrairement Ã  lâ€™ADL, lâ€™ADQ ne demande pas Ã  ce que les dispersions (covariances) soient homogÃ¨nes. NÃ©anmoins, lâ€™ADQ ne gÃ©nÃ¨re ni de scores, ni de loadings: il sâ€™agit dâ€™un outil pour prÃ©dire des catÃ©gories (classification), non pas dâ€™un outil dâ€™ordination.\n\n10.4.2.1.1 Application\nUtilisons les donnÃ©es dâ€™iris.\n\ndata(\"iris\")\n\nTestons la multinormalitÃ© par groupe. Rappelons-nous que pour considÃ©rer la distribution comme multinormale, la p-value de la distorsion ainsi que la statistique de Kurtosis doivent Ãªtre Ã©gale ou plus Ã©levÃ©e que 0.05. La fonction split sÃ©pare le tableau en listes et la fonction map applique la fonction spÃ©cifiÃ©e Ã  chaque Ã©lÃ©ment de la liste. Cela permet dâ€™effectuer des tests de multinormalitÃ© sur chacune des espÃ¨ces dâ€™iris.\n\niris %&gt;%\n  split(.$Species) %&gt;%\n  map(~ mvn(.x %&gt;% select(-Species),\n            mvnTest = \"mardia\")$multivariateNormality)\n\n$setosa\n             Test        Statistic           p value Result\n1 Mardia Skewness 25.6643445196298 0.177185884467652    YES\n2 Mardia Kurtosis 1.29499223711605 0.195322907441935    YES\n3             MVN             &lt;NA&gt;              &lt;NA&gt;    YES\n\n$versicolor\n             Test         Statistic           p value Result\n1 Mardia Skewness  25.1850115362466 0.194444483140265    YES\n2 Mardia Kurtosis -0.57186635893429 0.567412516528727    YES\n3             MVN              &lt;NA&gt;              &lt;NA&gt;    YES\n\n$virginica\n             Test         Statistic           p value Result\n1 Mardia Skewness  26.2705981752915 0.157059707690356    YES\n2 Mardia Kurtosis 0.152614173978342 0.878702546726567    YES\n3             MVN              &lt;NA&gt;              &lt;NA&gt;    YES\n\n\nLe test est passÃ© pour toutes les espÃ¨ces. Voyons maintenant lâ€™homogÃ©nÃ©itÃ© de la covariance. Pour ce faire, nous aurons besoin de la fonction boxM, disponible avec le module biotools. Pour que les covariances soient considÃ©rÃ©es comme Ã©gales, la p-vaule doit Ãªtre supÃ©rieure Ã  0.05.\n\nlibrary(\"heplots\")\n\nLoading required package: broom\n\nboxM(iris |&gt; select(-Species),\n     group = iris$Species)\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  select(iris, -Species)\nChi-Sq (approx.) = 140.94, df = 20, p-value &lt; 2.2e-16\n\n\nOn est loin dâ€™un cas oÃ¹ les distributions sont homogÃ¨nes. Nous allons nÃ©anmoins procÃ©der Ã  lâ€™analyse discriminante avec le module ade4. Nous aurons dâ€™abord besoin dâ€™effectuer une ACP avec la fonction dudi.pca de ade4 (en spÃ©cifiant une mise Ã  lâ€™Ã©chelle), que nous projetterons en ADL avec discrimin.\n\nlibrary(\"ade4\")\niris_pca &lt;- dudi.pca(df = iris |&gt; select(-Species),\n                     scannf = FALSE, # ne pas gÃ©nÃ©rer de graphique\n                     scale = TRUE)\niris_lda &lt;- discrimin(dudi = iris_pca,\n                      fac = iris$Species,\n                      scannf = FALSE)\n\nLa visualisation peut Ãªtre effectuÃ©e directement sur lâ€™objet issu de la fonction discrimin.\n\nplot(iris_lda)\n\n\n\n\n\n\n\nIl sâ€™agit toutefois dâ€™une visualisation pour le diagnostic davantage que pour la publication. Si lâ€™objectif est la publication, vous pourriez utiliser la fonction plotDA que jâ€™ai conÃ§ue Ã  cet effet. Jâ€™ai aussi conÃ§u une fonction similaire qui utilise le module graphique de base de R.\n\nsource(\"https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R\")\nplotDA(scores = iris_lda$li,\n       loadings = iris_lda$fa,\n       fac = iris$Species,\n       level=0.95,\n       facname = \"Species\",\n       propLoadings = 1) \n\nLoading required package: ellipse\n\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:car':\n\n    ellipse\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\n\nLoading required package: grid\n\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nÃ€ la diffÃ©rence de lâ€™ACP, lâ€™ADL maximise la sÃ©paration des groupes. Nous avions notÃ© avec lâ€™ACP que les dimensions des pÃ©tales distinguaient les groupes. Puisque nous avions justement des informations sur les groupes, nous aurions pu procÃ©der directement Ã  un ADL pour obtenir des conclusions plus directes. Si la longueur des pÃ©tales permet de distinguer lâ€™espÃ¨ce setosa des deux autres, la largeur des pÃ©tales permet de distinguer virginica et versicolor, bien que les nuages de points se superposent. De maniÃ¨re bivariÃ©e, les rÃ©gions de confiance des moyennes des scores discriminants (petites ellipses) montrent des diffÃ©rence significatives au seuil 0.05.\n\nExcercice. Si lâ€™on effectuait lâ€™ADL sur notre dauphin, avec la colonne anatomy comme variable de regroupement, quâ€™obtiendrions-nous? Si lâ€™on considÃ¨re la nageoire codale (queue) comme faisant partie du corps? Quelles sont les limitations?\n\n\n10.4.2.2 Analyse de redondance (RDA)\nEn anglais, on la nomme redundancy analysis, souvent abrÃ©gÃ©e RDA. Elle est utilisÃ©e pour rÃ©sumer les relations linÃ©aires entre des variables rÃ©ponse et des variables explicatives. La â€œredondanceâ€ se situe dans lâ€™utilisation de deux tableaux de donnÃ©es contenant de lâ€™information concordante. Lâ€™analyse de redondance est une maniÃ¨re Ã©lÃ©gante dâ€™effectuer une rÃ©gression linÃ©aire multiple, oÃ¹ la matrice de valeurs prÃ©dites par la rÃ©gression est assujettie Ã  une analyse en composantes principales. Il est ainsi possible de superposer les scores des variables explicatives Ã  ceux des variables rÃ©ponse.\nPlus prÃ©cisÃ©ment, une RDA effectue les Ã©tapes suivantes (Borcard et al.Â (2011)) entre une matrice de variables indÃ©pendantes (explicatives) \\(X\\) et une matrice de variables dÃ©pendantes (rÃ©ponse) \\(Y\\).\n\n10.4.2.2.1 1. RÃ©gression entre \\(Y\\) et \\(X\\)\n\nPour chacune des variables rÃ©ponse de \\(Y\\) (\\(y_1\\), \\(y_2\\), , \\(y_j\\)), effectuer une rÃ©gression linÃ©aire sur les variables explicatives \\(X\\).\n\\[\\hat{y}_j = b_j + m_{1, j} \\times x_1 + m_{2, j} \\times x_2 + ... + m_{i, j} \\times x_i\\]\n\\[\\hat{y}_j = y_j + y_{res, j}\\]\nPour chaque observation (\\(n\\)), nous obtenons une sÃ©rie de valeurs de \\(\\hat{y}_j\\) et de \\(y_{res, j}\\). Donc chaque cellule de la matrice \\(Y\\) a ses pendant \\(\\hat{y}\\) et \\(y_{res}\\). Nous obtenons ainsi une matrice de prÃ©diction \\(\\hat{Y}\\) et une matrice des rÃ©sidus \\(Y_{res} = Y - \\hat{Y}\\).\n\n10.4.2.2.2 2. Analyse en composantes principales\nEnsuite, on effectue une analyse en composantes principales (ACP) sur la matrice des prÃ©dictions \\(\\hat{Y}\\). On obtient ainsi ses valeurs et vecteurs propres. Nommons \\(U\\) ses vecteurs propres. Les fonctions de RDA mettent souvent ces vecteurs Ã  lâ€™Ã©chelle avant de les retourner Ã  lâ€™utilisateur. En ordination Ã©cologique, ces vecteurs mis Ã  lâ€™Ã©chelle sont souvent appelÃ©s les scores des espÃ¨ces, bien quâ€™il ne sâ€™agisse pas nÃ©cessairement dâ€™espÃ¨ces, mais plus gÃ©nÃ©ralement des variables de la matrice dÃ©pendante \\(Y\\).\nIl est aussi possible dâ€™effectuer une ACP sur \\(Y_{res}\\).\n\n10.4.2.2.3 3. Calculer les scores\nLes vecteurs propres \\(U\\) sont utilisÃ©s pour calculer les scores des sites, \\(Y \\times U\\), ainsi que les contraintes de site \\(\\hat{Y} \\times U\\).\n\n10.4.2.2.4 Application\nNous allons utiliser la fonction rda du module vegan. En ce qui a trait aux donnÃ©es, utilisons les donnÃ©es varespec (matrice Y) et varechem (matrice X). La fonction rda peut fonctionner avec lâ€™interface-formule de R, oÃ¹ Ã  gauche du ~ on retrouve le Y (la matrice de la communautÃ© Ã©cologique, i.e.Â les abondances dâ€™espÃ¨ces) contre le X (l), Ã  gauche, ce qui peut Ãªtre pratique pour lâ€™analyse dâ€™interactions. Mais pour comparer deux matrices, nous pouvons dÃ©finir X et Y. Ce qui est mÃ©langeant, câ€™est que vegan, contrairement aux conventions, dÃ©fini X comme Ã©tant la matrice rÃ©ponse et Y comme Ã©tant la matrice explicative.\n\nvare_rda &lt;- rda(X = varespec, Y = vareclr, scale = FALSE)\npar(mfrow = c(1, 2))\nordiplot(vare_rda, scaling = 1, type = \"text\", main = \"Scaling 1: triplot de distance\")\nordiplot(vare_rda, scaling = 2, type = \"text\", main = \"Scaling 2: triplot de corrÃ©lation\")\n\n\n\n\n\n\n\nLa fonction ordiplot permet de crÃ©er un triplot de base. La reprÃ©sentation des wascores est rÃ©putÃ©e plus robuste (moins susceptible dâ€™Ãªtre bruitÃ©e), mais leur interprÃ©tation porte Ã  confusion (Borcard et al.Â (2011)).\nTriplot de distance (scaling 1). Les angles entre les variables explicatives reprÃ©sentent leur corrÃ©lation (non pas les variables rÃ©ponse).\nTriplot de corrÃ©lation (scaling 2). Les angles entre les variables reprÃ©sentent leurs corrÃ©lation, que les variables soient rÃ©ponse ou explicative, ou entre variables rÃ©ponses et variables explicatives. Les distances entre les objets sur le triplot ne sont pas des approximation de leur distance euclidienne.\nLes triplots montrent que les variables ont toutes un rÃ´le important sur la dispersion des sites autour des axes principaux. Le premier axe principal est composÃ© de maniÃ¨re plus marquÃ©e par le clr de lâ€™Al et celui du Fe. Le deuxiÃ¨me axe principal est composÃ© de maniÃ¨re plus marquÃ©e par le clr du S, du P et du K. Le triplot de corrÃ©lation ne prÃ©sente pas de tendance apprÃ©ciable pour la plupart des espÃ¨ces, qui ne possÃ¨dent pas de niche particuliÃ¨re. Toutefois, lâ€™espÃ¨ce Cladstel, prÃ©sente surtout dans les sites 9 et 10, est liÃ©e Ã  de basses teneurs en N et Ã  de faibles valeurs de Baresoil (sol nu). Lâ€™espÃ¨ce Pleuschr est liÃ©e Ã  des sols oÃ¹ lâ€™on retrouve une grande Ã©paisseur dâ€™humus, ainsi que des teneurs Ã©levÃ©es en nutriment K, P, S, Ca, Mg et Zn. Elle semble apprÃ©cier les sols Ã  bas pH, mais Ã  faible teneur en Fe et Al. La teneur en N lui semble plus indiffÃ©rente (son vecteur Ã©tant presque perpendiculaire).\nOn pourra personnaliser les graphiques en extrayant les scores.\n\nscaling &lt;- 2\nsites &lt;- vegan::scores(vare_rda, display = \"wa\", scaling = scaling)\nspecies &lt;- vegan::scores(vare_rda, display = \"species\", scaling = scaling)\nenv &lt;- vegan::scores(vare_rda, display = \"reg\", scaling = scaling)\n\nplot(0, 0, type = \"n\", xlim = c(-3, 5), ylim = c(-3, 4), asp = 1)\nabline(h=0, v = 0, col = \"grey80\")\ntext(sites/2, labels = rownames(sites), cex = 0.7, col = \"grey50\")\ntext(species/2, labels = rownames(species), col = \"green\", cex = 0.7)\nsegments(x0 = 0, y0 = 0, x1 = env[, 1], y1 = env[, 2], col = \"blue\")\ntext(env, labels = rownames(env), col = \"blue\", cex = 1)\n\n\n\n\n\n\n\nOn pourra effectuer une analyse de Kaiser-Guttmann ou de broken-stick de la mÃªme maniÃ¨re que prÃ©cÃ©demment. Ã‰tant une collection de rÃ©gressions, une RDA est en mesure dâ€™effectuer des tests statistiques sur les coefficients de la rÃ©gression en utilisant des permutations pour tester la signification des coefficients et des axes dâ€™une RDA. On doit nÃ©anmoins obligatoirement effectuer la RDA avec lâ€™interface formule. La variable Ã  gauche du ~ peut Ãªtre une matrice ou un tableau, et celui de droite est dÃ©fini dans data. Le . dans lâ€™interface formule signifie â€œune combinaison linÃ©aire de toutes les variables, sans interactionâ€.\n\nvare_rda &lt;- rda(varespec ~ ., data = vareclr, scale = FALSE)\nperm_test_term &lt;- anova(vare_rda, by = \"term\")\n#perm_test_axis &lt;- anova(vare_rda, by = \"axis\")\n\nLa signification des axes est difficile Ã  interprÃ©ter. Toutefois, celui des variables prÃ©sente un intÃ©rÃªt.\n\nperm_test_term\n\nPermutation test for rda under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nModel: rda(formula = varespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Fv + Baresoil + Humdepth + pH, data = vareclr, scale = FALSE)\n         Df Variance      F Pr(&gt;F)   \nN         1   216.13 4.8470  0.006 **\nP         1   272.71 6.1159  0.005 **\nK         1   194.97 4.3724  0.012 * \nCa        1    24.92 0.5589  0.661   \nMg        1    52.61 1.1799  0.310   \nS         1   100.07 2.2441  0.089 . \nAl        1   177.91 3.9900  0.008 **\nFe        1   118.59 2.6595  0.050 * \nMn        1    25.96 0.5822  0.638   \nZn        1    35.81 0.8030  0.483   \nMo        1    23.51 0.5273  0.678   \nBaresoil  1    98.64 2.2122  0.103   \nHumdepth  1    43.59 0.9777  0.393   \npH        1    38.93 0.8730  0.447   \nResidual  9   401.31                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa p-value est la probabilitÃ© que les pentes calculÃ©es pour les variables Ã©mergent de distributions dont la moyenne est nulle. Au seuil 0.05, les variables significatives sont (les clr de) lâ€™azote, le phosphore, le potassium et lâ€™aluminium.\nDans le cas des matrices dâ€™abondance (ce nâ€™est pas le cas de varespec, constituÃ©e de donnÃ©es de recouvrement), il est prÃ©fÃ©rable avec les RDA de les transformer prÃ©alablement avec la transformation compositionnelle, de chord ou de Hellinger (chapitreÂ 9). Une autre option est dâ€™effectuer une RDA sur des matrices dâ€™association en passant par une analyse en coordonnÃ©es principales (Legendre et Anderson, 1999). Enfin, les donnÃ©es dâ€™abondance Ã  lâ€™Ã©tat brutes devraient plutÃ´t passer utiliser une analyse canonique des corrÃ©lations.\n\n10.4.2.3 Analyse canonique des correspondances (ACC)\nLâ€™analyse canonique des correspondances (Canonical correspondance analysis), ACC, a Ã©tÃ© Ã  lâ€™origine conÃ§ue pour Ã©tudier les liens entre des variables environnementales et lâ€™abondance (dÃ©compte) ou lâ€™occurrence (prÃ©sence-absence) dâ€™espÃ¨ces (ter Braak, 1986). Lâ€™ACC est Ã  la RDA ce que la CA est Ã  lâ€™ACP. Alors que la RDA prÃ©serve les distance euclidiennes entre variables dÃ©pendantes et indÃ©pendantes, lâ€™ACC prÃ©serve les distances du \\(\\chi^2\\). Tout comme lâ€™AC, elle hÃ©rite du coup une propriÃ©tÃ© importante de la distance du \\(\\chi^2\\): il y a davantage davantage dâ€™importance aux espÃ¨ces rares.\nLâ€™analyse des correspondances canoniques est souvent utilisÃ©e dans la littÃ©rature, mais dans bien des cas une RDA sur des donnÃ©es dâ€™abondance transformÃ©es donnera des rÃ©sultats davantage interprÃ©tables (Legendre et Gallagher, 2001).\n\n10.4.2.3.1 Application\nCet exemple dâ€™application concerne des donnÃ©es dâ€™abondance. Nous allons consÃ©quemment utiliser une CCA avec la fonction cca, toujours avec le module vegan.\nLes tableaux doubs_fish et doubs_env comprennent respectivement des donnÃ©es dâ€™abondance dâ€™espÃ¨ces de poissons et dans diffÃ©rents environnements de la riviÃ¨re Doubs (Europe) publiÃ©es dans Verneaux. (1973) et exportÃ©es du module ade4.\n\ndata(\"doubs\")\ndoubs_fish &lt;- doubs$fish\ndoubs_env &lt;- doubs$env\n\nSur le site no 8, aucun poisson nâ€™a pas Ã©tÃ© observÃ©. Les observations ne comprenant que des zÃ©ro doivent Ãªtre prÃ©alablement retirÃ©es.\n\ntot_spec &lt;- doubs_fish %&gt;%\n  transmute(tot_spec = apply(., 1, sum))\ndoubs_fish &lt;- doubs_fish %&gt;%\n  filter(tot_spec != 0)\n\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\nâ„¹ Please use one dimensional logical vectors instead.\n\ndoubs_env &lt;- doubs_env %&gt;%\n  filter(tot_spec != 0)\n\nDe la mÃªme maniÃ¨re quâ€™avec la fonction rda de vegan, nous utilisons cca pour lâ€™ACC.\n\ndoubs_cca &lt;- cca(doubs_fish ~ ., data = doubs_env, scale = FALSE)\n\nComparons les rÃ©sultats\n\npar(mfrow = c(1, 2))\nordiplot(doubs_cca, scaling = 1, type = \"text\", main = \"CCA - Scaling 1 - Triplot de distance\")\nordiplot(doubs_cca, scaling = 2, type = \"text\", main = \"CCA - Scaling 2 - Triplot de corrÃ©lation\")\n\n\n\n\n\n\n\nTriplot de distance (scaling 1).\n\n\nLa projection des variables rÃ©ponse Ã  angle droit sur les variables explicatives est une approximation de la rÃ©ponse sur lâ€™explication. (2) Un objet (site ou rÃ©ponse) situÃ© prÃ¨s dâ€™une variable explicative est plus susceptible dâ€™avoir le dÃ©compte 1. (3) Les distances entre les variables (rÃ©ponse et explicatives) approximent la distance du \\(\\chi^2\\) (traduction adaptÃ©e de Borcard et al.Â (2011)).\n\n\nTriplot de corrÃ©lation (scaling 2).\n\n\nLa valeur optmiale de lâ€™espÃ¨ce sur une variable environnementale quantitative peut Ãªtre obtenue en projetant lâ€™espÃ¨ce Ã  angle droit sur la variable. (2) Une espÃ¨ce se trouvant prÃ¨s dâ€™une variable environnementale est susceptible de se trouver en plus grande abondance aux sites de statut 1 pour cette variable. (3) Les distances nâ€™approximent pas la distance du \\(\\chi^2\\) (traduction adaptÃ©e de Borcard et al.Â (2011))."
  }
]