[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyse et modélisation d’agroécosystèmes sur R",
    "section": "",
    "text": "Préface\nCe cours a pour objectif de former les étudiants gradués en génie agroenvironnemental, génie civil, génie écologique, agronomie, biologie, foresterie et écologie en analyse et modélisation de systèmes vivants. Les sujets traités sont l’introduction au langage de programmation R, l’analyse statistique descriptive, la visualisation, la modélisation inférentielle, prédictive et déterministe.\nCe manuel est basé sur le cours Analyse et modélisation d’agroécosystèmes de Essi Parent (voir licence au bas de la page). La version proposée ici tient compte des mises à jour des différents outils présentés dans le manuel original. Elle est construite au format Quarto par Charles Frenette-Vallières et Andrés Felipe Silva Dimaté\nVoici la liste des modifications principales apportées jusqu’à présent par rapport à la version originale :"
  },
  {
    "objectID": "index.html#table-des-matières",
    "href": "index.html#table-des-matières",
    "title": "Analyse et modélisation d’agroécosystèmes sur R",
    "section": "Table des matières",
    "text": "Table des matières\n\nIntroduction\nLa science des données avec R\nOrganisation des données et opérations sur des tableaux\nVisualisation\nScience ouverte et reproductibilité\nIntroduction à Python\nBiostatistiques\nIntroduction à l’analyse bayésienne en écologie\nExplorer R\nAssociation, partitionnement et ordination\nDétection de valeurs aberrantes et imputation de données manquantes\nLes séries temporelles\nIntroduction à l’autoapprentissage\nLes données géospatiales\nModélisation de mécanismes écologiques\n\n\nAnalyse et modélisation d’agroécosystèmes de Essi Parent est mis à disposition selon les termes de la licence Creative Commons Attribution - Pas d’Utilisation Commerciale - Partage dans les Mêmes Conditions 4.0 International\n\nFondé(e) sur une œuvre à https://github.com/essicolo/ecologie-mathematique-R/."
  },
  {
    "objectID": "01-intro.html#définitions",
    "href": "01-intro.html#définitions",
    "title": "1  Introduction",
    "section": "\n1.1 Définitions",
    "text": "1.1 Définitions\nLes mathématiques confèrent aux humains une capacité d’abstraction suffisamment complexe pour leur permettre de toucher les étoiles et les atomes, de comprendre le passé et de prédire le futur, de toucher l’infini et de goûter à l’éternité. À partir des maths, on a pu créer des outils de calcul qui permettent de projeter des images de l’univers, bien au-delà de la Voie lactée. Mais appréhender le vivant, tout près de nous, demeure une tâche complexe.\n\n\n\n\nFigure 1.2: Domaines scientifiques de l’écologie mathématique.\n\n\n\nL’écologie mathématique couvre un large spectre de domaines (Figure 1.2), mais peut être divisée en deux branches: l’écologie théorique et l’écologie quantitative (Legendre et Legendre, 2012). Alors que l’écologie théorique s’intéresse à l’expression mathématique des mécanismes écologiques, l’écologie quantitative, plus empirique, en étudie principalement les phénomènes. La modélisation écologique vise à prévoir une situation selon des conditions données. Faisant partie à la fois de l’écologie théorique et de l’écologie quantitative, elle superpose souvent des mécanismes de l’écologie théorique et des phénomènes empiriques de l’écologie quantitative. L’écologie numérique comprend la branche descriptive de l’écologie quantitative, c’est-à-dire qu’elle s’intéresse à évaluer des effets à partir de données empiriques. L’exploration des données dans le but d’y découvrir des structures passe souvent par des techniques multivariées comme la classification hiérarchique ou la réduction d’axe (par exemple, l’analyse en composantes principales), qui sont davantage heuristiques (dans notre cas, bioheuristique) que statistiques. Les tests d’hypothèses et l’analyse des probabilités, quant à eux, relèvent de la biostatistique.\nLe génie écologique, une discipline intimement liée à l’écologie mathématique, est voué à l’analyse, la modélisation, la conception et la construction de systèmes vivants dans le but de résoudre de manière efficace des problèmes liés à l’écologie et à une panoplie de domaines qui lui sont raccordés. L’agriculture est l’un de ces domaines. C’est d’emblée la discipline qui sera prisée dans ce manuel. Néanmoins, les principes qui seront discutés sont transférables à l’écologie générale."
  },
  {
    "objectID": "01-intro.html#à-qui-sadresse-ce-manuel",
    "href": "01-intro.html#à-qui-sadresse-ce-manuel",
    "title": "1  Introduction",
    "section": "\n1.2 À qui s’adresse ce manuel?",
    "text": "1.2 À qui s’adresse ce manuel?\nLe cours vise à introduire des étudiant.e.s gradué.e.s en agronomie, biologie, écologie, sols, génie agroenvironnemental, génie civil et génie écologique à l’analyse et la modélisation dans leur domaine, tant pour les appuyer pour leurs travaux de recherche que pour leur fournir une trousse d’outils émancipatrice pour leur cheminement professionnel. Plus spécifiquement, vous serez accompagné à découvrir différents outils numériques qui vous permettront d’appréhender vos données, d’en faire émerger l’information et de construire des modèles. L’objectif de ce cours n’est pas de vous former en mathématiques, mais de vous aider à les utiliser. En ce sens, c’est un cours de pilotage, pas un cours de mécanique. Vous ferez tout de même un peu de mécanique pour mieux comprendre les réactions de notre machine.\nBien que des connaissances en programmation et en statistiques aideront grandement les étudiant.e.s à appréhender ce document, une littératie informatique n’est pas requise. Dans tous les cas, quiconque voudra tirer profit de ce manuel devra faire preuve d’autonomie. Vous serez guidés vers des ressources et des références, mais je vous suggère vivement de développer votre propre bibliothèque adaptée à vos besoins et à votre manière de comprendre."
  },
  {
    "objectID": "01-intro.html#les-logiciels-libres",
    "href": "01-intro.html#les-logiciels-libres",
    "title": "1  Introduction",
    "section": "\n1.3 Les logiciels libres",
    "text": "1.3 Les logiciels libres\nTous les outils numériques qui sont proposés dans ce cours sont des logiciels libres:\n\n« Logiciel libre » [free software] désigne des logiciels qui respectent la liberté des utilisateurs. En gros, cela veut dire que les utilisateurs ont la liberté d’exécuter, copier, distribuer, étudier, modifier et améliorer ces logiciels. Ainsi, « logiciel libre » fait référence à la liberté, pas au prix1 (pour comprendre ce concept, vous devez penser à « liberté d’expression », pas à « entrée libre »). - Projet GNU\n\nDonc: codes sources ouverts, développement souvent communautaire, gratuité. Plusieurs raisons éthiques, principalement liées au contrôle de l’environnement virtuel par les utilisateurs et les communautés, peuvent justifier l’utilisation de logiciels libres. Plusieurs raisons pratiques justifient aussi cette orientation. Les logiciels libres vous permettent de transporter vos outils avec vous, d’une entreprise à l’autre, au bureau, ou à la maison, et ce, sans vous soucier d’acheter de coûteuses licences.\nIl existe tout de même des risques liés aux possibles erreurs dans les codes des logiciels communautaires. Ces risques sont d’ailleurs les mêmes que ceux liés aux logiciels propriétaires. Pour les scientifiques, une erreur peut mener à une étude retirée de la littérature et même, potentiellement, des politiques publiques mal avisées. Pour les ingénieurs, les conséquences pourraient être dramatiques. Mais retenez qu’en toute circonstance, comme professionnel.le, vous êtes responsable des outils que vous utilisez: vous devez vous assurer de la bonne qualité d’un logiciel, qu’il soit propriétaire ou communautaire.\nAlors que la qualité des logiciels propriétaires est généralement suivie par audits, celle des logiciels libres est plutôt soumise à la vigilance communautaire. Chaque approche a ses avantages et inconvénients, mais elles ne sont pas exclusives. Ainsi, les logiciels libres peuvent être audités à l’externe par quiconque décide de le faire. Différentes entreprises, souvent concurrentes, participent tant à cette vigilance qu’au développement des logiciels libres: elles en sont même souvent les instigatrices (comme RStudio, Anaconda et Enthought).\nPar ailleurs, ce manuel est distribué librement sous licence Creative commons, selon les termes suivants.\n\n\nAnalyse et modélisation d’agroécosystèmes de Essi Parent est mis à disposition selon les termes de la licence Creative Commons Attribution - Pas d’Utilisation Commerciale - Partage dans les Mêmes Conditions 4.0 International\n\nFondé(e) sur une œuvre à https://github.com/essicolo/ecologie-mathematique-R/."
  },
  {
    "objectID": "01-intro.html#langage-de-programmation",
    "href": "01-intro.html#langage-de-programmation",
    "title": "1  Introduction",
    "section": "\n1.4 Langage de programmation",
    "text": "1.4 Langage de programmation\n\n1.4.1 R\nCe cours est basé sur le langage R. En plus d’être libre, R est un langage de programmation dynamique largement utilisé dans le monde universitaire, et dont l’utilisation s’étend de manière soutenue hors des tours d’ivoire.\n\nR is also the name of a popular programming language used by a growing number of data analysts inside corporations and academia. It is becoming their lingua franca partly because data mining has entered a golden age, whether being used to set ad prices, find new drugs more quickly or fine-tune financial models. New York Times, janvier 2009\n\nSon développement est supporté par la R Foundation for Statistical Computing, basée à l’Université de Vienne. Également, l’équipe de RStudio contribue largement au développement de modules génériques. R est principalement utilisé pour le calcul statistique, mais les récents développements le rendent un outil de choix pour tout ce qui entoure la science des données, de l’interaction avec les bases de données au déploiement d’outils d’intelligence artificielle en passant par la visualisation. Une fois implémenté avec des modules de calcul scientifique spécialisés en biologie, en écologie et en agronomie (que nous couvrirons au long du cours), R devient un outil de calcul convivial, rapide et fiable.\n\n1.4.2 Pourquoi pas Python?\nLa première mouture de ce cours se fondait sur le langage Python. Tout comme R, Python est un langage de programmation dynamique prisé pour le calcul scientifique. Python est un langage générique apprécié pour sa polyvalence et sa simplicité. Python est utilisé autant pour créer des logiciels ou des sites web que pour le calcul scientifique. Ainsi, Python peut être utilisé en interopérabilité avec une panoplie de logiciels libres, comme QGIS pour la cartographie et FreeCAD pour le dessin technique. Il est particulièrement apprécié en ingénierie pour ses modules de calcul par éléments finis (e.g. FeNICS) et en bioinformatique pour ses outils liés au séquençage (scikit-bio), mais ses lacunes en analyse statistique, en particulier en statistiques multivariées m’ont amené à favoriser R.\nBien que leurs possibilités se superposent largement, ce serait une erreur d’aborder R et Python comme des langages rivaux. Les deux langages s’expriment de manière similaire et s’inspirent mutuellement: apprendre à travailler avec l’un revient à apprendre l’autre. Les spécialistes en calcul scientifique tendent à apprendre à travailler avec plus d’un langage de programmation. Par ailleurs, il existe de plus en plus des moyens de travailler en R et en Python dans un même flux de travail. L’interface de calcul RStudio, que nous utiliserons pendant le cours, permet d’inclure des blocs de code en Python.\nDans la version mise à jour du manuel, une courte introduction facultative à Python est proposée.\n\n1.4.3 Pourquoi pas Matlab?\nParce qu’on est en 2024.\n\n1.4.4 Et… SAS?\nParce qu’on est à l’université.\n\n1.4.5 Mais pourquoi pas ______ ?\nD’autres langages, comme Julia, Scala, Javascript et même Ruby sont utilisés en calcul scientifique. Ils sont néanmoins moins garnis et moins documentés que R. Des langages de plus bas niveau, comme Fortran et C++, viennent souvent appuyer les fonctions des autres langages: ces langages sont plus ardus à utiliser au jour le jour, mais leur rapidité de calcul est imbattable."
  },
  {
    "objectID": "01-intro.html#contenu-du-manuel",
    "href": "01-intro.html#contenu-du-manuel",
    "title": "1  Introduction",
    "section": "\n1.5 Contenu du manuel",
    "text": "1.5 Contenu du manuel\nJe favorise une approche intuitive aux développements mathématiques. Nous aborderons l’analyse et la modélisation inférentielle, prédictive et mécanistique appliquée aux agroécosystèmes.\nChapitre 2 - Introduction au langage de programmation R. Qu’est-ce que R? Comment l’aborder? Quelles sont les fonctionnalités de base et comment tirer profit de tout l’écosystème de programmation?\nChapitre 3 - Organisation des données et opérations sur des tableaux. Les tableaux permettent d’enchâsser l’information dans un format prêt-à-porter pour R. Comment les importer, les exporter, les filtrer, et en faire des sommaires?\nChapitre 4 - Visualisation. Comment présenter l’information contenue dans un long tableau en un seul coup d’oeil?\nChapitre 5 - Le travail collaboratif, le suivi de version et la science ouverte. Ce chapitre offre une introduction à l’utilisation des outils de calcul collaboratif, ainsi qu’un aperçu du système de suivi de version git et de son utilisation sur GitHub.\nChapitre 6 - Introduction à Python (section facultative). Une très brève introduction au langage de programmation Python. Ce contenu est externe au cours et est là pour vous fournir des références si vous souhaitez explorer ce langage dans le futur.\nChapitre 7 - Biostatistiques. Il est audacieux de ne consacrer qu’un seul chapitre sur ce vaste sujet. Nous irons à l’essentiel… pour vous donner les outils qui permettront d’approfondir le sujet.\nChapitre 8 - Biostatistiques bayésiennes (section facultative). Une très brève introduction pour qui est intéressé à l’analyse bayésienne.\n?sec-expl - Explorer R. La science des données évolue rapidement. Vous gagnerez à vous tenir au courrant de son évolution, et immanquablement vous vous buterez sur des opérations qui vous sembleront insolubles. Ce chapitre vous accompagnera à rester à jour sur le développement de R, à poser de bonnes questions et proposera des modules intéressants en écologie mathématique.\nChapitre 10 - Association, partitionnement et ordination. Les écosystèmes diffèrent, mais en quoi sont-ils semblables, et en quoi dffèrent-ils? Ces questions importantes peuvent être abordés par l’écologie numérique, domaine d’étude au sein duquel l’association, le partitionnement et l’ordination sont des outils prédominants.\n?sec-imput - Détection de valeurs aberrantes et imputation. Une donnée aberrante sortira du lot, pour une raison ou pour une autre. Comment les détecter de manière systématique? D’autre part, que faire lorsqu’une donnée est manquante? Peut-on l’imputer? Comment?\n?sec-temps - Les séries temporelles. Les capteurs modernes permettent de générer des données en fonction du temps. Que ce soit des données météorologiques enregistrées quotidiennement ou des données de teneur en eau enregistrées au 5 secondes, les données en fonction du temps forment un signal. Comment analyser ces signaux?\n?sec-ia - L’autoapprentissage. Les applications de l’intelligence artificielle ne sont limitées que par votre imagination. Encore faut-il l’utiliser… intelligemment.\n?sec-geo - Les données spatiales. Ce chapitre porte sur l’utilisation de R comme système d’information géographique de base. Nous utiliserons aussi l’autoapprentissage et les modèles déterministes comme outils d’interpolation spatial.\n?sec-model - La modélisation mécanistique (section facultative). Les modèles sont des maquettes simplifiées. Comment utiliser les équations différentielles ordinaires pour créer ces maquettes?\nSi les chapitres 3 à 5 peuvent être considérés comme fondamentaux pour bien maîtriser R, les autres peuvent être feuilletés à la pièce, bien qu’ils forment une suite logique.\nChaque chapitre de ce manuel est rédigé en format Quarto, dans un environnement RStudio. Pour exécuter les commandes, vous pourrez soit les copier-coller dans R (ou RStudio), soit télécharger les fichiers-sources et exécuter les blocs de code.\nLe manuel original était rédigé au format R Markdown et est toujours disponible à l’adresse suivante."
  },
  {
    "objectID": "01-intro.html#objectifs-généraux",
    "href": "01-intro.html#objectifs-généraux",
    "title": "1  Introduction",
    "section": "\n1.6 Objectifs généraux",
    "text": "1.6 Objectifs généraux\nÀ la fin du cours, vous serez en mesure:\n\nde programmer en langage R\nd’importer, de manipuler (sélection des colonnes, filtres, sommaires statistiques) et d’exporter des tableaux\nde générer des graphiques d’utilisation commune\nde vous assurer que vos calculs soient auditables et reproductibles dans une perspective de science ouverte\nd’appréhender des données écologiques et agronomiques à l’aide de tests statistiques fréquentiels\nd’explorer par vous-même les possibilités offertes par la communauté de développement de modules R\nd’explorer les données à l’aide des outils de l’écologie numérique (association, partitionnement et ordination)\nd’imputer des données manquantes dans un tableau et de détecter des valeurs aberrantes\nde créer un modèle d’autoapprentissage\nd’effectuer une analyse de série temporelle\nd’interpoler des données spatiales\nde modéliser des équations différentielles ordinaires"
  },
  {
    "objectID": "01-intro.html#lectures-complémentaires",
    "href": "01-intro.html#lectures-complémentaires",
    "title": "1  Introduction",
    "section": "\n1.7 Lectures complémentaires",
    "text": "1.7 Lectures complémentaires\n\n1.7.1 Écologie mathématique\n\n\nHow to be a quantitative ecologist. Jason Mathipoulos vous prend par la main pour découvrir les notions de mathématiques fondamentales en écologie, appliquées avec le langage R.\n\n\nNumerical ecology. L’ouvrage hautement détaillé des frères Legendre est non seulement fondamental, mais aussi fondateur d’une science qui évolue encore aujourd’hui: l’analyse des données écologiques.\n\nA practical guide to ecological modelling. Soetaert et Herman portent une attention particulière à la présentation des principes de modélisation dans un langage accessible - ce qui est rarement le cas dans le domaine de la modélisation. Les modèles présentés concernent principalement les bilans de masse, en termes de systèmes de réactions chimiques et de relations biologiques.\n\nModélisation mathématique en écologie. Rare livre en modélisation écologique publié en français, la première partie s’attarde aux concepts mathématiques, alors que la deuxième planche à les appliquer. Si le haut niveau d’abstraction de la première partie vous rebute, n’hésitez pas débuter par la seconde partie et de vous référer à la première au besoin.\n\nA new ecology: systems perspective. Principalement grâce au soleil, la Terre forme un ensemble de gradients d’énergie qui se déclinent en des systèmes d’une étonnante complexité. C’est ainsi que le regretté Sven Erik Jørgensen (1934-2016, Figure 1.3)) et ses collaborateurs décrivent les écosystèmes dans cet ouvrage qui fait suite aux travaux fondateurs de Howard Thomas Odum.\nEcological engineering. Principle and Practice.\nEcological processes handbook.\nModeling complex ecological dynamics\n\n\n\n\n\nFigure 1.3: Sven Erik Jørgensen, Source: Elsevier.\n\n\n\n\n1.7.2 Programmation\n\n\nR for data science (2e). L’analyse de données est une branche importante de l’écologie mathématique. Ce manuel traite des matrices et la manipulation de données chapitre 3), de la visualisation (chapitre 4) ainsi que de l’apprentissage automatique (chapitre 14). R for data science (2e) repasse ces sujets plus en profondeur. En particulier, l’ouvrage de Garrett Grolemund, Hadley Wickham et Mine Çetinkaya-Rundel offre une introduction au module graphique ggplot2 et à tidyverse.\n\nNumerical ecology with R. Daniel Borcard enseigne l’écologie numérique à l’Université de Montréal. Son cours est condensé dans ce livre recettes voué à l’application des principes lourdement décrits dans Numerical ecology.\n\n1.7.3 Divers\n\n\nThe truthful art. Dans cet ouvrage, Alberto Cairo s’intéresse à l’utilisation des données et de leurs présentations pour fournir une information adéquate à différents publics."
  },
  {
    "objectID": "01-intro.html#besoin-daide",
    "href": "01-intro.html#besoin-daide",
    "title": "1  Introduction",
    "section": "\n1.8 Besoin d’aide?",
    "text": "1.8 Besoin d’aide?\nLes ouvrages de référence reconnus vous offrent des bases solides sur lesquelles vous pouvez vous appuyer dans vos travaux. Mais au-delà des principes, au jour le jour, vous vous buterez immanquablement à toutes sortes de petits problèmes. Quel module utiliser pour cette tâche précise? Que veut dire ce message d’erreur? Comment interpréter ce résultat? Pour tous les petits accrocs du quotidien en calcul scientifique, internet offre de nombreuses ressources qui sont très hétérogènes en qualité. Vous apprendrez à reconnaître les ressources fiables à celles qui sont douteuses. Les plateformes basées sur Stack Exchange, comme Stack Overflow et Cross Validated, m’ont souvent été d’une aide précieuse. Vous aurez avantage à vous construire une petite banque d’information avec un logiciel de prise de notes en collectant des liens, en prenant en notes certaines recettes et en suivant des sites d’intérêt avec des flux RSS."
  },
  {
    "objectID": "01-intro.html#à-propos-de-lauteur",
    "href": "01-intro.html#à-propos-de-lauteur",
    "title": "1  Introduction",
    "section": "\n1.9 À propos de l’auteur",
    "text": "1.9 À propos de l’auteur\nJe m’appelle Essi Parent. Je suis ingénieur écologue et professeur adjoint au Département des sols et de génie agroalimentaire de l’Université Laval, Québec, Canada. Je crois que la science est le meilleur moyen d’appréhender le monde pour prendre des décisions avisées."
  },
  {
    "objectID": "01-intro.html#un-cours-complémentaire-à-dautres-cours",
    "href": "01-intro.html#un-cours-complémentaire-à-dautres-cours",
    "title": "1  Introduction",
    "section": "\n1.10 Un cours complémentaire à d’autres cours",
    "text": "1.10 Un cours complémentaire à d’autres cours\nCe cours a été développé pour ouvrir des perspectives mathématiques en écologie et en agronomie à la FSAA de l’Université Laval. Il est complémentaire à certains cours offerts dans d’autres institutions académiques au Québec, dont ceux-ci.\n\n\nBIO2041. Biostatistiques 1, Université de Montréal\n\nBIO2042. Biostatistiques 2, Université de Montréal\n\nBIO109. Introduction à la programmation scientifique, Université de Sherbrooke\n\nBIO500. Méthodes en écologie computationnelle, Université de Sherbrooke."
  },
  {
    "objectID": "01-intro.html#contribuer-au-manuel",
    "href": "01-intro.html#contribuer-au-manuel",
    "title": "1  Introduction",
    "section": "\n1.11 Contribuer au manuel",
    "text": "1.11 Contribuer au manuel\nJe suis ouvert aux commentaires et suggestions. Pour contribuer directement, dirigez-vous sur le dépôt du manuel sur GitHub, puis ouvrez une Issue pour en discuter. Créez une nouvelle branche (fork), effectuez les modifications, puis lancer une requête de fusion (pull request)."
  },
  {
    "objectID": "02-R.html#statistiques-ou-science-des-données",
    "href": "02-R.html#statistiques-ou-science-des-données",
    "title": "2  La science des données avec R",
    "section": "\n2.1 Statistiques ou science des données?",
    "text": "2.1 Statistiques ou science des données?\nSelon Whitlock et Schluter (2015), la statistique est l’étude des méthodes pour décrire et mesurer des aspects de la nature à partir d’échantillon. Pour Grolemund et Wickham (2023), la science des données est une discipline excitante permettant de transformer des données brutes en compréhension, perspectives et connaissances. Oui, excitante! La différence entre les deux champs d’expertise est subtile, et certaines personnes n’y voient qu’une différence de ton.\n\n\n\nData Science is statistics on a Mac.\n\n— Big Data Borat (@BigDataBorat) 27 août 2013\n\nConfinées à ses applications traditionnelles, les statistiques sont davantage vouées à la définition de dispositifs expérimentaux et à l’exécution de tests d’hypothèses, alors que la science des données est moins linéaire, en particulier dans sa phase d’analyse, où de nouvelles questions (donc de nouvelles hypothèses) peuvent être posées au fur et à mesure de l’analyse. Cela arrive généralement davantage lorsque l’on fait face à de nombreuses observations sur lesquelles de nombreux paramètres sont mesurés.\nLa quantité de données et de mesures auxquelles nous avons aujourd’hui accès grâce aux technologies de mesure et de stockage relativement peu dispendieux rend la science des données une discipline particulièrement attrayante, pour ne pas dire sexy."
  },
  {
    "objectID": "02-R.html#débuter-en-r",
    "href": "02-R.html#débuter-en-r",
    "title": "2  La science des données avec R",
    "section": "\n2.2 Débuter en R",
    "text": "2.2 Débuter en R\nR est un langage de programmation dérivé du langage S, qui fut initialement lancé en 1976.\n\n\n\n\nFigure 2.2: Logo officiel du language R.\n\n\n\nR figure parmi les langages de programmation les plus utilisés au monde. Bien qu’il soit basé sur les langages statiques C et Fortran, R est un langage dynamique, c’est-à-dire que le code peut être exécuté ligne par ligne ou bloc par bloc: un avantage majeur pour des activités qui nécessitent des interactions fréquentes. Bien que R soit surtout utilisé pour le calcul statistique, il s’impose de plus en plus comme outil privilégié en sciences des données en raison des récents développements de modules d’analyse, de modélisation et de visualisation, dont plusieurs seront utilisés dans ce manuel.\nUn langage de programmation s’apprend un peu comme une langue. Au début, un code R peut sembler incompréhensible. Et face à son clavier, on ne sait pas trop comment exprimer ce que l’on désire. Au fur et à mesure de l’apprentissage, les symboles, les fonctions et le style deviennent de plus en plus familiers et on apprend tranquillement à traduire en code ce que l’on désire effectuer. Comme une langue s’apprend en la parlant dans la vie de tous les jours, un language de programmation s’apprend avantageusement en solutionnant vos propres problèmes (Figure 2.3).\n\n\n\n\nFigure 2.3: R avant et maintenant, Illustration de Allison Horst"
  },
  {
    "objectID": "02-R.html#préparer-son-flux-de-travail",
    "href": "02-R.html#préparer-son-flux-de-travail",
    "title": "2  La science des données avec R",
    "section": "\n2.3 Préparer son flux de travail",
    "text": "2.3 Préparer son flux de travail\nIl existe de nombreuses manières d’utiliser R. Parmi celles-ci, j’en couvrirai 3:\n\nInstallation classique (installation suggérée)\nInstallation avec Anaconda\nUtilisation infonuagique\n\n\n2.3.1 Installation classique\nInstallation suggérée. Sur Windows ou Mac, dirigez-vous ici, téléchargez et installez. Sur Linux, ouvrez votre gestionnaire d’application, chercher r-base (Ubuntu, Debian), R-base (openSuse) ou R-core (Fedora) et installez-le (assurez-vous que les librairies suivantes sont aussi installées: gcc, gcc-fortran, gcc-c++ et make), vous aurez peut-être besoin d’installer des librairies supplémentaires pour faire fonctionner certains modules.\n\nNote. Les modules présentés dans ce cours devraient être disponibles sur Linux, Windows et Mac. Ce n’est pas le cas pour tous les modules R. La plupart fonctionnent néanmoins sur Linux, dont les systèmes d’opération (je recommande Ubuntu ou l’une de ses dérivées comme elementary OS) sont de bonnes options pour le calcul scientifique.\n\nÀ cette étape, R devrait fonctionner dans un interpréteur de commande . Si vous lancez R dans un terminal (chercher cmd dans le menu si vous êtes sur Windows), vous obtiendrez quelque chose comme ceci.\n\n\n\n\nFigure 2.4: R dans le terminal.\n\n\n\nLe symbole &gt; indique que R attend que vos instructions. Vous voilà dans un état méditatif devant l’indéchiffrable vide du terminal 😵. Ne vous en faites pas: nous commencerons bientôt à jaser avec R.\nAvant cela, installons-nous au salon. Afin de travailler dans un environnement de travail plus confortable, je recommande l’installation de l’interface RStudio, gratuite et open source: téléchargez l’installateur et suivez les instructions. RStudio ressemble à ceci.\n\n\n\n\nFigure 2.5: Fenêtre de RStudio.\n\n\n\nEn haut à droite se trouve un menu Project (None). Il s’agit d’un menu de vos projets. Je recommande d’utiliser ces projets avec RStudio, qui vous permettront de mieux gérer vos sessions de travail, en particulier en lien avec les chemins vers vos données, graphiques, etc., que vous pouvez gérer relativement à l’emplacement de votre dossier de projet plutôt qu’à l’emplacement des fichiers sur votre machine: nous verrons plus en détails au chapitre 5.\n\nEn haut à gauche, vous avez vos feuilles de calcul, qui apparaîtront en tant qu’onglets. Une feuille de calcul R script est une série de commandes que vous lancez en séquence. Il peut aussi s’agir d’un document Quarto si vous choisissez de travailler ainsi. Ce format vous permettra de d’écrire du texte en format Markdown entre des blocs de code. Il est question du format Quarto au chapitre 5).\nEn bas à gauche apparaît la Console, où vous voyez les commandes envoyées à R ainsi que ses sorties.\nEn haut à droite, les différents onglets indiquent où vous en êtes dans vos calculs. En particulier, la liste sous Environment indique les objets qui ont été générés ou chargés jusqu’alors.\nEn bas à droite, on retrouve des onglets de nature variés. Files contient les sous-dossiers et fichiers du dossier de projets. Plots est l’endroit où apparaîtront vos graphiques. Packages contient la liste des modules déjà installés, ainsi qu’un outil de gestion des modules pour leur installation, leur désinstallation et leur mise à jour. Help affiche les fiches d’aide des fonctions (pour obtenir de l’aide sur une fonction dans RStudio, surlignez la fonction dans votre feuille de calcul, puis appuyez sur F1). Enfin, l’onglet Viewer affichera les sorties HTML, en particulier les graphiques interactifs que vous générerez par exemple avec le module plotly, ou alors le rendu de votre fichier Quarto. Si votre environnement de travail était un avion, R serait le moteur et RStudio serait le cockpit!\n\n\n\n\n\nFigure 2.6: Scène de Fifi Brindacier (Astrid Lindgren, 1945).\n\n\n\n\n2.3.2 Installation avec Anaconda\nSi vous cherchez une trousse complète d’analyse de données, comprenant R et Python, vous pourrez préférer Anaconda. Une fois installée, vous pourrez isoler un environnement de travail sur R, ou même isoler des environnements de travail particuliers pour vos projets. Une manière conviviale de créer des environnements de travail est de passer par l’interface Anaconda navigator, que vous lancerez soit dans le menu Windows, soit en ligne de commande anaconda-navigator sous Mac et Linux, puis d’installer r-essentials, rstudio et jupyterlab dans l’onglet Environment. Vous pourrez aussi installer RStudio et Jupyter lab via l’onglet Home de Anaconda navigator. Dans l’environnement de base, installez le package nb_conda_kernels pour vous assurer que tous les noyaux (R, Python, etc.) installés dans les environnements de travail soient automatiquement accessibles dans Jupyter. Si vous désirez utiliser dans Jupyter la version de R installée avec l’installation classique, référez-vous au guide présenté en extra au bas de la page.\n\n\n\n\nFigure 2.7: Anaconda navigator.\n\n\n\nJupyter lab est une interface notebook semblable à Quarto - les format Jupyter (*.ipynb) et Quarto (*.qmd) sont par ailleurs convertibles grâce au module jupytext. L’utilisation de R en Anaconda n’est pas tout à fait au point, et pourrait poser problème pour l’installation de certains modules. Si vous optez pour cette option, préparez-vous à avoir à bidouiller un peu. Plusieurs préfèrent Jupyter à RStudio (ce n’est pas mon cas).\n\n2.3.3 Utilisation infonuagique\nPas besoin d’avoir une machine super puissante pour travailler en R. Il existe une multitude de services infonuagiques (dans le cloud) vous permettant de lancer vos calculs sur des serveurs plutôt que sur votre Chromebook ou votre vieux laptop déglingué. Certains services sont gratuits, et d’autres souvent plus élaborés sont payants. Vous pouvez utiliser gratuitement Azure Notebooks ou un tour de passe-passe pour faire fonctionner Google colab en R. Une option gratuite de CoCalc vient avec un agressant bandeau rouge vif qui disparait avec l’option payante.\nÀ mon avis, le service Nextjournal est celui d’entre tous qui possède en ce moment les meilleures qualités dans sa version gratuite. Vous pourrez y travailler en mode collaboratif, comme dans Google docs. En outre, vous pouvez lancer ces notes de cours en les important dans Nextjournal. Vous devrez toutefois déposer les données dans l’interface, puis à chaque session installer les modules spécialisés. Le service gratuit offre peu de puissance de calcul, mais pour effectuer les applications de base, ça devrait être suffisant. La vidéo ci-dessous monter comment importer les notes de cours dans Nextjournal.\nVideo"
  },
  {
    "objectID": "02-R.html#premiers-pas-avec-r",
    "href": "02-R.html#premiers-pas-avec-r",
    "title": "2  La science des données avec R",
    "section": "\n2.4 Premiers pas avec R",
    "text": "2.4 Premiers pas avec R\nR ne fonctionne pas avec des menus, en faisant danser une souris sous une musique de clics. Vous devrez donc entrer des commandes avec votre clavier, que vous apprendrez par cœur au fur et à mesure, ou que vous retrouverez en lançant des recherches sur internet. Par expérience personnelle, lorsque je travaille avec R, j’ai toujours un navigateur ouvert prêt à recevoir une question.\nLes étapes qui suivent sont des premiers pas. Elles ne feront pas de vous des ceintures noires de la programmation. La plupart des utilisateurs de R ont appris en se pratiquant sur leurs données, en se butant sur des obstacles, en apprenant comment les surmonter ou les contourner…\nPour l’instant, ouvrez seulement un interpréteur de commande, et lancez R. Voyons si R est aussi libre qu’on le prétend.\n\n“La liberté, c’est la liberté de dire que deux et deux font quatre. Si cela est accordé, tout le reste suit.” - George Orwell, 1984\n\n\n2 + 2\n\n[1] 4\n\n\nEt voilà.\n\nLes opérations mathématiques sont effectuées telles que l’on devrait s’attendre.\n\n67.1 - 43.3\n\n[1] 23.8\n\n2 * 4\n\n[1] 8\n\n1 / 2\n\n[1] 0.5\n\n\nL’exposant peut être noté ^, comme c’est le cas dans Excel, ou ** comme c’est le cas en Python.\n\n2^4\n\n[1] 16\n\n\n\n2**4\n\n[1] 16\n\n\n\n1 / 2 # utilisez des espaces de part et d'autre des opérateurs (sauf pour l'exposant) pour éclaircir le code\n\n[1] 0.5\n\n\nR ne lit pas ce qui suit le caractère #. Cela vous laisse l’opportunité de commenter un code comprenant une séquence de plusieurs lignes. Remarquez également que la dernière opération comporte des espaces entre les nombres et l’opérateur /. Dans ce cas (ce n’est pas toujours le cas), les espaces ne signifient rien: ils aident seulement à éclaircir le code. Il existe des guides pour l’écriture de code en R. Je recommande fortement de suivre méticuleusement le guide de style de tidyverse.\nAssigner des objets à des variables est fondamental en programmation. En R, on assigne traditionnellement avec la flèche &lt;-, mais vous verrez parfois le =, qui est davantage utilisé comme standard dans d’autres langages de programmation. Par exemple.\n\na &lt;- 3\n\nTruc. Essayez d’inverser la flèche, e.g. 3 -&gt; a.\nTechniquement, a pointe vers le nombre entier 3. Conséquemment, on peut effectuer des opérations sur a.\n\na * 6\n\n[1] 18\n\n\n\nA + 2\n\nLe message d’erreur nous dit que A n’est pas défini. Sa version minuscule, a, l’est pourtant. La raison est que R considère la case dans la définition des objets. Utiliser la mauvaise case mène donc à des erreurs.\nNote. Les messages d’erreur ne sont pas toujours clairs, mais vous apprendrez à les comprendre. Dans tous les cas, ils sont fait pour vous aider. Lisez-les attentivement!\nEn général, le nom d’une variable doit toujours commencer par une lettre, et ne doit pas contenir de caractères réservés (espaces, +, *). Dans la définition des variables, plusieurs utilisent des symboles . pour délimiter les mots, mais la barre de soulignement _ est à préférer. En effet, dans d’autres langages de programmation comme Python, le . a une autre signification: son utilisation est à éviter autant que possible. De même, évitez l’utilisation de majuscules pour nommer vos objets (voir le guide de style de tidyverse pour nommer les objets).\nNote. À ce stade, vous serez probablement plus à l’aise de copier-coller ces commandes dans votre terminal.\n\nrendement_arbre &lt;- 50 # pomme/arbre\nnombre_arbre &lt;- 300 # arbre\nnombre_pomme &lt;- rendement_arbre * nombre_arbre\nnombre_pomme\n\n[1] 15000\n\n\nComme chez la plupart des langages de programmation, R respecte les conventions des priorités des opérations mathéatiques.\n\n10 - 9^0.5 * 2\n\n[1] 4\n\n\n\n2.4.1 Types de données\nJusqu’à maintenant, nous n’avons utilisé que des nombres entiers (integer ou int) et des nombres réels (numeric ou float64). R inclut d’autres types. La chaîne de caractère (string ou character) contient un ou plusieurs symboles. Elle est définie entre des doubles guillemets \" \" ou des apostrophes ' '. Il n’existe pas de standard sur l’utilisation de l’un ou de l’autre, mais en règle générale, on utilise les apostrophes pour les expressions courtes, contenant un simple mot ou une séquence de lettres, et les guillemets pour les phrases. Une raison pour cela: les guillemets sont utiles pour insérer des apostrophes dans une chaîne de caractère.\n\na &lt;- \"L'ours\"\nb &lt;- \"polaire\"\npaste(a, b)\n\n[1] \"L'ours polaire\"\n\n\nOn colle a et b avec la fonction paste. Notez que l’objet a a été défini précédemment. Il est possible en R de réassigner une variable, mais cela peut porter à confusion, jusqu’à générer des erreurs de calcul si une variable n’est pas assignée à l’objet auquel on voulait référer.\nCombien de caractères contient la chaîne \"L'ours polaire\"? R sait compter. Demandons-lui.\n\nc &lt;- paste(a, b)\nnchar(c)\n\n[1] 14\n\n\nQuatorze, c’est bien cela (comptez “L’ours polaire”, en incluant l’espace). Comme paste, nchar est une fonction incluse par défaut dans l’environnement de travail de R: plus précisément, ces fonctions sont incluses dans le module base, inclut par défaut lorsque R est lancé. La fonction est appelée en écrivant nchar(). Mais une fonction de quoi? Des arguments, qui se trouvent entre les parenthèses. Dans ce cas, il y a un seul argument: c.\nEn calcul scientifique, il est courant de lancer des requêtes déterminant si un résultat est vrai ou faux.\n\na &lt;- 17\na &lt; 10\n\n[1] FALSE\n\na &gt; 10\n\n[1] TRUE\n\na == 10\n\n[1] FALSE\n\na != 10\n\n[1] TRUE\n\na == 17\n\n[1] TRUE\n\n!(a == 17)\n\n[1] FALSE\n\n\nJe viens d’introduire un nouveau type de donnée: les données booléennes (boolean, ou logical), qui ne peuvent prendre que deux états - TRUE ou FALSE. En même temps, j’ai utilisé la fonction print parce que dans mon carnet, seule la dernière opération permet d’afficher le résultat. Si l’on veut forcer une sortie, on utilise print. Puis, on a vu plus haut que le symbole = est réservé pour assigner des objets: pour les tests d’égalité, on utilise le double égal, ==, ou != pour la non-égalité. Enfin, pour inverser une donnée de type booléenne, on utilise le point d’exclamation !.\n\n2.4.2 Les collections de données\nLes exercices précédents ont permis de présenter les types de données offerts par défaut sur R qui sont les plus importants pour le calcul scientifique: int (integer, ou nombre entier), numeric (nombre réel), character (string, ou chaîne de caractère) et logical (booléen). D’autres s’ajouteront tout au long du cours, comme les catégories (factor) et les unités de temps (date-heure).\nLorsque l’on procède à des opérations de calcul en science, nous utilisons rarement des valeurs uniques. Nous préférons les organiser et les traiter en collections. Par défaut, R offre quatre types importants de collections: les vecteurs, les matrices, les listes et les tableaux.\n\n2.4.2.1 Vecteurs\nD’abord, les vecteurs sont une série de variables de même type. Un vecteur est délimité par la fonction c( ) (c pour concaténation). Les éléments de la liste sont séparés par des virgules.\n\nespece &lt;- c(\"Petromyzon marinus\", \"Lepisosteus osseus\", \"Amia calva\", \"Hiodon tergisus\")\nespece\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\n\nPour accéder aux éléments d’une liste, one appelle la liste suivie de la position de l’objet désiré entre crochets.\n\nespece[1]\n\n[1] \"Petromyzon marinus\"\n\nespece[2]\n\n[1] \"Lepisosteus osseus\"\n\nespece[1:3]\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n\nespece[c(1, 3)]\n\n[1] \"Petromyzon marinus\" \"Amia calva\"        \n\n\nOn peut noter que le premier élément de la liste est noté 1, et non 0 comme c’est le cas de la plupart de langages. Le raccourcis 1:3 crée une liste de nombres entiers de 1 à 3 inclusivement, c’est-à-dire l’équivalent de c(1, 2, 3). En effet, on crée une liste d’indices pour soutirer des éléments d’une liste. On peut utiliser le symbole de soustraction pour retirer un ou plusieurs éléments d’un vecteur.\n\nespece[-c(1, 3)]\n\n[1] \"Lepisosteus osseus\" \"Hiodon tergisus\"   \n\n\nPour ajouter un élément à notre liste, on peut utiliser la fonction c( ).\n\nespece &lt;- c(espece, \"Cyprinus carpio\")\nespece\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"    \"Cyprinus carpio\"   \n\n\nNotez que l’on efface l’objet espece par une concaténation de l’objet espece, précédemment définie, et d’un autre élément.\nEn lançant espece[3] &lt;- \"Lepomis gibbosus\", il est possible de changer un élément de la liste.\n\nespece[3] &lt;- \"Lepomis gibbosus\"\nespece\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Lepomis gibbosus\"  \n[4] \"Hiodon tergisus\"    \"Cyprinus carpio\"   \n\n\n\n2.4.2.2 Matrices\nUne matrice est un vecteur de dimension plus élevée que 1. En écologie, on dépasse rarement la deuxième dimension, quoi que les matrices en N dimensions soient courantes en modélisation mathématique. Je ne considérerai pour le moment que des matrices 2D. Comme c’est la cas des vecteurs, les matrices contiennent des valeurs de même type. En R, on peut attribuer aux matrices 2D des noms de ligne et de colonne.\n\nmat &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), \n              ncol = 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\n\ncolnames(mat) &lt;- c(\"A\", \"B\", \"C\")\nrownames(mat) &lt;- c(\"site_1\", \"site_2\", \"site_3\", \"site_4\")\nmat\n\n       A B  C\nsite_1 1 5  9\nsite_2 2 6 10\nsite_3 3 7 11\nsite_4 4 8 12\n\n\nOn peut soutirer les noms de colonne et les noms de ligne. Le résultat est un vecteur.\n\ncolnames(mat)\n\n[1] \"A\" \"B\" \"C\"\n\nrownames(mat)\n\n[1] \"site_1\" \"site_2\" \"site_3\" \"site_4\"\n\n\n\n2.4.2.3 Listes\nLes listes sont des collections hétérogènes dans lesquelles on peut placer les objets désirés, sans distinction: elles peuvent même inclure d’autres listes. Chacun des éléments de la liste peut être identifié par une clé.\n\nma_liste &lt;- list(\n  especes = c(\n    \"Petromyzon marinus\", \"Lepisosteus osseus\",\n    \"Amia calva\", \"Hiodon tergisus\"\n  ),\n  site = \"A101\",\n  stations_meteos = c(\"746583\", \"783786\", \"856363\")\n)\nma_liste\n\n$especes\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\n$site\n[1] \"A101\"\n\n$stations_meteos\n[1] \"746583\" \"783786\" \"856363\"\n\n\nLes éléments de la liste peuvent être soutirés par le nom de la clé ou par l’indice, de cette manière.\n\nma_liste$especes\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\nma_liste[[1]]\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\n\nExercice. Accéder au deuxième élément du vecteur d’espèces dans la liste ma_liste.\n\n2.4.2.4 Tableaux\nEnfin, le type de collection de données le plus important est le tableau, ou data.frame. Techniquement, il s’agit d’une liste composée de vecteurs de même longueur. Chaque colonne peut ainsi prendre un type de donnée indépendamment des autres colonnes.\n\ntableau &lt;- data.frame(\n  espece = c(\n    \"Petromyzon marinus\", \"Lepisosteus osseus\",\n    \"Amia calva\", \"Hiodon tergisus\"\n  ),\n  poids = c(10, 13, 21, 4),\n  longueur = c(35, 44, 50, 8)\n)\ntableau\n\n              espece poids longueur\n1 Petromyzon marinus    10       35\n2 Lepisosteus osseus    13       44\n3         Amia calva    21       50\n4    Hiodon tergisus     4        8\n\n\nEn programmation classique en R (nous verrons plus loin la méthode tidyverse), les éléments d’un tableau se manipulent comme ceux d’une matrice et les colonnes peuvent être appelés comme les éléments d’une liste.\n\ntableau[, 2:3]\n\n  poids longueur\n1    10       35\n2    13       44\n3    21       50\n4     4        8\n\ntableau$poids\n\n[1] 10 13 21  4\n\n\nVous verrez aussi, quoi que rarement, ce format, qui à la différence du format $ génère un tableau.\n\ntableau[\"poids\"]\n\n  poids\n1    10\n2    13\n3    21\n4     4\n\n\nLe tableau est le format de collection à privilégier pour manipuler des données. Récemment, le format de tableau tibble a été créé par l’équipe de RStudio pour offrir un format plus moderne.\n\n2.4.3 Les fonctions\nLorsque vous écrivez une commande suivit de parenthèses, comme data.frame(especes = ...), vous demandez à R de passer à l’action en appelant une fonction. De manière très générale, une fonction transforme quelque chose en quelque chose d’autre (Figure 2.8).\n\n\n\n\nFigure 2.8: Schéma simplifié d’une fonction.\n\n\n\nPar exemple, la fonction mean() prend une collection de nombre comme entrée, puis en sort vous devinez quoi.\n\nmean(tableau$poids)\n\n[1] 12\n\n\nLes entrées sont appelés les arguments de la fonction. Leur définition est toujours disponible dans la documentation.\nExercice. Familiarisez-vous avec la documentation de R en lançant ?mean. Truc: si vous avez pris de l’avance et que vous travaillez déjà en RStudio, mettez le terme en surbrillance, puis appuyez sur F1.\nVous verrez dans la documentation que la fonction mean() demande trois arguments, x, trim et na.rm. Or nous avons seulement placé un vecteur, sans spécifier d’argument!\nEn effet. En l’absence d’une définition des arguments, R supposera que les arguments dans la parenthèse, séparés par une virgule, sont présentés dans le même ordre que celui spécifié dans la définition de la fonction (celle qui est présentée dans le fichier d’aide). Dans le cas qui nous intéresse, mean(tableau$poids) est équivalent à mean(x = tableau$poids).\nMaintenant, selon la fiche d’aide, l’argument na.rm est un valeur logique spécifiant si oui (TRUE) ou non (FALSE) les valeurs manquantes doivent être considérées (une moyenne d’un vecteur comprenant au moins un NA sera de NA). En ne spécifiant rien, R prend la valeur par défaut, telle que spécifiée dans la documentation. Il en va de même pour l’argument trim, qui permet d’élaguer des valeurs extrêmes. Dans la fiche d’aide, mean(x, trim = 0, na.rm = FALSE, ...) signifie que par défaut, l’argument x est vide (il doit donc être spécifié), l’argument trim est de 0 et l’argument na.rm est FALSE.\n\nmean(c(6, 1, 7, 4, 9, NA, 1))\n\n[1] NA\n\nmean(c(6, 1, 7, 4, 9, NA, 1), na.rm = TRUE)\n\n[1] 4.666667\n\n\nVous n’êtes pas emprisonné par les fonctions offertes par R. Vous pouvez installer des modules qui complètent les fonctions de base de R: on le verra un peu plus loin dans ce chapitre. Mais pour l’instant, voyons comment vous pouvez créer vos propres fonctions. Disons que vous voulez créer une fonction qui calcule la sortie de \\(x^3-2y+a\\). Pour obtenir la réponse, on a besoin des arguments x, y et a. La sortie de la fonction est ici triviale: la réponse de l’équation. L’opération function permet de prendre ça en charge.\n\noperation_f &lt;- function(x, y, a = 10) {\n  return(x^3 - 2 * y + a)\n}\n\nNotez que a a une valeur par défaut. La sortie de la fonction est ce qui se trouve entre les parenthèses de return. Vous pouvez maintenant utiliser la fonction operation_f au besoin.\n\noperation_f(x = 2, y = 3, a = 1)\n\n[1] 3\n\n\nUne telle fonction est peu utile. Mais l’utilisation de fonctions personnalisées vous permettra d’éviter de répéter la même opération plusieurs fois dans un flux de travail, en évitant de générer trop de code, donc aussi de potentielles erreurs. Personnellement, j’utilise les fonctions surtout pour générer des graphiques personnalisés.\nExercice. Afin d’acquérir de l’autonomie, vous devrez être en mesure de trouver le nom des commandes dont vous avez besoin pour effectuer la tâche que vous désirez effectuer. Cela peut causer des frustrations, mais vous vous sentirez toujours plus à l’aise avec R jour après jour. L’exercice ici est de trouver par vous-même la commande qui vous permettra mesurer la longueur d’un vecteur.\n\n2.4.4 Les boucles\nLes boucles permettent d’effectuer une même suite d’opérations sur plusieurs objets. Pour faire suite à notre exemple, nous désirons obtenir le résultat de l’opération f pour des paramètres que nous enregistrons dans ce tableau.\n\nparams &lt;- data.frame(\n  x = c(2, 4, 1, 5, 6),\n  y = c(3, 4, 8, 1, 0),\n  a = c(6, 1, 8, 2, 5)\n)\nparams\n\n  x y a\n1 2 3 6\n2 4 4 1\n3 1 8 8\n4 5 1 2\n5 6 0 5\n\n\nNous créons un vecteur vide, puis nous effectuons une itération ligne par ligne en remplissant le vecteur.\n\noperation_res &lt;- c()\nfor (i in 1:nrow(params)) {\n  operation_res[i] &lt;- operation_f(x = params[i, 1], y = params[i, 2], a = params[i, 3])\n}\noperation_res\n\n[1]   8  57  -7 125 221\n\n\nEn faisant varier i sur des valeurs du vecteur donné par la séquence de nombres entiers de 1 au nombre de ligne du tableau de paramètres, nous demandons à R d’effectuer la suite d’opération entre les accolades {}. À chaque boucle, i prend une valeur de la séquence. i est utilisé ici comme indice de la ligne à soutirer du tableau params, qui correspond à l’indice dans le vecteur operation_res.\nAinsi, chaque résultat est calculé dans l’ordre des lignes du tableau de paramètres et l’on pourra très bien y coller nos résultats:\n\nparams$resultats &lt;- operation_res\nparams\n\n  x y a resultats\n1 2 3 6         8\n2 4 4 1        57\n3 1 8 8        -7\n4 5 1 2       125\n5 6 0 5       221\n\n\nNotez que puisque la colonne resultat n’existe pas dans le tableau params, R crée automatiquement une nouvelle colonne.\nLes boucles for vous permettront par exemple de générer en peu de temps 10, 100, 1000 graphiques (autant que vous voulez), chacun issu de simulations obtenues à partir de conditions initiales différentes, et de les enregistrer dans un répertoire sur votre ordinateur. Un travail qui pourrait prendre des semaines sur Excel peut être effectué en R en quelques secondes.\nUn second outil est disponible pour les itérations: les boucles while. Elles effectuent une opération tant qu’un critère n’est pas atteint. Elles sont utiles pour les opérations où l’on cherche une convergence. Je les couvre rapidement puisqu’elles sont rarement utilisées dans les flux de travail courants. En voici un petit exemple.\n\nx &lt;- 100\nwhile (x &gt; 1.1) {\n  x &lt;- sqrt(x)\n  print(x)\n}\n\n[1] 10\n[1] 3.162278\n[1] 1.778279\n[1] 1.333521\n[1] 1.154782\n[1] 1.074608\n\n\nNous avons initié x à une valeur de 100. Puis, tant que (while) le test x &gt; 1.1 est vrai, attribuer à x la nouvelle valeur calculée en extrayant la racine de la valeur précédente de x. Enfin, indiquer la valeur avec print.\n\n2.4.5 Conditions: if, else if, else\n\n\nSi la condition 1 est remplie, effectuer une suite d’instructions 1. Si la condition 1 n’est pas remplie, et si la condition 2 est remplie, effectuer la suite d’instructions 2. Sinon, effectuer la suite d’instruction 3.\n\nVoilà comment on exprime une suite de conditions. Prenons l’exemple simple d’une discrétisation d’une valeur continue. Si \\(x&lt;10\\), il est classé comme faible. Si \\(10 \\leq x &lt;20\\), il est classé comme moyen. Si \\(x \\geq 20\\), il est classé comme élevé. Plaçons cette classification dans une fonction.\n\nclassification &lt;- function(x, lim1 = 10, lim2 = 20) {\n  if (x &lt; lim1) {\n    categorie &lt;- \"faible\"\n  } else if (x &lt; lim2) {\n    categorie &lt;- \"moyen\"\n  } else {\n    categorie &lt;- \"élevé\"\n  }\n  return(categorie)\n}\nclassification(-10)\n\n[1] \"faible\"\n\nclassification(15.4)\n\n[1] \"moyen\"\n\nclassification(1000)\n\n[1] \"élevé\"\n\n\nUne condition est définie avec le if, suivi du test à vrai ou faux entre parenthèses. Si le test retourne un vrai (TRUE), l’instruction entre accolades est exécutée. Si elle est fausse, on passe au suivant.\nExercice. Explorer les commandes ifelse et cut et réfléchissez à la manière qu’elles pourraient être utilisées pour effectuer une discrétisation plus efficacement qu’avec les if et les else.\n\n2.4.6 Installer et charger un module\nLa plupart des opérations d’ordre général (comme les racines carrées, les tests statistiques, la gestion de matrices et de tableau, les graphiques, etc.) sont accessibles grâce aux modules de base de R, qui sont installés et chargés par défaut lors du démarrage de R. Des équipes de travail ont néanmoins développé plusieurs modules pour répondre à leurs besoins spécialisés, et les ont laissés disponibles au grand public dans des modules que vous pouvez installer d’un dépôt CRAN (le AppStore de R), d’un dépôt Anaconda (le AppStore de Anaconda, si vous utilisez cette plate-forme), d’un dépôt Github (dépôts décentralisés), etc.\nRStudio possède un pratique bouton Install qui vous permet d’y inscrire une liste de modules. Le navigateur anaconda offre aussi une interface d’installation. La commande R pour installer un module est install.packages(\"ggplot2\"), si par exemple vous désirez installer ggplot2, le module graphique par excellence en R. C’est la commande que RStudio lancera tout seul si vous lui demandez d’installer ggplot2.\nLes modules sont l’équivalent des applications spécialisées que vous installez sur un téléphone mobile. Pour les utiliser, il faut les ouvrir.\nGénéralement, j’ouvre toutes les applications nécessaires à mon flux de travail au tout début de ma feuille de calcul (la prochaine cellule retournera un message d’erreur si les packages ne sont pas installés).\n\nlibrary(\"tidyverse\") # méta-package qui charge entre autres dplyr et ggplot2\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"vegan\")\n\nLoading required package: permute\nLoading required package: lattice\nThis is vegan 2.6-4\n\nlibrary(\"nlme\")\n\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nLes modules sont installés sur votre ordinateur à un endroit que vous pourrez retrouver avec la commande .libPaths()\nExercice. À partir d’ici jusqu’à la fin du cours, nous utiliserons RStudio. Ouvrez-le et familiarisez-vous avec l’interface! Quelques petits trucs:\n\npour lancer une ligne, placez votre curseur sur la ligne, puis appuyez sur Ctrl+Enter\npour lancer une partie de code précise, mettez le en surbrillance, puis Ctrl+Enter\nutilisez toujours le gestionnaire de projets, en haut à droite!\ninstallez le module tidyverse\n\nlancez data(\"iris\") pour obtenir un tableau d’exercice, puis cliquez sur l’objet dans la fenêtre environnement"
  },
  {
    "objectID": "02-R.html#enfin",
    "href": "02-R.html#enfin",
    "title": "2  La science des données avec R",
    "section": "\n2.5 Enfin…",
    "text": "2.5 Enfin…\nComme une langue, on n’apprend à s’exprimer en un langage informatique qu’en se mettant à l’épreuve, ce que vous ferez tout au long de ce cours. Pour vous encourager, voici quelques trucs pour apprendre à coder en R.\n\n\nR n’aime pas l’ambiguïté. Une simple virgule mal placée et il ne sait plus quoi faire. Cela peut être frustrant au début, mais cette rigidité est nécessaire pour effectuer du calcul scientifique.\n\nLe copier-coller est votre ami. En gardant à l’esprit que vous être responsable de votre code et que vous respectez les droits d’auteur, n’ayez pas peur de copier-coller des lignes de code et de personnaliser par la suite.\n\nL’erreur que vous obtenez: d’autres l’ont obtenue avant vous. Le site de question-réponse stackoverflow est une ressource inestimable où des gens ayant posté des questions ont reçu des réponses d’experts (les meilleures réponses et les meilleures questions apparaissent en premier). Apprenez à chercher intelligemment des réponses en formulant précisément vos questions!\n\nÉtudiez et pratiquez. Les messages d’erreur en R sont courants, même chez les personnes expérimentées. La meilleure manière d’apprendre une langue est de la parler, d’étudier ses susceptibilités, de les tester dans une conversation, etc."
  },
  {
    "objectID": "02-R.html#petit-truc",
    "href": "02-R.html#petit-truc",
    "title": "2  La science des données avec R",
    "section": "\n2.6 Petit truc!",
    "text": "2.6 Petit truc!\nRStudio peut être implémenté avec des extensions. L’une d’elle permet d’ajuster votre style de code. Par exemple, vous voulez vous assurer que toutes les allocations sont bien effectuées avec des &lt;- et non pas des =, qu’il y a bien des espaces de part et d’autre de &lt;-, que les retours de lignes sont bien placés, etc. Installez le module styler, et des options apparaîtront dans le menu Addins comme à la Figure 2.9.\n\n\n\n\nFigure 2.9: L’extension styler permet de formater votre code dans un style particulier"
  },
  {
    "objectID": "02-R.html#extra-jupyter",
    "href": "02-R.html#extra-jupyter",
    "title": "2  La science des données avec R",
    "section": "\n2.7 Extra: Utiliser R avec Jupyter",
    "text": "2.7 Extra: Utiliser R avec Jupyter\nPour utiliser R dans Jupyter notebook ou Jupyter lab, vous devez installer le module IRkernel dans la version de R que vous désirez utiliser avec Jupyter, puis de lancer la commande IRkernel::installspec(). La prochaine fois que vous ouvrirez Jupyter, le noyau de R devrait apparaître.\nJe n’ai aucune expérience sur Mac, mais semble-t-il cela fonctionne comme en Linux. Ouvrez R à partir d’un terminal (R + Enter), puis lancez IRkernel::installspec() après avoir installé IRkernel. Si vous travaillez en Windows, il vous faudra lancer R par son chemin complet dans l’invite de commande de Anaconda (Anaconda Powershell Prompt). Par exemple, ouvrir Anaconda Powershell Prompt, puis, si votre installation de R se trouve dans C:\\Program Files\\R\\R-3.6.2,\n(base) PS C:\\Users\\fifi&gt; cd \"C:\\Program Files\\R\\R-3.6.2\\bin\"\n(base) PS C:\\Program Files\\R\\R-3.6.2\\bin&gt; .\\R.exe\n\nR version 3.6.2 (2019-12-12) -- \"Dark and Stormy Night\"\nCopyright (C) 2019 The R Foundation for Statistical Computing\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\n\nR est un logiciel libre livré sans AUCUNE GARANTIE.\nVous pouvez le redistribuer sous certaines conditions.\nTapez 'license()' ou 'licence()' pour plus de détails.\n\nR est un projet collaboratif avec de nombreux contributeurs.\nTapez 'contributors()' pour plus d'information et\n'citation()' pour la façon de le citer dans les publications.\n\nTapez 'demo()' pour des démonstrations, 'help()' pour l'aide\nen ligne ou 'help.start()' pour obtenir l'aide au format HTML.\nTapez 'q()' pour quitter R.\n\n&gt; install.packages(\"IRkernel\")\nInstallation du package dans 'C:/Users/fifi/Documents/R/win-library/3.6'\n(car 'lib' n'est pas spécifié)\n--- SVP sélectionner un miroir CRAN pour cette session ---\nessai de l'URL 'https://cloud.r-project.org/bin/windows/contrib/3.6/IRkernel_1.1.zip'\nContent type 'application/zip' length 138696 bytes (135 KB)\ndownloaded 135 KB\n\nle package 'IRkernel' a été décompressé et les sommes MD5 ont été vérifiées avec succés\n\nLes packages binaires téléchargés sont dans\n       C:\\Users\\fifi\\AppData\\Local\\Temp\\Rtmp6xJtB3\\downloaded_packages\n\n&gt; IRkernel::installspec()\n[InstallKernelSpec] Installed kernelspec ir in C:\\Users\\fifi\\AppData\\Roaming\\jupyter\\kernels\\ir\n&gt; qui()"
  },
  {
    "objectID": "03-tableaux.html#les-collections-de-données",
    "href": "03-tableaux.html#les-collections-de-données",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.1 Les collections de données",
    "text": "3.1 Les collections de données\nDans le chapitre 2, nous avons survolé différents types d’objets : réels, entiers, chaînes de caractères et booléens. Les données peuvent appartenir à d’autres types : dates, catégories ordinales (ordonnées : faible, moyen, élevé) et nominales (non ordonnées : espèces, cultivars, couleurs, unité pédologique, etc.). Comme mentionné en début de chapitre, une donnée est une valeur associée à une variable. Les données peuvent être organisées en collections.\nNous avons aussi vu au chapitre 2 que la manière privilégiée d’organiser des données était sous forme de tableaux. De manière générale, un tableau de données est une organisation de données en deux dimensions, comportant des lignes et des colonnes. Il est préférable de respecter la convention selon laquelle les lignes sont des observations et les colonnes sont des variables. Ainsi, un tableau est une liste de vecteurs de même longueur, chaque vecteur représentant une variable. Chaque variable est libre de prendre le type de données approprié. La position d’une donnée dans le vecteur correspond à une observation. Lorsque les vecteurs sont posés les uns à côté des autres, la position dans le vecteur devient une ligne qui définit les valeurs des variables d’une observation.\nImaginez que vous consignez des données météorologiques comme les précipitations totales ou la température moyenne pour chaque jour, pendant une semaine sur les sites A, B et C. Chaque site possède ses propres caractéristiques, comme la position en longitude-latitude. Il est redondant de répéter la position du site pour chaque jour de la semaine. Vous préférerez créer deux tableaux : un pour décrire vos observations, et un autre pour décrire les sites. De cette manière, vous créez une collection de tableaux interreliés : une base de données. Nous couvrirons cette notion un peu plus loin. R peut soutirer des données des bases de données grâce au module DBI, qui n’est pas couvert à ce stade de développement du cours.\nDans R, les données structurées en tableaux, ainsi que les opérations sur les tableaux, peuvent être gérées grâce aux modules readr, dplyr et tidyr, tous des modules faisant partie du méta-module tidyverse, qui est un genre de Microsoft Office sur R : plusieurs modules fonctionnant en interopérabilité. Mais avant de se lancer dans l’utilisation de ces modules, voyons quelques règles à suivre pour bien structurer ses données en format tidy, un jargon du tidyverse qui signifie proprement organisé."
  },
  {
    "objectID": "03-tableaux.html#organiser-un-tableau-de-données",
    "href": "03-tableaux.html#organiser-un-tableau-de-données",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.2 Organiser un tableau de données",
    "text": "3.2 Organiser un tableau de données\nAfin de repérer chaque cellule d’un tableau, on attribue à chaque ligne et à chaque colonne un identifiant unique, que l’on nomme indice pour les lignes et entête pour les colonnes.\n\nRègle no 1. Une variable par colonne, une observation par ligne, une valeur par cellule.\n\nLes unités expérimentales sont décrites par une ou plusieurs variables, par des chiffres ou des lettres. Chaque variable devrait être présente en une seule colonne, et chaque ligne devrait correspondre à une unité expérimentale où ces variables ont été mesurées. La règle parait simple, mais elle est rarement respectée. Prenez par exemple le tableau suivant.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nTable 3.1: Rendements obtenus sur les sites expérimentaux selon les traitements.\n\nSite\nTraitement A\nTraitement B\nTraitement C\n\n\n\nSainte-Souris\n4.1\n8.2\n6.8\n\n\nSainte-Fourmi\n5.8\n5.9\nNA\n\n\nSaint-Ours\n2.9\n3.4\n4.6\n\n\n\n\n\n\nQu’est-ce qui cloche avec ce tableau? Chaque ligne est une observation, mais contient plusieurs observations d’une même variable, le rendement, qui devient étalé sur plusieurs colonnes. À bien y penser, le type de traitement est une variable et le rendement en est une autre:\n\n\n\n\nTable 3.2: Rendements obtenus sur les sites expérimentaux selon les traitements.\n\nSite\nTraitement\nRendement\n\n\n\nSainte-Souris\nTraitement A\n4.1\n\n\nSainte-Souris\nTraitement B\n8.2\n\n\nSainte-Souris\nTraitement C\n6.8\n\n\nSainte-Fourmi\nTraitement A\n5.8\n\n\nSainte-Fourmi\nTraitement B\n5.9\n\n\nSainte-Fourmi\nTraitement C\nNA\n\n\nSaint-Ours\nTraitement A\n2.9\n\n\nSaint-Ours\nTraitement B\n3.4\n\n\nSaint-Ours\nTraitement C\n4.6\n\n\n\n\n\n\nPlus précisément, l’expression à bien y penser suggère une réflexion sur la signification des données. Certaines variables peuvent parfois être intégrées dans une même colonne, parfois pas. Par exemple, les concentrations en cuivre, zinc et plomb dans un sol contaminé peuvent être placées dans la même colonne “Concentration” ou déclinées en plusieurs colonnes Cu, Zn et Pb. La première version trouvera son utilité pour créer des graphiques (chapitre 4), alors que la deuxième favorise le traitement statistique (chapitre 7). Il est possible de passer d’un format à l’autre grâce à la fonction pivot_longer() et pivot_wider() du module tidyr.\n\nRègle no 2. Un tableau par unité observationnelle: ne pas répéter les informations.\n\nReprenons la même expérience. Supposons que vous mesurez la précipitation à l’échelle du site.\n\n\n\n\nTable 3.3: Rendements et précipitations obtenus sur les sites expérimentaux selon les traitements.\n\nSite\nTraitement\nRendement\nPrécipitations\n\n\n\nSainte-Souris\nTraitement A\n4.1\n813\n\n\nSainte-Souris\nTraitement B\n8.2\n813\n\n\nSainte-Souris\nTraitement C\n6.8\n813\n\n\nSainte-Fourmi\nTraitement A\n5.8\n642\n\n\nSainte-Fourmi\nTraitement B\n5.9\n642\n\n\nSainte-Fourmi\nTraitement C\nNA\n642\n\n\nSaint-Ours\nTraitement A\n2.9\n1028\n\n\nSaint-Ours\nTraitement B\n3.4\n1028\n\n\nSaint-Ours\nTraitement C\n4.6\n1028\n\n\n\n\n\n\nSegmenter l’information en deux tableaux serait préférable.\n\n\n\n\nTable 3.4: Précipitations sur les sites expérimentaux.\n\nSite\nPrécipitations\n\n\n\nSainte-Souris\n813\n\n\nSainte-Fourmi\n642\n\n\nSaint-Ours\n1028\n\n\n\n\n\n\nLes tableaux Table 3.2 et Table 3.4, ensemble, forment une base de données (collection organisée de tableaux). Les opérations de fusion entre les tableaux peuvent être effectuées grâce aux fonctions de jointure (left_join(), par exemple) du module tidyr. Une jointure de Table 3.4 vers Table 3.2 donnera le tableau Table 3.3.\n\nRègle no 3. Ne pas bousiller les données.\n\nPar exemple.\n\n\nAjouter des commentaires dans des cellules. Si une cellule mérite d’être commentée, il est préférable de placer les commentaires soit dans un fichier décrivant le tableau de données, soit dans une colonne de commentaire juxtaposée à la colonne de la variable à commenter. Par exemple, si vous n’avez pas mesuré le pH pour une observation, n’écrivez pas “échantillon contaminé” dans la cellule, mais annoter dans un fichier d’explication que l’échantillon no X a été contaminé. Si les commentaires sont systématiques, il peut être pratique de les inscrire dans une colonne commentaire_pH.\n\nInscription non systématique. Il arrive souvent que des catégories d’une variable ou que des valeurs manquantes soient annotées différemment. Il arrive même que le séparateur décimal soit non systématique, parfois noté par un point, parfois par une virgule. Par exemple, une fois importés dans votre session, les catégories St-Ours et Saint-Ours seront traitées comme deux catégories distinctes. De même, les cellules correspondant à des valeurs manquantes ne devraient pas être inscrites parfois avec une cellule vide, parfois avec un point, parfois avec un tiret ou avec la mention NA. Le plus simple est de laisser systématiquement ces cellules vides.\n\nInclure des notes dans un tableau. La règle “une colonne, une variable” n’est pas respectée si on ajoute des notes un peu n’importe où sous ou à côté du tableau.\n\nAjouter des sommaires. Si vous ajoutez une ligne sous un tableau comprenant la moyenne de chaque colonne, qu’est-ce qui arrivera lorsque vous importerez votre tableau dans votre session de travail? La ligne sera considérée comme une observation supplémentaire.\n\nInclure une hiérarchie dans les entêtes. Afin de consigner des données de texture du sol, comprenant la proportion de sable, de limon et d’argile, vous organisez votre entête en plusieurs lignes. Une ligne pour la catégorie de donnée, Texture, fusionnée sur trois colonnes, puis trois colonnes intitulées Sable, Limon et Argile. Votre tableau est joli, mais il ne pourra pas être importé conformément dans un votre session de calcul : on recherche une entête unique par colonne. Votre tableau de données devrait plutôt porter les entêtes Texture sable, Texture limon et Texture argile. Un conseil : réserver le travail esthétique à la toute fin d’un flux de travail."
  },
  {
    "objectID": "03-tableaux.html#formats-de-tableau",
    "href": "03-tableaux.html#formats-de-tableau",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.3 Formats de tableau",
    "text": "3.3 Formats de tableau\nPlusieurs outils sont à votre disposition pour créer des tableaux. Je vous présente ici les plus communs.\n\n3.3.1 xls ou xlsx\n\nMicrosoft Excel est un logiciel de type tableur, ou chiffrier électronique. L’ancien format xls a été remplacé par le format xlsx avec l’arrivée de Microsoft Office 2010. Il s’agit d’un format propriétaire, dont l’alternative libre la plus connue est le format ods, popularisé par la suite bureautique LibreOffice. Les formats xls, xlsx ou ods sont davantage utilisés comme outils de calcul que d’entreposage de données. Ils contiennent des formules, des graphiques, du formatage de cellule, etc. Je ne les recommande pas pour stocker des données.\n\n3.3.2 csv\n\nLe format csv, pour comma separated values, est un fichier texte, que vous pouvez ouvrir avec n’importe quel éditeur de texte brut (Bloc note, VSCode, Notepad++, etc.). Chaque colonne doit être délimitée par un caractère cohérent (conventionnellement une virgule, mais en français un point-virgule ou une tabulation pour éviter la confusion avec le séparateur décimal) et chaque ligne du tableau est un retour de ligne. Il est possible d’ouvrir et d’éditer les fichiers csv dans un éditeur texte, mais il est plus pratique de les ouvrir avec des tableurs (LibreOffice Calc, Microsoft Excel, Google Sheets, etc.).\nEncodage des fichiers texte. Puisque le format csv est un fichier texte, un souci particulier doit être porté sur la manière dont le texte est encodé. Les caractères accentués pourraient être importés incorrectement si vous importez votre tableau en spécifiant le mauvais encodage. Pour les fichiers en langues occidentales, l’encodage UTF-8 devrait être utilisé. Toutefois, par défaut, Excel utilise un encodage de Microsoft. Si le csv a été généré par Excel, il est préférable de l’ouvrir avec votre éditeur texte et de l’enregistrer dans l’encodage UTF-8.\n\n3.3.3 json\n\nComme le format csv, le format json indique un fichier en texte clair. En permettant des structures de tableaux emboîtés et en ne demandant pas que chaque colonne ait la même longueur, le format json permet plus de souplesse que le format csv, mais il est plus compliqué à consulter et prend davantage d’espace sur le disque que le csv. Il est utilisé davantage pour le partage de données des applications web, mais en ce qui concerne la matière du cours, ce format est surtout utilisé pour les données géoréférencées. L’encodage est géré de la même manière qu’un fichier csv.\n\n3.3.4 SQLite\nSQLite est une application pour les bases de données relationnelles de type SQL qui n’a pas besoin de serveur pour fonctionner. Les bases de données SQLite sont encodés dans des fichiers portant l’extension db, qui peuvent être facilement partagés.\n\n3.3.5 Suggestion\nEn csv pour les petits tableaux, en sqlite pour les bases de données plus complexes. Ce cours se concentre toutefois sur les données de type csv."
  },
  {
    "objectID": "03-tableaux.html#entreposer-ses-données",
    "href": "03-tableaux.html#entreposer-ses-données",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.4 Entreposer ses données",
    "text": "3.4 Entreposer ses données\nLa manière la plus sécurisée pour entreposer ses données est de les confiner dans une base de données sécurisée sur un serveur sécurisé dans un environnement sécurisé et d’encrypter les communications. C’est aussi… la manière la moins accessible. Des espaces de stockage nuagiques, comme Dropbox ou d’autres options similaires, peuvent être pratiques pour les backups et le partage des données avec une équipe de travail (qui risque en retour de bousiller vos données). Le suivi de version est possible chez certains fournisseurs d’espace de stockage. Mais pour un suivi de version plus rigoureux, les espaces de développement (comme GitHub et GitLab) sont plus appropriés (couverts au chapitre 5). Dans tous les cas, il est important de garder (1) des copies anciennes pour y revenir en cas d’erreurs et (2) un petit fichier décrivant les changements effectués sur les données."
  },
  {
    "objectID": "03-tableaux.html#manipuler-des-données-en-mode-tidyverse",
    "href": "03-tableaux.html#manipuler-des-données-en-mode-tidyverse",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.5 Manipuler des données en mode tidyverse",
    "text": "3.5 Manipuler des données en mode tidyverse\nLe méta-module tidyverse regroupe une collection de précieux modules pour l’analyse de données en R. Il permet d’importer des données dans votre session de travail avec readr, de les explorer avec le module de visualisation ggplot2, de les transformer avec tidyr et dplyr et de les exporter avec readr. Les tableaux de classe data.frame, comme ceux de la plus moderne classe tibble, peuvent être manipulés à travers le flux de travail pour l’analyse et la modélisation. Comme ce sera le cas pour le chapitre sur la visualisation, ce chapitre est loin de couvrir les nombreuses fonctionnalités qui sont offertes dans le tidyverse.\n\n3.5.1 Importer vos données dans votre session de travail\nSupposons que vous avez bien organisé vos données en mode tidy. Pour les importer dans votre session et commencer à les inspecter, vous lancerez une des commandes du module readr, décrites dans la documentation dédiée.\n\n\nread_csv() si le séparateur de colonne est une virgule\n\nread_csv2() si le séparateur de colonne est un point-virgule et que le séparateur décimal est une virgule\n\nread_tsv() si le séparateur de colonne est une tabulation\n\nread_table() si le séparateur de colonne est un espace blanc\n\nread_delim() si le séparateur de colonne est un autre caractère (comme le point-virgule) que vous spécifierez dans l’argument delim = \";\"\n\n\nLes principaux arguments sont les suivants.\n\n\nfile: le chemin vers le fichier. Ce chemin peut aussi bien être une adresse locale (data/…) qu’une adresse internet (https://…).\n\ndelim: le symbole délimitant les colonnes dans le cas de read_delim.\n\ncol_names: si TRUE, la première ligne est l’entête du tableau, sinon FALSE. Si vous spécifiez un vecteur numérique, ce sont les numéros des lignes utilisées pour le nom de l’entête. Si vous utilisez un vecteur de caractères, ce sont les noms des colonnes que vous désirez donner à votre tableau.\n\nna: le symbole spécifiant une valeur manquante. L’argument na='' signifie que les cellules vides sont des données manquantes. Si les valeurs manquantes ne sont pas uniformes, vous pouvez les indiquer dans un vecteur, par exemple na = c(\"\", \"NA\", \"NaN\", \".\", \"-\").\n\nlocal: cet argument prend une fonction local() qui peut inclure des arguments de format de temps, mais aussi d’encodage (voir documentation)\n\nD’autres arguments peuvent être spécifiés au besoin, et les répéter ici dupliquerait l’information de la documentation de la fonction read_csv de readr.\nJe déconseille d’importer des données en format xls ou xlsx. Si toutefois cela vous convient, je vous réfère au module readxl.\nL’aide-mémoire de readr (Figure 3.1) est à afficher près de soi.\n\n\n\n\nFigure 3.1: Aide-mémoire de readr, Source: https://rstudio.github.io/cheatsheets/data-import.pdf\n\n\n\nNous allons charger des données de culture de la chicouté (Rubus chamaemorus), un petit fruit nordique, tiré de Parent et al. (2013). Ouvrons d’abord le fichier pour vérifier les séparateurs de colonnes et de décimales (Figure 3.2).\n\n\n\n\nFigure 3.2: Aperçu brut d’un fichier csv.\n\n\n\nLe séparateur de colonnes est un point-virgule et le décimal est une virgule.\nAvec Atom, mon éditeur texte préféré (il y en a d’autres), je vais dans Edit &gt; Select Encoding et j’obtiens bien le UTF-8 (Figure 3.3).\n\n\n\n\nFigure 3.3: Changer l’encodage d’un fichier csv.\n\n\n\nNous allons donc utiliser read_csv2() avec ses arguments par défaut.\n\nlibrary(\"tidyverse\")\nchicoute &lt;- read_csv2(\"data/chicoute.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 90 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): CodeTourbiere, Ordre, Traitement, DemiParcelle, SousTraitement\ndbl (26): ID, Site, Latitude_m, Longitude_m, Rendement_g_5m2, TotalRamet_nom...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nQuelques commandes utiles inspecter le tableau:\n\n\nhead() présente l’entête du tableau, soit ses 6 premières lignes\n\nstr() et glimpse() présentent les variables du tableau et leur type - glimpse()est la fonction tidyverse et str() est la fonction classique (je préfère str())\n\nsummary() présente des statistiques de base du tableau\n\nnames() ou colnames() sort les noms des colonnes sous forme d’un vecteur\n\ndim() donne les dimensions du tableau, ncol() son nombre de colonnes et nrow() son nombre de lignes\n\nskim est une fonction du module skimr montrant un portrait graphique et numérique du tableau\n\nExtra 1. Plusieurs modules ne se trouvent pas dans les dépôts CRAN, mais sont disponibles sur GitHub. Pour les installer, installez d’abord le module devtools disponible sur CRAN. Vous pourrez alors installer les packages de GitHub comme on le fait avec le package skimr.\nExtra 2. Lorsque je désire utiliser une fonction, mais sans charger le module dans la session, j’utilise la notation module::fonction. Comme dans ce cas, pour skimr.\n\nskimr::skim(chicoute)\n\n\nData summary\n\n\nName\nchicoute\n\n\nNumber of rows\n90\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n26\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nCodeTourbiere\n0\n1.00\n1\n4\n0\n12\n0\n\n\nOrdre\n0\n1.00\n1\n2\n0\n20\n0\n\n\nTraitement\n50\n0.44\n6\n11\n0\n2\n0\n\n\nDemiParcelle\n50\n0.44\n4\n5\n0\n2\n0\n\n\nSousTraitement\n50\n0.44\n1\n7\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n45.50\n26.12\n1.00\n23.25\n45.50\n67.75\n90.00\n▇▇▇▇▇\n\n\nSite\n0\n1.00\n6.33\n5.49\n1.00\n2.00\n4.00\n9.00\n20.00\n▇▃▁▁▁\n\n\nLatitude_m\n0\n1.00\n5701839.86\n1915.50\n5695688.00\n5701868.50\n5702129.00\n5702537.00\n5706394.00\n▁▂▅▇▁\n\n\nLongitude_m\n0\n1.00\n485295.54\n6452.33\n459873.00\n485927.00\n486500.00\n486544.75\n491955.00\n▁▁▁▂▇\n\n\nRendement_g_5m2\n50\n0.44\n13.33\n21.56\n0.00\n0.00\n0.95\n15.63\n72.44\n▇▁▁▁▁\n\n\nTotalRamet_nombre_m2\n0\n1.00\n251.26\n156.06\n40.74\n122.70\n212.92\n347.80\n651.90\n▇▇▃▂▂\n\n\nTotalVegetatif_nombre_m2\n4\n0.96\n199.02\n139.13\n22.92\n86.26\n161.25\n263.78\n580.60\n▇▇▂▂▁\n\n\nTotalFloral_nombre_m2\n4\n0.96\n52.08\n40.41\n4.80\n22.92\n43.00\n69.52\n198.62\n▇▅▂▁▁\n\n\nTotalMale_nombre_m2\n4\n0.96\n24.40\n26.87\n0.00\n3.30\n15.28\n36.51\n104.41\n▇▂▂▁▁\n\n\nTotalFemelle_nombre_m2\n4\n0.96\n27.53\n29.83\n2.55\n10.34\n17.19\n31.96\n187.17\n▇▁▁▁▁\n\n\nFemelleFruit_nombre_m2\n18\n0.80\n19.97\n23.79\n0.40\n7.64\n11.46\n22.83\n157.88\n▇▂▁▁▁\n\n\nFemelleAvorte_nombre_m2\n4\n0.96\n8.49\n14.52\n0.00\n1.27\n3.07\n10.14\n76.80\n▇▁▁▁▁\n\n\nSterileFleur_nombre_m2\n4\n0.96\n0.26\n0.71\n0.00\n0.00\n0.00\n0.00\n3.82\n▇▁▁▁▁\n\n\nC_pourc\n0\n1.00\n50.28\n1.61\n46.72\n49.14\n50.45\n51.58\n53.83\n▃▆▅▇▁\n\n\nN_pourc\n0\n1.00\n2.20\n0.40\n1.53\n1.89\n2.12\n2.58\n3.10\n▃▇▃▃▂\n\n\nP_pourc\n0\n1.00\n0.14\n0.04\n0.07\n0.12\n0.14\n0.16\n0.23\n▃▆▇▂▂\n\n\nK_pourc\n0\n1.00\n0.89\n0.27\n0.35\n0.69\n0.86\n1.13\n1.54\n▃▇▇▇▁\n\n\nCa_pourc\n0\n1.00\n0.39\n0.10\n0.19\n0.32\n0.37\n0.44\n0.88\n▅▇▂▁▁\n\n\nMg_pourc\n0\n1.00\n0.50\n0.08\n0.36\n0.45\n0.48\n0.52\n0.86\n▇▇▂▁▁\n\n\nS_pourc\n0\n1.00\n0.13\n0.04\n0.07\n0.11\n0.13\n0.14\n0.28\n▅▇▂▁▁\n\n\nB_pourc\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▂▅▃▇▃\n\n\nCu_pourc\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▇▁▁▁▁\n\n\nZn_pourc\n0\n1.00\n0.01\n0.00\n0.00\n0.01\n0.01\n0.01\n0.02\n▇▇▂▁▁\n\n\nMn_pourc\n0\n1.00\n0.03\n0.03\n0.00\n0.01\n0.03\n0.05\n0.10\n▇▅▃▂▁\n\n\nFe_pourc\n0\n1.00\n0.02\n0.01\n0.01\n0.01\n0.01\n0.02\n0.05\n▇▂▁▁▁\n\n\nAl_pourc\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n▇▅▁▁▁\n\n\n\n\n\nExercice. Inspectez le tableau.\n\n3.5.2 Comment sélectionner et filtrer des données ?\nOn utilise le terme sélectionner lorsque l’on désire choisir une ou plusieurs lignes et colonnes d’un tableau (la plupart du temps des colonnes). L’action de filtrer signifie de sélectionner des lignes selon certains critères.\n\n3.5.2.1 Sélectionner\nVoici 4 manières de sélectionner une colonne en R.\n\nUne méthode rapide mais peu expressive consiste à indiquer les valeurs numériques de l’indice de la colonne entre des crochets. Il s’agit d’appeler le tableau suivi de crochets. L’intérieur des crochets comprend deux éléments séparés par une virgule. Le premier élément sert à filtrer selon l’indice, le deuxième sert à sélectionner selon l’indice. Ainsi:\n\n\nchicoute[, 1]: sélectionner la première colonne\n\nchicoute[, 1:10]: sélectionner les 10 premières colonnes\n\nchicoute[, c(2, 4, 5)]: sélectionner les colonnes 2, 4 et 5\n\nchicoute[c(10, 13, 20), c(2, 4, 5)]: sélectionner les colonnes 2, 4 et 5 et les lignes 10, 13 et 20.\n\n\nUne autre méthode rapide, mais plus expressive, consiste à appeler le tableau, suivi du symbole $, puis le nom de la colonne, e.g. chicoute$Site.\n\n\nTruc. La plupart des IDE, comme RStudio, peuvent vous proposer des colonnes dans une liste. Après avoir saisi le $, taper sur la touche de tabulation: vous pourrez sélectionner la colonne dans une liste défilante (Figure 3.4).\n\n\n\n\n\nFigure 3.4: Autocomplétion dans RStudio.\n\n\n\n\nVous pouvez aussi inscrire le nom de la colonne, ou du vecteur des colonnes, entre des crochets suivant le nom du tableau, c’est-à-dire chicoute[c(\"Site\", \"Latitude_m\", \"Longitude_m\")].\nEnfin, dans une séquence d’opérations en mode pipeline (chaque opération est mise à la suite de la précédente en plaçant le pipe |&gt; entre chacune), il peut être préférable de sélectionner des colonnes avec la fonction select(), i.e.\n\n\nchicoute |&gt; \n  select(Site, Latitude_m, Longitude_m)\n\n\nNote sur le mode pipeline : Le pipe |&gt; a été introduit dans R-base en 2021. Auparavant, on utilisait la fonction %&gt;% introduite dans le module magrittr, inclus dans tidyverse. La plupart du temps, les deux fonctionnent sensiblement de la même façon, mais il existe quelques différences dans leur interaction avec certaines fonctions. Puisque |&gt; fait partie de R-base, je vous suggère de l’utiliser par défaut, mais il est fort probable que vous trouviez l’ancienne version %&gt;% lors de vos recherches sur internet (ou même dans ce guide si j’oublie d’effectuer les modifications). Pour insérer un pipe, il suffit d’utiliser le raccourci clavier Ctrl + Shift + M. Vous pouvez modifier la forme par défaut dans les options de RStudio, comme sur la Figure 3.5.\n\n\n\n\n\nFigure 3.5: Modifier le pipe par défaut dans RStudio.\n\n\n\nLa fonction select() permet aussi de travailler en exclusion. Ainsi pour enlever des colonnes, on placera un - (signe de soustraction) devant le nom de la colonne.\n⚠️ Attention. Plusieurs modules utilisent la fonction select (et filter, plus bas). Lorsque vous lancez select et que vous obtenez un message d’erreur comme\nError in select(., ends_with(\"pourc\")) : \n  argument inutilisé (ends_with(\"pourc\"))\nil se pourrait bien que R utilise la fonction select d’un autre module. Pour spécifier que vous désirez la fonction select du module dplyr, spécifiez dplyr::select.\nD’autre arguments de select() permettent une sélection rapide. Par exemple, pour obtenir les colonnes contenant des pourcentages:\n\nchicoute |&gt; \n  select(ends_with(\"pourc\")) |&gt; \n  head(3)\n\n# A tibble: 3 × 13\n  C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1    51.5    1.72  0.108     1.21    0.435    0.470  0.0976 0.00258 0.000175\n2    51.3    2.18  0.0985    1.22    0.337    0.439  0.0996 0.00258 0.000407\n3    50.6    2.12  0.0708    1.05    0.373    0.420  0.104  0.00258 0.000037\n# ℹ 4 more variables: Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;,\n#   Al_pourc &lt;dbl&gt;\n\n\n\n3.5.2.2 Filtrer\nComme c’est le cas de la sélection, on pourra filtrer un tableau de plusieurs manières. J’ai déjà présenté comment filtrer selon les indices des lignes. Les autres manières reposent néanmoins sur une opération logique ==, &lt;, &gt; ou %in% (le %in% signifie se trouve parmi et peut être suivi d’un vecteur de valeurs que l’on désire accepter).\nLes conditions booléennes peuvent être combinées avec les opérateurs et, &, et ou, |. Pour rappel,\n\n\nOpération\nRésultat\n\n\n\nVrai et Vrai\nVrai\n\n\nVrai et Faux\nFaux\n\n\nFaux et Faux\nFaux\n\n\nVrai ou Vrai\nVrai\n\n\nVrai ou Faux\nVrai\n\n\nFaux ou Faux\nFaux\n\n\n\n\nLa méthode classique consiste à appliquer une opération logique entre les crochets, par exemple chicoute[chicoute$CodeTourbiere == \"BEAU\", ]\n\nLa méthode tidyverse, plus pratique en mode pipeline, passe par la fonction filter(), i.e.\n\nchicoute |&gt; \n  filter(CodeTourbiere == \"BEAU\")\nCombiner le tout.\n\nchicoute |&gt; \n  filter(Ca_pourc &lt; 0.4 & CodeTourbiere %in% c(\"BEAU\", \"MB\", \"WTP\")) |&gt; \n  select(contains(\"pourc\"))\n\n# A tibble: 4 × 13\n  C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1    51.3    2.18  0.0985   1.22     0.337    0.439  0.0996 0.00258 0.000407\n2    50.6    2.12  0.0708   1.05     0.373    0.420  0.104  0.00258 0.000037\n3    53.8    2.04  0.115    0.947    0.333    0.472  0.106  0.00258 0.000037\n4    52.6    2.11  0.0847   0.913    0.328    0.376  0.111  0.00296 0.000037\n# ℹ 4 more variables: Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;,\n#   Al_pourc &lt;dbl&gt;\n\n\n\n3.5.3 Le format long et le format large\nDans le tableau chicoute, chaque élément possède sa propre colonne. Si l’on voulait mettre en graphique les boxplot des facettes de concentrations d’azote, de phosphore et de potassium dans les différentes tourbières, il faudrait obtenir une seule colonne de concentrations.\nPour ce faire, nous utiliserons la fonction pivot_longer(). L’argument obligatoire (excluant le tableau, qui est implicite dans la chaîne d’opérations), est cols, le nom des colonnes à allonger. Pour obtenir des noms de colonnes allongées personnalisées, on spécifie le nom des variables consistant aux anciens noms de colonnes avec names_to et celui de la nouvelle colonne contenant les valeurs dans values_to. La suite consiste à décrire les colonnes à inclure ou à exclure. Dans le cas qui suit, j’exclue CodeTourbiere de la refonte et j’utilise slice_sample() pour présenter un échantillon aléatoire du résultat. Notez la ligne comprenant la fonction mutate, que l’on verra plus loin. Cette fonction ajoute une colonne au tableau. Dans ce cas-ci, j’ajoute une colonne constituée d’une séquence de nombres allant de 1 au nombre de lignes du tableau (il y en a 90). Cet identifiant unique pour chaque ligne permettra de reconstituer par la suite le tableau initial.\n\nchicoute_long &lt;- chicoute |&gt; \n  select(CodeTourbiere, N_pourc, P_pourc, K_pourc) |&gt; \n  mutate(ID = 1:n())  |&gt;  # mutate ajoute une colonne au tableau\n  # pour l'identifiant, on peut aussi utiliser la commande cur_group_rows()\n  pivot_longer(cols = contains(\"pourc\"), names_to = \"nutrient\", values_to = \"concentration\")\nchicoute_long |&gt;  slice_sample(n = 10)\n\n# A tibble: 10 × 4\n   CodeTourbiere    ID nutrient concentration\n   &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 SSP              58 N_pourc         1.98  \n 2 2                26 P_pourc         0.158 \n 3 BS2              62 N_pourc         2.44  \n 4 MB               32 P_pourc         0.117 \n 5 BEAU              4 P_pourc         0.0909\n 6 MB               35 P_pourc         0.0847\n 7 2                25 K_pourc         1.24  \n 8 1                70 N_pourc         2.47  \n 9 1                77 P_pourc         0.138 \n10 MB               34 N_pourc         2.35  \n\n\nL’opération inverse est pivot_wider(), avec laquelle nous sélectionnons une colonne spécifiant les nouvelles colonnes à construire (names_from) ainsi que les valeurs à placer dans ces colonnes (values_from).\n\nchicoute_large &lt;- chicoute_long |&gt; \n  pivot_wider(names_from = nutrient, values_from = concentration)\nchicoute_large |&gt;  slice_sample(n = 10)\n\n# A tibble: 10 × 5\n   CodeTourbiere    ID N_pourc P_pourc K_pourc\n   &lt;chr&gt;         &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 NTP              51    2.05  0.104    0.398\n 2 2                24    2.72  0.181    1.14 \n 3 1                76    2.13  0.140    0.683\n 4 BEAU              4    1.95  0.0909   1.19 \n 5 NBM              50    2.42  0.156    0.825\n 6 2                22    2.92  0.226    1.22 \n 7 MR               37    1.90  0.129    0.958\n 8 BEAU              1    1.72  0.108    1.21 \n 9 BEAU              5    2.04  0.115    0.947\n10 1                78    2.31  0.156    0.833\n\n\n\n3.5.4 Combiner des tableaux\nNous avons introduit plus haut la notion de base de données. Nous voudrions peut-être utiliser le code des tourbières pour inclure leur nom, le type d’essai mené à ces tourbières, etc. Importons d’abord le tableau des noms liés aux codes.\n\ntourbieres &lt;- read_csv2(\"data/chicoute_tourbieres.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 11 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (4): Tourbiere, CodeTourbiere, Type, TypeCulture\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntourbieres\n\n# A tibble: 11 × 4\n   Tourbiere               CodeTourbiere Type        TypeCulture\n   &lt;chr&gt;                   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;      \n 1 Beaulieu                BEAU          calibration naturel    \n 2 Brador Path             BP            calibration naturel    \n 3 Lichen (BS2E)           2             validation  cultive sec\n 4 Mannys Brook            MB            calibration naturel    \n 5 Middle Bay Road         MR            calibration naturel    \n 6 North Est of Smelt Pond NESP          calibration naturel    \n 7 North of Blue Moon      NBM           calibration naturel    \n 8 South of Smelt Pond     SSP           calibration naturel    \n 9 Sphaigne (BS2F)         BS2           validation  cultive sec\n10 Sphaigne (BS2F)         1             calibration naturel    \n11 West of Trout Pond      WTP           calibration naturel    \n\n\nNotre information est organisée en deux tableaux, liés par la colonne CodeTourbiere. Comment fusionner l’information pour qu’elle puisse être utilisée dans son ensemble? La fonction left_join effectue cette opération typique avec les bases de données.\n\nchicoute_merge &lt;- left_join(x = chicoute, y = tourbieres, by = \"CodeTourbiere\")\n# ou bien chicoute |&gt;  left_join(y = tourbieres, by = \"CodeTourbiere\")\nchicoute_merge |&gt;  slice_head(n = 4)\n\n# A tibble: 4 × 34\n     ID CodeTourbiere Ordre  Site Traitement DemiParcelle SousTraitement\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;         \n1     1 BEAU          A         1 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n2     2 BEAU          A         2 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n3     3 BEAU          A         3 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n4     4 BEAU          A         4 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n# ℹ 27 more variables: Latitude_m &lt;dbl&gt;, Longitude_m &lt;dbl&gt;,\n#   Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;,\n#   TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;,\n#   TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;,\n#   FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;,\n#   SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, P_pourc &lt;dbl&gt;,\n#   K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, S_pourc &lt;dbl&gt;, …\n\n\nD’autres types de jointures sont possibles, et décrites en détails dans la documentation.\nGarrick Aden-Buie a préparé de jolies animations pour décrire les différents types de jointures.\nleft_join(x, y) colle y à x seulement ce qui dans y correspond à ce que l’on trouve dans x.\n\nright_join(x, y) colle y à x seulement ce qui dans x correspond à ce que l’on trouve dans y.\n\ninner_join(x, y) colle x et y en excluant les lignes où au moins une variable de jointure est absente dans x et y.\n\nfull_join(x, y)garde toutes les lignes et les colonnes de x et y.\n\n\n3.5.5 Opérations sur les tableaux\nLes tableaux peuvent être segmentés en éléments sur lesquels on calculera ce qui nous chante.\nOn pourrait vouloir obtenir :\n\nla somme avec la function sum()\n\nla moyenne avec la function mean() ou la médiane avec la fonction median()\n\nl’écart-type avec la function sd()\n\nles maximum et minimum avec les fonctions min() et max()\n\nun décompte d’occurrence avec la fonction n() ou count()\n\n\nPar exemple,\n\nmean(chicoute$Rendement_g_5m2, na.rm = TRUE)\n\n[1] 13.32851\n\n\nEn mode classique, pour effectuer des opérations sur des tableaux, on utilisera la fonction apply(). Cette fonction prend, comme arguments, le tableau, l’axe (opération par ligne = 1, opération par colonne = 2), puis la fonction à appliquer.\n\napply(chicoute |&gt;  select(contains(\"pourc\")), 2, mean)\n\n     C_pourc      N_pourc      P_pourc      K_pourc     Ca_pourc     Mg_pourc \n5.027911e+01 2.199411e+00 1.388959e-01 8.887000e-01 3.884391e-01 4.980142e-01 \n     S_pourc      B_pourc     Cu_pourc     Zn_pourc     Mn_pourc     Fe_pourc \n1.347177e-01 3.090922e-03 4.089891e-04 6.662155e-03 3.345239e-02 1.514885e-02 \n    Al_pourc \n2.694979e-03 \n\n\nLes opérations peuvent aussi être effectuées par ligne, par exemple une somme (je garde seulement les 10 premiers résultats).\n\napply(chicoute |&gt;  select(contains(\"pourc\")), 1, sum)[1:10]\n\n [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991\n [9] 55.06295 55.16774\n\n\nLa fonction à appliquer peut être personnalisée, par exemple:\n\napply(\n  chicoute |&gt;  select(contains(\"pourc\")), 2,\n  function(x) (prod(x))^(1 / length(x))\n)\n\n     C_pourc      N_pourc      P_pourc      K_pourc     Ca_pourc     Mg_pourc \n50.253429104  2.165246915  0.133754530  0.846193827  0.376192724  0.491763884 \n     S_pourc      B_pourc     Cu_pourc     Zn_pourc     Mn_pourc     Fe_pourc \n 0.129900753  0.003014675  0.000000000  0.006408775  0.024140327  0.014351745 \n    Al_pourc \n 0.002450982 \n\n\nVous reconnaissez cette fonction? C’était la moyenne géométrique (la fonction prod() étant le produit d’un vecteur).\nEn mode tidyverse, on aura besoin principalement des fonction suivantes:\n\n\ngroup_by() pour effectuer des opérations par groupe, l’opération group_by() sépare le tableau en plusieurs petits tableaux, en attendant de les recombiner. C’est un peu l’équivalent des facettes avec le module de visualisation ggplot2, que nous explorons au chapitre 4.\n\nsummarise() pour réduire plusieurs valeurs en une seule, il applique un calcul sur le tableau ou s’il y a lieu sur chaque petit tableau segmenté. Il en existe quelques variantes.\n\n\nsummarise_all() applique la fonction à toutes les colonnes\n\nsummarise_at() applique la fonction aux colonnes spécifiées\n\nsummarise_if() applique la fonction aux colonnes qui ressortent comme TRUE selon une opération booléenne\n\n\n\nmutate() pour ajouter une nouvelle colonne\n\nSi l’on désire ajouter une colonne à un tableau, par exemple le sommaire calculé avec summarise(). À l’inverse, la fonction transmute() retournera seulement le résultat, sans le tableau à partir duquel il a été calculé. De même que summarise(), mutate() et transmute() possèdent leurs équivalents _all(), _at() et _if().\n\n\n\narrange() pour réordonner le tableau\n\nCette fonction est parfois utile lors de la mise en page de tableaux ou de graphiques. Il ne s’agit pas d’une opération sur un tableau, mais plutôt un changement d’affichage en changeant l’ordre d’apparition des données.\n\n\n\nCes opérations sont décrites dans l’aide-mémoire Data Transformation Cheat Sheet (Figure 3.6).\n\n\n\n\nFigure 3.6: Aide-mémoire pour la transformation des données, https://rstudio.github.io/cheatsheets/data-transformation.pdf\n\n\n\nPour effectuer des statistiques par colonne, on utilisera summarise pour des statistiques effectuées sur une seule colonne. summarise peut prendre le nombre désiré de statistiques dont la sortie est un scalaire.\n\nchicoute |&gt; \n  summarise(\n    moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE),\n    ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 2\n  moyenne ecart_type\n    &lt;dbl&gt;      &lt;dbl&gt;\n1    52.1       40.4\n\n\nSi l’on désire un sommaire sur toutes les variables sélectionnées, on utilisera summarise_all(). Pour spécifier que l’on désire la moyenne et l’écart-type, on inscrit les noms des fonctions dans list().\n\nchicoute |&gt; \n  select(contains(\"pourc\")) |&gt; \n  summarise_all(list(mean, sd))\n\n# A tibble: 1 × 26\n  C_pourc_fn1 N_pourc_fn1 P_pourc_fn1 K_pourc_fn1 Ca_pourc_fn1 Mg_pourc_fn1\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1        50.3        2.20       0.139       0.889        0.388        0.498\n# ℹ 20 more variables: S_pourc_fn1 &lt;dbl&gt;, B_pourc_fn1 &lt;dbl&gt;,\n#   Cu_pourc_fn1 &lt;dbl&gt;, Zn_pourc_fn1 &lt;dbl&gt;, Mn_pourc_fn1 &lt;dbl&gt;,\n#   Fe_pourc_fn1 &lt;dbl&gt;, Al_pourc_fn1 &lt;dbl&gt;, C_pourc_fn2 &lt;dbl&gt;,\n#   N_pourc_fn2 &lt;dbl&gt;, P_pourc_fn2 &lt;dbl&gt;, K_pourc_fn2 &lt;dbl&gt;,\n#   Ca_pourc_fn2 &lt;dbl&gt;, Mg_pourc_fn2 &lt;dbl&gt;, S_pourc_fn2 &lt;dbl&gt;,\n#   B_pourc_fn2 &lt;dbl&gt;, Cu_pourc_fn2 &lt;dbl&gt;, Zn_pourc_fn2 &lt;dbl&gt;,\n#   Mn_pourc_fn2 &lt;dbl&gt;, Fe_pourc_fn2 &lt;dbl&gt;, Al_pourc_fn2 &lt;dbl&gt;\n\n\nOn utilisera group_by() pour segmenter le tableau, et ainsi obtenir des statistiques pour chaque groupe.\n\nchicoute |&gt; \n  group_by(CodeTourbiere) |&gt; \n  summarise(\n    moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE),\n    ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)\n  )\n\n# A tibble: 12 × 3\n   CodeTourbiere moyenne ecart_type\n   &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n 1 1                72.1      32.7 \n 2 2                37.1      32.9 \n 3 BEAU            149.       53.2 \n 4 BP               60.4      30.6 \n 5 BS2              27.2      15.5 \n 6 MB               64.7      40.8 \n 7 MR               35.1      10.5 \n 8 NBM              35.1      16.6 \n 9 NESP             21.4       4.88\n10 NTP              47.6      15.9 \n11 SSP              25.7      11.1 \n12 WTP              50.2      28.3 \n\n\nDans le cas de summarise_all, les résultats s’affichent de la même manière.\n\nchicoute |&gt; \n  group_by(CodeTourbiere) |&gt; \n  select(N_pourc, P_pourc, K_pourc) |&gt; \n  summarise_all(list(mean, sd))\n\nAdding missing grouping variables: `CodeTourbiere`\n\n\n# A tibble: 12 × 7\n   CodeTourbiere N_pourc_fn1 P_pourc_fn1 K_pourc_fn1 N_pourc_fn2 P_pourc_fn2\n   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1                    2.26      0.156        0.880      0.250      0.0193 \n 2 2                    2.76      0.181        1.12       0.178      0.0283 \n 3 BEAU                 2.00      0.0967       1.12       0.179      0.0172 \n 4 BP                   2.05      0.158        0.747      0.161      0.00625\n 5 BS2                  2.08      0.103        1.12       0.420      0.0218 \n 6 MB                   2.15      0.109        0.675      0.114      0.0165 \n 7 MR                   1.99      0.127        0.830      0.0802     0.0131 \n 8 NBM                  2.01      0.127        0.854      0.310      0.0202 \n 9 NESP                 1.76      0.135        0.945      0.149      0.0108 \n10 NTP                  1.83      0.0873       0.402      0.166      0.0103 \n11 SSP                  1.83      0.130        0.700      0.160      0.00383\n12 WTP                  1.79      0.0811       0.578      0.132      0.00587\n# ℹ 1 more variable: K_pourc_fn2 &lt;dbl&gt;\n\n\nPour obtenir des statistiques à chaque ligne, mieux vaut utiliser apply(), tel que vu précédemment. Le point, ., représente le tableau dans une fonction qui n’a pas été conçue pour fonctionner de facto avec dplyr.\n\nchicoute |&gt; \n  select(contains(\"pourc\")) |&gt; \n  apply(1, sum)\n\n [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991\n [9] 55.06295 55.16774 56.41123 55.47917 55.43537 55.79175 55.44561 54.85448\n[17] 54.34262 55.03075 54.40533 51.89319 54.70172 54.62176 54.30250 53.86976\n[25] 53.44731 53.86244 52.43280 54.34978 53.96756 51.46672 55.44267 54.70350\n[33] 55.30711 56.16200 56.64710 55.95499 54.76370 54.32775 54.95419 53.37094\n[41] 53.07855 53.04541 52.09520 52.40456 51.92376 53.33248 56.56405 56.35004\n[49] 56.27185 55.56986 53.81654 55.39638 55.51961 54.88098 54.74774 51.08921\n[57] 51.31462 53.46819 53.15640 52.82020 57.78038 57.94636 56.65558 56.28845\n[65] 55.54463 56.51751 55.36497 56.00594 55.64247 56.56967 56.81674 55.87070\n[73] 55.72308 56.14116 56.42611 55.35650 54.90469 54.03674 53.42991 53.99334\n[81] 53.09085 53.23222 53.28212 53.63192 53.48102 52.31131 51.72026 51.10534\n[89] 51.49055 51.59297\n\n\nPrenons ce tableau des espèces menacées issu de l’Union internationale pour la conservation de la nature distribué par l’OCDE.\n\nlibrary(\"tidyverse\")\nespeces_menacees &lt;- read_csv(\"data/WILD_LIFE_14012020030114795.csv\")\n\nNous exécutons le pipeline suivant.\n\nespeces_menacees |&gt; \n  dplyr::filter(IUCN == \"CRITICAL\", SPEC == \"VASCULAR_PLANT\") |&gt; \n  dplyr::select(Country, Value) |&gt; \n  dplyr::group_by(Country) |&gt; \n  dplyr::summarise(n_critical_plants = sum(Value)) |&gt; \n  dplyr::arrange(desc(n_critical_plants)) |&gt; \n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   Country         n_critical_plants\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 United States                1222\n 2 Japan                         525\n 3 Canada                        315\n 4 Czech Republic                284\n 5 Spain                         271\n 6 Belgium                       253\n 7 Austria                       172\n 8 Slovak Republic               155\n 9 Australia                     148\n10 Italy                         128\n\n\nCe pipeline consiste à:\nprendre le tableau especes_menacees, puis\n  \nfiltrer pour n'obtenir que les espèces critiques dans la catégorie des plantes vasculaires, puis\n  \nsélectionner les colonnes des pays et des valeurs (nombre d'espèces), puis\n\nsegmenter le tableau en plusieurs tableaux selon le pays, puis\n\nappliquer la fonction sum pour chacun de ces petits tableaux (et recombiner ces sommaires), puis\n\ntrier les pays en nombre décroissant de décompte d'espèces, puis\n\nafficher le top 10\nNotez qu’il aurait aussi été possible d’utiliser la fonction dplyr::slice_max(n_critical_plants, n = 10) pour afficher directement le top 10, sans faire le tri préalable.\n\n3.5.6 Exemple (difficile)\nPour revenir à notre tableau chicoute, imaginez que vous aviez une station météo (station_A) située aux coordonnées (490640, 5702453) et que vous désiriez calculer la distance entre l’observation et la station. Prenez du temps pour réfléchir à la manière dont vous procéderez…\n\nOn pourra créer une fonction qui mesure la distance entre un point x, y et les coordonnées de la station A…\n\ndist_station_A &lt;- function(x, y) {\n  return(sqrt((x - 490640)^2 + (y - 5702453)^2))\n}\n\n… puis ajouter une colonne avec mutate grâce à une fonction prenant les arguments x et y spécifiés.\n\nchicoute |&gt; \n  mutate(dist = dist_station_A(x = Longitude_m, y = Latitude_m)) |&gt; \n  select(ID, CodeTourbiere, Longitude_m, Latitude_m, dist) |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 5\n      ID CodeTourbiere Longitude_m Latitude_m    dist\n   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1    63 BS2                486545    5702197 4103.  \n 2    11 2                  486522    5702582 4120.  \n 3    72 1                  486488    5702129 4165.  \n 4    15 2                  486498    5702643 4146.  \n 5    18 2                  486501    5702627 4143.  \n 6    88 WTP                487060    5700775 3954.  \n 7    64 BS2                486530    5702199 4118.  \n 8    73 1                  486488    5702129 4165.  \n 9    76 1                  486465    5702112 4189.  \n10     2 BEAU               490634    5702452    6.08\n\n\nNous pourrions procéder de la même manière pour fusionner des données climatiques. Le tableau chicoute ne possède pas d’indicateurs climatiques, mais il est possible de les soutirer de stations météo placées près des sites. Ces données ne sont pas disponibles pour le tableau de la chicouté, alors j’utiliserai des données fictives pour l’exemple.\nVoici ce qui pourrait être fait.\n\nCréer un tableau des stations météo ainsi que des indices météorologiques associés à ces stations.\nLier chaque site à une station (à la main où selon la plus petite distance entre le site et la station).\nFusionner les indices climatiques aux sites, puis les sites aux mesures de rendement.\n\nCes opérations demandent habituellement du tâtonnement. Il serait surprenant que même une personne expérimentée soit en mesure de compiler ces opérations sans obtenir de message d’erreur, et retravailler jusqu’à obtenir le résultat souhaité. L’objectif de cette section est de vous présenter un flux de travail que vous pourriez être amenés à effectuer et de fournir quelques éléments nouveaux pour mener à bien une opération. Il peut être frustrant de ne pas saisir toutes les opérations: passez à travers cette section sans jugement. Si vous devez vous frotter à un problème semblable, vous saurez que vous trouverez dans ce manuel une recette intéressante.\n\nmes_stations &lt;- data.frame(\n  Station = c(\"A\", \"B\", \"C\"),\n  Longitude_m = c(490640, 484870, 485929),\n  Latitude_m = c(5702453, 5701870, 5696421),\n  t_moy_C = c(13.8, 18.2, 16.30),\n  prec_tot_mm = c(687, 714, 732)\n)\nmes_stations\n\n  Station Longitude_m Latitude_m t_moy_C prec_tot_mm\n1       A      490640    5702453    13.8         687\n2       B      484870    5701870    18.2         714\n3       C      485929    5696421    16.3         732\n\n\nLa fonction suivante calcule la distance entre des coordonnées x et y et chaque station d’un tableau de stations, puis retourne le nom de la station dont la distance est la moindre.\n\ndist_station &lt;- function(x, y, stations_df) {\n  # stations est le tableau des stations à trois colonnes\n  # 1iere: nom de la station\n  # 2ieme: longitude\n  # 3ieme: latitude\n  distance &lt;- c()\n  for (i in 1:nrow(stations_df)) {\n    distance[i] &lt;- sqrt((x - stations_df[i, 2])^2 + (y - stations_df[i, 3])^2)\n  }\n  nom_station &lt;- as.character(stations_df$Station[which.min(distance)])\n  return(nom_station)\n}\n\nTestons la fonction avec des coordonnées.\n\ndist_station(x = 459875, y = 5701988, stations_df = mes_stations)\n\n[1] \"B\"\n\n\nNous appliquons cette fonction à toutes les lignes du tableau, puis en retournons un échantillon.\n\nchicoute |&gt; \n  rowwise() |&gt; \n  mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) |&gt; \n  select(ID, CodeTourbiere, Longitude_m, Latitude_m, Station) |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 90 × 5\n# Rowwise: \n      ID CodeTourbiere Longitude_m Latitude_m Station\n   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;  \n 1     1 BEAU               490627    5702454 A      \n 2     2 BEAU               490634    5702452 A      \n 3     3 BEAU               490638    5702461 A      \n 4     4 BEAU               490647    5702453 A      \n 5     5 BEAU               490654    5702445 A      \n 6     6 BP                 484865    5706394 B      \n 7     7 BP                 484054    5706307 B      \n 8     8 BP                 484742    5702280 B      \n 9     9 BP                 484761    5706324 B      \n10    10 BP                 484780    5706364 B      \n# ℹ 80 more rows\n\n\nCela semble fonctionner. On peut y ajouter un left_join() pour joindre les données météo au tableau principal.\n\nchicoute_weather &lt;- chicoute |&gt; \n  rowwise() |&gt; \n  mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) |&gt; \n  left_join(y = mes_stations, by = \"Station\")\nchicoute_weather |&gt;  slice_sample(n = 10)\n\n# A tibble: 90 × 36\n# Rowwise: \n      ID CodeTourbiere Ordre  Site Traitement DemiParcelle SousTraitement\n   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;         \n 1     1 BEAU          A         1 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 2     2 BEAU          A         2 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 3     3 BEAU          A         3 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 4     4 BEAU          A         4 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 5     5 BEAU          A         5 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 6     6 BP            H         1 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 7     7 BP            H         2 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 8     8 BP            H         3 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 9     9 BP            H         4 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n10    10 BP            H         5 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n# ℹ 80 more rows\n# ℹ 29 more variables: Latitude_m.x &lt;dbl&gt;, Longitude_m.x &lt;dbl&gt;,\n#   Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;,\n#   TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;,\n#   TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;,\n#   FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;,\n#   SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, …\n\n\n\n3.5.7 Exporter un tableau\nSimplement avec write_csv().\n\nwrite_csv(chicoute_weather, \"data/chicoute_weather.csv\")\n\n\n3.5.8 Aller plus loin dans le tidyverse\nLe livre R for data science (2e), de Hadley Wickham et Garrett Grolemund (couverture à la Figure 3.7), est un incontournable.\n\n\n\n\nFigure 3.7: Couverture du libre de Hadley Wickham, Mine Çetinkaya-Rundel et Garrett Grolemund, Source: https://r4ds.hadley.nz/"
  },
  {
    "objectID": "03-tableaux.html#références",
    "href": "03-tableaux.html#références",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.6 Références",
    "text": "3.6 Références\nParent L.E., Parent, S.É., Herbert-Gentile, V., Naess, K. et Lapointe, L. 2013. Mineral Balance Plasticity of Cloudberry (Rubus chamaemorus) in Quebec-Labrador Bogs. American Journal of Plant Sciences, 4, 1508-1520. DOI: 10.4236/ajps.2013.47183"
  },
  {
    "objectID": "04-visualisation.html#pourquoi-explorer",
    "href": "04-visualisation.html#pourquoi-explorer",
    "title": "4  Visualisation",
    "section": "\n4.1 Pourquoi explorer graphiquement?",
    "text": "4.1 Pourquoi explorer graphiquement?\nLa plupart des graphiques que vous générerez ne seront pas destinés à être publiés. Ils viseront probablement d’abord à explorer des données. Cela vous permettra de mettre en évidence de nouvelles perspectives.\nPrenons par exemple deux variables, \\(X\\) et \\(Y\\). Vous calculez leur moyenne, écart-type et la corrélation entre les deux variables (nous verrons les statistiques plus en détail dans un prochain chapitre).\n\nlibrary(\"tidyverse\")\ndatasaurus &lt;- read_tsv(\"data/DatasaurusDozen.tsv\")\n\ncor_datasaurus &lt;- datasaurus |&gt; \n  group_by(dataset) |&gt; \n  summarise(cor = cor(x = x, y = y, method = \"pearson\"))\n\ndatasaurus |&gt; \n  group_by(dataset) |&gt; \n  summarise_all(list(mean = mean, sd = sd)) |&gt; \n  left_join(cor_datasaurus, by = \"dataset\")\n\n# A tibble: 13 × 6\n   dataset    x_mean y_mean  x_sd  y_sd     cor\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 away         54.3   47.8  16.8  26.9 -0.0641\n 2 bullseye     54.3   47.8  16.8  26.9 -0.0686\n 3 circle       54.3   47.8  16.8  26.9 -0.0683\n 4 dino         54.3   47.8  16.8  26.9 -0.0645\n 5 dots         54.3   47.8  16.8  26.9 -0.0603\n 6 h_lines      54.3   47.8  16.8  26.9 -0.0617\n 7 high_lines   54.3   47.8  16.8  26.9 -0.0685\n 8 slant_down   54.3   47.8  16.8  26.9 -0.0690\n 9 slant_up     54.3   47.8  16.8  26.9 -0.0686\n10 star         54.3   47.8  16.8  26.9 -0.0630\n11 v_lines      54.3   47.8  16.8  26.9 -0.0694\n12 wide_lines   54.3   47.8  16.8  26.9 -0.0666\n13 x_shape      54.3   47.8  16.8  26.9 -0.0656\n\n\nLes moyennes, écarts-types et corrélations sont à peu près les mêmes pour tous les groupes. Peut-on conclure que tous les groupes sont semblables? Pas encore.\nPour démontrer que ces statistiques ne vous apprendront pas grand chose sur la structure des données, Matejka et Fitzmaurice (2017) ont généré 12 jeux de données \\(X\\) et \\(Y\\), ayant chacun pratiquement les mêmes statistiques. Mais avec des structures bien différentes (Figure 4.2)!\n\n\n\n\nFigure 4.2: Animation montrant la progression du jeu de données Datasaurus pour toutes les formes visées. Source: Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing"
  },
  {
    "objectID": "04-visualisation.html#publier-un-graphique",
    "href": "04-visualisation.html#publier-un-graphique",
    "title": "4  Visualisation",
    "section": "\n4.2 Publier un graphique",
    "text": "4.2 Publier un graphique\nVous voilà sensibilisés à l’importance d’explorer les données graphiquement. Mais ce qui ultimement émanera d’un projet sera le rapport que vous déposerez, l’article scientifique que vous ferez publier ou le billet de blogue que vous partagerez sur les réseaux sociaux. Les graphiques inclus dans vos publications méritent une attention particulière pour que votre audience puisse comprendre les découvertes et perspectives offertes par vos travaux. Pour ce faire, un graphique doit répondre honnêtement à la question posée tout en étant attrayant.\n\n4.2.1 Cinq qualités d’un bon graphique\nAlberto Cairo, chercheur spécialisé en visualisation de données, a fait paraître en 2016 le livre The Truthful art. Il note cinq qualités d’une visualisation bien conçue (les citations de cette section proviennent de ma traduction de Alberto Cairo, The Truthful Art (2016), p. 45.).\n\n1- Elle est véritable, puisqu’elle est basée sur une recherche exhaustive et honnête.\n\nCela vaut autant pour les graphiques que pour l’analyse de données. Il s’agit froidement de présenter les données selon l’interprétation la plus exacte. Les pièges à éviter sont le picorage de cerises et la surinterprétation des données. Le picorage, c’est lorsqu’on réduit les perspectives afin de soutenir un argumentaire. Par exemple, retirer des données d’une région ou d’une décennie qui rendraient factice une conclusion fixée a priori. Ceci vaut autant pour les graphiques que pour les statistiques (nous parlerons du p-hacking au prochain chapitre). La surinterprétation, c’est lorsque l’on saute rapidement aux conclusions: par exemple, que l’on génère des corrélations, voire même des relations de causalités à partir de ce qui n’est que du bruit de fond. À ce titre, lors d’une conférence, Heather Krause insiste sur l’importance de faire en sorte que les représentations graphiques répondent correctement aux questions posées dans une étude (Figure 4.3).\n\n\n\n\nFigure 4.3: The F word: Protect your work from four hidden fallacies when working with data, une conférence de Heather Krause, 2018\n\n\n\n\n2- Elle est fonctionnelle, puisqu’elle constitue une représentation précise des données, et qu’elle est construite de manière à laisser les observateurs.trices prendre des initiatives conséquentes.\n\n“La seule chose qui est pire qu’un diagramme en pointe de tarte, c’est d’en présenter plusieurs” (Edward Tufte, designer, cité par Alberto Cairo, 2016, p. 50). Choisir le bon graphique pour représenter vos données est beaucoup moins une question de bon goût qu’une question de démarche rationnelle sur l’objectif visé par la présentation d’un graphique. Je présenterai des lignes guides pour sélectionner le type de graphique qui présentera vos données de manière fonctionnelle en fonction de l’objectif d’un graphique (d’ailleurs, avez-vous vraiment besoin d’un graphique?).\n\n3- Elle est attrayante et intrigante, et même esthétiquement plaisante pour l’audience visée - les scientifiques d’abord, mais aussi le public en général.\n\nEn sciences naturelles, la pensée rationnelle, la capacité à organiser la connaissance et créer de nouvelles avenues sont des qualités qui sont privilégiées au talent artistique. Que vous ayez où non des aptitudes en art visuel, présentez de l’information, pas des décorations. Excel vous permet d’ajouter une perspective 3D à un diagramme en barres. La profondeur contient-elle de l’information? Non. Cette décoration ne fait qu’ajouter de la confusion. Minimalisez, fournissez le plus d’information possible avec le moins d’éléments graphiques possibles. C’est ce que vous proposent les guides graphiques que j’introduirai plus loin.\n\n4- Elle est pertinente, puisqu’elle révèle des évidences scientifiques autrement difficilement accessibles.\n\nIl s’agit de susciter un eurêka, dans le sens qu’elle génère une idée, et parfois une initiative, en un coup d’œil. Le graphique en bâton de hockey est un exemple où l’on a spontanément une idée de la situation. Cette situation peut être la présence d’un phénomène comme l’augmentation de la température globale, mais aussi l’absence de phénomènes pourtant attendus.\n\n5- Elle est instructive, parce que si l’on saisit et accepte les évidences scientifiques qu’elle décrit, cela changera notre perception pour le mieux.\n\nEn présentant cette qualité, Alberto Cairo voulait inciter ses lecteurs.trices à choisir des sujets de discussion visuelle de manière à participer à un monde meilleur. En ce qui nous concerne, il s’agit de bien sélectionner l’information que l’on désire transmettre. Imaginez que vous avez travaillé quelques jours pour créer un graphique, dont vous êtes fier, mais vous (ou un collègue hiérarchiquement favorisé) vous rendez compte que le graphique soutient peu ou pas le propos ou l’objectif de votre thèse/mémoire/rapport/article. Si c’est bien le cas, vous feriez mieux de laisser tomber votre oeuvre et considérer votre démarche comme une occasion d’apprentissage.\nAlberto Cairo résume son livre The Truthful Art dans une entrevue avec le National Geographic."
  },
  {
    "objectID": "04-visualisation.html#choisir-type-graph",
    "href": "04-visualisation.html#choisir-type-graph",
    "title": "4  Visualisation",
    "section": "\n4.3 Choisir le type de graphique le plus approprié",
    "text": "4.3 Choisir le type de graphique le plus approprié\nDe nombreuses manières de présenter les données sont couramment utilisées, comme les nuages de points, les lignes, les histogrammes, les diagrammes en barres et en pointes de tarte. Les principaux types de graphiques seront couverts dans ce chapitre. D’autres types spécialisés seront couverts dans les chapitres appropriés (graphiques davantage orientés vers les statistiques, les biplots, les dendrogrammes, les diagrammes ternaires, les cartes, etc.).\nLa visualisation de données est aujourd’hui devenue un métier pour plusieurs personnes ayant des affinités pour la science, les arts et la communication, dont certaines partagent leur expertise sur le web. À ce titre, le site from data to viz est à conserver dans vos marque-pages. Il comprend des arbres décisionnels qui vous guident vers les options appropriées pour présenter vos données, puis fournissent des exemples pour produire ces visualisations en R. Également, je suggère le site internet de Ann K. Emery, qui présente des lignes guide pour présenter le graphique adéquat selon les données en main. De nombreuses recettes sont également proposées sur r-graph-gallery.com. En ce qui a trait aux couleurs, le choix n’est pas anodin. Si vous avez le souci des détails sur les éléments esthétiques de vos graphiques, je recommande la lecture de ce billet de blog de Lisa Charlotte Rost.\nRetenez néanmois que La couleur est une information. Les couleurs devraient être sélectionnées d’abord pour être lisibles par les personnes ne percevant pas les couleurs (Figure 4.4), selon le support (apte à être photocopié, lisible à l’écran, lisible sur des documents imprimés en noir et blanc) et selon le type de données. Vous pouvez aussi utiliser certains modules comme RColorBrewer comme expliqué dans le billet suivant qui permet d’adopter directement les palettes sélectionnées.\n\nDonnées continues ou catégorielles ordinales: gradient (transition graduelle d’une couleur à l’autre), séquence (transition saccadée selon des groupes de données continues) ou divergentes (transition saccadée d’une couleur à l’autre vers des couleurs divergentes, par exemple orange vers blanc vers bleu).\nDonnées catégorielles nominales: couleurs éloignées d’une catégorie à une autre (plus il y a de catégories, plus les couleurs sont susceptibles de se ressembler).\n\n\n\n\n\nFigure 4.4: Capture d’écran de colorbrewer2.org, qui propose des palettes de couleurs pour créer des cartes, mais l’information est pertinente pour tout type de graphique.\n\n\n\nLe Financial Times offre également ce guide visuel (Figure 4.5).\n\n\n\n\nFigure 4.5: Guide de sélection de graphique du Financial Times\n\n\n\nCairo (2016) propose de procéder en suivant ces étapes:\n\nRéfléchissez au message que vous désirez transmettre: comparer les catégories \\(A\\) et \\(B\\), visualiser une transition ou un changement de \\(A\\) vers \\(B\\), présenter une relation entre \\(A\\) et \\(B\\) ou la distribution de \\(A\\) et \\(B\\) sur une carte.\nEssayez différentes représentations: si le message que vous désirez transmettre a plusieurs volets, il se pourrait que vous ayez besoin de plus d’un graphique.\nMettez de l’ordre dans vos données. C’était le sujet du chapitre 3.\nTestez le résultat. “Hé, qu’est-ce que tu comprends de cela?” Si la personne hausse les épaules, il va falloir réévaluer votre stratégie."
  },
  {
    "objectID": "04-visualisation.html#choisir-son-outil-de-visualisation",
    "href": "04-visualisation.html#choisir-son-outil-de-visualisation",
    "title": "4  Visualisation",
    "section": "\n4.4 Choisir son outil de visualisation",
    "text": "4.4 Choisir son outil de visualisation\nLes modules et logiciels de visualisation sont basés sur des approches que l’on pourrait placer sur un spectre allant de l’impératif au déclaratif.\n\n4.4.1 Approche impérative\nSelon cette approche, vous indiquez comment placer l’information dans un espace graphique. Vous indiquer les symboles, les couleurs, les types de ligne, etc. Peu de choses sont automatisées, ce qui laisse une grande flexibilité, mais demande de vouer beaucoup d’énergie à la manière de coder pour obtenir le graphique désiré. Le module graphique de Excel, ainsi que le module graphique de base de R, utilisent des approches impératives.\n\n4.4.2 Approche déclarative\nLes stratégies d’automatisation graphique se sont grandement améliorées au cours des dernières années. Plutôt que de vouer vos énergies à créer un graphique, il est maintenant possible de spécifier ce que l’on veut présenter.\n\nLa visualisation déclarative vous permet de penser aux données et à leurs relations, plutôt que des détails accessoires.\nJake Vanderplas, Declarative Statistical Visualization in Python with Altair (ma traduction)\n\nL’approche déclarative passe souvent par une grammaire graphique, c’est-à-dire un langage qui explique ce que l’on veut présenter - en mode impératif, on spécifie plutôt comment on veut présenter les données. Le module ggplot2 est le module déclaratif par excellence en R."
  },
  {
    "objectID": "04-visualisation.html#visualisation-en-r",
    "href": "04-visualisation.html#visualisation-en-r",
    "title": "4  Visualisation",
    "section": "\n4.5 Visualisation en R",
    "text": "4.5 Visualisation en R\nEn R, votre trousse d’outils de visualisation mériterait de comprendre les modules suivants.\n\n\nbase. Le module de base de R contient des fonctions graphiques très polyvalentes. Les axes sont générés automatiquement, on peut y ajouter des titres et des légendes, on peut créer plusieurs graphiques sur une même figure, on peut y ajouter différentes géométries (points, lignes et polygones), avec différents types de points ou de traits, différentes couleurs, etc. Les modules spécialisés viennent souvent avec leurs graphiques spécialisés, construits à partir du module de base. En tant que module graphique impératif, on peut tout faire ou presque (pas d’interactivité), mais l’écriture du code est peut expressive.\n\nggplot2. C’est le module graphique par excellence en R (et j’ose dire: en calcul scientifique). ggplot2 se base sur une grammaire graphique. À partir d’un tableau de données, une colonne peut définir l’axe des x, une autre l’axe des y, une autre la couleur des points ou leur dimension. Une autre colonne définissant des catégories peut segmenter la visualisation en plusieurs graphiques alignés horizontalement ou verticalement. Des extensions de ggplot2 permettent de générer des cartes (ggmap), des diagrammes ternaires (ggtern), des animations (gganimate), etc.\n\nplotly. plotly est un module graphique particulièrement utile pour générer des graphiques interactifs. plotly offre une fonction toute simple pour rendre interactif un graphique ggplot2.\n\nNous survolerons rapidement le module de base, irons plus en profondeur avec ggplot2, puis je présenterai brièvement les graphiques interactifs avec plotly."
  },
  {
    "objectID": "04-visualisation.html#module-de-base-pour-les-graphiques",
    "href": "04-visualisation.html#module-de-base-pour-les-graphiques",
    "title": "4  Visualisation",
    "section": "\n4.6 Module de base pour les graphiques",
    "text": "4.6 Module de base pour les graphiques\nNous allons d’abord survoler le module de base, en mode impératif. La fonction de base pour les graphiques en R est plot(). Pour nous exercer avec cette fonction, chargeons d’abord le tableau de données d’exercice iris, publié en 1936 par le célèbre biostatisticien Ronald Fisher.\n\ndata(\"iris\")\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nLe tableau iris contient 5 colonnes, les 4 premières décrivant les longueurs et largeurs des pétales et sépales de différentes espèces d’iris dont le nom apparaît à la 5ième colonne. Vous avez déjà vu au chapitre précédent comment extraire les colonnes d’un tableau; une méthode consiste à appeler le tableau, suivi du $, puis du nom de la colonne, par exemple iris$Species. Pour générer un graphique avec la fonction plot():\n\nplot(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\nPar défaut, le premier argument est le vecteur définissant l’axe des x et le deuxième est celui définissant l’axe des y. Vous rencontrerez souvent de telles utilisations d’arguments implicites, mais je préfère être explicite en définissant bien les arguments: plot(x = iris$Sepal.Length, y = iris$Petal.Length). Le graphique précédent peut être amplement personnalisé en utilisant différents arguments (Figure 4.6).\n\n\n\n\nFigure 4.6: Éléments personnalisables d’un graphique de base\n\n\n\nExercice. Utilisez ces arguments dans la cellule de code de la figure plot(iris$Sepal.Length, iris$Petal.Length).\nRemarquez que la fonction a décidé toute seule de créer un nuage de point. La fonction plot() est conçue pour créer le graphique approprié selon le type des données spécifiées: lignes, boxplot, etc. Si l’on spécifiait les espèces comme argument x:\n\nplot(x = iris$Species, y = iris$Petal.Length)\n\n\n\n# ou bien\n# iris |&gt; \n#   select(Species, Petal.Length) |&gt; \n#   plot()\n\nDe même, la fonction plot() appliquée à un tableau de données générera une représentation bivariée.\n\nplot(iris)\n\n\n\n\nIl est possible d’encoder des attributs grâce à des vecteurs de facteurs (catégories).\n\nplot(iris, col = iris$Species)\n\n\n\n\nL’argument type = \"\" permet de personnaliser l’apparence:\n\n\ntype = \"p\": points\n\ntype = \"l\": ligne\n\ntype = \"o\" et type = \"b\": ligne et points\n\ntype = \"n\": ne rien afficher\n\nCréons un jeu de données.\n\ntime &lt;- seq(from = 0, to = 100, by = 10)\nheight &lt;- abs(time * 0.1 + rnorm(length(time), 0, 2)) \n# abs pour valeur absolue (changement de signe si négatif)\nplot(x = time, y = height, type = \"b\", lty = 2, lwd = 1)\n\n\n\n\nLe type de ligne est spécifié par l’argument lty (qui peut prendre un chiffre ou une chaîne de caractères, i.e. 1 est équivalent de \"solid\", 2 de \"dashed\", 3 de \"dotted\", etc.) et la largeur du trait (valeur numérique), par l’argument lwd.\nLa fonction hist() permet quant à elle de créer des histogrammes. Parmi ses arguments, breaks est particulièrement utile, car il permet d’ajuster la segmentation des incréments.\n\nhist(iris$Petal.Length, breaks = 60)\n\n\n\n\nExercice. Ajustez le titre de l’axe des x, ainsi que les limites de l’axe des x. Êtes-vous en mesure de colorer l’intérieur des barres en bleu?\nLa fonction plot() peut être suivie de plusieurs autres couches comme des lignes (lines() ou abline()), des points (points()), du texte (text()), des polygones (polygon(), des légendes (legend())), etc. On peut aussi personnaliser les couleurs, les types de points, les types de lignes, etc. L’exemple suivant ajoute une ligne au graphique. Ne prêtez pas trop attention aux fonctions predict() et lm() pour l’instant: nous les verrons au chapitre 7.\n\nplot(x = time, y = height)\nlines(x = time, y = predict(lm(height ~ time)))\n\n\n\n\nPour exporter un graphique, vous pouvez passer par le menu Export de RStudio. Mais pour des graphiques destinés à être publiés, je vous suggère d’exporter vos graphiques avec une haute résolution à la suite de la commande png() (ou jpg() ou svg()).\n\nsvg(filename = \"images/mon-graphique.svg\", width = 3000, height = 2000)\n# png(filename = 'images/mon-graphique.png', width = 3000, height=2000, res=300)\nplot(\n  x = iris$Petal.Length,\n  y = iris$Sepal.Length,\n  col = iris$Species,\n  cex = 3, # dimension des points\n  pch = 16 # type de points\n)\ndev.off()\n\npng \n  2 \n\n\nLe format svg crée une version vectorielle du graphique, c’est-à-dire que l’image exportée est un fichier contenant les formes, non pas les pixels. Cela vous permet d’éditer votre graphique dans un logiciel de dessin vectoriel (comme Inkscape).\nDans le bloc de code précédent, j’ai mis en commentaire (# ...) le format d’image png, utile pour les images de type graphique, avec des changements de couleurs drastiques. J’y ai spécifié une haute résolution, à 300 pixels par pouce. Pour les photos, vous préférerez le format jpg. Des éditeurs demanderont peut-être des formats vectoriels comme pdf ou eps. Si vous ne trouvez pas de moyen de modifier un aspect du graphique dans le code (bouger des étiquettes ou des légendes, ajouter des éléments graphiques), vous pouvez exporter votre graphique en format svg et éditer votre graphique dans Inkscape.\nLe module de base de R comprend une panoplie d’autres particularités que je ne couvrirai pas ici, en faveur du module ggplot2."
  },
  {
    "objectID": "04-visualisation.html#la-grammaire-graphique-ggplot2",
    "href": "04-visualisation.html#la-grammaire-graphique-ggplot2",
    "title": "4  Visualisation",
    "section": "\n4.7 La grammaire graphique ggplot2\n",
    "text": "4.7 La grammaire graphique ggplot2\n\nLe module esquisse est une extension de RStudio permettant de générer du code pour le module graphique ggplot2. La vidéo suivant, où j’utilise esquisse, montre ce en quoi consiste une grammaire graphique.\nVideo\nChaque colonne est un élément graphique qui peut être encodé pour former la position en x, en y, la taille des points, leur couleur, ou même le panneau (facet). Mais quelle forme prendra le bidule positionné? Des points, lignes, boxplots, barres? C’est ce que définit une grammaire graphique. Brièvement, une grammaire graphique permet de schématiser des données avec des marqueurs (points, lignes, etc.) sur des attributs visuels (couleurs, dimension, forme). Cette approche permet de dégager 5 composantes.\n\n\nLes données. Votre tableau est bien sûr un argument nécessaire pour générer le graphique.\n\nLes marqueurs. Un terme abstrait pour désigner les points, les lignes, les polygones, les barres, les flèches, etc. En ggplot2, ce sont des géométries, par exemple geom_point() pour définir une géométrie de points.\n\nLes attributs encodés. La position, la dimension, la couleur ou la forme que prendront les géométries. En ggplot2, on les nomme les aesthetics.\n\nLes attributs globaux. Les attributs sont globaux lorsqu’ils sont constants (ils ne dépendent pas d’une variable). Les valeurs par défaut conviennent généralement, mais certains attributs peuvent être spécifiés: par exemple la forme ou la couleur des points, le type de ligne, etc.\n\nLes thèmes. Le thème du graphique permet de personnaliser la manière dont le graphique est rendu. Il existe des thèmes prédéfinis, que vous pouvez ajuster, mais il est possible de créer vos propres thèmes (nous ne couvrirons pas cela dans ce cours).\n\n\n\n\n\nFigure 4.7: Créer une oeuvre d’art avec ggplot2, dessin de @allison_horst.\n\n\n\nLe flux de travail pour créer un graphique à partir d’une grammaire ressemble donc à ceci:\nAvec mon tableau,\nCréer un marqueur (\nencoder(position X = colonne A,\nposition Y = colonne B,\ncouleur = colonne C),\nforme globale = 1)\nAvec un thème noir et blanc\nLe module tidyverse installera des modules utilisés de manière récurrente dans ce cours, comme ggplot2, dplyr, tidyr et readr. Je recommande de le charger au début de vos sessions de travail.\n\nlibrary(\"tidyverse\")\n\nL’approche tidyverse est une grammaire des données. Le module ggplot2, qui en fait partie, est une grammaire graphique (d’où le gg de ggplot)."
  },
  {
    "objectID": "04-visualisation.html#mon-premier-ggplot",
    "href": "04-visualisation.html#mon-premier-ggplot",
    "title": "4  Visualisation",
    "section": "\n4.8 Mon premier ggplot",
    "text": "4.8 Mon premier ggplot\nPour notre premier exercice, je vais charger un tableau depuis le fichier de données abalone.data. Pour plus de détails sur les tableaux de données, consultez le chapitre 3. Le fichier de données porte sur un escargot de mer et comprend le sexe (M: mâle, F: femelle et I: enfant), des poids et dimensions des individus observés, et le nombre d’anneaux comptés dans la coquille.\n\nabalone &lt;- read_csv(\"data/abalone.csv\")\n\nRows: 4177 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Type\ndbl (8): LongestShell, Diameter, Height, WholeWeight, ShuckedWeight, Viscera...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nInspectons l’entête du tableau avec la fonction head().\n\nhead(abalone)\n\n# A tibble: 6 × 9\n  Type  LongestShell Diameter Height WholeWeight ShuckedWeight VisceraWeight\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 M            0.455    0.365  0.095       0.514        0.224         0.101 \n2 M            0.35     0.265  0.09        0.226        0.0995        0.0485\n3 F            0.53     0.42   0.135       0.677        0.256         0.142 \n4 M            0.44     0.365  0.125       0.516        0.216         0.114 \n5 I            0.33     0.255  0.08        0.205        0.0895        0.0395\n6 I            0.425    0.3    0.095       0.352        0.141         0.0775\n# ℹ 2 more variables: ShellWeight &lt;dbl&gt;, Rings &lt;dbl&gt;\n\n\nSuivant la grammaire graphique ggplot2, on pourra créer ce graphique de points comprenant les attributs suivants.\n\n\ndata = abalone, le fichier de données.\n\nmapping = aes(...), spécifié comme attribut de la fonction ggplot(), cet encodage (ou aesthetic) reste l’encodage par défaut pour tous les marqueurs du graphique. Toutefois, l’encodage mapping = aes() peut aussi être spécifié dans la fonction du marqueur (par exemple geom_point()). Dans l’encodage global du graphique, on place en x la longueur de la coquille (x = LongestShell) et on place en y le poids de la coquille (y = ShellWeight).\nPour ajouter une fonction à ggplot, comme une nouvelle couche de marqueur ou des éléments de thème, on utilise le +. Généralement, on change aussi de ligne.\nLe marqueur ajouté est un point, geom_point(), dans lequel on spécifie un encodage de couleur sur la variable Type (colour = Type) et un encodage de dimension du point sur la variable rings (size = Rings). L’attribut alpha = 0.5 se situe hors du mapping et de la fonction aes(): c’est un attribut identique pour tous les points.\n\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)\n\n\n\n\nIl existe plusieurs types de marqueurs:\n\n\ngeom_point() pour les points\n\ngeom_line() pour les lignes\n\ngeom_bar() pour les diagrammes en barre en décompte, geom_col en terme de grandeur et geom_histogram pour les histogrammes\n\ngeom_boxplot() pour les boxplots\n\ngeom_errorbar(), geom_pointrange() ou geom_crossbar() pour les marges d’erreur\n\ngeom_map() pour les cartes\netc.\n\nIl existe plusieurs attributs d’encodage:\n\nla position x, y et z (z pertinent notamment pour le marqueur geom_tile())\nla taille size\n\nla forme des points shape\n\nla couleur, qui peut être discrète ou continue :\n\n\ncolour, pour la couleur des contours\n\nfill, pour la couleur de remplissage\n\n\nle type de ligne linetype\n\nla transparence alpha\n\net d’autres types spécialisés que vous retrouverez dans la documentation des marqueurs\n\nLes types de marqueurs et leurs encodages sont décrits dans la documentation de ggplot2, qui fournit des feuilles aide-mémoire qu’il est commode d’imprimer et d’afficher près de soi (Figure 4.8).\n\n\n\n\nFigure 4.8: Aide-mémoire de ggplot2, source: https://rstudio.github.io/cheatsheets/html/data-visualization.html\n\n\n\n\n4.8.0.1 Les facettes\nDans ggplot2, les facetttes sont un type spécial d’encodage utilisé pour définir des grilles de graphiques. Elles prennent deux formes:\n\nLe collage, facet_wrap(). Une variable catégorielle est utilisée pour segmenter les graphiques en plusieurs graphiques, qui sont placés l’un à la suite de l’autre dans un arrangement spécifié par un nombre de colonnes ou un nombre de lignes.\nLa grille, facet_grid(). Une ou deux variables segmentent les graphiques selon les colonnes et les lignes.\n\nLes facettes peuvent être spécifiées n’importe où dans la chaîne de commande de ggplot2 mais, conventionnellement, on les place tout de suite après la fonction ggplot().\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  facet_wrap(~Type, ncol = 2) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)\n\n\n\n\nLa fonction cut() permet de discrétiser des variables continues en catégories ordonnées - les fonctions peuvent être utilisées à l’intérieur de la fonction ggplot.\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  facet_grid(Type ~ cut(Rings, breaks = seq(0, 30, 5))) +\n  geom_point(mapping = aes(colour = Type), alpha = 0.5)\n\n\n\n\nPar défaut, les axes des facettes, ainsi que leurs dimensions, sont les mêmes. Une telle représentation permet de comparer les facets sur une même échelle. Les axes peuvent être définis selon les données avec l’argument scales, tandis que l’espace des facettes peut être conditionné selon l’argument space - pour plus de détails, voir la fiche de documentation.\nExercice. Personnalisez le graphique avec les données abalone en remplaçant les variables et en réorganisant les facettes.\n\n4.8.1 Plusieurs sources de données\nIl peut arriver que les données pour générer un graphique proviennent de plusieurs tableaux. Lorsqu’on ne spécifie pas la source du tableau dans un marqueur, la valeur par défaut est le tableau spécifié dans l’amorce ggplot(). Il est néanmoins possible de définir une source personnalisée pour chaque marqueur en spécifiant data = ... comme argument du marqueur.\n\nabalone_siteA &lt;- data.frame(\n  LongestShell = c(0.3, 0.8, 0.7),\n  ShellWeight = c(0.05, 0.81, 0.77)\n)\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +\n  geom_point(data = abalone_siteA, size = 8, shape = 4)\n\n\n\n\n\n4.8.2 Exporter avec style\nLe fond gris est une marque distinctive de ggplot2. Il n’est toutefois pas apprécié de tout le monde. D’autres thèmes dits complets peuvent être utilisés (liste des thèmes complets). Les thèmes complets sont appelés avant la fonction theme(), qui permet d’effectuer des ajustements précis dont la liste exhaustive se trouve dans la documentation de ggplot2.\nVous pouvez aussi personnaliser le titre des axes (xlab() et ylab()) ou du graphique (ggtitle()), ou bien tout spécifier dans une même fonction ou bien tout en même temps dans labs(x = \"...\", y = \"...\", title = \"...\"). Il est possible d’utiliser des exposants dans le titre des axes avec la fonction expression(), par exemple labs(x = expression(\"Dose (kg ha\"^\"-1\"~\")\")) pour intituler l’axe des x avec \\(Dose~(kg~ha^{-1})\\). Aussi convient parfois de spécifier les limites (xlim() et ylim(), ou expand_limits(x = c(0, 1), y = c(0, 1))).\nPour exporter un ggplot, on pourra utiliser les commandes de R png(), svg() ou pdf(), ou les outils de RStudio. Toutefois, ggplot2 offre la fonction ggsave(), que l’on place en remorque du graphique, en spécifiant les dimensions (width et height) ainsi que la résolution (dpi). La résolution d’un graphique destiné à la publication est typiquement de plus de 300 dpi.\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +\n  #xlab(\"Length (mm)\") +\n  #ylab(\"Shell weight (g)\") +\n  #ggtitle(\"Abalone\") + # préférablement dans une même ligne\n  labs(x = \"Length (mm)\", y = \"Shell weight (g)\", title = \"Abalone\") +\n  xlim(c(0, 1)) +\n  theme_classic() +\n  theme(\n    axis.title = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.text.y = element_text(size = 20, angle = 90, hjust = 0.5),\n    legend.box = \"horizontal\"\n  )\n\n\n\nggsave(\"images/abalone.png\", width = 8, height = 8, dpi = 300)\n\nNous allons maintenant couvrir différents types de graphiques, accessibles selon différents marqueurs:\n\nles nuages de points\nles diagrammes en ligne\nles boxplots\nles histogrammes\nles diagrammes en barres\n\n4.8.3 Nuages de points\nL’exemple précédent est un nuage de points, que nous avons généré avec le marqueur geom_point(), qui a déjà été passablement introduit. L’exploration de ces données a permis de détecter une croissance exponentielle du poids de la coquille en fonction de sa longueur. Il est clair que les abalones juvéniles (Type I) sont plus petits et moins lourds, mais nous devrons probablement procéder à des tests statistiques pour vérifier s’il y a des différences entre mâles et femelles.\nLe graphique étant très chargé, nous avons utilisé des stratégies pour l’alléger en utilisant de la transparence et des facettes. Le marqueur geom_jitter() peut permettre de mieux apprécier la dispersion des points en ajoutant une dispersion randomisée en x ou en y.\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_jitter(mapping = aes(colour = Type, size = Rings), alpha = 0.5, width = 0.05, height = 0.1)\n\n\n\n\nDans ce cas-ci, ça ne change pas beaucoup, mais retenons-le pour la suite.\n\n4.8.4 Diagrammes en lignes\nLes lignes sont utilisées pour exprimer des liens entre une suite d’information. Dans la plupart des cas, il s’agit d’une suite d’information dans le temps que l’on appelle les séries temporelles (plus sur ce sujet au chapitre 12. En l’occurrence, les lignes devraient être évitées si la séquence entre les variables n’est pas évidente. Nous allons utiliser un tableau de données de R portant sur la croissance des orangers.\n\ndata(\"Orange\")\nhead(Orange)\n\n  Tree  age circumference\n1    1  118            30\n2    1  484            58\n3    1  664            87\n4    1 1004           115\n5    1 1231           120\n6    1 1372           142\n\n\nLa première colonne spécifie le numéro de l’arbre mesuré, la deuxième son âge et la troisième sa circonférence. Le marqueur geom_line() permet de tracer la tendance de la circonférence selon l’âge. En encodant la couleur de la ligne à l’arbre, nous pourrons tracer une ligne pour chacun d’entre eux.\n\nggplot(data = Orange, mapping = aes(x = age, y = circumference)) +\n  geom_line(aes(colour = Tree))\n\n\n\n\nLa légende ne montre pas les numéros d’arbre en ordre croissant. En effet, la légende (tout comme les facettes) classe les catégories prioritairement selon l’ordre des catégories si elles sont ordinales, ou par ordre alphabétique si les catégories sont nominales. Inspectons la colonne Tree en inspectant le tableau avec la commande str() - la commande glimpse() du tidyverse donne un sommaire moins complet que str().\n\nstr(Orange)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  35 obs. of  3 variables:\n $ Tree         : Ord.factor w/ 5 levels \"3\"&lt;\"1\"&lt;\"5\"&lt;\"2\"&lt;..: 2 2 2 2 2 2 2 4 4 4 ...\n $ age          : num  118 484 664 1004 1231 ...\n $ circumference: num  30 58 87 115 120 142 145 33 69 111 ...\n - attr(*, \"formula\")=Class 'formula'  language circumference ~ age | Tree\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Time since December 31, 1968\"\n  ..$ y: chr \"Trunk circumference\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(days)\"\n  ..$ y: chr \"(mm)\"\n\n\nEn effet, la colonne Tree est un facteur ordinal dont les niveaux sont dans le même ordre que celui la légende.\n\n4.8.5 Les histogrammes\nNous avons vu les histogrammes dans la brève section sur les fonctions graphiques de base dans R: il s’agit de segmenter l’axe des x en incréments, puis de présenter sur l’axe de y le nombre de données que l’on retrouve dans cet incrément. Le marqueur à utiliser est geom_histogram().\nRevenons à nos escargots. Comment présenteriez-vous la longueur de la coquille selon la variable Type? Selon des couleurs ou des facettes? La couleur, dans le cas des histogrammes, est celle du pourtour des barres. Pour colorer l’intérieur des barres, l’argument à utiliser est fill.\n\nggplot(data = abalone, mapping = aes(x = LongestShell)) +\n  geom_histogram(mapping = aes(fill = Type), colour = \"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOn n’y voit pas grand chose. Essayons plutôt les facettes.\n\nggplot(data = abalone, mapping = aes(x = LongestShell)) +\n  facet_grid(Type ~ .) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLes facettes permettent maintenant de bien distinguer la distribution des longueurs des juvéniles. L’argument bins, tout comme l’argument breaks du module graphique de base, permet de spécifier le nombre d’incréments, ce qui peut être très utile en exploration de données.\n\nggplot(data = abalone, mapping = aes(x = LongestShell)) +\n  facet_grid(Type ~ .) +\n  geom_histogram(bins = 60, colour = \"white\")\n\n\n\n\nLe nombre d’incréments est un paramètre qu’il ne faut pas sous-estimer. À preuve, ce tweet de @NicholasStrayer:\n\n\nHistograms are fantastic, but make sure your bin-width/number is chosen well. This is the exact same data, plotted with different bin-widths. Notice that the pattern doesn't necessarily get clearer as bin num increases. #dataviz pic.twitter.com/3MhSFwTVPH\n\n— Nick Strayer (@NicholasStrayer) 7 août 2018\n\n\n4.8.6 Boxplots\nLes boxplots sont une autre manière de visualiser des distributions. L’astuce est de créer une boîte qui s’étend du premier quartile (valeur à laquelle 25% des données ont une valeur inférieure) au troisième quartile (valeur à laquelle 75% des données ont une valeur inférieure). Une barre à l’intérieur de cette boîte est placée à la médiane (qui est en fait le second quartile). De part et d’autre de la boîte, on retrouve des lignes spécifiant l’étendue hors quartiles. Cette étendue peut être déterminée de plusieurs manières, mais dans le cas de ggplot2, il s’agit de 1.5 fois l’étendue de la boîte (l’écart interquartile). Au-delà de ces lignes, on retrouve les points représentant les valeurs extrêmes. Le marqueur à utiliser est geom_boxplot(). L’encodage x est la variable catégorielle et l’encodage y est la variable continue.\n\nggplot(data = abalone, mapping = aes(x = Type, y = LongestShell)) +\n  geom_boxplot()\n\n\n\n\nExercice. On suggère parfois de présenter les mesures sur les boxplots. Utiliser geom_jitter() avec un bruit horizontal.\n\n4.8.7 Les diagrammes en barre\nLes diagrammes en barre représentent une variable continue associée à une catégorie. Les barres sont généralement horizontales et ordonnées. Nous y reviendrons à la fin de ce chapitre, mais retenez pour l’instant que dans tous les cas, les diagrammes en barre doivent inclure le zéro pour éviter les mauvaises interprétations.\nPour les diagrammes en barre, nous allons utiliser les données de l’union internationale pour la conservation de la nature distribuées par l’OCDE.\n\n# Certaines  colonnes de caractères sont considérées comme booléennes\n# mieux vaut définir leur type pour s'assurer que le bon type\n# soit attribué\nespeces_menacees &lt;- read_csv(\"data/WILD_LIFE_14012020030114795.csv\",\n  col_types = list(\n    \"c\", \"c\", \"c\", \"c\",\n    \"c\", \"c\", \"c\", \"c\",\n    \"d\", \"c\", \"c\", \"c\",\n    \"d\", \"c\", \"c\"\n  )\n)\nhead(especes_menacees)\n\n# A tibble: 6 × 15\n  IUCN       `IUCN Category`       SPEC  Species COU   Country `Unit Code` Unit \n  &lt;chr&gt;      &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;\n1 TOT_KNOWN  Total number of know… MAMM… Mammals AUS   Austra… NBR         Numb…\n2 ENDANGERED Number of endangered… MAMM… Mammals AUS   Austra… NBR         Numb…\n3 CRITICAL   Number of critically… MAMM… Mammals AUS   Austra… NBR         Numb…\n4 VULNERABLE Number of vulnerable… MAMM… Mammals AUS   Austra… NBR         Numb…\n5 THREATENED Total number of thre… MAMM… Mammals AUS   Austra… NBR         Numb…\n6 TOT_KNOWN  Total number of know… MAMM… Mammals AUT   Austria NBR         Numb…\n# ℹ 7 more variables: `PowerCode Code` &lt;dbl&gt;, PowerCode &lt;chr&gt;,\n#   `Reference Period Code` &lt;chr&gt;, `Reference Period` &lt;chr&gt;, Value &lt;dbl&gt;,\n#   `Flag Codes` &lt;chr&gt;, Flags &lt;chr&gt;\n\n\nL’exercice consiste à créer un diagramme en barres horizontales du nombre de plantes vasculaires menacées de manière critique pour les 10 pays qui en contiennent le plus. Je vais effectuer quelques opérations sur ce tableau afin d’en arriver avec un tableau que nous pourrons convenablement mettre en graphique: si vous avez bien suivi le dernier chapitre, ces opérations devraient vous être familières!\nNous allons filtrer le tableau pour obtenir le nombre de plantes vasculaires critiquement menacées, sélectionner seulement le pays et le nombre d’espèces, les grouper par pays, additionner toutes les espèces pour chaque pays et enfin sélectionner et arranger les 10 premiers en ordre décroissant. Comme vous le voyez, la création de graphique est liée de près avec la manipulation des tableaux!\n\nespeces_crit &lt;- especes_menacees |&gt; \n  filter(IUCN == \"CRITICAL\", SPEC == \"VASCULAR_PLANT\") |&gt; \n  dplyr::select(Country, Value) |&gt; \n  group_by(Country) |&gt; \n  summarise(n_critical_species = sum(Value)) |&gt; \n  slice_max(n = 10, order_by = n_critical_species)\nespeces_crit\n\n# A tibble: 10 × 2\n   Country         n_critical_species\n   &lt;chr&gt;                        &lt;dbl&gt;\n 1 United States                 1222\n 2 Japan                          525\n 3 Canada                         315\n 4 Czech Republic                 284\n 5 Spain                          271\n 6 Belgium                        253\n 7 Austria                        172\n 8 Slovak Republic                155\n 9 Australia                      148\n10 Italy                          128\n\n\nLe premier type de diagramme en barre que nous allons couvrir est obtenu par le marqueur geom_col().\n\nggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_col()\n\n\n\n\nCe graphique est perfectible. Les barres sont verticales et non ordonnées. Souvenons-nous que ggplot2 ordonne par ordre alphabétique si aucun autre ordre est spécifié. Nous pouvons changer l’ordre en changeant l’ordre des niveaux de la variable Country selon le nombre d’espèces grâce à la fonction fct_reorder.\n\nespeces_crit &lt;- especes_crit %&gt;%\n  mutate(Country = fct_reorder(Country, n_critical_species))\n\nPour faire pivoter le graphique, nous ajoutons coord_flip() à la séquence.\n\nggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\nUne autre méthode, geom_bar(), est un raccourci permettant de compter le nombre d’occurrence d’une variable unique. Par exemple, dans le tableau abalone, le nombre de fois que chaque niveau de la variable Type.\n\nggplot(data = abalone, mapping = aes(x = Type)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\nPersonnellement, j’aime bien passer par un diagramme en lignes avec le marqueur geom_segment(). Cela me donne la flexibilité pour définir un largeur de trait et éventuellement d’ajouter un point au bout pour en faire un diagramme en suçon. Tenez, j’en profite aussi pour y ajouter du texte (décalé horizontalement) et étendre les limtes pour m’assurer que les chiffres apparaissent bien.\n\nggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) +\n  geom_point(size = 5, colour = \"black\") +\n  geom_text(aes(label = n_critical_species), hjust = -0.5) + # si ce ne sont pas des valeurs entières, arrondir avec signif()\n  expand_limits(y = c(0, 1300)) +\n  coord_flip() +\n  theme_bw()\n\n\n\n\nLes diagrammes en barre peuvent être placés en relation avec d’autres. Reprenons notre manipulation de données précédente, mais en incluant tous les pays, pour les trois niveaux d’alerte, pour les poissons.\n\nespeces_pays_iucn &lt;- especes_menacees |&gt; \n  filter(IUCN %in% c(\"ENDANGERED\", \"VULNERABLE\", \"CRITICAL\"), SPEC == \"FISH_TOT\") |&gt; \n  dplyr::select(IUCN, Country, Value) |&gt; \n  group_by(Country, IUCN) |&gt; \n  summarise(n_species = sum(Value)) |&gt; \n  group_by(Country) |&gt; \n  mutate(n_tot = sum(n_species)) |&gt; \n  ungroup() |&gt;  # pour pouvoir modifier Country, non modifiable tant qu'elle est une variable de regroupement (voir group_by)\n  mutate(Country = fct_reorder(Country, n_tot))\n\n`summarise()` has grouped output by 'Country'. You can override using the\n`.groups` argument.\n\nhead(especes_pays_iucn)\n\n# A tibble: 6 × 4\n  Country   IUCN       n_species n_tot\n  &lt;fct&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 Australia CRITICAL           8    48\n2 Australia ENDANGERED        16    48\n3 Australia VULNERABLE        24    48\n4 Austria   CRITICAL           6    39\n5 Austria   ENDANGERED        18    39\n6 Austria   VULNERABLE        15    39\n\n\nPour placer les barres les unes à côté des autres, nous spécifions position = \"dodge\".\n\nggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +\n  geom_col(aes(fill = IUCN), position = \"dodge\") +\n  coord_flip()\n\n\n\n\nIl est parfois plus pratique d’utiliser les facettes.\n\nggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +\n  facet_grid(IUCN ~ .) +\n  geom_col() +\n  coord_flip()\n\n\n\n\nPour perfectionner encore ce graphique, on pourrait réordonner les facettes individuellement, mais ne nous égarons par trop.\n\n4.8.8 Exporter un graphique\nPlus besoin d’utiliser la fonction png() en mode ggplot2. Utilisons plutôt ggsave().\n\nggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +\n  facet_grid(IUCN ~ .) +\n  geom_col(aes(fill = IUCN)) +\n  coord_flip()\n\n\n\nggsave(\"images/especes_pays_iucn.png\", width = 6, height = 8, dpi = 300)"
  },
  {
    "objectID": "04-visualisation.html#les-graphiques-comme-outil-dexploration-des-données",
    "href": "04-visualisation.html#les-graphiques-comme-outil-dexploration-des-données",
    "title": "4  Visualisation",
    "section": "\n4.9 Les graphiques comme outil d’exploration des données",
    "text": "4.9 Les graphiques comme outil d’exploration des données\n\n\n\n\nFigure 4.9: Explorer les données avec ggplot2, dessin de @allison_horst.\n\n\n\nLa plupart des graphiques que vous créerez ne seront pas destinés à être publiés, mais serviront d’outil d’exploration des données. Le jeu de données datasaurus, présenté en début de chapitre, permet de saisir l’importance des outils graphiques pour bien comprendre les données.\n\ndatasaurus &lt;- read_tsv(\"data/DatasaurusDozen.tsv\")\n\nRows: 1846 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): dataset\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(datasaurus)\n\n# A tibble: 6 × 3\n  dataset     x     y\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 dino     55.4  97.2\n2 dino     51.5  96.0\n3 dino     46.2  94.5\n4 dino     42.8  91.4\n5 dino     40.8  88.3\n6 dino     38.7  84.9\n\n\nProjetons d’abord les coordonnées x et y sur un graphique.\n\nggplot(data = datasaurus, mapping = aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\nCe graphique pourrait ressembler à une distribution binormale, ou un coup de 12 dans une porte de grange. Mais on aperçoit des données alignées, parfois de manière rectiligne, parfois en forme d’ellipse. Le tableau datasaurus a une colonne d’information supplémentaire. Utilisons-la comme catégorie pour générer des couleurs différentes.\n\nggplot(data = datasaurus, mapping = aes(x = x, y = y)) +\n  geom_point(mapping = aes(colour = dataset))\n\n\n\n\nCe n’est pas vraiment plus clair. Il y a toutefois des formes qui se dégagent, comme des ellipses et des lignes. Et si je regarde bien, j’y vois une étoile. La catégorisation pourrait-elle être mieux utilisée si on segmentait par facettes au lieu des couleurs?\n\nggplot(data = datasaurus, mapping = aes(x = x, y = y)) +\n  facet_wrap(~dataset, nrow = 2) +\n  geom_point(size = 0.5) +\n  coord_equal()\n\n\n\n\nVoilà! Fait intéressant : ni les statistiques, ni les algorithmes de regroupement ne nous auraient été utiles pour différencier les groupes!\n\n4.9.1 Des graphiques interactifs!\nLes graphiques sont traditionnellement des images statiques. Toutefois, les graphiques n’étant pas dépendants de supports papiers peuvent être utilisés de manière différente, en ajoutant une couche d’interaction. Conçue à Montréal, plotly est un module graphique interactif en soi. Il peut être utilisé grâce à son outil web, tout comme il peut être interfacé avec R, Python, javascript, etc. Mais ce qui retient notre attention ici est son interface avec ggplot2.\nLes graphiques ggplot2 peuvent être enregistrés en tant qu’objets. Il peuvent conséquemment être manipulés par des fonctions. La fonction ggplotly permet de rendre votre ggplot interactif.\n\nlibrary(\"plotly\")\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nespeces_crit_bar &lt;- ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) +\n  geom_point(size = 6) +\n  coord_flip()\nggplotly(especes_crit_bar)\n\n\n\n\n\nVous pouvez publier votre graphique plotly en ligne pour le partager ou l’inclure dans une publication web. Il vous faudra créer un compte plotly, puis générer une clé d’utilisation dans Settings &gt; API Keys &gt; Generate key. Pour des raisons de sécurité, la clé du bloc ci-dessous ne fonctionnera pas. J’ai désactivé le bloc de code, mais le résultat se trouve en suivant le lien généré par plotly: https://plot.ly/~essicolo/152/.\nSys.setenv(\"plotly_username\"=\"essicolo\")\nSys.setenv(\"plotly_api_key\"=\"iavd1ycE2iiqOp9YD45I\")\n\nchart_link &lt;- api_create(x = ggplotly(especes_crit_bar), \n                         filename = \"public-graph\",\n                         sharing = \"public\",\n                         fileopt = \"overwrite\")\nchart_link\n\n4.9.2 Des extensions de ggplot2\n\nggplot2 est un module graphique élégant et polyvalent. Il a pourtant bien des limitations. Justement, le module est conçu pour être implémenté avec des extensions. Vous en trouverez plusieurs sur exts.ggplot2.tidyverse.org, mais en trouverez de nombreuses autres en cherchant avec le terme ggplot2 sur github.com, probablement la plate-forme (voire un réseau social) de développement de logiciels la plus utilisée dans le monde. En voici quelques unes.\n\n\nggthemr: spécifier un thème graphique une seule fois dans votre session, et tout le reste suit.\n\ncowplot et patchwork permettent de créer des graphiques prêts pour la publication, par exemple en créant des grilles de plusieurs ggplots, en les numérotant, etc.\nSi les thèmes de base ne vous conviennent pas, vous en trouverez d’autres en installant ggthemes.\n\nggmap et ggspatial sont deux extensions pour créer des cartes. Un chapitre sur les données spatiales est en développement.\n\nggtern permet de créer des diagrammes ternaires, qui sont utiles pour la visualisation de proportions incluant trois composantes, par exemple pour la granulométrie des sols.\n\nggprism permet de personnaliser les ggplots et leur donner un aspect similaire aux graphiques du logiciel statistique prism\n\n\n4.9.3 Aller plus loin avec ggplot2\n\n\n\nClaus O. Wilke est professeur en biologie intégrative à l’Université du Texas à Austin. Son livre Fundamentals of Data Visualization est un guide théorique et pratique pour la visualisation de données avec ggplot2.\nLe site data-to-viz.com vous accompagne dans le choix du graphique à créer selon vos données.\nLe site r-graph-gallery.com offre des recettes pour créer des graphiques avec ggplot2.\nLe livre R Graphics Cookbook, disponible entièrement en ligne, offre aussi des recettes pour réaliser différents graphiques."
  },
  {
    "objectID": "04-visualisation.html#extra-règles-particulières",
    "href": "04-visualisation.html#extra-règles-particulières",
    "title": "4  Visualisation",
    "section": "\n4.10 Extra: Règles particulières",
    "text": "4.10 Extra: Règles particulières\n\nLes mauvais graphiques peuvent survenir à cause de l’ignorance, bien sûr, mais souvent ils existent pour la même raison que la boeuferie [bullhist] verbale ou écrite. Parfois, les gens ne se soucient pas de la façon dont ils présentent les données aussi longtemps que ça appuie leurs arguments et, parfois, ils ne se soucient pas que ça porte à confusion tant qu’ils ont l’air impressionnant. \\(-\\) Carl Bergstorm et Jevin West, Calling Bullshit Read-Along Week 6: Data Visualization\n\nUne représentation visuelle est un outil tranchant qui peut autant présenter un état véritable des données qu’une perspective trompeuse. Bien souvent, une ou plusieurs des 5 qualités ne sont pas respectées. Les occasions d’erreur ne manquent pas - j’en ai fait mention dans la section Choisir le bon type de graphique. Maintenant, notons quelques règles particulières.\n\n4.10.1 Ne tronquez pas inutilement l’axe des \\(y\\)\n\nTronquer l’axe vertical peut amener à porter de fausses conclusions.\n\n\n\n\nFigure 4.10: Effets sur la perception d’utiliser différentes références. Source: Yau (2015), Real Chart Rules to Follow.\n\n\n\n\n\nEffets sur la perception d’utiliser différentes références. Source: Yau (2015), Real Chart Rules to Follow.\n\nLa règle semble simple: les diagrammes en barre (utilisés pour représenter une grandeur) devraient toujours présenter le 0 et les diagrammes en ligne (utilisés pour présenter des tendances) ne requièrent pas nécessairement le zéro (Bergstrom et West, Calling bullshit: Misleading axes on graphs. Mais le zéro n’est pas toujours lié à une quantité particulière : par exemple, la température ou un log-ratio. De plus, avec un diagramme en ligne, on pourra toujours magnifier des tendances en zoomant sur une variation somme toute mineure. On arrive donc moins à une règle qu’une qualité d’un bon graphique, en particulier la qualité no 1 de Cairo: offrir une représentation honnête des données. Par exemple, Nathan Yau, auteur du blogue Flowing Data, propose de présenter des résultats de manière relative à la mesure initiale. C’est d’ailleurs ce qui a été fait pour générer le graphique de Michael Mann et al. au tout début de ce chapitre à la Figure 4.1, où le zéro correspond à la moyenne des températures enregistrées entre 1961 et 1990.\nIl peut être tentant de tronquer l’axe des \\(y\\) lorsque l’on désire superposer deux axes verticaux. Souvent, l’utilisation de plusieurs axes verticaux amène une perception de causalité dans des situations de fausses corrélations. On ne devrait pas utiliser plusieurs axes verticaux.\n\n4.10.2 Utilisez un encrage proportionnel\nCette règle a été proposée par Edward Tufte dans Visual Display of Quantitative Information. Une des raisons pour lesquelles on évite de tronquer l’axe des \\(y\\) en particulier pour les diagrammes en barre est que l’aire représentant une mesure (la quantité d’“encre” nécessaire pour la dessiner) devrait être proportionnelle à sa magnitude. Les diagrammes en barre sont particulièrement sensibles à cette règle, étant donnée que la largeur des barres peuvent amplifier l’aire occupée. Deux solutions dans ce cas: (1) utiliser des barres minces ou (2) préférer des “diagrammes de points” (dot charts, à ne pas confondre aux nuages de points).\nL’encrage a beau être proportionnel, la difficulté que les humains éprouvent à comparer la dimension des cercles, et a fortiori la dimension de parties de cercle, donne peu d’avantage à utiliser des diagrammes en pointe de tarte, souvent utilisés pour illustrer des proportions. Nathan Yau suggère de les utiliser avec suspicions et d’explorer d’autres options.\n\nPour comparer deux proportions, une avenue intéressante est le diagramme en pente, suggéré notamment par Ann K. Emery.\n\nPar extension, le diagramme en pente devient un diagramme en ligne lorsque plusieurs types de proportions sont comparées, ou lorsque des proportions évoluent selon des données continues.\nDe la même manière, les diagrammes en bulles ne devraient pas être représentatifs de la quantité, mais permettent plutôt de contextualiser des données. Justement, le graphique tiré des données de Gap minder présenté plus haut est une contextualisation: l’aire d’un cercle ne permet pas de saisir la population d’un pays, mais de comparer grossièrement la population d’un pays par rapport aux autres.\n\n4.10.3 Publiez vos données\nVous avez peut-être déjà feuilleté un article et voulu avoir accès aux données incluses dans un graphique. Il existe des outils pour digitaliser des graphiques pour en extraire les données. Mais le processus est fastidieux, long, souvent peu précis. De plus en plus, les chercheurs sont encouragés à publier leurs données et leurs calculs. Matplotlib et Seaborn sont des outils graphiques classiques qui devraient être accompagnés des données et calculs ayant servi à les générer. Mais ce n’est pas idéal non plus. En revanche, les outils graphiques modernes comme Plotly et Altair peuvent être exportés en code javascipt, qui contient toutes les informations sur les données et la manière de les représenter graphiquement. Ce chapitre a pour objectif de vous familiariser avec les outils de base les plus communément utilisés en calcul scientifique avec R, mais je vous encourage à explorer la nouvelle génération d’outils graphiques. Nous verrons ça au chapitre 5.\n\n4.10.4 Visitez Junk Charts de temps à autre\nLe statisticien et blogueur Kaiser Fung s’affaire quotidiennement à proposer des améliorations à de mauvais graphiques sur son blogue Junk Charts."
  },
  {
    "objectID": "05-github.html#un-code-reproductible",
    "href": "05-github.html#un-code-reproductible",
    "title": "5  Science ouverte et reproductibilité",
    "section": "\n5.1 Un code reproductible",
    "text": "5.1 Un code reproductible\n\n\n\n\nFigure 5.2: A Guide to Reproducible Code in Ecology and Evolution, BES 2017\n\n\n\nLa British ecological society offre des lignes guide pour créer un flux de travail reproductible (BES, 2017). En outre, les principes suivants doivent être respectés (ma traduction, avec ajouts).\n\nCommencez votre analyse à partir d’une copie des données brutes. Les données doivent être fournies dans un format ouvert (csv, json, sqlite, etc.). Évitez de démarrer une analyse par un chiffrier électronique ou un logiciel propriétaire (qui n’est pas open source). En ce sens, démarrer avec Excel (xls ou xlsx) est à éviter, tout comme le sont les données encodées pour SPSS ou SAS.\nToute opération sur les données, que ce soit du nettoyage, des fusions, des transformations, etc. devrait être effectuée avec du code, non pas manuellement. S’il s’agit d’une erreur de frappe dans un tableau, on peut déroger à la règle. Mais s’il s’agit par exemple d’éliminer des outliers, ne supprimez pas des entrées de vos données brutes. De même, n’effectuez pas de transformation de vos données brutes à l’extérieur du code. En somme, vos calculs devraient être en mesure d’être lancés d’un seul coup, sans opérations manuelles intermédiaires.\nSéparez vos opérations en unités logiques thématiques. Par exemple, vous pourriez séparer votre code en parties: (i) charger, fusionner et nettoyer les données, (ii) analyser les données, (iii) créer des fichiers comme des tableaux et des figures.\nÉliminez la duplication du code en créant des fonctions personnalisées. Assurez-vous de commenter vos fonctions en détails, expliquez ce qui est attendu comme entrées et comme sorties, ce qu’elles font et pourquoi.\nDocumentez votre code et vos données à même les feuilles de calcul ou dans un fichier de documentation séparé.\nTout fichier intermédiaire devrait être séparé de vos données brutes.\n\n\n5.1.1 Structure d’un projet\nUn projet de calcul devrait être contenu en un seul dossier. Si vous n’avez que quelques projets, il est assez facile de garder l’info en mémoire. Toutefois, en particulier en milieu d’entreprise, il se pourrait fort bien que vous ayez à mener plusieurs projets de front. Certaines entreprises créent des numéros de projet: vous aurez avantage à nommer vos dossiers avec ces numéros, incluant une brève description. Pour ma part, j’ordonne mes projets chronologiquement par année, avec un descriptif.\n📁 2019_abeille-canneberge\nNotez que je n’utilise ni espace, ni caractère spécial dans le nom du fichier, pour éviter les erreurs potentielles avec des logiciels capricieux.\nÀ l’intérieur du dossier racine du projet, j’inclus l’information générale: données source (souvent des fichiers Excel), manuscrit (mémoire, thèse, article, etc.) documentation particulière (pour les articles, j’utilise Zotero, un gestionnaire de référence), photos et, évidemment, mon dossier de code (par exemple rstats).\n\n📂 2019_abeille-canneberge\n|-📁 documentation\n|-📁 manuscrit\n|-📁 photos\n|-📁 rstats\n|-📁 source\nSi vous rédigez votre manuscrit à même votre code (en Latex, Lyx, markdown, R markdown ou Quarto que nous verrons plus loin), vous pouvez très bien l’inclure dans votre fichier de calcul.\nÀ l’intérieur du fichier de calcul, vous aurez votre projet RStudio et vos feuilles de calcul séquencées. J’utilise 01-, et non pas 1- pour éviter que le 10- suive le 1- dans le classement en ordre alpha-numérique au cas où j’aurais plus de 10 feuilles de calcul. J’inclus un fichier README.md (extension md pour markdown), qui contient les informations générales de mes calculs. Les données brutes (csv) sont placées dans un dossier data, mes graphiques sont exportés dans un dossier images, mes tableaux sont exportés dans un dossier tables et mes fonctions externes sont exportées dans un dossier lib.\n\n📂 rstats\n|-📁 data\n|-📁 images\n|-📁 lib\n|-📁 tables\n📄 bees.Rproj\n📄 01_clean-data.R\n📄 02_data-mining.R\n📄 03_data-analysis.R\n📄 04_data-modeling.R\n📄 README.md\nJe décris les noms de fichiers dans la langue de communication utile pour le rendu final du projet, souvent en anglais lors de publications académiques. J’évite les noms de fichier qui ne sont pas informatifs, par exemple 01.R ou Rplot1.png, ainsi que les majuscules, les caractères spéciaux et les espaces comme dans Deuxième essai.R (le README.md est une exception).\nPour partager un dossier de projet sur R, on n’a qu’à le compresser (zip), puis à l’envoyer. Pour que le code fonctionne sur un autre ordinateur, les liens vers les fichiers de données à importer ou les graphiques exportés doivent être relatifs au fichier R ouvert dans votre projet, non pas le chemin complet sur votre ordinateur.\n\n\n\n\nFigure 5.3: Retrouvez votre chemin, dessin de @Allison Horst\n\n\n\nTout comme la BSE, l’organisme sans but lucratif rOpenSci offre un guide sur la reproductibilité (le répertoire est maintenant archivé et n’est plus mis à jour depuis 2022, mais vous y avez tout de même accès en lecture seule).\n\n5.1.2 Les formats markdown\n\nUn code reproductible est un code bien décrit. La structure de projet présentée précédemment propose de segmenter le code en plusieurs fichiers R. Cette manière de procéder est optionnelle. Si le fichier de calcul n’est pas trop encombrant, on pourra n’en utiliser qu’un seul, par exemple stats.R. À l’intérieur même des feuilles de calcul R, vous devrez commenter votre code pour en expliquer les étapes, par exemple:\n#############\n## Titre 1 ##\n#############\n\n# Titre 2\n## Titre 3\ndata &lt;- read_csv(\"data/abeilles.csv\") # commentaire particulier\nRStudio a développé une approche plus conviviale avec son format R markdown. Le langage markdown permet de formater un texte avec un minimum de décorations, et R markdown permet d’intégrer du texte et des codes. Le manuel original de notes de cours était par ailleurs entièrement écrit en R markdown.\n\n\n\n\nFigure 5.4: La magie de R markdown, dessin de @Allison Horst\n\n\n\nDepuis quelques années, Quarto a fait son entrée en scène. Il s’agit de la nouvelle version de R markdown qui se veut plus attrayante, accessible, stable et polyvalente. Quarto a la versatilité d’utiliser, dans un seul document, des morceaux de code provenant de différents langages, puis de produire des fichiers sous différents formats en une seule étape. De plus, vous pouvez l’utiliser directement en RStudio ou dans Jupyter. La version actuelle du manuel est montée à l’aide de Quarto, tout comme les diapositives présentées dans les capsules vidéo du cours.\n\n\n\n\n\n(a) La versatilité de Quarto\n\n\n\n\n\n\n\n(b) Les étapes de rendu de Quarto\n\n\n\nFigure 5.5: Dessins de la présentation Hello, Quarto par Julia Lowndes et Mine Çetinkaya-Rundel, présentée à la conférence RStudio de 2022. Illustrés par @Allison Horst.\n\n\n\n5.1.2.1 Le langage markdown\nLe langage markdown est ce qu’on appelle “un langage de balises léger” qui vous permet d’introduire dans votre texte brut des balises simples pour effectuer le formatage. Un fichier portant l’extension .md ou .markdown est un fichier texte clair (que vous pouvez ouvrir et éditer dans votre éditeur texte préféré), tout comme un fichier .R. RStudio permet notamment d’éditer un fichier .md. Il existe aussi de nombreux éditeurs de texte spécialisés en édition markdown - mon préféré est Typora. Les décorations (ou balises) principales en markdown sont les suivantes (les citations utilisées ci-après sont tirées du roman Dune, de Frank Herbert).\nItalique. Pour accentuer en italique, balisez le texte avec des astérisques simples *. Par exemple, “Pourrais-je porter parmi vous le nom de *Paul-Muad'dib*?” devient “Pourrais-je porter parmi vous le nom de Paul-Muad’dib?”\nGras. Pour accentuer en gras, balisez le texte avec des doubles astérisques **. Par exemple, “L'espérance **ternit** l'observation.” devient “L’espérance ternit l’observation”.\nLargeur fixe. Pour un texte à largeur fixe (signifiant du code), balisez le texte avec des accents graves. Par exemple, “Quel nom donnez-vous à la petite `souris`, celle qui saute ?” devient “Quel nom donnez-vous à la petite souris, celle qui saute?”\nListes. Pour effectuer une liste numérotée, utilisez le chiffre 1. Par exemple,\n1. Paul\n1. Leto\n1. Alia\ndevient\n\nPaul\nJessica\nAlia\n\nDe même, pour une liste à puces, changez le 1. par le - ou le *.\nEntêtes. Les titres sont précédés par des #. Un # pour un titre 1, deux ## pour un titre 2, etc. Par exemple,\n\n# Imperium\n## Landsraad\n### Maison des Atréides\n### Maison des Harkonnen\n## CHOAM\n# Guilde des navigateurs\nInsérera les titres appropriés (que je n’insère pas pour ne pas bousiller la structure de ce texte).\nLiens. Pour insérer des liens, le texte est entre crochets directement suivi du lien entre parenthèses. Par exemple, “Longue vie aux [combattants](https://youtu.be/Cv87NJ2xX0k?t=59)” devient “Longue vie aux combattants”.\nÉquations. Les équations suivent la syntaxe Latex entre deux $$ pour les équations sur une ligne et entre des doubles $$ $$ pour les équations sur un paragraphe. Par exemple, $c = \\sqrt{a^2 + b^2}$ devient \\(c = \\sqrt{a^2 + b^2}\\).\nImages. Pour insérer une image, ![nom de l'image](images/spice-must-flow.png).\nUne liste exhaustive des balises markdown est disponible sous forme d’aide-mémoire. L’extension de RStudio remedy, installable de la même façon qu’un module, fera apparaître une section REMEDY dans le menu Addins, où vous trouverez toutes sortes d’options de formatage automatique (Figure 5.6). Toutefois, vous verrez plus loin dans la section sur Quarto que d’autres outils encore plus conviviaux existent désormais.\n\n\n\n\nFigure 5.6: Menu des extensions de RStudio, avec l’extension remedy\n\n\n\n\n5.1.2.2 R markdown\nDans RStudio, ouvrez un R markdown par File &gt; New file &gt; R Markdown. Si le module rmarkdown n’est pas installé, RStudio vous demandera de l’installer. Une fenêtre apparaîtra.\n\n\n\n\nFigure 5.7: Nouveau fichier R markdown\n\n\n\nLes options d’exportation pourront être modifiées par la suite.\nUn fichier d’exemple sera créé, et vous pourrez le modifier. Les parties de texte sont écrits en markdown, et le code R est enchâssé entre les balises ```{r} et ```. Je nommerai ces parties de code des cellules ou des blocs de code. Vous pouvez utiliser le raccourci clavier Ctrl + Alt + I pour insérer rapidement un bloc de code.\nDes options de code peuvent être utilisées à l’intérieur des accolades {r}. Par exemple\n\n\n{r, filtre-outliers} donne le nom filtre-outliers au bloc de code, qui permet nommément de nommer les images créées dans le bloc de code.\n\n{r, eval = FALSE} permet d’activer (TRUE, valeur par défaut) ou de désactiver (FALSE) le calcul de la cellule.\n\n{r, echo = FALSE} permet de n’afficher que la sortie de la cellule de code en n’affichant pas le code, par exemple un graphique ou le sommaire d’une régression.\n\n{r, results = FALSE} permet de n’afficher que le code, mais pas la sortie.\n\n{r, warning = FALSE, message = FALSE, error = FALSE} n’affichera pas les avertissements, les messages automatiques et les messages d’erreur.\n\n{r, fig.width = 10, fig.height = 5, fig.align = \"center\"} affichera les graphiques dans les dimensions voulues, alignée au centre (\"center\"), à gauche (\"left\") ou à droite (\"right\").\n\nNotez que vous pouvez exécuter rapidement du code sur une ligne avec la formulation `r `, par exemple la moyenne des nombres `\\r a&lt;-round(runif(4, 0, 10)); a` est de `\\r mean(a)`, en enlevant les \\ devant les r (ajoutées artificiellement pour éviter que le code soit calculé), sera la moyenne des nombres 7, 2, 2, 8 est de 4.75\nUne fois que vous serez satisfait de votre document, cliquer sur Knit  et le fichier de sortie sera généré. Le guide qui permet de générer le fichier de sortie est tout en haut du fichier. Nous l’appelons le YAML (acronyme récursif de YAML Ain’t Markup Language). Prenez le YAML suivant.\n---\ntitle: \"Dune\"\nauthor: \"Frank Herbert\"\ndate: \"1965-08-01\"\noutput: github_document\n---\nLe titre, l’auteur et la date sont spécifiés. Pour indiquer la date courante, on peut simplement la générer avec R en remplaçant \"1965-08-01\" par 2024-02-29. La spécification output indique le type de document à générer, par exemple html_document pour une page web, pdf_document pour un pdf, ou word_document pour un docx. Dans ce cas-ci, j’indique github_document pour créer un fichier markdown comprenant notamment des liens relatifs vers les images des graphiques générés. Pourquoi un github_document? C’est le sujet de la section 5.2. Mais avant cela, je vous réfère à un autre aide-mémoire.\n\n\n\n\nFigure 5.8: Aide-mémoire pour R Markdown, Source: RStudio\n\n\n\n\n5.1.3 Quarto markdown\nAvant d’utiliser Quarto, il vous faudra l’installer sur votre ordinateur. Lorsque ce sera fait, une fois RStudio redémarré, vous devriez pouvoir créer un document Quarto par File &gt; New file &gt; Quarto Document. Toutefois, je vous recommande plutôt de créer directement un nouveau projet, et de choisir le type de projet s’appliquant à votre situation (typiquement un projet Quarto, mais le manuel de cours par exemple est un livre Quarto).\n\n\n\n\nFigure 5.9: Création d’un nouveau projet Quarto.\n\n\n\nEn créant votre projet dans un nouveau répertoire, vous pouvez choisir le moteur (ici nous utilisons knitr, mais vous pourriez aussi choisir Jupyter si vous préférez) et activer le suivi d’environnement reproductible renv (vous en apprendrez plus sur renv à la section 5.3). De plus, vous pouvez directement créer un répertoire git, ce qui facilitera les étapes d’initialisation de votre répertoire si vous souhaitez en faire un. Pour l’instant, je vous suggère de garder ces cases vides si vous souhaitez seulement utiliser Quarto.\n\n\n\n\nFigure 5.10: Options de création d’un nouveau projet Quarto.\n\n\n\nIl existe un guide très détaillé sur l’utilisation de Quarto sur leur site internet. Je vous suggère pour bien démarrer la lecture suivante.\n\n5.1.3.1 Balises des blocs de code\nEn général, vous pouvez utiliser les mêmes balises dans Quarto qu’avec R markdown. Les mêmes options peuvent être utilisées à l’intérieur des accolades {r} de la même manière (par exemple : {r, important-figure, fig.width = 10, fig.height = 5, fig.align = \"center\"}), ou alors de façon plus claire avec le style YAML de la façon suivante :\n```{r}\n#| label: important-figure\n#| fig-width: 10\n#| fig-height: 5\n#| fig-align: center\n\nknitr::include_graphics(\"images/important-figure.png\")\n```\n\n5.1.3.2 Rendu de documents\nPour créer le rendu de votre document, au lieu d’utiliser le bouton Knit, vous pouvez utiliser Render. Si vos options permettent la sortie de plusieurs formats de fichiers, vous verrez une liste déroulante vous permettant de choisir le format à produire.\n\n\n\n\nFigure 5.11: Créer un rendu de votre document Quarto (Tutorial: Hello Quarto).\n\n\n\nCe bouton est bien pratique lorsque vous avez un seul document simple .qmd dans votre dossier ou alors que vous voulez rapidement visualiser une sortie, mais il est préférable de prendre l’habitude d’utiliser la commande quarto_render pour faire le rendu de tous les fichiers dans le dossier. Certaines options de cette commande vous permettent par exemple de faire le rendu d’un seul format de fichier.\n```{r}\ninstall.packages(\"quarto\")\nquarto::quarto_render(\"index.qmd\")\n```\n\n5.1.3.3 Options YAML\nComme pour R markdown, nous utilisons le guide YAML pour spécifier les options de rendements de documents. On peut l’utiliser directement en haut du fichier .qmd en en-tête, mais en général je recommande plutôt d’utiliser le fichier _quarto.yml qui devrait avoir été créé dans votre dossier de projet lors de la création du projet. Vous pouvez y spécifier par exemple le dossier de sortie des fichiers créés, donner des paramètres pour chaque format de fichiers de sortie, etc. Les informations dans ce fichier sont les consignes qui seront suivies lors de l’interprétation et de la création de vos documents. Voici un exemple du contenu d’un fichier YAML inspiré du manuel des notes de cours.\nproject:\n  type: book\n  output-dir: docs\n  \nbook:\n  title: \"Titre livre\"\n  author:\n  - name: \"Nom auteur 1\"\n  - name: \"Nom auteur 2\"\n  date: today\n  date-format: iso\n  chapters:\n    - href: index.qmd\n      text: Préface\n    - 01-chap1.qmd\n    - 02-chap2.qmd\n  cover-image: images/cover.png\n  \nformat:\n  html:\n    theme: flatly\n    code-link: true\n    css: styles.css\n    toc: true\n  pdf:\n    documentclass: scrreprt\n    toc: true\n\n\nAide-mémoire de Quarto\n\n\n5.1.3.4 Éditeur visuel\nEnfin, un des grands avantages de Quarto en termes d’accessibilité est l’éditeur visuel. Personnellement, je préfère généralement travailler avec l’éditeur de code Source, qui me permet de mieux voir ce qu’il se passe exactement et d’avoir un meilleur contrôle sur le formatage. Toutefois, il m’arrive de passer à l’éditeur visuel lorsque j’ignore comment introduire certains éléments.\n\n\n\n\nFigure 5.12: Éditeur visuel vs source (Tutorial: Hello Quarto).\n\n\n\nEn mode d’édition visuelle, il est très facile d’insérer un nouvel élément : il vous suffit de taper / : une liste déroulante devrait alors apparaître. Vous pouvez alors choisir un élément dans la liste, ou d’abord spécifier votre recherche (comme à la Figure 5.13).\n\n\n\n\nFigure 5.13: Insérer une équation avec l’éditeur visuel. (Tutorial: Authoring).\n\n\n\nPour finir, il existe des modules vous permettant de créer directement un document à partir de formats de référence. Par exemple, quarto-journals vous permet de créer rapidement des fichiers au format de votre revue préférée. Puisque Quarto est plutôt récent, ces outils sont toujours en développement et peuvent parfois être imparfaits (ou inexistants pour votre revue), mais ils seront plus performants avec le temps et des revues devraient être ajoutées."
  },
  {
    "objectID": "05-github.html#sec-intro-git",
    "href": "05-github.html#sec-intro-git",
    "title": "5  Science ouverte et reproductibilité",
    "section": "\n5.2 Introduction à GitHub",
    "text": "5.2 Introduction à GitHub\nLe système de suivi de version git (open source) a été créé par Linus Torvalds, aussi connu pour avoir créé Linux. git prend une photo de votre répertoire de projet à chaque fois que vous commettez un changement. Vous pourrez revenir sans problème sur d’anciennes versions si quelque chose tourne mal, et vous pourrez publier le résultat final sur un service d’hébergement utilisant git.\nIl existe plusieurs services pour rendre git utilisable en ligne, mais GitHub est définitivement le plus utilisé d’entre tous. La plateforme GitHub est presque devenue un réseau social de développement. GitHub, maintenant la propriété de Microsoft, n’est en soi pas open source. Si comme moi vous avez un penchant pour l’open source, je vous redirige vers la plateforme GitLab, qui fonctionne à peu près de la même manière que GitHub, mais dans sa version gratuite GitLab vous octroie autant de répertoires privés que vous désirez. Seul hic, alors que la plateforme GitHub sera fort probablement toujours vivante dans plusieurs années, on en est moins sûr pour GitLab. C’est pourquoi, en règle générale, j’utilise GitHub à des fins professionnelles mais GitLab à des fins personnelles.\nPour suivre cette partie du cours, je vous invite à créer un compte sur GitHub ou GitLab, à votre choix. Créez un nouveau dépôt (New repository).\n\n\n\n\nFigure 5.14: Nouveau dépôt avec GitHub\n\n\n\n\n\n\n\nFigure 5.15: Nouveau dépôt avec GitLab\n\n\n\nPour utiliser git, vous pourrez toujours travailler en ligne de commande (c’est ce que je préfère personnellement mais je ne le conseille pas nécessairement; vous trouverez toute la documentation nécessaire sur le site de GitHub). Il est aussi possible de travailler avec les outils intégrés dans RStudio, mais je vous suggère d’utiliser GitHub Desktop (qui fonctionne aussi sur GitLab) - évidemment, d’autres logiciels similaires existent. Github Desktop vous permettra d’abord de cloner un répertoire en ligne. Le clonage vous permet de créer une copie locale (sur votre ordinateur) du répertoire.\n\n\n\n\nFigure 5.16: Cloner dépôt avec GitHub\n\n\n\n\n\n\n\nFigure 5.17: Cloner dépôt avec GitLab\n\n\n\nUne fois que le dépôt est cloné, il est sur votre ordinateur. Lorsque vous effectuez un changement, vous devez commettre (commit), puis envoyer (push) vos changements vers le dépôt en ligne. Pour que votre document markdown soit lisible par GitHub et GitLab, il doit être exporté au format html. Un fichier .md sera créé, et inclura les détails de votre feuille de calculs, images y compris!\nDe plus, puisque les fichiers HTML générés par Quarto remplacent certains paramètres des fichiers HTML classiques, vous devez ajouter dans le dossier racine de votre projet un fichier vide .nojekyll à partir de votre terminal. Ce fichier indiquera GitHub d’ignorer les procédures Jekyll qu’il lance normalement sur les fichiers HTML (voir site web de Quarto pour plus de détails).\n\n\n\n\n\n\n\nMac/Linux\n\n\nTerminal\n\ntouch .nojekyll\n\n\n\nWindows\n\n\nTerminal\n\ncopy NUL .nojekyll\n\n\n\n\n\n\n\n\nFigure 5.18: Commettre et déployer un dépôt avec GitHub\n\n\n\nL’interface de GitHub Desktop vous permet de revenir en arrière en éliminant des commits précédents.\n\n\n\n\nFigure 5.19: Revenir en arrière avec GitHub desktop\n\n\n\nVous pourrez ajouter des collaborateurs à votre dépôt, pour que plusieurs personnes travaillent de front sur un même dépôt. Il est aussi possible de créer une branche d’un dépôt, fusionner la branche de développement avec la branche principale, commenter les codes, suggérer des changements, etc., mais cela sort du cadre d’un cours sur la reproductibilité.\nSi vous souhaitez créer un site internet comme celui des notes de cours à partir de votre répertoire GitHub, vous devez activer votre page avec GitHub Pages, créer un lien, choisir la branche et le dossier contenant les fichiers HTML et publier.\n\n\n\n\nFigure 5.20: Activer la page avec GitHub Pages\n\n\n\nEnfin, pour renvoyer un article vers votre matériel supplémentaire, insérez le lien dans la section méthodologie. Il peut s’agir du lien complet, ou bien d’un lien raccourci (avec par exemple tinyurl ou bitly). Par exemple,\n\nThe data and the R code used to compute the results are both available as supplementary material at https://git.io/fhHEj.\n\nNotez que RStudio offre une interface pour utiliser git via un onglet afiché en haut à droite dans l’affichage par défaut. Ne l’ayant jamais utilisé, je ne me sens pas à l’aise d’en suggérer l’utilisation, mais libre à vous d’explorer cet outil et de vous l’approprier!\n\n\n\n\nFigure 5.21: L’outil Git de RStudio"
  },
  {
    "objectID": "05-github.html#sec-git-renv",
    "href": "05-github.html#sec-git-renv",
    "title": "5  Science ouverte et reproductibilité",
    "section": "\n5.3 Introduction à renv\n",
    "text": "5.3 Introduction à renv\n\nAlors que les modules sont continuellement mis à jour, on doit s’assurer que l’on sache exactement quelle version a été utilisée si l’on désire être strict sur la reproductibilité. Lorsque je révise un article, je demande à ce que le nom des modules utilisés et leur numéro de version soient explicitement cités et référencés. Par exemple, dans un article sur l’analyse de compositions foliaires de laitues inoculées par une bactérie, j’écrivais:\n\nComputations were performed in the R statistical language version 3.4.1 (R Development Core Team, 2017). The main packages used in the data analysis workflow were the vegan package version 2.4-3 (Oksanen et al., 2017) for ordination, the compositions package version 1.40-1 (van den Boogaart and Tolosana-Delgado, 2013) for ilr transformations, the nlme version 3.1-131 (Pinheiro et al., 2017) package to compute the random experimental effect, the mvoutlier package version 2.0.8 (Filzmoser and Gschwandtner, 2017) for multivariate outlier detection, and the ggplot2 package version 2.2.1 (Wickham and Chang, 2017) for data visualization. The data and computations are publicly available at https://github.com/essicolo/Nicolas-et-al_Infected-lettuce-ionomics. Nicolas et al., 2019\n\nDe cette manière, une personne (que ce soit vos collègues, quiconque voudra auditer ou évaluer votre code ou vous-même dans le futur) pourra reproduire le code publié sur GitHub en installant les versions de R et des modules cités. Mais cela est fastidieux. C’est pourquoi l’équipe de RStudio (oui, encore ceux-là) ont développé le module renv, qui permet d’installer les modules à même votre dossier de projet (le dossier contenant le fichier .Rproj).\nPour l’utiliser à tout moment en cours de projet, il suffit de lancer la commande renv::init(). Cette commande configure l’infrastructure du projet, installe les libraries utilisées dans le dossier renv à l’intérieur du dossier de projet et bloque leurs versions actuelles dans le fichier renv.lock, crée un fichier .Rprofile qui traquera les installations futures de librairies supplémentaires, puis redémarre la session R. Ouf!\n\n\n\n\nFigure 5.22: L’outil renv de RStudio\n\n\n\nDans le dossier renv, le .gitignore contient tous les documents et les types de documents qui sont ignorés par git. L’option par défaut est d’ignorer le dossier library, qui contient les modules installés, mais de garder les fichiers activate.R et settings.json, qui contiennent le script d’installation des modules non installés (qui devront être installés par les autres personnes utilisant votre projet) ainsi que les paramètres du projet. Mieux vaut garder les options par défaut. Initialiser renv revient à scanner vos documents de projet pour trouver les modules utilisés et créer un paquet contenant tout cela à même votre projet, dans un dossier renv.\n\n📂 rstats\n|-📁 data\n|-📁 docs\n|-📁 images\n|-📁 lib\n|-📁 renv\n|-📁 tables\n📄 _quarto.yml\n📄 sentier-d-or.Rproj\n📄 stats.qmd\n📄 README.md\n📄 renv.lock\n📄 .Rprofile\n📄 .gitignore\nCe dossier contiendra tout ce qu’il faut pour utiliser les modules du projet d’une personne que l’on nommera Leto. Lorsqu’une autre personne, appellons-la Ghanima, utilisera le projet de Leto, RStudio vérifiera si le module renv est bien installé, et l’installera s’il ne l’est pas. Pour utiliser les modules du projet et non pas les modules de son ordinateur, Ghanima lancera la fonction renv::restore(), qui installera les versions décrites dans le lockfile (renv.lock). Si Leto décide de mettre à jour ses modules en cours de projet, il lancera la fonction renv::install() pour installer de nouveaux modules, renv::update() pour faire la mise à jour de tous les modules utilisés, puis renv::snapshot() pour que ces nouveaux modules soit intégrés à son projet dans le lockfile. Lorsque Leto commettra (commit) ses changements dans git et les publiera (push) sur GitHub, puis lorsque Ghanima mettra à jour (fetch) son dépôt local git lié au dépôt GitHub, elle devra à nouveau lancer renv::restore() pour que les modules soient bel et bien ceux utilisés par Leto.\nNotez qu’avec renv, vous n’installez pas réellement les modules complets à chaque fois : renv garde en mémoire cache l’installation des modules sur votre ordinateur, si bien que lorsque vous installez plusieurs la même version d’un même module, vous ne compilez pas à chaque fois et vous n’utilisez pas plus d’espace sur votre disque dur qu’il n’en faut."
  },
  {
    "objectID": "05-github.html#pour-terminer-le-reprex",
    "href": "05-github.html#pour-terminer-le-reprex",
    "title": "5  Science ouverte et reproductibilité",
    "section": "\n5.4 Pour terminer, le reprex\n",
    "text": "5.4 Pour terminer, le reprex\n\nLorsque j’ai découvert un bogue dans le module weathercan, j’ai ouvert une issue sur GitHub en indiquant le message d’erreur obtenu, en espérant que l’origine du bogue puisse être facilement déduit. Un développeur de weathercan m’a demandé un reprex. J’ai été déçu lorsque j’ai compris que le reprex n’était pas une espèce de dinosaure, mais plutôt un exemple reproductible (reproducible example).\n\n📗 Reprex: Un exemple reproductible.\n\n\nJ’ai essayé d’isoler le problème pour reproduire l’erreur avec le minimum de code possible. À partir d’un code de plus de 7000 lignes (les présentes notes de cours), j’en suis arrivé à ceci:\n\nstations &lt;- data.frame(A = 1)\n\nlibrary(\"weathercan\")\nmont_bellevue &lt;- weather_dl(\n  station_ids = c(5397, 48371),\n  start = \"2019-02-01\",\n  end = \"2019-02-07\",\n  interval = \"hour\",\n  verbose = TRUE\n)\n\n, qui me retournait l’erreur\nGetting station: 5397\nFormatting station data: 5397\nError in strptime(xx, f, tz = tz) : valeur 'tz' incorrecte\nLe bogue: la fonction weather_dl() utilisait à l’interne un objet nommé stations, qui entrait en conflit avec un objet stations s’il était défini hors de la fonction.\nSynthétiser une question n’est pas facile (créer cet exemple reprductible m’a pris près de 2 heures). Mais répondre à une question non synthétisée, c’est encore plus difficile. C’est pourquoi on (moi y compris) vous demandera systématiquement un reprex lorsque vous poserez une question liée à une erreur systématique, le plus souvent en programmation.\n\nUn exemple reproductible permet à quelqu’un de recréer l’erreur que vous avez obtenue simplement en copiant-collant votre code. - Hadley Wickham\n\nSelon Hadley Wickham (gourou de R), un reprex devrait comprendre quatre éléments (je joue à l’hérétique en me permettant d’adapter le document du gourou):\n\nLes modules devraient être chargés en début de code.\nPuis vous chargez des données, qui peuvent être des données d’exemple ou des données incluses à même le code R (comme des données générées au hasard).\nAssurez-vous que votre code est un exemple minimal (retirer le superflu) et qu’il soit facilement lisible.\nIncluez la sortie de la fonction sessionInfo(), qui indique la plateforme matérielle et logicielle sur laquelle vous avez généré l’erreur. Ceci est important en particulier s’il s’agit d’un bogue.\n\nLorsque vous pensez avoir généré votre reprex, redémarrez R (Session &gt; Restart R dans RStudio), puis lancez votre code pour vous assurer que l’erreur puisse être générée dans un nouvel environnement tout propre.\nLa librarie reprex de tidyverse vous aide à générer et à tester des exemples reproductibles en vous fournissant un bloc de code au format markdown que vous pouvez directement copier-coller sur GitHub, StackOverflow ou Discourse (les lieux fréquents où vous poserez vos questions)."
  },
  {
    "objectID": "06-python.html#quest-ce-que-python",
    "href": "06-python.html#quest-ce-que-python",
    "title": "6  Introduction à Python",
    "section": "6.1 Qu’est-ce que Python ?",
    "text": "6.1 Qu’est-ce que Python ?\nPython est un langage de programmation de haut niveau (comme R). Ce langage est apparu en février 1991 et a été créé par Guido van Rossum. Un des objectifs principaux de Van Rossum était de créer un langage libre, simple et intuitif, mais puissant comme d’autres langages déjà existants. Python a été amplement adopté partout dans le monde et est devenu un des langages de programmation les plus populaires d’après différents rangs comme les indices TIOBE et PYPL et les tendances des questions dans Stack Overflow.\nDans les dernières années Python est devenu l’un des outils les plus utilisés pour le calcul scientifique et pour l’analyse de données, même si ce langage n’était pas conçu spécifiquement pour ces tâches. L’utilisation de Python dans la science de données a été poussé par le développement de différents modules qui permettent la manipulation et l’analyse de données, certains des modules les plus populaires pour l’analyse de données en Python étant :\n\nnumpy : Ce module permet de manipuler et de stocker de façon efficiente les données dans des objets connus comme tableaux (en anglais, array).\npandas : Permet de travailler avec des données tabulaires avec des étiquettes de file et de colonne, l’objet primaire de ce module est le DataFrame.\nmatplotlib : C’est le module le plus utilisé pour la visualisation de données sur Python, il peut être considéré comme le module de base pour la visualisation en Python.\nSciPy : Ce module a différentes fonctions pour la computation scientifique comme le calcul numérique, le traitement de signaux et d’images, et certains statistiques.\nscikit-learn : C’est une des modules les plus utilisés pour l’apprentissage automatique. scikit-learn a des innombrables algorithmes d’apprentissage supervisé et non-supervisé utilisés pour la classification ou la régression."
  },
  {
    "objectID": "06-python.html#installation",
    "href": "06-python.html#installation",
    "title": "6  Introduction à Python",
    "section": "6.2 Installation",
    "text": "6.2 Installation\nIl y a différentes façons d’installer Python dans un ordinateur. Dans le cas de Mac et Linux Python vient par défaut avec ces systèmes d’exploitation. Dans le cas de windows il faut le télécharger et l’installer. Une façon d’installer Python sur windows est de télécharger le fichier d’installation directement du site web de Python, une fois le fichier téléchargé, l’exécuter et suivre les instructions pour l’installation.\nUne autre alternative pour télécharger Python est à partir de Anaconda. Anaconda c’est une distribution de Python, dont l’objectif est de simplifier la gestion et le déploiement des modules. Le gestionnaire de modules de Anaconda s’appelle conda. Il y a deux options pour installer Anaconda sur un ordinateur :\n\nTélécharger la version complète qui vient avec Python, conda et 1 500 modules pre-installés.\nTélécharger une version minimale appellé Miniconda qui ne vient qu’avec Python et conda.\n\nLes deux différences principales entre Anaconda et Miniconda sont (1) l’espace réquis pour l’installation qui est de 3 GO et 400 MO, respectivement, et (2) le temps d’installation puisque ça prends moins de temps à installer Miniconda que Anaconda. L’utilisateur peut choisir Anaconda si n’a pas d’expérience et s’il ne veut pas se préoccuper à installer des modules. L’installation de Miniconda est récommandé pour des utilisateurs qui sont plus experimentés et qui savent déjà quels modules ils vont utiliser. Enfin, si l’ordinateur n’a pas beaucoup d’espace, il est recommandé d’installer Miniconda. Dans ce manuel, les exemples d’éxecution de Python et l’installation des modules se fera à partir d’un terminal de commande nommé « Anaconda prompt »."
  },
  {
    "objectID": "06-python.html#chapitre-en-construction",
    "href": "06-python.html#chapitre-en-construction",
    "title": "6  Introduction à Python",
    "section": "6.3 Chapitre en construction",
    "text": "6.3 Chapitre en construction"
  },
  {
    "objectID": "06-python.html#anaconda-prompt",
    "href": "06-python.html#anaconda-prompt",
    "title": "6  Introduction à Python",
    "section": "6.4 Anaconda prompt",
    "text": "6.4 Anaconda prompt\n\nconda bash\n“Bonjour monde”\nRstudio et Python\nVS Code\nOperations Python\nTypes d’objets\nBoucles\nFonctions\nLes gestionnaires de modules\nNumpy, Pandas, Visualisation\nFin"
  },
  {
    "objectID": "07a-biostats.html#populations-et-échantillons",
    "href": "07a-biostats.html#populations-et-échantillons",
    "title": "7  Biostatistiques",
    "section": "\n7.1 Populations et échantillons",
    "text": "7.1 Populations et échantillons\nLe principe d’inférence consiste à généraliser des conclusions à l’échelle d’une population à partir d’échantillons issus de cette population. Alors qu’une population contient tous les éléments étudiés, un échantillon d’une population est une observation unique. Une expérience bien conçue fera en sorte que les échantillons soient représentatifs de la population qui, la plupart du temps, ne peut être observée entièrement pour des raisons pratiques.\nLes principes d’expérimentation servant de base à la conception d’une bonne méthodologie sont présentés dans le cours Dispositifs expérimentaux (BVG-7002). Également, je recommande le livre Principes d’expérimentation: planification des expériences et analyse de leurs résultats de Pierre Dagnelie (2012), disponible en ligne en format PDF. Un bon aperçu des dispositifs expérimentaux est aussi présenté dans Introductory Statistics with R, de Peter Dalgaard (2008), que vous pouvez télécharger du site de la bibliothèque de l’Université Laval vous avez un identifiant autorisé.\nUne population est échantillonnée pour induire des paramètres: un rendement typique dans des conditions météorologiques, édaphiques et managériales données, la masse typique des faucons pèlerins, mâles et femelles, le microbiome typique d’un sol agricole ou forestier, etc. Une statistique est une estimation d’un paramètre calculée à partir des données, par exemple une moyenne et un écart-type, ou une ordonnée à l’origine (intercept) et une pente.\nPar exemple, la moyenne (\\(\\mu\\)) et l’écart-type (\\(\\sigma\\)) d’une population sont estimés par les moyennes (\\(\\bar{x}\\)) et écarts-types (\\(s\\)) calculés sur les données issues de l’échantillonnage.\nChaque paramètre est liée à une perspective que l’on désire connaître chez une population. Ces angles d’observations sont les variables."
  },
  {
    "objectID": "07a-biostats.html#les-variables",
    "href": "07a-biostats.html#les-variables",
    "title": "7  Biostatistiques",
    "section": "\n7.2 Les variables",
    "text": "7.2 Les variables\nNous avons abordé au chapitre 3 la notion de variable par l’intermédiaire d’une donnée. Une variable est l’observation d’une caractéristique décrivant un échantillon. Si la charactéristique varie d’un échantillon à un autre sans que vous en expliquiez la raison (i.e. si identifier la source de la variabilité ne fait pas partie de votre expérience), on parlera de variable aléatoire. Même le hasard est régi par certaines lois: ce qui est aléatoire dans une variable peut être décrit par des lois de probabilité, que nous verrons plus bas.\nMais restons aux variables pour l’instant. Par convention, on peut attribuer aux variables un symbole mathématique. Par exemple, on peut donner à la masse volumique d’un sol (qui est le résultat d’une méthodologie précise) le symbole \\(\\rho\\). Lorsque l’on attribue une valeur à \\(\\rho\\), on parle d’une donnée. Chaque donnée d’une observation a un indice qui lui est propre, que l’on désigne souvent par \\(i\\), que l’on place en indice \\(\\rho_i\\). Pour la première donnée, on a \\(i=1\\), donc \\(\\rho_1\\). Pour un nombre \\(n\\) d’échantillons, on aura \\(\\rho_1\\), \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_n\\), formant le vecteur \\(\\rho = \\left[\\rho_1, \\rho_2, \\rho_3, ..., \\rho_n \\right]\\).\nEn R, une variable est associée à un vecteur ou une colonne d’un tableau.\n\nrho &lt;- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D\ndata &lt;- data.frame(rho = rho) # tableau\ndata\n\n   rho\n1 1.34\n2 1.52\n3 1.26\n4 1.43\n5 1.39\n\n\nIl existe plusieurs types de variables, qui se regroupent en deux grandes catégories: les variables quantitatives et les variables qualitatives.\n\n7.2.1 Variables quantitatives\nCes variables peuvent être continues dans un espace échantillonnal réel ou discrètes dans un espace échantillonnal ne considérant que des valeurs fixes. Notons que la notion de nombre réel est toujours une approximation en sciences expérimentales comme en calcul numérique, étant donnée que l’on est limité par la précision des appareils comme par le nombre d’octets à utiliser. Bien que les valeurs fixes des distributions discrètes ne soient pas toujours des valeurs entières, c’est bien souvent le cas en biostatistiques comme en démographie, où les décomptes d’individus sont souvent présents (et où la notion de fraction d’individus n’est pas acceptée).\n\n7.2.2 Variables qualitatives\nOn exprime parfois qu’une variable qualitative est une variable impossible à mesurer numériquement: une couleur, l’appartenance à une espèce ou à une série de sol. Pourtant, dans bien des cas, les variables qualitatives peuvent être encodées en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile à un loam sableux, qui autrement est décrit par la classe texturale d’un sol. Pour une couleur, on peut lui associer une longueur d’onde ou des pourcentages de rouge, vert et bleu, ainsi qu’un ton. En ce qui a trait aux variables ordonnées, il est possible de supposer un étalement. Par exemple, une variable d’intensité faible-moyenne-forte peut être transformée linéairement en valeurs quantitatives -1, 0 et 1. Attention toutefois, l’étalement peut parfois être quadratique ou logarithmique. Les séries de sol peuvent être encodées par la proportion de gleyfication (Parent et al., 2017). Quant aux catégories difficilement transformables en quantités, on pourra passer par l’encodage catégoriel, souvent appelé dummyfication, que nous verrons plus loin. L’analyse qualitative consiste en l’analyse de verbatims, essentiellement utile en sciences sociales: nous n’en n’aurons pas besoin ici. Nous considérerons les variables qualitatives comme des variables quantitatives qui n’ont pas subi de prétraitement."
  },
  {
    "objectID": "07a-biostats.html#les-probabilités",
    "href": "07a-biostats.html#les-probabilités",
    "title": "7  Biostatistiques",
    "section": "\n7.3 Les probabilités",
    "text": "7.3 Les probabilités\n\n« Nous sommes si éloignés de connaître tous les agens de la nature, et leurs divers modes d’action ; qu’il ne serait pas philosophique de nier les phénomènes, uniquement parce qu’ils sont inexplicables dans l’état actuel de nos connaissances. Seulement, nous devons les examiner avec une attention d’autant plus scrupuleuse, qu’il paraît plus difficile de les admettre ; et c’est ici que le calcul des probabilités devient indispensable, pour déterminer jusqu’à quel point il faut multiplier les observations ou les expériences, afin d’obtenir en faveur des agens qu’elles indiquent, une probabilité supérieure aux raisons que l’on peut avoir d’ailleurs, de ne pas les admettre. » — Pierre-Simon de Laplace\n\nUne probabilité est la vraisemblance qu’un évènement se réalise chez un échantillon. Les probabilités forment le cadre des systèmes stochastiques, c’est-à-dire des systèmes trop complexes pour en connaître exactement les aboutissants, auxquels on attribue une part de hasard. Ces systèmes sont prédominants dans les processus vivants.\nOn peut dégager deux perspectives sur les probabilités: l’une passe par une interprétation fréquentielle, l’autre bayésienne.\n\nL’interprétation fréquentielle représente la fréquence des occurrences après un nombre infini d’évènements. Par exemple, si vous jouez à pile ou face un grand nombre de fois, le nombre de pile sera égal à la moitié du nombre de lancers. L’approche fréquentielle teste si les données concordent avec un modèle du réel. Il s’agit de l’interprétation communément utilisée.\nL’interprétation bayésienne vise à quantifier l’incertitude des phénomènes. Dans cette perspective, plus l’information s’accumule, plus l’incertitude diminue. Cette approche gagne en notoriété notamment parce qu’elle permet de décrire des phénomènes qui, intrinsèquement, ne peuvent être répétés infiniment (absence d’asymptote), comme ceux qui sont bien définis dans le temps ou sur des populations limitées. L’approche bayésienne évalue la probabilité que le modèle soit réel.\n\nUne erreur courante consiste à aborder des statistiques fréquentielles comme des statistiques bayésiennes. Par exemple, si l’on désire évaluer la probabilité de l’existence de vie sur Mars, on devra passer par le bayésien, car avec les stats fréquentielles, on devra plutôt conclure si les données sont conformes ou non avec l’hypothèse de la vie sur Mars (exemple tirée du blogue Dynamic Ecology).\nDes rivalités factices s’installent entre les tenants des différentes approches, dont chacune, en réalité, répond à des questions différentes dont il convient réfléchir sur les limitations. Bien que les statistiques bayésiennes soient de plus en plus utilisées, nous ne couvrirons dans ce chapitre que l’approche fréquentielle. L’approche bayésienne est néanmoins traitée dans le chapitre 8, qui est facultatif au cours."
  },
  {
    "objectID": "07a-biostats.html#les-distributions",
    "href": "07a-biostats.html#les-distributions",
    "title": "7  Biostatistiques",
    "section": "\n7.4 Les distributions",
    "text": "7.4 Les distributions\nUne variable aléatoire peut prendre des valeurs selon des modèles de distribution des probabilités. Une distribution est une fonction mathématique décrivant la probabilité d’observer une série d’évènements. Ces évènements peuvent être des valeurs continues, des nombres entiers, des catégories, des valeurs booléennes (Vrai/Faux), etc. Dépendemment du type de valeur et des observations obtenues, on peut associer des variables à différentes lois de probabilité. Toujours, l’aire sous la courbe d’une distribution de probabilité est égale à 1.\nEn statistiques inférentielles, les distributions sont les modèles, comprenant certains paramètres comme la moyenne et la variance pour les distributions normales, à partir desquelles les données sont générées.\nIl existe deux grandes familles de distribution: discrètes et continues. Les distributions discrètes sont contraintes à des valeurs prédéfinies (finies ou infinies), alors que les distributions continues prennent nécessairement un nombre infini de valeurs, dont la probabilité ne peut pas être évaluée ponctuellement, mais sur un intervalle.\nL’espérance mathématique est une fonction de tendance centrale, souvent décrite par un paramètre. Il s’agit de la moyenne d’une population pour une distribution normale. La variance, quant à elle, décrit la variabilité d’une population, i.e. son étalement autour de l’espérance. Pour une distribution normale, la variance d’une population est aussi appelée variance, souvent présentée par l’écart-type (égal à la racine carrée de la variance).\n\n7.4.1 Distribution binomiale\nEn tant que scénario à deux issues possibles, des tirages à pile ou face suivent une loi binomiale, comme toute variable booléenne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la présence/absence d’une espèce, d’une maladie, d’un trait phylogénétique, ainsi que les catégories encodées. Lorsque l’opération ne comprend qu’un seul échantillon (i.e. un seul tirage à pile ou face), il s’agit d’un cas particulier d’une loi binomiale que l’on nomme une loi de Bernouilli.\nPour 25 tirages à pile ou face indépendants (i.e. dont l’ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilités est de 1. La fonction dbinom est une fonction de distribution de probabilités. Les fonctions de distribution de probabilités discrètes sont appelées des fonctions de masse.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nx &lt;- 0:25\ny &lt;- dbinom(x = x, size = 25, prob = 0.5)\nprint(paste('La somme des probabilités est de', sum(y)))\n\n[1] \"La somme des probabilités est de 1\"\n\nggplot(data = tibble(x, y), mapping = aes(x, y)) +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = \"grey50\") +\n  geom_point()\n\n\n\n\n\n7.4.2 Distribution de Poisson\nLa loi de Poisson (avec un P majuscule, introduite par le mathématicien français Siméon Denis Poisson et non pas l’animal) décrit des distributions discrètes de probabilité d’un nombre d’évènements se produisant dans l’espace ou dans le temps. Les distributions de Poisson décrivent ce qui tient du décompte. Il peut s’agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants d’asclépiades se trouvant sur une terre cultivée, ou du nombre d’évènements de précipitation au mois de juin, etc. La distribution de Poisson n’a qu’un seul paramètre, \\(\\lambda\\), qui décrit la moyenne des décomptes.\nPar exemple, en un mois de 30 jours, et une moyenne de 8 évènements de précipitation pour ce mois, on obtient la distribution suivante.\n\nx &lt;- 1:30\ny &lt;- dpois(x, lambda = 8)\nprint(paste('La somme des probabilités est de', sum(y)))\n\n[1] \"La somme des probabilités est de 0.999664536835124\"\n\nggplot(data = data.frame(x, y), mapping = aes(x, y)) +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = \"grey50\") +\n  geom_point()\n\n\n\n\n\n7.4.3 Distribution uniforme\nLa distribution la plus simple est probablement la distribution uniforme. Si la variable est discrète, chaque catégorie est associée à une probabilité égale. Si la variable est continue, la probabilité est directement proportionnelle à la largeur de l’intervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour décrire des a priori vagues pour l’analyse bayésienne (ce sujet est traité dans le chapitre 8). Nous utilisons la fonction dunif. À la différence des distributions discrètes, les fonctions de distribution de probabilités continues sont appelées des fonctions de densité d’une loi de probabilité (probability density function).\n\nincrement &lt;- 0.01\nx &lt;- seq(-4, 4, by = increment)\ny1 &lt;- dunif(x, min = -3, max = 3)\ny2 &lt;- dunif(x, min = -2, max = 2)\ny3 &lt;- dunif(x, min = -1, max = 1)\n\nprint(paste('La somme des probabilités est de', sum(y3 * increment)))\n\n[1] \"La somme des probabilités est de 1.005\"\n\ngg_unif &lt;- data.frame(x, y1, y2, y3) |&gt;  \n  pivot_longer(-x, names_to = \"variable\", values_to = \"value\")\n\nggplot(data = gg_unif, mapping = aes(x = x, y = value)) +\n  geom_line(aes(colour = variable))\n\n\n\n\n\n7.4.4 Distribution normale\nLa plus répandue de ces lois est probablement la loi normale, parfois nommée loi gaussienne et plus rarement loi laplacienne. Il s’agit de la distribution classique en forme de cloche.\nLa loi normale est décrite par une moyenne, qui désigne la tendance centrale, et une variance, qui désigne l’étalement des probabilités autour de la moyenne. La racine carrée de la variance est l’écart-type.\nLes distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approximées par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne d’une loi log-normale est la moyenne géométrique.\n\nincrement &lt;- 0.01\nx &lt;- seq(-10, 10, by = increment)\ny1 &lt;- dnorm(x, mean = 0, sd = 1)\ny2 &lt;- dnorm(x, mean = 0, sd = 2)\ny3 &lt;- dnorm(x, mean = 0, sd = 3)\n\nprint(paste('La somme des probabilités est de', sum(y3 * increment)))\n\n[1] \"La somme des probabilités est de 0.999147010743368\"\n\ngg_norm &lt;- data.frame(x, y1, y2, y3) |&gt;  \n  pivot_longer(-x, names_to = \"variable\", values_to = \"value\")\n\nggplot(data = gg_norm, mapping = aes(x = x, y = value)) +\n  geom_line(aes(colour = variable))\n\n\n\n\nQuelle est la probabilité d’obtenir le nombre 0 chez une observation continue distribuée normalement dont la moyenne est 0 et l’écart-type est de 1? Réponse: 0. La loi normale étant une distribution continue, les probabilités non-nulles ne peuvent être calculées que sur des intervalles. Par exemple, la probabilité de retrouver une valeur dans l’intervalle entre -1 et 2 est calculée en soustrayant la probabilité cumulée à -1 de la probabilité cumulée à 2.\n\nincrement &lt;- 0.01\nx &lt;- seq(-5, 5, by = increment)\ny &lt;- dnorm(x, mean = 0, sd = 1)\n\nprob_between &lt;- c(-1, 2)\n\ngg_norm &lt;- data.frame(x, y)\ngg_auc &lt;- gg_norm |&gt; \n  filter(x &gt; prob_between[1], x &lt; prob_between[2]) |&gt; \n  rbind(c(prob_between[2], 0)) |&gt; \n  rbind(c(prob_between[1], 0))\n\nggplot(data.frame(x, y), aes(x, y)) +\n  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadécimal\n  geom_line()\n\n\n\nprob_norm_between &lt;- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1)\nprint(paste(\"La probabilité d'obtenir un nombre entre\",\n            prob_between[1], \"et\",\n            prob_between[2], \"est d'environ\",\n            round(prob_norm_between, 2) * 100, \"%\"))\n\n[1] \"La probabilité d'obtenir un nombre entre -1 et 2 est d'environ 82 %\"\n\n\nLa courbe normale peut être utile pour évaluer la distribution d’une population. Par exemple, on peut calculer les limites de région sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et d’autre de la moyenne. Il s’agit ainsi de l’intervalle de confiance sur la déviation de la distribution.\n\nincrement &lt;- 0.01\nx &lt;- seq(-5, 5, by = increment)\ny &lt;- dnorm(x, mean = 0, sd = 1)\n\nalpha &lt;- 0.05\nprob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1),\n                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1))\n\ngg_norm &lt;- data.frame(x, y)\ngg_auc &lt;- gg_norm |&gt; \n  filter(x &gt; prob_between[1], x &lt; prob_between[2]) |&gt; \n  rbind(c(prob_between[2], 0)) |&gt; \n  rbind(c(prob_between[1], 0))\n\nggplot(data = data.frame(x, y), mapping = aes(x, y)) +\n  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadécimal\n  geom_line() +\n  geom_text(data = data.frame(x = prob_between,\n                              y = c(0, 0),\n                              labels = round(prob_between, 2)),\n            mapping = aes(label = labels))\n\n\n\n\nOn pourrait aussi être intéressé à l’intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont l’écart-type est noté erreur standard. On calcule cette erreur en divisant la variance par le nombre d’observation, ou en divisant l’écart-type par la racine carrée du nombre d’observations. Ainsi, pour 10 échantillons:\n\nincrement &lt;- 0.01\nx &lt;- seq(-5, 5, by = increment)\ny &lt;- dnorm(x, mean = 0, sd = 1)\n\nalpha &lt;- 0.05\nprob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10),\n                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10))\n\ngg_norm &lt;- data.frame(x, y)\ngg_auc &lt;- gg_norm |&gt; \n  filter(x &gt; prob_between[1], x &lt; prob_between[2]) |&gt; \n  rbind(c(prob_between[2], 0)) |&gt; \n  rbind(c(prob_between[1], 0))\n\nggplot(data = data.frame(x, y), mapping = aes(x, y)) +\n  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadécimal\n  geom_line() +\n  geom_text(data = data.frame(x = prob_between,\n                              y = c(0, 0),\n                              labels = round(prob_between, 2)),\n            mapping = aes(label = labels))"
  },
  {
    "objectID": "07a-biostats.html#statistiques-descriptives",
    "href": "07a-biostats.html#statistiques-descriptives",
    "title": "7  Biostatistiques",
    "section": "\n7.5 Statistiques descriptives",
    "text": "7.5 Statistiques descriptives\nOn a vu comment générer des statistiques sommaires en R avec la fonction summary(). Reprenons les données d’iris.\n\ndata(\"iris\")\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nPour précisément effectuer une moyenne et un écart-type sur un vecteur, passons par les fonctions mean() et sd().\n\nmean(iris$Sepal.Length)\n\n[1] 5.843333\n\nsd(iris$Sepal.Length)\n\n[1] 0.8280661\n\n\nPour effectuer un sommaire de tableau piloté par une fonction, nous passons par la gamme de fonctions summarise(), de dplyr. Dans ce cas, avec group_by(), nous fragmentons le tableau par espèce pour effectuer un sommaire sur toutes les variables.\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise_all(mean)\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 versicolor         5.94        2.77         4.26       1.33 \n3 virginica          6.59        2.97         5.55       2.03 \n\n\nVous pourriez être intéressé par les quartiles à 25, 50 et 75%. Mais la fonction summarise() n’autorise que les fonctions dont la sortie est d’un seul objet, alors faisons sorte que l’objet soit une liste - lorsque l’on imbrique une fonction funs, le tableau à insérer dans la fonction est indiqué par un ..\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise_all(list(q25 = ~ quantile(., probs = 0.25),\n                     q50 = ~ quantile(., probs = 0.50),\n                     q75 = ~ quantile(., probs = 0.75)))\n\n# A tibble: 3 × 13\n  Species    Sepal.Length_q25 Sepal.Width_q25 Petal.Length_q25 Petal.Width_q25\n  &lt;fct&gt;                 &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n1 setosa                 4.8             3.2               1.4             0.2\n2 versicolor             5.6             2.52              4               1.2\n3 virginica              6.22            2.8               5.1             1.8\n# ℹ 8 more variables: Sepal.Length_q50 &lt;dbl&gt;, Sepal.Width_q50 &lt;dbl&gt;,\n#   Petal.Length_q50 &lt;dbl&gt;, Petal.Width_q50 &lt;dbl&gt;, Sepal.Length_q75 &lt;dbl&gt;,\n#   Sepal.Width_q75 &lt;dbl&gt;, Petal.Length_q75 &lt;dbl&gt;, Petal.Width_q75 &lt;dbl&gt;\n\n\nEn mode programmation classique de R, on pourra générer les quartiles à la pièce.\n\nquantile(iris$Sepal.Length[iris$Species == 'setosa'])\n\n  0%  25%  50%  75% 100% \n 4.3  4.8  5.0  5.2  5.8 \n\nquantile(iris$Sepal.Length[iris$Species == 'versicolor'])\n\n  0%  25%  50%  75% 100% \n 4.9  5.6  5.9  6.3  7.0 \n\nquantile(iris$Sepal.Length[iris$Species == 'virginica'])\n\n   0%   25%   50%   75%  100% \n4.900 6.225 6.500 6.900 7.900 \n\n\nLa fonction table() permettra d’obtenir des décomptes par catégorie, ici par plages de longueurs de sépales. Pour obtenir les proportions du nombre total, il s’agit d’encapsuler le tableau croisé dans la fonction prop.table().\n\ntableau_croise &lt;- table(iris$Species,\n                        cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length)))\ntableau_croise\n\n            \n             (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9]\n  setosa            35        14         0         0\n  versicolor         4        20        17         9\n  virginica          1         5        18        26\n\n\n\nprop.table(tableau_croise)\n\n            \n               (4.3,5.1]   (5.1,5.8]   (5.8,6.4]   (6.4,7.9]\n  setosa     0.234899329 0.093959732 0.000000000 0.000000000\n  versicolor 0.026845638 0.134228188 0.114093960 0.060402685\n  virginica  0.006711409 0.033557047 0.120805369 0.174496644"
  },
  {
    "objectID": "07a-biostats.html#tests-dhypothèses-à-un-et-deux-échantillons",
    "href": "07a-biostats.html#tests-dhypothèses-à-un-et-deux-échantillons",
    "title": "7  Biostatistiques",
    "section": "\n7.6 Tests d’hypothèses à un et deux échantillons",
    "text": "7.6 Tests d’hypothèses à un et deux échantillons\nUn test d’hypothèse permet de décider si une hypothèse est confirmée ou rejetée à un seuil de probabilité prédéterminé.\nCette section est inspirée du chapitre 5 de Dalgaard, 2008.\n\nInformation: l’hypothèse nulle. Les tests d’hypothèse évaluent des effets statistiques (qui ne sont pas nécessairement des effets de causalité). L’effet à évaluer peut être celui d’un traitement, d’indicateurs météorologiques (e.g. précipitations totales, degré-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est menée pour évaluer l’hypothèse que l’on retrouve des différences entre des unités expérimentales. Par convention, l’hypothèse nulle (écrite \\(H_0\\)) est l’hypothèse qu’il n’y ait pas d’effet (c’est l’hypothèse de l’avocat du diable 😈) à l’échelle de la population (et non pas à l’échelle de l’échantillon). À l’inverse, l’hypothèse alternative (écrite \\(H_1\\)) est l’hypothèse qu’il y ait un effet à l’échelle de la population.\n\nÀ titre d’exercice en stats, on débute souvent en testant si deux vecteurs de valeurs continues proviennent de populations à moyennes différentes ou si un vecteur de valeurs a été généré à partir d’une population ayant une moyenne donnée. Dans cette section, nous utiliserons la fonction t.test() pour les tests de t et la fonction wilcox.test() pour les tests de Wilcoxon (aussi appelé de Mann-Whitney).\n\n7.6.1 Test de t à un seul échantillon\nNous devons assumer, pour ce test, que l’échantillon est recueillit d’une population dont la distribution est normale, \\(\\mathcal{N} \\sim \\left( \\mu, \\sigma^2 \\right)\\), et que chaque échantillon est indépendant l’un de l’autre. L’hypothèse nulle est souvent celle de l’avocat du diable, que la moyenne soit égale à une valeur donnée (donc la différence entre la moyenne de la population et une moyenne donnée est de zéro): ici, que \\(\\mu = \\bar{x}\\). L’erreur standard sur la moyenne (ESM) de l’échantillon, \\(\\bar{x}\\) est calculée comme suit.\n\\[ESM = \\frac{s}{\\sqrt{n}}\\]\noù \\(s\\) est l’écart-type de l’échantillon et \\(n\\) est le nombre d’échantillons.\nPour tester l’intervalle de confiance de l’échantillon, on multiplie l’ESM par l’aire sous la courbe de densité couvrant une certaine proportion de part et d’autre de l’échantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et d’autre.\n\nset.seed(33746)\nx &lt;- rnorm(20, 16, 4)\n\nlevel &lt;-  0.95\nalpha &lt;- 1-level\n\nx_bar &lt;- mean(x)\ns &lt;- sd(x)\nn &lt;- length(x)\n\nerror &lt;- qnorm(1 - alpha/2) * s / sqrt(n)\nerror\n\n[1] 1.483253\n\n\nL’intervalle de confiance est l’erreur de part et d’autre de la moyenne.\n\nc(x_bar - error, x_bar + error)\n\n[1] 14.35630 17.32281\n\n\nSi la moyenne de la population est de 16, un nombre qui se situe dans l’intervalle de confiance on accepte l’hypothèse nulle au seuil 0.05. Si le nombre d’échantillon est réduit (généralement &lt; 30), on passera plutôt par une distribution de t, avec \\(n-1\\) degrés de liberté.\n\nerror &lt;- qt(1 - alpha/2, n-1) * s / sqrt(n)\nc(x_bar - error, x_bar + error)\n\n[1] 14.25561 17.42351\n\n\nPlus simplement, on pourra utiliser la fonction t.test() en spécifiant la moyenne de la population. Nous avons généré 20 données avec une moyenne de 16 et un écart-type de 4. Nous savons donc que la vraie moyenne de l’échantillon est de 16. Mais disons que nous testons l’hypothèse que ces données sont tirées d’une population dont la moyenne est 18 (et implicitement que son écart-type est de 4).\n\nt.test(x, mu = 18)\n\n\n    One Sample t-test\n\ndata:  x\nt = -2.8548, df = 19, p-value = 0.01014\nalternative hypothesis: true mean is not equal to 18\n95 percent confidence interval:\n 14.25561 17.42351\nsample estimates:\nmean of x \n 15.83956 \n\n\nLa fonction retourne la valeur de t (t-value), le nombre de degrés de liberté (\\(n-1 = 19\\)), une description de l’hypothèse alternative (alternative hypothesis: true mean is not equal to 18), ainsi que l’intervalle de confiance au niveau de 95%. Le test contient aussi la p-value.\n\n\n7.6.1.1 Information: la p-value\n\nLa p-value, ou valeur-p ou p-valeur, est utilisée pour trancher si, oui ou non, un résultat est significatif. En langage scientifique, le mot significatif ne devrait être utilisé que lorsque l’on réfère à un test d’hypothèse statistique. Vous retrouverez des p-values partout en stats. Les p-values indiquent la probabilité que les données ait été échantillonnées d’une population où un effet est observable selon le modèle statistique utilisé.\n\nLa p-value est la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l’hypothèse nulle est vraie.\n\nUne p-value élevée indique que le modèle appliqué à vos données concorde avec la conclusion que l’hypothèse nulle est vraie, et inversement si la p-value est faible. Le seuil arbitraire utilisée en écologie et en agriculture, comme dans plusieurs domaines, est de 0.05. L’utilisation d’un seuil est toutefois contestée avec raison. Une enquête menée dans la littérature scientifiques a révélé que 49% des 791 articles étudiés interprétaient un effet non significatif comme un effet nul (Amrhein et al., 2019). En effet, une catégorisation de la p-value avec un seuil de significativité brouille le jugement sur l’importance des effets et de leur incertitude. Les six principes de l’American Statistical Association guident l’interprétation des p-values. [ma traduction]\n\nLes p-values indiquent l’ampleur de l’incompatibilité des données avec le modèle statistique\nLes p-values ne mesurent pas la probabilité que l’hypothèse étudiée soit vraie, ni la probabilité que les données ont été générées uniquement par la chance.\nLes conclusions scientifiques et décisions d’affaire ou politiques ne devraient pas être basées sur l’atteinte d’une p-value à un seuil spécifique.\nUne inférence appropriée demande un rapport complet et transparent.\nUne p-value, ou une signification statistique, ne mesure pas l’ampleur d’un effet ou l’importance d’un résultat.\nEn tant que tel, une p-value n’offre pas une bonne mesure des évidences d’un modèle ou d’une hypothèse.\n\n\nDans le cas précédent, la p-value était de 0.01014. Pour aider notre interprétation, prenons l’hypothèse alternative: true mean is not equal to 18. L’hypothèse nulle était bien que la vraie moyenne est égale à 18. Insérons la p-value dans la définition: la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l’hypothèse nulle est vraie est de 0.01014. Il est donc très peu probable que les données soient tirées d’un échantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette l’hypothèse nulle et l’on conclut qu’à ce seuil de confiance, l’échantillon ne provient pas d’une population ayant une moyenne de 18.\n\n7.6.2 Attention: mauvaises interprétations des p-values\n\n\n“La p-value n’a jamais été conçue comme substitut au raisonnement scientifique” Ron Wasserstein, directeur de l’American Statistical Association [ma traduction].\n\nUn résultat montrant une p-value plus élevée que 0.05 est-il pertinent?\nLors d’une conférence, Dr Evil ne présente que les résultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importants… En écartant ces résultats, Dr Evil commet 3 erreurs:\n\nLa p-value n’est pas un bon indicateur de l’importance d’un test statistique. L’importance d’une variable dans un modèle devrait être évaluée par la valeur de son coefficient. Son incertitude devrait être évaluée par sa variance. Une manière plus intuitive d’évaluer la variance est l’écart-type ou l’intervalle de confiance. À un certain seuil d’intervalle de confiance, la p-value traduira la probabilité qu’un coefficient réellement nul ait pu générer des données démontrant un coefficient égal ou supérieur.\nIl est tout aussi important de savoir que le traitement fonctionne que de savoir qu’il ne fonctionne pas. Les résultats démontrant des effets sont malheureusement davantage soumis aux journaux et davantage publiés que ceux ne démontrant pas d’effets (Decullier et al., 2005).\nLe seuil de 0.05 est arbitraire.\n\n\n\n7.6.2.1 Attention au p-hacking\n\nLe p-hacking (ou data dredging) consiste à manipuler les données et les modèles pour faire en sorte d’obtenir des p-values favorables à l’hypothèse testée et, éventuellement, aux conclusions recherchées. À éviter dans tous les cas. Toujours. Toujours. Toujours.\n\n\n\n\nUn sketch humoristique de John Oliver sur le p-hacking, Last week tonight, 2016 (en anglais)\n\n\n\n\n7.6.3 Test de Wilcoxon à un seul échantillon\nLe test de t suppose que la distribution des données est normale… ce qui est rarement le cas, surtout lorsque les échantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: c’est un test non-paramétrique basé sur le tri des valeurs.\n\nwilcox.test(x, mu = 18)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x\nV = 39, p-value = 0.01208\nalternative hypothesis: true location is not equal to 18\n\n\nLe V est la somme des rangs positifs. Dans ce cas, la p-value est semblable à celle du test de t, et les mêmes conclusions s’appliquent.\n\n7.6.4 Tests de t à deux échantillons\nLes tests à un échantillon servent plutôt à s’exercer: rarement en aura-t-on besoin en recherche, où plus souvent, on voudra comparer les moyennes de deux unités expérimentales. L’expérience comprend donc deux séries de données continues, \\(x_1\\) et \\(x_2\\), issues de lois de distribution normale \\(\\mathcal{N} \\left( \\mu_1, \\sigma_1^2 \\right)\\) et \\(\\mathcal{N} \\left( \\mu_2, \\sigma_2^2 \\right)\\), et nous testons l’hypothèse nulle que \\(\\mu_1 = \\mu_2\\). La statistique t est calculée comme suit.\n\\[t = \\frac{\\bar{x_1} - \\bar{x_2}}{ESDM}\\]\nL’ESDM est l’erreur standard de la différence des moyennes:\n\\[ESDM = \\sqrt{ESM_1^2 + ESM_2^2}\\]\nSi vous supposez que les variances sont identiques, l’erreur standard (\\(s\\)) est calculée pour les échantillons des deux groupes, puis insérée dans le calcul des ESM. La statistique t sera alors évaluée à \\(n_1 + n_2 - 2\\) degrés de liberté. Si vous supposez que la variance est différente (procédure de Welch), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degrés de liberté calculé à partir des erreurs standards et du nombre d’échantillon dans les groupes: cette procédure est considérée comme plus prudente (Dalgaard, 2008, page 101).\nPrenons les données d’iris pour l’exemple en excluant l’iris setosa étant donnée que les tests de t se restreignent à deux groupes. Nous allons tester la longueur des pétales.\n\niris_pl &lt;- iris |&gt; \n    filter(Species != \"setosa\") |&gt; \n    select(Species, Petal.Length)\nslice_sample(iris_pl, n = 5)\n\n     Species Petal.Length\n1  virginica          5.1\n2 versicolor          4.0\n3  virginica          5.0\n4 versicolor          4.6\n5 versicolor          4.1\n\n\nDans la prochaine cellule de code, nous introduisons l’interface-formule de R, où l’on retrouve typiquement le ~, entre les variables de sortie à gauche et les variables d’entrée à droite. Dans notre cas, la variable de sortie est la variable testée, Petal.Length, qui varie en fonction du groupe Species, qui est la variable d’entrée (variable explicative) - nous verrons les types de variables plus en détails dans la section sur les modèles statistiques (chapitre 7.8).\n\nt.test(formula = Petal.Length ~ Species,\n       data = iris_pl, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Length by Species\nt = -12.604, df = 95.57, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\nNous obtenons une sortie similaire aux précédentes. L’intervalle de confiance à 95% exclut le zéro, ce qui est cohérent avec la p-value très faible, qui nous indique le rejet de l’hypothèse nulle au seuil 0.05. Les données montrent que les groupes ont des moyennes de longueur de pétales différentes.\n\n\n7.6.4.1 Enregistrer les résultats d’un test\nIl est possible d’enregistrer un test dans un objet.\n\ntt_pl &lt;- t.test(formula = Petal.Length ~ Species,\n                data = iris_pl, var.equal = FALSE)\nsummary(tt_pl)\n\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \np.value     1      -none- numeric  \nconf.int    2      -none- numeric  \nestimate    2      -none- numeric  \nnull.value  1      -none- numeric  \nstderr      1      -none- numeric  \nalternative 1      -none- character\nmethod      1      -none- character\ndata.name   1      -none- character\n\nstr(tt_pl)\n\nList of 10\n $ statistic  : Named num -12.6\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 95.6\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 4.9e-22\n $ conf.int   : num [1:2] -1.5 -1.09\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num [1:2] 4.26 5.55\n  ..- attr(*, \"names\")= chr [1:2] \"mean in group versicolor\" \"mean in group virginica\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"difference in means between group versicolor and group virginica\"\n $ stderr     : num 0.103\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Welch Two Sample t-test\"\n $ data.name  : chr \"Petal.Length by Species\"\n - attr(*, \"class\")= chr \"htest\"\n\n\n\n7.6.5 Comparaison des variances\nPour comparer les variances, on a recours au test de F (F pour Fisher).\n\nvar.test(formula = Petal.Length ~ Species,\n         data = iris_pl)\n\n\n    F test to compare two variances\n\ndata:  Petal.Length by Species\nF = 0.72497, num df = 49, denom df = 49, p-value = 0.2637\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.411402 1.277530\nsample estimates:\nratio of variances \n         0.7249678 \n\n\nIl semble que l’on pourrait relancer le test de t sans la procédure Welch, avec var.equal = TRUE.\n\n7.6.6 Tests de Wilcoxon à deux échantillons\nCela ressemble au test de t!\n\nwilcox.test(formula = Petal.Length ~ Species,\n       data = iris_pl, var.equal = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Petal.Length by Species\nW = 44.5, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n7.6.7 Les tests pairés\nLes tests pairés sont utilisés lorsque deux échantillons proviennent d’une même unité expérimentale: il s’agit en fait de tests sur la différence entre deux observations.\n\nset.seed(2555)\n\nn &lt;- 20\navant &lt;- rnorm(n, 16, 4)\napres &lt;- rnorm(n, 18, 3)\n\nIl est important de spécifier que le test est pairé, la valeur par défaut de paired étant FALSE.\n\nt.test(avant, apres, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  avant and apres\nt = -1.5168, df = 19, p-value = 0.1458\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.5804586  0.7311427\nsample estimates:\nmean difference \n      -1.924658 \n\n\nL’hypothèse nulle qu’il n’y ait pas de différence entre l’avant et l’après traitement est acceptée au seuil 0.05.\nExercice. Effectuer un test de Wilcoxon pairé."
  },
  {
    "objectID": "07a-biostats.html#lanalyse-de-variance",
    "href": "07a-biostats.html#lanalyse-de-variance",
    "title": "7  Biostatistiques",
    "section": "\n7.7 L’analyse de variance",
    "text": "7.7 L’analyse de variance\nL’analyse de variance consiste à comparer des moyennes de plusieurs groupes distribués normalement et de même variance. Cette section sera élaborée prochainement plus en profondeur. Considérons-la pour le moment comme une régression sur une variable catégorielle.\n\npl_aov &lt;- aov(Petal.Length ~ Species, iris)\nsummary(pl_aov)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  437.1  218.55    1180 &lt;2e-16 ***\nResiduals   147   27.2    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa prochaine section, justement, est vouée aux modèles statistiques explicatifs, qui incluent la régression."
  },
  {
    "objectID": "07a-biostats.html#sec-bios-models",
    "href": "07a-biostats.html#sec-bios-models",
    "title": "7  Biostatistiques",
    "section": "\n7.8 Les modèles statistiques",
    "text": "7.8 Les modèles statistiques\nLa modélisation statistique consiste à lier de manière explicite des variables de sortie \\(y\\) (ou variables-réponse / variables dépendantes) à des variables explicatives \\(x\\) (ou variables prédictives / indépendantes / covariables). Les variables-réponse sont modélisées par une fonction des variables explicatives ou prédictives.\nPourquoi garder les termes explicatives et prédictives? Parce que les modèles statistiques (basés sur des données et non pas sur des mécanismes) sont de deux ordres. D’abord, les modèles prédictifs sont conçus pour prédire de manière fiable une ou plusieurs variables-réponse à partir des informations contenues dans les variables qui sont, dans ce cas, prédictives (par exemple : Les séries temporelles au ?sec-temps). Lorsque l’on désire tester des hypothèses pour évaluer quelles variables expliquent la réponse, on parlera de modélisation (et de variables) explicative. En inférence statistique, on évaluera les corrélations entre les variables explicatives et les variables-réponse. Un lien de corrélation n’est pas un lien de causalité. L’inférence causale peut en revanche être évaluée par des modèles d’équations structurelles.\nCette section couvre la modélisation explicative. Les variables qui contribuent à créer les modèles peuvent être de différentes natures et distribuées selon différentes lois de probabilité. Alors que les modèles linéaires simples (lm) impliquent une variable-réponse distribuée de manière continue, les modèles linéaires généralisés peuvent aussi expliquer des variables de sorties discrètes.\nDans les deux cas, on distinguera les variables fixes et les variables aléatoires. Les variables fixes sont les variables testées lors de l’expérience: dose du traitement, espèce/cultivar, météo, etc. Les variables aléatoires sont les sources de variation qui génèrent du bruit dans le modèle: les unités expérimentales ou le temps lors de mesures répétées. Les modèles incluant des effets fixes seulement sont des modèles à effets fixes. Généralement, les modèles incluant des variables aléatoires incluent aussi des variables fixes: on parlera alors de modèles mixtes. Nous couvrirons ces deux types de modèles.\n\n7.8.1 Modèles à effets fixes\nLes tests de t et de Wilcoxon, explorés précédemment, sont des modèles statistiques à une seule variable. Nous avons vu dans l’interface-formule qu’une variable-réponse peut être liée à une variable explicative avec le tilde ~. En particulier, le test de t est une régression linéaire univariée (à une seule variable explicative) dont la variable explicative comprend deux catégories. De même, l’anova est une régression linéaire univariée dont la variable explicative comprend plusieurs catégories. Or l’interface-formule peut être utilisé dans plusieurs circonstances, notamment pour ajouter plusieurs variables de différents types: on parlera de régression multivariée.\nLa plupart des modèles statistiques peuvent être approximés comme une combinaison linéaire de variables: ce sont des modèles linéaires. Les modèles non-linéaires impliquent des stratégies computationnelles complexes qui rendent leur utilisation plus difficile à manœuvrer.\nUn modèle linéaire univarié prendra la forme \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), où \\(\\beta_0\\) est l’intercept et \\(\\beta_1\\) est la pente et \\(\\epsilon\\) est l’erreur.\nVous verrez parfois la notation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\). La notation avec le chapeau \\(\\hat{y}\\) exprime qu’il s’agit des valeurs générées par le modèle. En fait, \\(y = \\hat{y} - \\epsilon\\).\n\n7.8.1.1 Modèle linéaire univarié avec variable continue\nPrenons les données lasrosas.corn incluses dans le module agridat, où l’on retrouve le rendement d’une production de maïs à dose d’azote variable, en Argentine.\n\nlibrary(\"agridat\")\ndata(\"lasrosas.corn\")\nslice_sample(lasrosas.corn, n = 10)\n\n   year       lat      long yield nitro topo     bv rep nf\n1  1999 -33.05207 -63.84230 69.57   0.0   LO 185.67  R1 N0\n2  1999 -33.05137 -63.84383 67.41  53.0    E 175.12  R2 N2\n3  1999 -33.05104 -63.84323 68.33  29.0   LO 168.70  R3 N1\n4  1999 -33.05162 -63.84456 68.06  53.0    E 171.71  R1 N2\n5  1999 -33.05180 -63.84386 63.99   0.0   LO 172.46  R1 N0\n6  2001 -33.05065 -63.84578 35.85  50.6   HT 194.85  R1 N2\n7  1999 -33.05170 -63.84553 58.89 131.5   HT 187.98  R1 N5\n8  2001 -33.05077 -63.84502 50.95 124.6   HT 184.66  R2 N5\n9  1999 -33.05181 -63.84202 78.75 106.0   LO 169.25  R2 N4\n10 1999 -33.05154 -63.84468 68.58  29.0    E 169.35  R1 N1\n\n\nCes données comprennent plusieurs variables. Prenons le rendement (yield) comme variable de sortie et, pour le moment, ne retenons que la dose d’azote (nitro) comme variable explicative: il s’agit d’une régression univariée. Les deux variables sont continues. Explorons d’abord le nuage de points de l’une et l’autre.\n\nggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) +\n    geom_point()\n\n\n\n\nL’hypothèse nulle est que la dose d’azote n’affecte pas le rendement, c’est à dire que le coefficient de pente et nul. Une autre hypothèse est que l’intercept est nul, c’est à dire qu’à une dose de 0, le rendement est de 0. Un modèle linéaire à variable de sortie continue est créé avec la fonction lm(), pour linear model.\n\nmodlin_1 &lt;- lm(yield ~ nitro, data = lasrosas.corn)\nsummary(modlin_1)\n\n\nCall:\nlm(formula = yield ~ nitro, data = lasrosas.corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.843213   0.608573 108.193  &lt; 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,   Adjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n\n\nLe diagnostic du modèle comprend plusieurs informations. D’abord, la formule utilisée est affichée pour la traçabilité. Vient ensuite un aperçu de la distribution des résidus. La médiane devrait s’approcher de la moyenne des résidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en considération l’échelle de y, et ce -3.079 est exprimé en terme de rendement, ici en quintaux (i.e. 100 kg) par hectare. La distribution des résidus mérite d’être davantage investiguée. Nous verrons cela un peu plus tard.\nLes coefficients apparaissent ensuite. Les estimés sont les valeurs des effets. R fournit aussi l’erreur standard associée, la valeur de t ainsi que la p-value (la probabilité d’obtenir cet effet ou un effet plus extrême si en réalité il y avait absence d’effet). L’intercept est bien sûr plus élevé que 0 (à dose nulle, on obtient un rendement de 65.8 quintaux par hectare en moyenne). La pente de la variable nitro est de ~0.06: pour chaque augmentation d’un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de maïs. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que l’intercept. Soulignons que l’ampleur du coefficient est très important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait qu’elle est inférieure à 0.05 (ce qui arrive souvent dans la littérature), serait très insuffisant pour l’interprétation des statistiques. La p-value nous indique néanmoins qu’il serait très improbable qu’une telle pente ait été générée alors que celle-ci est nulle en réalité. Les étoiles à côté des p-values indiquent l’ampleur selon l’échelle Signif. codes indiquée en-dessous du tableau des coefficients.\nSous ce tableau, R offre d’autres statistiques. En outre, les R² et R² ajustés indiquent si la régression passe effectivement par les points. Le R² prend un maximum de 1 lorsque la droite passe exactement sur les points.\nEnfin, le test de F génère une p-value indiquant la probabilité que les coefficients de pente ait été générés si les vrais coefficients étaient nuls. Dans le cas d’une régression univariée, cela répète l’information sur l’unique coefficient.\nOn pourra également obtenir les intervalles de confiance avec la fonction confint().\n\nconfint(modlin_1, level = 0.95)\n\n                  2.5 %      97.5 %\n(Intercept) 64.65001137 67.03641474\nnitro        0.04629164  0.07714271\n\n\nOu soutirer l’information de différentes manières, comme avec la fonction coefficients().\n\ncoefficients(modlin_1)\n\n(Intercept)       nitro \n65.84321305  0.06171718 \n\n\nÉgalement, on pourra exécuter le modèle sur les données qui ont servi à le générer:\n\npredict(modlin_1)[1:5]\n\n       1        2        3        4        5 \n73.95902 73.95902 73.95902 73.95902 73.95902 \n\n\nOu sur des données externes.\n\nnouvelles_donnees &lt;- data.frame(nitro = seq(from = 0, to = 100, by = 5))\npredict(modlin_1, newdata = nouvelles_donnees)[1:5]\n\n       1        2        3        4        5 \n65.84321 66.15180 66.46038 66.76897 67.07756 \n\n\n\n7.8.1.2 Analyse des résidus\nLes résidus sont les erreurs du modèle. C’est le vecteur \\(\\epsilon\\), qui est un décalage entre les données et le modèle. Le R² est un indicateur de l’ampleur du décalage, mais une régression linéaire explicative en bonne et due forme devrait être accompagnée d’une analyse des résidus. On peut les calculer par \\(\\epsilon = y - \\hat{y}\\), ou alors simplement utiliser la fonction residuals().\n\nres_df &lt;- data.frame(nitro = lasrosas.corn$nitro,\n                     residus_lm = residuals(modlin_1),\n                     residus_calcul = lasrosas.corn$yield - predict(modlin_1))\nslice_sample(res_df, n = 10)\n\n     nitro residus_lm residus_calcul\n2931 124.6  24.666827      24.666827\n1793 124.6  11.126827      11.126827\n2006  99.8  25.417413      25.417413\n116   66.0 -11.636547     -11.636547\n1235 131.5  11.460978      11.460978\n2426  75.4 -18.686688     -18.686688\n1132  29.0  -1.763011      -1.763011\n15   131.5 -11.289022     -11.289022\n1691 131.5  -5.639022      -5.639022\n38   131.5 -13.129022     -13.129022\n\n\nDans une bonne régression linéaire, on ne retrouvera pas de structure identifiable dans les résidus, c’est-à-dire que les résidus sont bien distribués de part et d’autre du modèle de régression.\n\nggplot(res_df, aes(x = nitro, y = residus_lm)) +\n  geom_point() +\n  labs(x = \"Dose N\", y = \"Résidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nBien que le jugement soit subjectif, on peut dire avec confiance qu’il n’y a pas structure particulière. En revanche, on pourrait générer un \\(y\\) qui varie de manière quadratique avec \\(x\\), un modèle linéaire montrera une structure évidente.\n\nset.seed(36164)\nx &lt;- 0:100\ny &lt;- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50)\nmodlin_2 &lt;- lm(y ~ x)\nggplot(data.frame(y, residus = residuals(modlin_2)),\n       aes(x = x, y = residus)) +\n  geom_point() +\n  labs(x = \"x\", y = \"Résidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\n\n\n\nÉgalement, les résidus ne devraient pas croître avec \\(x\\).\n\nset.seed(3984)\nx &lt;- 0:100\ny &lt;-  10 + x + x * rnorm(length(x), 0, 2)\nmodlin_3 &lt;- lm(y ~ x)\nggplot(data.frame(x, residus = residuals(modlin_3)),\n       aes(x = x, y = residus)) +\n  geom_point() +\n  labs(x = \"x\", y = \"Résidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\n\n\n\nOn pourra aussi inspecter les résidus avec un graphique de leur distribution. Reprenons notre modèle de rendement du maïs.\n\nggplot(res_df, aes(x = residus_lm)) +\n  geom_histogram(binwidth = 2, color = \"white\") +\n  labs(x = \"Residual\")\n\n\n\n\nL’histogramme devrait présenter une distribution normale. Les tests de normalité comme le test de Shapiro-Wilk peuvent aider, mais ils sont généralement très sévères.\n\nshapiro.test(res_df$residus_lm)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res_df$residus_lm\nW = 0.94868, p-value &lt; 2.2e-16\n\n\nL’hypothèse nulle que la distribution est normale est rejetée au seuil 0.05. Dans notre cas, il est évident que la sévérité du test n’est pas en cause car les résidus semblent générer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilité de la variable-réponse.\n\n7.8.1.3 Régression multiple\nComme c’est le cas pour bien des phénomènes en écologie, le rendement d’une culture n’est certainement pas expliqué seulement par la dose d’azote.\nLorsque l’on combine plusieurs variables explicatives, on crée un modèle de régression multivariée, ou une régression multiple. Bien que les tendances puissent sembler non-linéaires, l’ajout de variables et le calcul des coefficients associés reste un problème d’algèbre linéaire.\nOn pourra en effet généraliser les modèles linéaires, univariés et multivariés, de la manière suivante.\n\\[ y = X \\beta + \\epsilon \\]\noù:\n\\(X\\) est la matrice du modèle à \\(n\\) observations et \\(p\\) variables.\n\\[ X = \\left( \\begin{matrix}\n1 & x_{11} & \\cdots & x_{1p}  \\\\\n1 & x_{21} & \\cdots & x_{2p}  \\\\\n\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n1 & x_{n1} & \\cdots & x_{np}\n\\end{matrix} \\right) \\]\n\\(\\beta\\) est la matrice des \\(p\\) coefficients, \\(\\beta_0\\) étant l’intercept qui multiplie la première colonne de la matrice \\(X\\).\n\\[ \\beta = \\left( \\begin{matrix}\n\\beta_0  \\\\\n\\beta_1  \\\\\n\\vdots \\\\\n\\beta_p\n\\end{matrix} \\right) \\]\n\\(\\epsilon\\) est l’erreur de chaque observation.\n\\[ \\epsilon = \\left( \\begin{matrix}\n\\epsilon_0  \\\\\n\\epsilon_1  \\\\\n\\vdots \\\\\n\\epsilon_n\n\\end{matrix} \\right) \\]\n\n7.8.1.4 Modèles linéaires univariés avec variable catégorielle nominale\n\nUne variable catégorielle nominale (non ordonnée) utilisée à elle seule dans un modèle comme variable explicative, est un cas particulier de régression multiple. En effet, l’encodage catégoriel (ou dummyfication) transforme une variable catégorielle nominale en une matrice de modèle comprenant une colonne désignant l’intercept (une série de 1) désignant la catégorie de référence, ainsi que des colonnes pour chacune des autres catégories désignant l’appartenance (1) ou la non appartenance (0) de la catégorie désignée par la colonne.\n\n7.8.1.4.1 L’encodage catégoriel\nUne variable à \\(C\\) catégories pourra être déclinée en \\(C\\) variables dont chaque colonne désigne par un 1 l’appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour l’exemple, créons un vecteur désignant le cultivar de pomme de terre.\n\ndata &lt;- data.frame(cultivar = factor(c('Superior', 'Superior', 'Superior', 'Russet', 'Kenebec', 'Russet')))\nmodel.matrix(~cultivar, data)\n\n  (Intercept) cultivarRusset cultivarSuperior\n1           1              0                1\n2           1              0                1\n3           1              0                1\n4           1              1                0\n5           1              0                0\n6           1              1                0\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$cultivar\n[1] \"contr.treatment\"\n\n\nNous avons trois catégories, encodées en trois colonnes. La première colonne est un intercept et les deux autres décrivent l’absence (0) ou la présence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que l’appartenance à une catégorie est mutuellement exclusive, c’est-à-dire qu’un échantillon ne peut être assigné qu’à une seule catégorie, on peut déduire une catégorie à partir de l’information sur toutes les autres. Par exemple, si cultivar_Russet et cultivar_Superior sont toutes deux égales à \\(0\\), on conclura que cultivar_Kenebec est nécessairement égal à \\(1\\). Et si l’un d’entre cultivar_Russet et cultivar_Superior est égal à \\(1\\), cultivar_Kenebec est nécessairement égal à \\(0\\). L’information contenue dans un nombre \\(C\\) de catégories peut être encodée dans un nombre \\(C-1\\) de colonnes. C’est pourquoi, dans une analyse statistique, on désignera une catégorie comme une référence, que l’on détecte lorsque toutes les autres catégories sont encodées avec des \\(0\\): cette référence sera incluse dans l’intercept. La catégorie de référence par défaut en R est la première catégorie dans l’ordre alphabétique. On pourra modifier cette référence avec la fonction relevel().\n\ndata$cultivar &lt;- relevel(data$cultivar, ref = \"Superior\")\nmodel.matrix(~cultivar, data)\n\n  (Intercept) cultivarKenebec cultivarRusset\n1           1               0              0\n2           1               0              0\n3           1               0              0\n4           1               0              1\n5           1               1              0\n6           1               0              1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$cultivar\n[1] \"contr.treatment\"\n\n\nPour certains modèles, vous devrez vous assurer vous-même de l’encodage catégoriel. Pour d’autre, en particulier avec l’interface par formule de R, ce sera fait automatiquement.\n\n7.8.1.4.2 Exemple d’application\nPrenons la topographie du terrain, qui peut prendre plusieurs niveaux.\n\nlevels(lasrosas.corn$topo)\n\n[1] \"E\"  \"HT\" \"LO\" \"W\" \n\n\nExplorons le rendement selon la topographie.\n\nggplot(lasrosas.corn, aes(x = topo, y = yield)) +\n    geom_boxplot()\n\n\n\n\nLes différences sont évidentes, et la modélisation devrait montrer des effets différents.\nL’encodage catégoriel peut être visualisé en générant la matrice de modèle avec la fonction model.matrix() et l’interface-formule - sans la variable-réponse.\n\nmodel.matrix(~ topo, data = lasrosas.corn) |&gt; \n    as_tibble() |&gt;  # as_tibble pour transformer la matrice en tableau\n    slice_sample(n = 10)\n\n# A tibble: 10 × 4\n   `(Intercept)` topoHT topoLO topoW\n           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1             1      0      0     0\n 2             1      0      1     0\n 3             1      0      0     1\n 4             1      0      0     1\n 5             1      0      1     0\n 6             1      0      0     1\n 7             1      1      0     0\n 8             1      0      1     0\n 9             1      0      0     1\n10             1      0      0     0\n\n\nDans le cas d’un modèle avec une variable catégorielle nominale seule, l’intercept représente la catégorie de référence, ici E. Les autres colonnes spécifient l’appartenance (1) ou la non-appartenance (0) de la catégorie pour chaque observation.\nCette matrice de modèle utilisée pour la régression donnera un intercept, qui indiquera l’effet de la catégorie de référence, puis les différences entre les catégories subséquentes et la catégorie de référence.\n\nmodlin_4 &lt;- lm(yield ~ topo, data = lasrosas.corn)\nsummary(modlin_4)\n\n\nCall:\nlm(formula = yield ~ topo, data = lasrosas.corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.371 -11.933  -1.593  11.080  44.119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  78.6653     0.5399 145.707   &lt;2e-16 ***\ntopoHT      -30.0526     0.7500 -40.069   &lt;2e-16 ***\ntopoLO        6.2832     0.7293   8.615   &lt;2e-16 ***\ntopoW       -11.8841     0.7039 -16.883   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.59 on 3439 degrees of freedom\nMultiple R-squared:  0.4596,    Adjusted R-squared:  0.4591 \nF-statistic:   975 on 3 and 3439 DF,  p-value: &lt; 2.2e-16\n\n\nLe modèle linéaire est équivalent à l’anova, mais les résultats de lm sont plus élaborés.\n\nsummary(aov(yield ~ topo, data = lasrosas.corn))\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \ntopo           3 622351  207450     975 &lt;2e-16 ***\nResiduals   3439 731746     213                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nL’analyse de résidus peut être effectuée de la même manière.\n\n7.8.1.5 Modèles linéaires univariés avec variable catégorielle ordinale\n\nBien que j’introduise la régression sur variable catégorielle ordinale à la suite de la section sur les variables nominales, nous revenons dans ce cas à une régression simple, univariée. Voyons un cas à 5 niveaux.\n\nstatut &lt;- c(\"Totalement en désaccord\",\n            \"En désaccord\",\n            \"Ni en accord, ni en désaccord\",\n            \"En accord\",\n            \"Totalement en accord\")\nstatut_o &lt;- factor(statut, levels = statut, ordered=TRUE)\nmodel.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) où 5 est le nombre de niveaux\n\n  (Intercept)    statut_o.L statut_o.Q    statut_o.C statut_o^4\n1           1 -6.324555e-01  0.5345225 -3.162278e-01  0.1195229\n2           1 -3.162278e-01 -0.2672612  6.324555e-01 -0.4780914\n3           1 -3.510833e-17 -0.5345225  1.755417e-16  0.7171372\n4           1  3.162278e-01 -0.2672612 -6.324555e-01 -0.4780914\n5           1  6.324555e-01  0.5345225  3.162278e-01  0.1195229\nattr(,\"assign\")\n[1] 0 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$statut_o\n[1] \"contr.poly\"\n\n\nLa matrice de modèle a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres désignant différentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils linéairement? De manière quadratique, cubique ou plus loin dans des distributions polynomiales?\n\nmodmat_tidy &lt;- data.frame(statut, model.matrix(~statut_o)[, -1]) |&gt; \n    pivot_longer(-statut, names_to = \"variable\", values_to = \"valeur\")\nmodmat_tidy$statut &lt;- factor(modmat_tidy$statut,\n                             levels = statut,\n                             ordered=TRUE)\nggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) +\n    facet_wrap(. ~ variable) +\n    geom_point() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nRègle générale, pour les variables ordinales, on préférera une distribution linéaire, et c’est l’option par défaut de la fonction lm(). L’utilisation d’une autre distribution peut être effectuée à la mitaine en utilisant dans le modèle la colonne désirée de la sortie de la fonction model.matrix().\n\n7.8.1.6 Régression multiple à plusieurs variables\nReprenons le tableau de données du rendement de maïs.\n\nhead(lasrosas.corn)\n\n  year       lat      long yield nitro topo     bv rep nf\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5\n\n\nPour ajouter des variables au modèle dans l’interface-formule, on additionne les noms de colonne. La variable lat désigne la latitude, la variable long désigne la longitude et la variable bv (brightness value) désigne la teneur en matière organique du sol (plus bv est élevée, plus faible est la teneur en matière organique).\n\nmodlin_5 &lt;- lm(yield ~ lat + long + nitro + topo + bv,\n               data = lasrosas.corn)\nsummary(modlin_5)\n\n\nCall:\nlm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.405 -11.071  -1.251  10.592  40.078 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.946e+05  3.309e+04   5.882 4.45e-09 ***\nlat          5.541e+03  4.555e+02  12.163  &lt; 2e-16 ***\nlong         1.776e+02  4.491e+02   0.395    0.693    \nnitro        6.867e-02  5.451e-03  12.597  &lt; 2e-16 ***\ntopoHT      -2.665e+01  1.087e+00 -24.520  &lt; 2e-16 ***\ntopoLO       5.565e+00  1.035e+00   5.378 8.03e-08 ***\ntopoW       -1.465e+01  1.655e+00  -8.849  &lt; 2e-16 ***\nbv          -5.089e-01  3.069e-02 -16.578  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.47 on 3435 degrees of freedom\nMultiple R-squared:  0.5397,    Adjusted R-squared:  0.5387 \nF-statistic: 575.3 on 7 and 3435 DF,  p-value: &lt; 2.2e-16\n\n\nL’ampleur des coefficients est relatif à l’échelle de la variable. En effet, un coefficient de 5541 sur la variable lat n’est pas comparable au coefficient de la variable bv, de -0.5089, étant donné que les variables ne sont pas exprimées avec la même échelle. Pour les comparer sur une même base, on peut centrer (soustraire la moyenne) et réduire (diviser par l’écart-type).\n\nlasrosas.corn_sc &lt;- lasrosas.corn |&gt; \n  mutate_at(c(\"lat\", \"long\", \"nitro\", \"bv\"), scale)\n\nmodlin_5_sc &lt;- lm(yield ~ lat + long + nitro + topo + bv,\n               data = lasrosas.corn_sc)\nsummary(modlin_5_sc)\n\n\nCall:\nlm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.405 -11.071  -1.251  10.592  40.078 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  78.9114     0.6666 118.376  &lt; 2e-16 ***\nlat           3.9201     0.3223  12.163  &lt; 2e-16 ***\nlong          0.3479     0.8796   0.395    0.693    \nnitro         2.9252     0.2322  12.597  &lt; 2e-16 ***\ntopoHT      -26.6487     1.0868 -24.520  &lt; 2e-16 ***\ntopoLO        5.5647     1.0347   5.378 8.03e-08 ***\ntopoW       -14.6487     1.6555  -8.849  &lt; 2e-16 ***\nbv           -4.9253     0.2971 -16.578  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.47 on 3435 degrees of freedom\nMultiple R-squared:  0.5397,    Adjusted R-squared:  0.5387 \nF-statistic: 575.3 on 7 and 3435 DF,  p-value: &lt; 2.2e-16\n\n\nTypiquement, les variables catégorielles, qui ne sont pas mises à l’échelle, donneront des coefficients plus élevées, et devrons être évaluées entre elles et non comparativement aux variables mises à l’échelle. Une manière conviviale de représenter des coefficients consiste à utiliser la fonction tidy du module broom, qui génère un tableau contenant les coefficients ainsi que leurs intervalles de confiance, que nous pourrons ensuite porter graphiquement.\n\nlibrary(\"broom\") # ou bien charger le méta-module tidymodels\nintervals &lt;- tidy(modlin_5_sc, conf.int = TRUE, conf.level = 0.95)\nintervals\n\n# A tibble: 8 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   78.9       0.667   118.    0            77.6      80.2 \n2 lat            3.92      0.322    12.2   2.34e- 33     3.29      4.55\n3 long           0.348     0.880     0.395 6.93e-  1    -1.38      2.07\n4 nitro          2.93      0.232    12.6   1.33e- 35     2.47      3.38\n5 topoHT       -26.6       1.09    -24.5   1.74e-122   -28.8     -24.5 \n6 topoLO         5.56      1.03      5.38  8.03e-  8     3.54      7.59\n7 topoW        -14.6       1.66     -8.85  1.39e- 18   -17.9     -11.4 \n8 bv            -4.93      0.297   -16.6   1.92e- 59    -5.51     -4.34\n\n\nLa valeur par défaut de l’argument conf.level est de 0.95, mais je vous suggère de toujours l’écrire de manière explicite, ne serait-ce que pour rappeler à vous-même ainsi qu’à vos collègues que cette valeur est arbitraire: il s’agit d’une décision d’analyse, non pas d’une valeur à utiliser par convention.\nPour le graphique, on aura avantage à séparer les effets catégoriels aux effets numériques pour mieux interpréter leurs effets entre eux. J’utilise la fonction dplyr::case_when() pour créer une nouvelle colonne qui catégorisera les termes de l’équation. Cette catégorie me permettra d’effectuer un facet_wrap().\n\nintervals |&gt; \n  mutate(type = case_when(\n    term %in% c(\"topoHT\", \"topoLO\", \"topoW\") ~ \"Catégorie\", # condition ~ résultat\n    term == \"(Intercept)\" ~ \"Intercept\",  # condition ~ résultat\n    TRUE ~ \"numérique\" # pour toute autre condition (TRUE) ~ résultat\n  )) |&gt; \n  ggplot(mapping = aes(x = estimate, y = term)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  geom_segment(mapping = aes(x = conf.low, xend = conf.high, yend = term)) +\n  geom_point() +\n  labs(x = \"Coefficient standardisé\", y = \"\") +\n  facet_wrap(~type, scales = \"free\", ncol = 1, strip.position = \"right\")\n\n\n\n\nOn y voit qu’à l’exception de la variable long, tous les coefficients sont éloignés de 0. Le coefficient bv est négatif, indiquant que plus la valeur de bv est élevé (donc plus le sol est pauvre en matière organique), plus le rendement est faible. Plus la latitude est élevée (plus on se dirige vers le Nord de l’Argentine), plus le rendement est élevé. La dose d’azote a aussi un effet statistique positif sur le rendement.\nQuant aux catégories topographiques, elles sont toutes éloignées de la catégorie E, placée à zéro. De plus, les intervalles de confiance à 0.95 ne se chevauchant pas, on peut conclure que la variabilité du phénomène échantillonné n’est pas suffisante pour expliquer les différences importantes entre elles.\nOn pourra retrouver des cas où l’effet combiné de plusieurs variables diffère de l’effet des deux variables prises séparément. Par exemple, on pourrait évaluer l’effet de l’azote et celui de la topographie dans un même modèle, puis y ajouter une interaction entre l’azote et la topographie, qui définira des effets supplémentaires de l’azote selon chaque catégorie topographique. C’est ce que l’on appelle une interaction.\nDans l’interface-formule, l’interaction entre l’azote et la topographie est notée nitro:topo. Pour ajouter cette interaction, la formule deviendra yield ~ nitro + topo + nitro:topo. Une approche équivalente est d’utiliser le raccourci yield ~ nitro * topo.\n\nmodlin_5_sc &lt;- lm(yield ~ nitro * topo,\n               data = lasrosas.corn_sc)\nsummary(modlin_5_sc)\n\n\nCall:\nlm(formula = yield ~ nitro * topo, data = lasrosas.corn_sc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.984 -11.985  -1.388  10.339  40.636 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   78.6999     0.5322 147.870  &lt; 2e-16 ***\nnitro          1.8131     0.5351   3.388 0.000711 ***\ntopoHT       -30.0052     0.7394 -40.578  &lt; 2e-16 ***\ntopoLO         6.2026     0.7190   8.627  &lt; 2e-16 ***\ntopoW        -11.9628     0.6939 -17.240  &lt; 2e-16 ***\nnitro:topoHT   1.2553     0.7461   1.682 0.092565 .  \nnitro:topoLO   0.5695     0.7186   0.792 0.428141    \nnitro:topoW    0.7702     0.6944   1.109 0.267460    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.38 on 3435 degrees of freedom\nMultiple R-squared:  0.4756,    Adjusted R-squared:  0.4746 \nF-statistic: 445.1 on 7 and 3435 DF,  p-value: &lt; 2.2e-16\n\n\nLes résultats montrent des effets de l’azote et des catégories topographiques, mais il y a davantage d’incertitude sur les interactions, indiquant que l’effet statistique de l’azote est sensiblement le même indépendamment des niveaux topographiques.\nDans le cas des régressions multiples, les résidus ne peuvent pas être présentés selon une variable explicative \\(x\\), puisqu’il y en a plusieurs. On fera l’analyse des résidus selon la variable réponse \\(y\\).\n\ntibble(\n  y = lasrosas.corn_sc$yield,\n  residus = residuals(modlin_5_sc)\n) |&gt; \n  ggplot(aes(x = y, y = residus)) +\n  geom_point() +\n  labs(x = \"y\", y = \"Résidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\n\n\n\nDans ce modèle, il y a clairement une structure qui nous échappe! L’ajout d’autres variables nous permettrait éventuellement d’obtenir une distribution qui s’approche d’un bruit.\n\n7.8.1.7 Les interactions\nUne interaction est un effet supplémentaire qui est investigué pour des combinaisons de variables. L’interaction entre l’azote et la topographie est une nouvelle variable créée par la multiplication de l’azote, une variable numérique, et de la topographie, qui ici est une variable catégorielle.\n\nmodel.matrix(~ nitro * topo, data = lasrosas.corn_sc) |&gt; head()\n\n  (Intercept)    nitro topoHT topoLO topoW nitro:topoHT nitro:topoLO\n1           1 1.571194      0      0     1            0            0\n2           1 1.571194      0      0     1            0            0\n3           1 1.571194      0      0     1            0            0\n4           1 1.571194      0      0     1            0            0\n5           1 1.571194      0      0     1            0            0\n6           1 1.571194      0      0     1            0            0\n  nitro:topoW\n1    1.571194\n2    1.571194\n3    1.571194\n4    1.571194\n5    1.571194\n6    1.571194\n\n\nL’entête de la matrice modèle montre que l’interaction est l’addition de trois variables, qui sont nulles si la catégorie topographique est absente, mais qui prend la dose d’azote pour la catégorie présente seulement.\nL’interprétation d’une interaction est spécifique au modèle utilisé. Une manière de l’interpréter est de se demander dans quelles unités elle est exprimée. Dans notre exemple, il s’agit de kg/ha standardisés.\nPrenons un autre exemple, cette fois-ci avec des données fictives. Une enquête a été menée où des personnes évaluaient le karma (échelle 0 à 10) de pieds nus, en bas (chaussettes) et/ou en sandales.\n\nkarma_df &lt;- read_csv(\"data/karma_df.csv\")\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): ID, sandales, bas, karma\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNous désirons savoir quelle est l’effet des bas et des sandales sur le karma, donc 💖 ~ 👡 + 🧦.\n\ntidy(lm(karma ~ sandales + bas, karma_df))\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.67     0.147      25.0 1.11e-94\n2 sandales        2.29     0.166      13.8 7.99e-38\n3 bas             1.77     0.168      10.5 6.31e-24\n\n\nÀ partir du scénario à pieds nus d’un karma de 3.67, les sandales ajoutent 2.29 de points de karma, alors que les bas en ajoutent 1.8. Mais ce modèle est incomplet, cas on n’évalue pas l’effet des bas ET des sandales, donc 💖 ~ 👡 * 🧦.\n\ntidy(lm(karma ~ sandales * bas, karma_df))\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      2.68     0.134      20.0 2.22e-68\n2 sandales         3.68     0.159      23.2 4.07e-85\n3 bas              3.20     0.162      19.8 1.40e-67\n4 sandales:bas    -5.25     0.309     -17.0 3.79e-53\n\n\nLe modèle est plus clair. Sans interaction, les effets sur le karma des bas et des sandales étaient négativement affectés par l’effet d’interaction sandales:bas, le karma étant poussé à la baisse par le bas blanc dans vos sandales.\nIl est possible d’ajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a d’interactions, plus votre modèle comprendra de variables et vos tests d’hypothèse perdront en puissance statistique.\n\n7.8.1.8 Les modèles linéaires généralisés\nDans un modèle linéaire ordinaire, un changement constant dans les variables explicatives résulte en un changement constant de la variable-réponse. Cette supposition ne serait pas adéquate si la variable-réponse était un décompte, si elle est booléenne ou si, de manière générale, la variable-réponse ne suivait pas une distribution continue. De manière plus spécifique, elle ne s’applique pas aux cas où il n’y a pas moyen de retrouver une distribution normale des résidus. On pourra bien sûr transformer les variables. Mais il pourrait s’avérer impossible ou tout simplement non souhaitable de transformer les variables. Le modèle linéaire généralisé (MLG, ou generalized linear model - GLM) est une généralisation du modèle linéaire ordinaire chez qui la variable-réponse peut être caractérisé par une distribution de Poisson, de Bernouilli, etc.\nPrenons d’abord le cas d’un décompte de vers fil-de-fer (worms) retrouvés dans des parcelles sous différents traitements (trt). Les décomptes sont typiquement distribués selon une loi de Poisson.\n\ncochran.wireworms |&gt; ggplot(aes(x = worms)) + geom_histogram(bins = 10)\n\n\n\n\nExplorons les décomptes selon les traitements.\n\ncochran.wireworms |&gt; ggplot(aes(x = trt, y = worms)) + geom_boxplot()\n\n\n\n\nLes traitements semblent à première vue avoir un effet comparativement au contrôle. Lançons un MLG avec la fonction glm(), et spécifions que la sortie est une distribution de Poisson. Bien que la fonction de lien (link = \"log\") soit explictement imposée, le log est la valeur par défaut pour les distributions de Poisson. Ainsi, les coefficients du modèles devront être interprétés selon un modèle \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\).\n\nmodglm_1 &lt;- glm(worms ~ trt, cochran.wireworms, family = stats::poisson(link=\"log\"))\nsummary(modglm_1)\n\n\nCall:\nglm(formula = worms ~ trt, family = stats::poisson(link = \"log\"), \n    data = cochran.wireworms)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.1823     0.4082   0.447 0.655160    \ntrtM          1.6422     0.4460   3.682 0.000231 ***\ntrtN          1.7636     0.4418   3.991 6.57e-05 ***\ntrtO          1.5755     0.4485   3.513 0.000443 ***\ntrtP          1.3437     0.4584   2.931 0.003375 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 64.555  on 24  degrees of freedom\nResidual deviance: 38.026  on 20  degrees of freedom\nAIC: 125.64\n\nNumber of Fisher Scoring iterations: 5\n\n\nL’interprétation spécifique des coefficients d’une régression de Poisson doit passer par la fonction de lien \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). Le traitement de référence (K), qui correspond à l’intercept, sera accompagné d’un nombre de vers de \\(exp \\left(0.1823\\right) = 1.20\\) vers, et le traitement M, à \\(exp \\left(1.6422\\right) = 5.17\\) vers. Cela correspond à ce que l’on observe sur les boxplots plus haut.\nIl est très probable (p-value de ~0.66) qu’un intercept (traitement K) de 0.18 ayant une erreur standard de 0.4082 ait été généré depuis une population dont l’intercept est nul. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils être considérés comme équivalents?\n\nintervals &lt;- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l'intercept\n                    LL = confint(modglm_1)[, 1], # [-1, ] enlever la première ligne, celle de l'intercept\n                    UL = confint(modglm_1)[, 2],\n                    variable = names(coefficients(modglm_1)))\n\nWaiting for profiling to be done...\nWaiting for profiling to be done...\n\nintervals\n\n# A tibble: 5 × 4\n  Estimate     LL    UL variable   \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n1    0.182 -0.740 0.888 (Intercept)\n2    1.64   0.840 2.62  trtM       \n3    1.76   0.972 2.74  trtN       \n4    1.58   0.766 2.56  trtO       \n5    1.34   0.509 2.34  trtP       \n\n\n\nggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) +\n    geom_vline(xintercept = 0, lty = 2) +\n    geom_segment(mapping = aes(x = LL, xend = UL,\n                               y = variable, yend = variable)) +\n    geom_point() +\n    labs(x = \"Coefficient\", y = \"\")\n\n\n\n\nLes intervalles de confiance se superposant, on ne peut pas conclure qu’un traitement est lié à une réduction plus importante de vers qu’un autre, au seuil 0.05.\nMaintenant, à défaut de trouver un tableau de données plus approprié, prenons le tableau mtcars, qui rassemble des données sur des modèles de voitures. La colonne vs, pour v-shaped, inscrit 0 si les pistons sont droits et 1 s’ils sont placés en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du véhicule (wt)?\n\nmtcars |&gt; slice_sample(n = 6)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat 128          32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nVolvo 142E        21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nChrysler Imperial 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n\n\n\nmtcars |&gt; \n    ggplot(aes(x = wt, y = vs)) + geom_point()\n\n\n\n\nIl semble y avoir une tendance: les véhicules plus lourds ont plutôt des pistons droits (vs = 0). Vérifions cela.\n\nmodglm_2 &lt;- glm(vs ~ wt, data = mtcars, family = stats::binomial())\nsummary(modglm_2)\n\n\nCall:\nglm(formula = vs ~ wt, family = stats::binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   5.7147     2.3014   2.483  0.01302 * \nwt           -1.9105     0.7279  -2.625  0.00867 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.860  on 31  degrees of freedom\nResidual deviance: 31.367  on 30  degrees of freedom\nAIC: 35.367\n\nNumber of Fisher Scoring iterations: 5\n\n\nExercice. Analyser les résultats.\n\n7.8.1.9 Les modèles non-linéaires\nLa hauteur d’un arbre en fonction du temps n’est typiquement pas linéaire. Elle tend à croître de plus en plus lentement jusqu’à un plateau. De même, le rendement d’une culture traitée avec des doses croissantes de fertilisants tend à atteindre un maximum, puis à se stabiliser.\nCes phénomènes ne peuvent pas être approximés par des modèles linéaires. Examinons les données du tableau engelstad.nitro.\n\nengelstad.nitro |&gt; slice_sample(n = 10)\n\n         loc year nitro yield\n1  Knoxville 1966     0  63.0\n2  Knoxville 1965   335  61.2\n3    Jackson 1965   335  73.0\n4    Jackson 1966   201  61.3\n5    Jackson 1966   335  59.8\n6  Knoxville 1964     0  60.9\n7  Knoxville 1964    67  75.9\n8    Jackson 1966    67  45.2\n9    Jackson 1962   201  73.1\n10   Jackson 1964   335  67.8\n\n\n\nengelstad.nitro |&gt; \n    ggplot(aes(x = nitro, y = yield)) +\n        facet_grid(year ~ loc) +\n        geom_line() +\n        geom_point()\n\n\n\n\nLe modèle de Mitscherlich pourrait être utilisé.\n\\[ y = A \\left( 1 - e^{-R \\left( E + x \\right)} \\right) \\]\noù \\(y\\) est le rendement, \\(x\\) est la dose, \\(A\\) est l’asymptote vers laquelle la courbe converge à dose croissante, \\(E\\) est l’équivalent de dose fourni par l’environnement et \\(R\\) est le taux de réponse.\nExplorons la fonction.\n\nmitscherlich_f &lt;- function(x, A, E, R) {\n    A * (1 - exp(-R*(E + x)))\n}\n\nx &lt;- seq(0, 350, by = 5)\ny &lt;- mitscherlich_f(x, A = 75, E = 30, R = 0.02)\n\nggplot(tibble(x, y), aes(x, y)) +\n    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +\n    geom_line() + ylim(c(0, 100))\n\n\n\n\nExercice. Changez les paramètres pour visualiser comment la courbe réagit.\nNous pouvons décrire le modèle grâce à l’interface formule dans la fonction nls(). Notez que les modèles non-linéaires demandent des stratégies de calcul différentes de celles des modèles linéaires. En tout temps, nous devons identifier des valeurs de départ raisonnables pour les paramètres dans l’argument start. Vous réussirez rarement à obtenir une convergence du premier coup avec vos paramètres de départ. Le défi est d’en trouver qui permettront au modèle de converger. Parfois, le modèle ne convergera jamais. D’autres fois, il convergera vers des solutions différentes selon les variables de départ choisies.\nmodnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))),\n                data = engelstad.nitro,\n                start = list(A = 50, E = 10, R = 0.2))\nLe modèle ne converge pas (le bloc de calcul est désactivé). Essayons les valeurs prises plus haut, lors de la création du graphique, qui semblent bien s’ajuster.\n\nmodnl_1 &lt;-  nls(yield ~ A * (1 - exp(-R*(E + nitro))),\n                data = engelstad.nitro,\n                start = list(A = 75, E = 30, R = 0.02))\n\nBingo! Voyons maintenant le sommaire.\n\nsummary(modnl_1)\n\n\nFormula: yield ~ A * (1 - exp(-R * (E + nitro)))\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nA 75.023427   3.331860  22.517   &lt;2e-16 ***\nE 66.164111  27.251591   2.428   0.0184 *  \nR  0.012565   0.004881   2.574   0.0127 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.34 on 57 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 8.058e-06\n\n\nLes paramètres sont différents de zéro, et donnent la courbe suivante.\n\nx &lt;- seq(0, 350, by = 5)\ny &lt;- mitscherlich_f(x,\n                    A = coefficients(modnl_1)[1],\n                    E = coefficients(modnl_1)[2],\n                    R = coefficients(modnl_1)[3])\n\nggplot(tibble(x, y), aes(x, y)) +\n    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +\n    geom_line() + ylim(c(0, 100))\n\n\n\n\nEt les résidus…\n\ntibble(res = residuals(modnl_1)) |&gt; \n    ggplot(aes(x = res)) + geom_histogram(bins = 20)\n\n\n\n\n\ntibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) |&gt; \n    ggplot(aes(x = nitro, y = res)) +\n        geom_point() +\n        geom_hline(yintercept = 0, colour = \"red\")\n\n\n\n\nLes résidus ne sont pas distribués normalement, mais semblent bien partagés de part et d’autre de la courbe.\n\n7.8.2 Modèles à effets mixtes\nLorsque l’on combine des variables fixes (testées lors de l’expérience) et des variables aléatoire (variation des unités expérimentales), on obtient un modèle mixte. Les modèles mixtes peuvent être univariés, multivariés, linéaires (ordinaires ou généralisés) ou non linéaires.\nÀ la différence d’un effet fixe, un effet aléatoire sera toujours distribué normalement avec une moyenne de 0 et une certaine variance. Dans un modèle linéaire où l’effet aléatoire est un décalage d’intercept, cet effet s’additionne aux effets fixes:\n\\[ y = X \\beta + Z b + \\epsilon \\]\noù:\n\\(Z\\) est la matrice du modèle à \\(n\\) observations et \\(p\\) variables aléatoires. Les variables aléatoires sont souvent des variables nominales qui subissent un encodage catégoriel.\n\\[ Z = \\left( \\begin{matrix}\nz_{11} & \\cdots & z_{1p}  \\\\\nz_{21} & \\cdots & z_{2p}  \\\\\n\\vdots & \\ddots & \\vdots  \\\\\nz_{n1} & \\cdots & z_{np}\n\\end{matrix} \\right) \\]\n\\(b\\) est la matrice des \\(p\\) coefficients aléatoires.\n\\[ b = \\left( \\begin{matrix}\nb_0  \\\\\nb_1  \\\\\n\\vdots \\\\\nb_p\n\\end{matrix} \\right) \\]\nLe tableau lasrosas.corn, utilisé précédemment, contenait trois répétitions effectuées au cours de deux années, 1999 et 2001. Étant donné que la répétition R1 de 1999 n’a rien à voir avec la répétition R1 de 2001, on dit qu’elle est emboîtée dans l’année.\nLe module nlme nous aidera à monter notre modèle mixte.\n\nlibrary(\"nlme\")\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nmmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv,\n                 random = ~ 1|year/rep,\n                 data = lasrosas.corn,\n                 control = lmeControl(opt = \"optim\"))\n\nÀ ce stade vous devriez commencer à être familier avec l’interface formule et vous devriez saisir l’argument fixed, qui désigne l’effet fixe. L’effet aléatoire est random, suit un tilde ~. À gauche de la barre verticale |, on place les variables désignant les effets aléatoire sur la pente. Nous n’avons pas couvert cet aspect, alors nous le laissons à 1. À droite, on retrouve un structure d’emboîtement désignant l’effet aléatoire: le premier niveau est l’année, dans laquelle est emboîtée la répétition.\n\nsummary(mmodlin_1)\n\nLinear mixed-effects model fit by REML\n  Data: lasrosas.corn \n       AIC      BIC    logLik\n  26535.37 26602.93 -13256.69\n\nRandom effects:\n Formula: ~1 | year\n        (Intercept)\nStdDev:    20.35426\n\n Formula: ~1 | rep %in% year\n        (Intercept) Residual\nStdDev:    11.17447 11.35617\n\nFixed effects:  yield ~ lat + long + nitro + topo + bv \n                 Value Std.Error   DF    t-value p-value\n(Intercept) -1379436.9  55894.55 3430 -24.679273   0.000\nlat           -25453.0   1016.53 3430 -25.039084   0.000\nlong           -8432.3    466.05 3430 -18.092988   0.000\nnitro              0.0      0.00 3430   1.739757   0.082\ntopoHT           -27.7      0.92 3430 -30.122438   0.000\ntopoLO             6.8      0.88 3430   7.804733   0.000\ntopoW            -16.7      1.40 3430 -11.944793   0.000\nbv                -0.5      0.03 3430 -19.242424   0.000\n Correlation: \n       (Intr) lat    long   nitro  topoHT topoLO topoW \nlat     0.897                                          \nlong    0.866  0.555                                   \nnitro   0.366  0.391  0.247                            \ntopoHT  0.300 -0.017  0.582  0.024                     \ntopoLO -0.334 -0.006 -0.621 -0.038 -0.358              \ntopoW   0.403 -0.004  0.762  0.027  0.802 -0.545       \nbv     -0.121 -0.012 -0.214 -0.023 -0.467  0.346 -0.266\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.32360269 -0.66781575 -0.07450856  0.61587533  3.96434001 \n\nNumber of Observations: 3443\nNumber of Groups: \n         year rep %in% year \n            2             6 \n\n\nLa sortie est semblable à celle de la fonction lm().\n\n7.8.2.1 Modèles mixtes non-linéaires\nLe modèle non linéaire créé plus haut liait le rendement à la dose d’azote. Toutefois, les unités expérimentales (le site loc et l’année year) n’étaient pas pris en considération. Nous allons maintenant les considérer.\nNous devons décider la structure de l’effet aléatoire, et sur quelles variables il doit être appliqué - la décision appartient à l’analyste. Il me semble plus convenable de supposer que le site et l’année affectera le rendement maximum plutôt que l’environnement et le taux: les effets aléatoires seront donc affectés à la variable A. Les effets aléatoires n’ont pas de structure d’emboîtement. L’effet de l’année sur A sera celui d’une pente et l’effet de site sera celui de l’intercept. La fonction que nous utiliserons est nlme().\n\nmm &lt;- nlme(yield ~ A * (1 - exp(-R*(E + nitro))),\n           data = engelstad.nitro,\n           start = c(A = 75, E = 30, R = 0.02),\n           fixed = list(A ~ 1, E ~ 1, R ~ 1),\n           random = A ~ year | loc)\nsummary(mm)\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: yield ~ A * (1 - exp(-R * (E + nitro))) \n  Data: engelstad.nitro \n       AIC     BIC    logLik\n  477.2286 491.889 -231.6143\n\nRandom effects:\n Formula: A ~ year | loc\n Structure: General positive-definite, Log-Cholesky parametrization\n              StdDev       Corr  \nA.(Intercept)  2.608588499 A.(In)\nA.year         0.003066584 -0.556\nResidual      11.152757993       \n\nFixed effects:  list(A ~ 1, E ~ 1, R ~ 1) \n                 Value Std.Error DF   t-value p-value\nA.(Intercept) 74.58222  4.722715 56 15.792235  0.0000\nE             65.56721 25.533994 56  2.567840  0.0129\nR              0.01308  0.004808 56  2.720215  0.0087\n Correlation: \n  A.(In) E     \nE  0.379       \nR -0.483 -0.934\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.83373140 -0.89293039  0.07418166  0.68353578  1.82434344 \n\nNumber of Observations: 60\nNumber of Groups: 2 \n\n\nEt sur graphique:\n\nengelstad.nitro |&gt; \n  ggplot(aes(x = nitro, y = yield)) +\n  facet_grid(year ~ loc) +\n  geom_line(data = tibble(nitro = engelstad.nitro$nitro,\n                          yield = predict(mm, level = 0)),\n            colour = \"grey35\") +\n  geom_point() +\n  ylim(c(0, 95))\n\n\n\n\nLes modèles mixtes non linéaires peuvent devenir très complexes lorsque les paramètres, par exemple A, E et R, sont eux-même affectés linéairement par des variables (par exemple A ~ topo). Pour aller plus loin, consultez Parent et al. (2017) ainsi que les calculs associés à l’article. Ou écrivez-moi un courriel pour en discuter!\nNote. L’interprétation de p-values sur les modèles mixtes est controversée. À ce sujet, Douglas Bates a écrit une longue lettre à la communauté de développement du module lme4, une alternative à nlme, qui remet en cause l’utilisation des p-values, ici. De plus en plus, pour les modèles mixtes, on se tourne vers les statistiques bayésiennes, couvertes dans le chapitre 8 avec le module greta. Mais en ce qui a trait aux modèles mixtes, le module brms automatise bien des aspects de l’approche bayésienne.\n\n7.8.3 Aller plus loin\n\n7.8.3.1 Statistiques générales:\n\nThe analysis of biological data\n\n7.8.3.2 Statistiques avec R\n\nDisponibles en version électronique à la bibliothèque de l’Université Laval:\n\nIntroduction aux statistiques avec R: Introductory statistics with R\n\nApprofondir les statistiques avec R: The R Book, Third edition\n\nApprofondir les modèles à effets mixtes avec R: Mixed Effects Models and Extensions in Ecology with R\n\n\n\n\nModernDive, un livre en ligne offrant une approche moderne avec le package moderndive."
  },
  {
    "objectID": "07b-bayes.html",
    "href": "07b-bayes.html",
    "title": "8  Introduction à l’analyse bayésienne en écologie",
    "section": "",
    "text": "Objectifs spécifiques:\nCe chapitre est un extra. Il ne fait pas partie des objectifs du cours. Il ne sera pas évalué.\nÀ la fin de ce chapitre, vous\n\nserez en mesure de définir ce que sont les statistiques bayésiennes\nserez en mesure de calculer des statistiques descriptives de base en mode bayésien avec le module greta.\n\n\nÀ venir!"
  },
  {
    "objectID": "08-explorer.html#r-sur-le-web",
    "href": "08-explorer.html#r-sur-le-web",
    "title": "9  Explorer R",
    "section": "\n9.1 R sur le web",
    "text": "9.1 R sur le web\nDans un environnement de travail en évolution rapide et constante, il est difficile de considérer que ses compétences sont abouties. Rester informé sur le développement de R vous permettra de trouver et de résoudre des problèmes persistants de manière plus efficace ou par de nouvelles avenues, et vous offrira même l’occasion de dénicher des problèmes dont vous ne soupçonniez pas l’existence. Plusieurs sources d’information vous permettront de vous tenir à jour sur le développement de R, de ses environnements de travail (RStudio, Jupyter, etc.) et des nouveaux modules qui s’y greffent. Plus largement, vous gagnerez à vous informer sur les dernières tendances en calcul scientifique sur d’autres plate-forme que R (Python, Javascript, Julia, etc.). Évidemment, nos tâches quotidiennes ne nous permettent pas de tout suivre. Même si vous pouviez n’attrapper que 1% du défilement, ce serait déjà 1% de plus que rien du tout.\n\nÉvidemment, rester au courant aide parce que vous en apprenez davantage sur les outils et leurs applications. Mais ça aide aussi parce que ça vous permet de connaître des gens et des organisations! Il est très utile de savoir qui travaille sur quoi et où se déroulent les développements sur un sujet donné, car si vous cherchez consciemment quelque chose plus tard, ça vous aidera à trouver votre chemin plus facilement. - Maëlle Salmon, Keeping up to date with R news (ma traduction)\n\nJe vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez à l’aventure!\n\n\n\n\nTiré du film The Hobbit: An Unexpected Journey, de Peter Jackson (2012).\n\n\n\n\n9.1.1 GitHub\nNous avons vu chapitre 5 l’importance d’utilser des outils d’archivage et de suivi de version, comme git, dans le déploiement de la science ouverte. En effet, GitHub est une plateforme git sur Internet, acquise par Microsoft, qui est devenue un réseau social de développement informatique. De nombreux modules de R y sont développés. Au chapitre 5, vous avez appris à y ouvrir un compte et à y archiver du contenu. Vous pourrez alors suivre (dans le même sens que sur d’autres réseaux sociaux) le développement de projets et suivre les travaux des personnes qui vous semblent d’intérêt.\n\n9.1.2 X (anciennement Twitter)\nLe hashtag #rstats rassemble (ou plutôt rassemblait) sur Twitter ce qui se publie sur le sujet. Depuis le rachat du réseau social par Elon Musk et les divers changements qui y ont été effectués, plusieurs s’en sont détournés pour privilégier des plateformes opensource, par exemple les réseaux sociaux décentralisés de [Mastodon](https://joinmastodon.org/fr.\nOn y retrouve les comptes de R-bloggers, RStudio et rOpenSci. Certaines communautés y sont aussi actives, comme R4DS online learning community, qui partage des nouvelles sur R, et R-Ladies Global, qui vise à amener davantage de diversité à la communauté de R. Des comptes thématiques comme Daily R Cheatsheets, R for the Rest of Us et One R Package a Day permettent de découvrir quotidiennement de nouvelles possibilités. Enfin, plusieurs personnes contribuent positivement à la communauté R. Hadley Wickham brille parmi les étoiles de R. Les comptes de Mara Averick, Claus Wilke et David Robinson sont aussi intéressants.\n\n9.1.3 Les serveurs Mastodon\nMastodon est un réseau social opensource décentralisé, gratuit et interopérable qui fait partie de l’Univers Fediverse. Il est composé de plusieurs serveurs indépendants pouvant communiquer entre eux, et est assez similaire à Twitter. Depuis quelques années, la communauté de programmation R s’est largement tournée vers cette option, et vous trouverez la plupart des personnes ou des groupes mentionnés à la section précédente sur ce site :\n\nPosit\nrOpenSci\nR4DS online learning community\nR-Ladies Global\nOne R Package a Day\nHadley Wickham\n\nMara Averick,\nMaëlle Salmon\n\nEt pour les hashtags :\n\n#rstats\n#tidytuesday\n\nPeu importe votre choix de serveur, vous pourrez ensuite effectuer une recherche en utilisant le @ pour trouver des comptes et le # pour chercher des mots-clés. Bref, n’hésitez pas à fouiller et à suivre ce qui vous intéresse!\n\n9.1.4 Nouvelles\nLe site d’aggrégation R-bloggers, mis à jour quotidiennement, republie des articles en anglais tirés d’un peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux développements. Deux fois par mois, l’organisation rOpenSci offre un portrait de l’univ-R (#dadjoke), ce que R Weekly offre de manière hebdomadaire (l’information sera probablement redondante). Le tidyverse a quant à lui son propre blogue.\n\n9.1.5 Des questions?\nBien que davantage voués à la résolution de problème qu’ à l’exploration de nouvelles opportunités, Stackoverflow et Cross Validated sont des plateformes prisées. De plus, la liste de courriels r-sig-ecology permet des échanges entre professionnels et novices en analyse de données écologiques avec R.\n\n9.1.6 Participer\nR est un logiciel basé sur une communauté de développement, d’utilisation et de vulgarisation. Des personnes offrent généreusement du temps de support. Si vous vous sentez à l’aise, offrez aussi le vôtre!\n\n9.1.7 Mise en garde\nLes modules de R sont développés par quiconque le veut bien: leur qualité n’est pas nécessairement auditée. Souvent, ils ne sont vérifiés que par une vigilance communautaire: dans ce cas, vous êtes les cobailles. Ce qui n’est pas nécessairement une mauvaise chose, mais cela nécessite de prendre ses précautions. Dans sa conférence How to be a resilient R user, Maëlle Salmon propose quelques guides pour juger de la qualité d’un module.\n1. Le module est-il activement développé?\nBien!\n\nAttention!\n\n2. Le module est-il bien testé?\nVérifiez si le module a fait l’objet d’une publication scientifique, s’il a été utilisé avec succès dans la littérature ou dans des documents crédibles.\n3. Le module est-il bien documenté?\nUn site internet dédié est-il utilisé pour documenter l’utilisation du module? Les fichiers d’aide sont-ils complets, et sont-ils de bonne qualité?\n4. Le module est-il largement utilisé?\nUn module peu populaire n’est pas nécessessairement de mauvaise qualité: peut-être est-il seulement destiné à des applications de niche. S’il n’est pas un indicateur à lui seul de la solidité ou la validité d’un module, une masse critique indique que le module a passé sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peut être évalué par le nombre d’étoiles attribué au module (équivalent à un J’aime).\n\n5. Le module est-il développé par une personne ou une organisation crédible?\nOn peut affirmer sans trop se compromettre que l’équipe de RStudio (Posit) développe des modules de confiance. Tout comme il faudrait se méfier d’un module développé par une personne anonyme.\nLe module packagemetrics permet d’évaluer ces critères.\n\n# devtools::install_github(\"sfirke/packagemetrics\")\nlibrary(\"packagemetrics\")\npm &lt;- package_list_metrics(c(\"dplyr\", \"ggplot2\", \"vegan\", \"greta\"))\nmetrics_table(pm)\n\n\n9.1.8 Prendre tout ça en note\nUn logiciel de prise de notes (il en existe plein, mais je vous suggère d’opter pour les options encryptées) pourrait vous être utile pour retrouver l’information soutirée de vos flux d’information. Mais certaines personnes consignent simplement leurs informations dans un carnet ou un document de traitement de texte."
  },
  {
    "objectID": "08-explorer.html#r-en-chaire-et-en-os",
    "href": "08-explorer.html#r-en-chaire-et-en-os",
    "title": "9  Explorer R",
    "section": "\n9.2 R en chaire et en os",
    "text": "9.2 R en chaire et en os\nL’Université Laval (institution auprès de laquelle ce manuel est développé) est hôte à tous les 2 ans de la conférence R à Québec. La prochaine conférence aura lieu en 2021."
  },
  {
    "objectID": "08-explorer.html#quelques-outils-en-écologie-mathématique-avec-r",
    "href": "08-explorer.html#quelques-outils-en-écologie-mathématique-avec-r",
    "title": "9  Explorer R",
    "section": "\n9.2 Quelques outils en écologie mathématique avec R",
    "text": "9.2 Quelques outils en écologie mathématique avec R\n\n9.2.1 Prétraitement des données\nIl arrive souvent ques les données brutes ne soient pas exprimées de manière appropriée ou optimale pour l’analyse statistique ou la modélisation. Vous devrez alors effectuer un prétraitement sur ces données. Lors du chapitre 7, nous avons abordé la mise à l’échelle, où des variables numériques étaient transformées pour avoir une moyenne de zéro et un écart-type de 1. Cette opération permettait d’apprécier les coefficients et leur incertitude sur une même échelle. L’encodage catégorielle a quant à lui permi d’utiliser des méthodes quantitatives sur des données qualitatives. Dans les deux cas, nous n’avons pas utilisé le terme, mais il s’agissait d’un prétraitement, c’est-à-dire une transformation des données préalable à l’analyse ou la modélisation.\nUn prétraitement peut consister simplement en une transformation logarithmique ou exponentielle. Nous verrons les transformations les plus communes comme la standardisation, la mise à l’échelle sur une étendue et la normalisation. Puis nous verrons comment ces opérations de prétraitement sont offertes dans le module recipes.\nrecipes n’est pas en mesure d’effectuer toutes les transformations imaginables. Pour des opérations plus spécialisées, si vos données forment une partie d’un tout (exprimées en pourcentages ou fractions), vous devriez probablement utiliser un prétraitement grâce aux outils de l’analyse compositionnelle. Avant de les aborder, nous allons traiter des transformations de base.\n\n9.2.1.1 Standardisation\nLa standardisation consiste à centrer vos données à une moyenne de 0 et à les échelonner à une variance de 1, c’est-à-dire\n\\[x_{standard} = \\frac{x - \\bar{x}}{\\sigma}\\]\noù \\(\\bar{x}\\) est la moyenne du vecteur \\(x\\) et où \\(\\sigma\\) est son écart-type.\nCe prétraitement des données peut s’avérér utile lorsque la modélisation tient compte de l’échelle de vos mesures (par exemple, les paramètres de régression vus au chapitre 7 ou les distances que nous verrons au chapitre 10). En effet, les pentes d’une régression linéaire multiple ne pourront être comparées entre elles que si elles sont une même échelle. Par exemple, on veut modéliser la consommation en miles au gallon (mpg) de voitures en fonction de leur puissance (hp), le temps en secondes pour parcourir un quart de mile (qsec) et le nombre de cylindre.\n\ndata(\"mtcars\")\nmodl &lt;- lm(mpg ~ hp + qsec + cyl, mtcars)\nsummary(modl)\n\n\nCall:\nlm(formula = mpg ~ hp + qsec + cyl, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3223 -1.9483 -0.5656  1.5452  7.7773 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 55.30540    9.03697   6.120 1.33e-06 ***\nhp          -0.03552    0.01622  -2.190  0.03700 *  \nqsec        -0.89424    0.42755  -2.092  0.04567 *  \ncyl         -2.26960    0.54505  -4.164  0.00027 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.003 on 28 degrees of freedom\nMultiple R-squared:  0.7757,    Adjusted R-squared:  0.7517 \nF-statistic: 32.29 on 3 and 28 DF,  p-value: 3.135e-09\n\n\nLes pentes signifient que la distance parcourue par gallon d’essence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. L’interprétation est conviviale à cette échelle. Mais lequel de ces effets est le plus important? L t value indique que ce seraient les cylindres. Mais pour juger l’importance en terme de pente, il vaudrait mieux standardiser.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nstandardise &lt;- function(x) (x-mean(x))/sd(x)\nmtcars_sc &lt;- mtcars %&gt;%\n  mutate_if(is.numeric, standardise) # ou bien scale(mtcars, center = TRUE, scale = TRUE)\nmodl_sc &lt;- lm(mpg ~ hp + qsec + cyl, mtcars_sc)\nsummary(modl_sc)\n\n\nCall:\nlm(formula = mpg ~ hp + qsec + cyl, data = mtcars_sc)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.71716 -0.32326 -0.09384  0.25639  1.29042 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.061e-16  8.808e-02   0.000  1.00000    \nhp          -4.041e-01  1.845e-01  -2.190  0.03700 *  \nqsec        -2.651e-01  1.268e-01  -2.092  0.04567 *  \ncyl         -6.725e-01  1.615e-01  -4.164  0.00027 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4983 on 28 degrees of freedom\nMultiple R-squared:  0.7757,    Adjusted R-squared:  0.7517 \nF-statistic: 32.29 on 3 and 28 DF,  p-value: 3.135e-09\n\n\nLes valeurs des pentes ne peuvent plus être interprétées directement, mais peuvent maintenant être comparées entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile.\nLes algorithmes basés sur des distances auront, de même, avantage à être standardisés.\n\n9.2.1.2 À l’échelle de la plage\nSi vous désirez préserver le zéro dans le cas de données positives ou plus généralement vous voulez que vos données prétraitées soient positives, vous pouvez les transformer à l’échelle de la plage, c’est-à-dire les forcer à s’étaler de 0 à 1:\n\\[ x_{range01} = \\frac{x - x_{min}}{x_{max} - x_{min}}  \\]\nCette transformation est sensible aux valeurs aberrantes, et une fois le vecteur transformé les valeurs aberrantes seront toutefois plus difficiles à détecter.\n\nrange_01 &lt;- function(x) (x-min(x))/(max(x) - min(x))\nmtcars %&gt;%\n  mutate_if(is.numeric, range_01) %&gt;% # en fait, toutes les colonnes sont numériques, alors mutate_all aurait pu être utilisé au lieu de mutate_if\n  sample_n(4)\n\n                    mpg cyl       disp        hp      drat         wt      qsec\nMerc 230      0.5276596 0.0 0.17385882 0.1519435 0.5345622 0.41856303 1.0000000\nMazda RX4 Wag 0.4510638 0.5 0.22175106 0.2049470 0.5253456 0.34824853 0.3000000\nVolvo 142E    0.4680851 0.0 0.12446994 0.2014134 0.6221198 0.32395807 0.4880952\nHonda Civic   0.8510638 0.0 0.01147418 0.0000000 1.0000000 0.02608029 0.4785714\n              vs am gear      carb\nMerc 230       1  0  0.5 0.1428571\nMazda RX4 Wag  0  1  0.5 0.4285714\nVolvo 142E     1  1  0.5 0.1428571\nHonda Civic    1  1  0.5 0.1428571\n\n\n\n9.2.1.3 Normaliser\nLe terme normaliser est associé à des opérations différentes dans la littérature. Nous prendrons la nomenclature de scikit-learn, pour qui la normalisation consiste à faire en sorte que la longueur du vecteur (sa norme, d’où normaliser) soit unitaire. Cette opération est le plus souvent utilisée par observation (ligne), non pas par variable (colonne). Il existe plusieurs manières de mesurer la distance d’un vecteur, mais la plus commune est la distance euclidienne. La seule fois que j’ai eu à utiliser ce prétraitement était en analyse spectrale (Chemometrics with R, Ron Wehrens, 2011, chapitre 3.5). En R,\n\nlibrary(\"pls\")\n\n\nAttaching package: 'pls'\n\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\ndata(\"gasoline\")\nspectro &lt;- gasoline$NIR %&gt;% unclass() %&gt;% as_tibble()\n\nnormalise &lt;- function(x) x/sqrt(sum(x^2))\nspectro_norm &lt;- spectro %&gt;% \n  rowwise() %&gt;% # différentes approches possibles pour les opérations sur les lignes\n  normalise()\nspectro_norm[1:4, 1:4]\n\n         900 nm        902 nm        904 nm        906 nm\n1 -0.0011224834 -0.0010265446 -0.0009434425 -0.0008314021\n2 -0.0009890637 -0.0008856332 -0.0007977676 -0.0006912734\n3 -0.0010481029 -0.0009227116 -0.0008269742 -0.0007035061\n4 -0.0010444801 -0.0009446277 -0.0008623530 -0.0007718261\n\n\n\n9.2.1.4 Le module recipes\n\nNous avons vu comment standardiser avec notre propre fonction. Certaines personnes préfèrent utiliser la fonction scale(). Mais une nouvelle approche est en train de s’installer, avec le module recipes, un module de l’ombrelle tidymodels, un méta module en développement visant à faire de R un outil de modélisation plus convivial.\nrecipes fonctionne en mode tidyverse, c’est-à-dire en suites d’opérations. De nombreuses fonctions sont offertes, dont des fonctions d’imputation, que nous verrons au chapitre @ref(chapitre-outliers). Nous couvrirons ici la standardisation et la mise à l’échelle, juste pour l’apéro 🍳.\nLe module ne s’appelle pas recette pour rien. Il fonctionne en trois étapes:\n\nMonter la liste des ingrédients: spécifier ce qu’il faut faire\nMélanger les ingrédients: transformer tout ce qu’il faut faire en une procédure\nCuire les ingrédients: appliquer la procédure à un tableau.\n\nVoici une petite application sur le tableau lasrosas.corn.\n\nlibrary(\"tidymodels\")\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5      ✔ rsample      1.2.0 \n✔ dials        1.2.1      ✔ tune         1.1.2 \n✔ infer        1.0.6      ✔ workflows    1.1.4 \n✔ modeldata    1.3.0      ✔ workflowsets 1.0.1 \n✔ parsnip      1.2.0      ✔ yardstick    1.3.0 \n✔ recipes      1.0.10     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\ndata(lasrosas.corn, package = \"agridat\")\nlasrosas.corn %&gt;% \n  head()\n\n  year       lat      long yield nitro topo     bv rep nf\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5\n\n\nDisons que pour mon modèle statistique, ma variable de sortie est le rendement (yield), que je désire lier à la dose d’azote (nitro), à un indicateur de la teneur en matière organique du sol (bv) et à la topographie (topo).\nMais pour rendre le modèle prédictif (et non pas seulement descriptif), je dois l’évaluer sur des données qui n’ont pas servies à lisser le modèle (nous verrons en plus de détails ça au chapitre 15). Je vais donc séparer mon tableau au hasard en un tableau d’entraînement comprenant 70% des observations et un autre pour tester le modèle comprenant le 30% restant.\n\ntrain_test_split &lt;- lasrosas.corn %&gt;% \n  select(yield, nitro, bv, topo) %&gt;% \n  initial_split(prop = 0.7)\ntrain_df &lt;- training(train_test_split)\ntest_df &lt;- testing(train_test_split)\n\nVoici ma recette. Je l’expliquerai tout de suite après.\n\nrecette &lt;- recipe(yield ~ ., data = train_df) %&gt;% \n  step_zv(all_numeric()) %&gt;% \n  step_normalize(all_numeric(), -all_outcomes()) %&gt;% \n  # step_downsample(topo) %&gt;%\n  step_dummy(topo) %&gt;% \n  prep()\n\nLa recette peut se baser sur l’interface formule - souvenez-vous que le . signifie “toutes les autres variables du tableau”. Elle se base toujours sur le jeu d’entraînement. La prochaine étape (souvenez-vous que %&gt;% signifie “puis” ou “ensuite”) consiste à retirer les variables dont la variance est non-nulle, ce qui est pratique pour éviter que la standardisation divise par \\(0\\). Cette étape est appliquée à toutes les variables numériques all_numeric(). Ensuite, je standardise avec la fonction step_normalize() - il y a beaucoup de confusion entre la notion de standardisation et de normalisation dans les fonctions comme dans la littérature. Dans cette fonction, je spécifie que la standardisation n’est applicable que sur les entrées numériques du modèle (all_numeric(), -all_outcomes()). L’étape step_downsample() retire des obsservations pour faire en sorte que les catégories d’une variable apparaissent toutes en même nombre. Bien que l’interface-formule de R s’en occupe automatiquement, je spécifie ensuite que je désire que la variable topo subisse un encodage catégoriel. Puis je mélange mes ingrédients avec la fonction prep(). J’obtiens un objet de type recette.\n\nrecette\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Training information \n\n\nTraining data contained 2410 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Zero variance filter removed: &lt;none&gt; | Trained\n\n\n• Centering and scaling for: nitro and bv | Trained\n\n\n• Dummy variables from: topo | Trained\n\n\nLa recette étant bien mélangée, on peut en extraire le jus avec la fonction bake(), qui permet de générer le tableau transformé.\n\ntest_proc &lt;- bake(recette, test_df)\ntest_proc %&gt;% sample_n(5)\n\n# A tibble: 5 × 6\n    nitro     bv yield topo_HT topo_LO topo_W\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 -0.326   0.432  46.1       1       0      0\n2  0.250  -0.698 104         0       1      0\n3  0.0319 -0.203  57.6       0       0      1\n4  0.960   1.24   60.7       1       0      0\n5  1.39    0.555  56.6       0       0      1\n\n\nLa fonction bake() peut aussi être appliquée au données d’entraînement, mais certaines étapes de recette doivent passer par des opérations particulières, comme step_downsample() Il est donc préférable, pour les données d’entrâinement, d’en extraire le jus avec la fonction juice().\n\ntrain_proc &lt;- bake(recette, train_df)\ntrain_proc %&gt;% sample_n(5)\n\n# A tibble: 5 × 6\n   nitro     bv yield topo_HT topo_LO topo_W\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1  0.250 -0.296  94.6       0       1      0\n2 -0.827  0.600  59.2       0       0      0\n3  0.960 -1.84   72.1       0       1      0\n4  0.960  0.813  74.3       0       1      0\n5 -0.270 -0.275  68.1       0       0      0\n\n\nLe tableau train_proc peut être envoyé dans un modèle de votre choix! Par exemple,\n\nlm(yield ~ ., train_proc) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = yield ~ ., data = train_proc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.888 -11.133  -2.347  10.705  41.205 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  77.8903     0.6199 125.654  &lt; 2e-16 ***\nnitro         2.4698     0.2842   8.689  &lt; 2e-16 ***\nbv           -5.4303     0.3568 -15.221  &lt; 2e-16 ***\ntopo_HT     -23.8887     0.9537 -25.048  &lt; 2e-16 ***\ntopo_LO       3.0660     0.8631   3.553 0.000389 ***\ntopo_W      -10.4725     0.8029 -13.043  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.95 on 2404 degrees of freedom\nMultiple R-squared:  0.5116,    Adjusted R-squared:  0.5106 \nF-statistic: 503.7 on 5 and 2404 DF,  p-value: &lt; 2.2e-16\n\n\n\n9.2.1.5 Analyse compositionnelle en R\nEn 1898, le statisticien Karl Pearson nota que des corrélations étaient induites lorsque l’on effectuait des ratios par rapport à une variable commune.\n Source Karl Pearson, 1897. Mathematical contributions to the theory of evolution.—on a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of London\nFaisons l’exercice! Nous générons au hasard 1000 données (comme le proposait Pearson) pour trois dimensions: le fémur, le tibia et l’humérus. Ces dimensions ne sont pas générées par des distributions corrélées.\n\nset.seed(3570536)\nn &lt;- 1000\nbones &lt;- tibble(femur = rnorm(n, 10, 3),\n                tibia = rnorm(n, 8, 2),\n                humerus = rnorm(n, 6, 2))\nplot(bones)\n\n\n\ncor(bones)\n\n               femur        tibia      humerus\nfemur    1.000000000 -0.069006171  0.002652292\ntibia   -0.069006171  1.000000000 -0.008994704\nhumerus  0.002652292 -0.008994704  1.000000000\n\n\nPourtant, si j’utilise des ratios allométriques avec l’humérus comme base,\n\nbones_r &lt;- bones %&gt;% \n  transmute(fh = femur/humerus,\n            th = tibia/humerus)\nplot(bones_r)\ntext(30, 20, paste(\"corrélation =\", round(cor(bones_r$fh, bones_r$th), 2)), col = \"blue\")\n\n\n\n\nNous avons induit ce que Pearson appelait une fausse corrélation (spurious correlation). En 1960, Chayes proposa que de telles fausses corrélations sont induites non seulement sur des ratios de valeurs absolues, mais aussi sur des ratios d’une somme totale. Par exemple, dans une composition simple de deux types d’utilisation du territoire, si une proportion augmente, l’autre doit nécessairement diminuer.\n\nn &lt;- 100\ntibble(A = runif(n, 0, 1)) %&gt;% \n  mutate(B = 1 - A) %&gt;% \n  ggplot(aes(x=A, y=B)) +\n  geom_point()\n\n\n\n\nLes variables exprimées relativement à une somme totale sont dites compositionnelles. Elles possèdent les caractéristiques suivantes.\n\n\nRedondance d’information. Un système de deux proportions ne contient qu’une seule variable du fait que l’on puisse déduire l’une en soutrayant l’autre de la somme totale. Un vecteur compositionnel contient de l’information redondante. Pourtant, effectuer des statistiques sur l’une plutôt que sur l’autre donnera des résultats différents.\n\nDépendance d’échelle. Les statistiques devraient être indépendantes de la somme totale utilisée. Pourtant, elles différeront sur l’on utilise par exemple, une proportion des mâles d’une part et des femelles d’autre part, ou la proportion de la somme des deux, de même que les résultats d’un test sanguin différera si l’on utilise une base sèche ou une base humide.\n\nDistribution théorique des données. Étant donnée que les proportions sont confinées entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui s’étend de -∞ à +∞) n’est souvent pas appropriée. On pourra utiliser la distribution de Dirichlet ou la distribution logitique-normale, mais d’autres approches sont souvent plus pratiques.\n\nPour illustrer l’effet de la distribution, voyons un diagramme ternaire incluant le sable, le limon et l’argile. En utilisant des écart-types univariés, nous obtenons l’ellipse en rouge, qui non seulement représente peu l’étalement des données, mais elle dépasse les bornes du triangle, admettant ainsi des proportions négatives. En bleu, la distribution logistique normale (issue des méthodes présentées plus loin dans cette section) convient davantage.\n\nLes conséquences d’effectuer des statistiques linéaires sur des données compositionnelles brutes peuvent être majeures. En outre, Pawlowksy-Glahn et Egozcue (2006), s’appuyant en outre sur Rock (1988), note les problèmes suivants (exprimés en mes mots).\n\nles régressions, les regroupements et les analyses en composantes principales peuvent avoir peu ou pas de signification\nles propriétés des distributions peuvent être générées par l’opération de fermeture de la composition (s’assurer que le total des proportions donne 100%)\nles résultats d’analyses discriminantes linéaires sont propices à être illusoires\ntous les coefficients de corrélation seront affectés à des degrés inconnus\nles résultats des tests d’hypothèses seront intrinsèquement faussés\n\nPour contourner ces problèmes, il faut d’abord aborder les données compositionnelles pour ce qu’elles sont: des données intrinsèquement multivariées. Elles sont un nuage de point, et non pas une collection de variables individuelles. Ceci qui n’empêche pas d’effectuer des analyses consciencieusement sous des angles particuliers.\nEn R, on pourra aisément rapporter une composition en somme unitaire grâce à la fonction apply. Mais auparavant, chargeons le module compositions (n’oubliez pas de l’installer au préalable) pour accéder à des données fictives de proportions de sable, limon et argile dans des sédiments.\n\nlibrary(\"compositions\")\ndata(\"ArcticLake\")\nArcticLake &lt;- ArcticLake %&gt;% as_tibble()\nhead(ArcticLake)\n\n# A tibble: 6 × 4\n   sand  silt  clay depth\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  77.5  19.5   3    10.4\n2  71.9  24.9   3.2  11.7\n3  50.7  36.1  13.2  12.8\n4  52.2  40.9   6.6  13  \n5  70    26.5   3.5  15.7\n6  66.5  32.2   1.3  16.3\n\n\n\ncomp &lt;- ArcticLake %&gt;%\n  select(-depth) %&gt;%\n  apply(., 1, function(x) x/sum(x)) %&gt;% \n  t()\ncomp[1:5, ]\n\n          sand      silt      clay\n[1,] 0.7750000 0.1950000 0.0300000\n[2,] 0.7190000 0.2490000 0.0320000\n[3,] 0.5070000 0.3610000 0.1320000\n[4,] 0.5235707 0.4102307 0.0661986\n[5,] 0.7000000 0.2650000 0.0350000\n\n\nOn pourra aussi utiliser la fonction acomp (pour Aitchison-composition) pour fermer la composition à une somme de 1.\n\ncomp &lt;- ArcticLake %&gt;%\n  select(-depth) %&gt;%\n  acomp(.)\ncomp[1:5, ]\n\n     sand        silt        clay       \n[1,] \"0.7750000\" \"0.1950000\" \"0.0300000\"\n[2,] \"0.7190000\" \"0.2490000\" \"0.0320000\"\n[3,] \"0.5070000\" \"0.3610000\" \"0.1320000\"\n[4,] \"0.5235707\" \"0.4102307\" \"0.0661986\"\n[5,] \"0.7000000\" \"0.2650000\" \"0.0350000\"\nattr(,\"class\")\n[1] \"acomp\"\n\n\nCette stratégie a pour avantage d’attribuer à la variable comp la classe acomp, qui automatise les opérations dans l’espace compositionnel (que l’on nomme aussi le simplex). La représentation ternaire est souvent utilisée pour présenter des compositions. Toutefois, il est difficile d’interpréter les compositions de plus de trois parties. La classe acomp automatise aussi la représentation teranaire.\n\nplot(comp)\n\n\n\n\nAfin de transposer cet espace clôt en un espace ouvert, on pourra diviser chaque proportion par une proportion de référence choisie parmi n’importe quelle proportion. Du coup, on retire une dimension redondante! Dans ce ratio, on choisit d’utiliser la proportion de référence au dénominateur, ce qui est arbitraire. En utilisant le log du ratio, l’inverse du ratio ne sera qu’un changement de signe, ce qui est pratique en statistiques linéaries. Cette solution, proposée par Aitchison (1986), s’applique non seulement sur les compositions à deux composantes, mais sur toute composition. Il s’agit alors d’utiliser une composition de référence pour effecteur les ratios. Pour une composition de \\(A\\), \\(B\\), \\(C\\), \\(D\\) et \\(E\\):\n\\[alr_A = log \\left( \\frac{A}{E} \\right), alr_B = log \\left( \\frac{B}{E} \\right), alr_C = log \\left( \\frac{C}{E} \\right), alr_D = log \\left( \\frac{D}{E} \\right)\\]\nDans R, la colonne de référence est par défaut la dernière colonne de la matrice des compositions.\n\nadd_lr &lt;- alr(comp)\n\nCette dernière stratégie se nomme les log-ratios aditifs (\\(alr\\) pour additive log-ratio). Bien que valide pour effectuer des tests statistiques, cette stratégie a le désavantage de dépendre de la décision arbitraire de la composante à utiliser au numérateur. Deuxième restriction des alr: les axes de l’espace des alr n’étant pas orthogonaux, ils ne peuvent pas être utilisés pour effectuer des statistiques basées sur les distances (que nous couvrirons au chapitre 10).\nL’autre stratégie proposée par Aitchison était d’effectuer un log-ratio entre chaque composante et la moyenne géométrique de toutes les composantes. Cette transformation se nomme le log-ratio centré (\\(clr\\), pour centered log-ratio)\n\\[clr_i = log \\left( \\frac{x_i}{g \\left( x \\right)} \\right)\\]\nEn R,\n\ncen_lr &lt;- clr(comp)\n\nAvec des CLRs, les distances sont valides. Mais… nous restons avec le problème de la redondance d’information. En fait, la somme de chacunes des lignes d’une matrice de clr est de 0. Pas très pratique lorsque l’on effectue des statistiques incluant une inversion de la matrice de covariance (distance de Mahalanobis, géostatistiques, etc.)\ncen_lr %&gt;% \n  cov() %&gt;% \n  solve()\n Error in solve.default(.) : le système est numériquement singulier : conditionnement de la réciproque = 4.44407e-17\nEnfin, une autre méthode de transformation développée par Egoscue et al. (2003), les log-ratios isométriques (ou isometric log-ratios, ilr) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonormées. Ces dimensions doivent doivent être préalablement établie dans un dendrogramme de bifurcation, où chaque composante ou groupe de composante est successivement divisé en deux embranchement. La manière d’arranger ces balances importe peu, mais on aura avantage à créer des balances interprétables.\nLe diagramme de balances peut être encodé dans une partition binaire séquentielle (ou sequential bianry partition, sbp). Une sbp est une matrice de contraste ou chaque ligne représente une partition entre deux variables ou groupes de variables. Une composante étiquettée +1 correspondra au groupe du numérateur, une composante étiquettée -1 au dénominateur et une composante étiquettée 0 sera exclue de la partition (Parent et al., 2013). J’ai reformulé la fonction CoDaDendrogram pour que l’on puisse ajouter des informations intéressantes sur les balants horizontaux. Cette fonction est disponible sur github.\n\nsource(\"https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R\")\n\nsbp &lt;- matrix(c(1, 1,-1,\n                1,-1, 0),\n              byrow = TRUE,\n              ncol = 3)\n\nCoDaDendrogram2(comp, V = gsi.buildilrBase(t(sbp)), ylim = c(0, 1),\n                equal.height = TRUE)\n\n\n\n\nSi la SBP est plus imposante, il pourrait être plus aisé de monter dans un chiffrier, puis de l’importer dans R via un fichier csv.\nLe calcul des ILRs est effectué comme suit.\n\\[ilr_j = \\sqrt{\\frac{n_j^+ n_j^-}{n_j^+ + n_j^-}} log \\left( \\frac{g \\left( c_j^+ \\right)}{g \\left( c_j^+ \\right)} \\right)\\]\nou, à la ligne \\(j\\) de la SBP, \\(n_j^+\\) et \\(n_j^-\\) sont respectivement le nombre de composantes au numérateur et au dénominateur, \\(g \\left( c_j^+ \\right)\\) est la moyenne géométrique des composantes au numérateur et \\(g \\left( c_j^- \\right)\\) est la moyenne géométrique des composantes au dénominateur.\nLes balances sont conventionnellement notées [A,B | C,D], ou les composantes A et B au dénominateur sont balancées avec les composantes C and D au numérateur. Une balance positive signifie que la moyenne géométrique des concentrations au numérateur est supérieur à celle au dénominateur, et inversement, alors qu’une balance nulle signifie que les moyennes géométriques sont égales (équilibre). Ainsi, en modélisation linéaire, un coefficient positif sur [A,B | C,D] signifie que l’augmentation de l’importance de C et D comparativement à A et B est associé à une augmentation de la variable réponse du modèle.\nEn R,\n\niso_lr &lt;- ilr(comp, V = gsi.buildilrBase(t(sbp)))\n\nNotez la forme gsi.buildilrBase(t(sbp)) est une opération pour obtenir la matrice d’orthonormalité à partir de la SBP.\nLes ILRs sont des balances multivariées sur lesquelles on pourra effectuer des statistiques linéaries. Bien que l’interprétation des résultats comme collection d’interprétations sur des balances univariées pourra être affectée par la structure de la SBP, ni les statistiques linéaires multivariées, ni la distance entre les points ne seront affectés. En effet, chaque variante de la SBP est une rotation (d’un facteur de 60°) par rapport à l’origine:\n\nsource(\"lib/ilr-rotation-sbp.R\")\n\n\nPour les transformations inverses, vous pourrez utiliser les fonctions alrInv, clrInv et ilrInv. Dans tous les cas, si vous tenez à garder la trace de vos données dans leur format original, vous aurez avantage à ajouter à votre vecteur compositionnel la valeur de remplissage, constitué d’un amalgame des composantes non mesurées. Par exemple,\n\npourc &lt;- c(N = 0.03, P = 0.001, K = 0.01)\nacomp(pourc) # vous perdez la trace des proportions originales\n\n           N            P            K \n\"0.73170732\" \"0.02439024\" \"0.24390244\" \nattr(,\"class\")\n[1] \"acomp\"\n\n\n\npourc &lt;- c(N = 0.03, P = 0.001, K = 0.01)\nFv &lt;- 1 - sum(pourc)\ncomp &lt;- acomp(c(pourc, Fv = Fv))\ncomp\n\n      N       P       K      Fv \n\"0.030\" \"0.001\" \"0.010\" \"0.959\" \nattr(,\"class\")\n[1] \"acomp\"\n\n\n\niso_lr &lt;- ilr(comp) # avec une sbp par défaut\nilrInv(iso_lr)\n\n      N       P       K      Fv \n\"0.030\" \"0.001\" \"0.010\" \"0.959\" \nattr(,\"class\")\n[1] \"acomp\"\n\n\nSi vos données font partie d’un tout, je vous recommande chaudement d’utiliser des méthodes compositionnelles autant pour l’analyse que la modélisation. Pour en savoir davantage, le livre Compositional data analysis with R, de van den Boogart et Tolosana-Delgado, est disponible en format électronique à la bibliothèque de l’Université Laval.\nPour aller plus loin, j’ai écri un billet à ce sujet (auquel à ce jour il manque toujours un cas d’étude): We should use balances and machine learning to diagnose ionomes.\n\n9.2.2 Acquérir des données météo\nUne tâche commune en écologie est de lier des observations à la météo… qui sont rarement collectés lors d’expériences. Environnement Canada possède son réseau de stations. Les données sont disponibles sur internet en libre accès. Vous pouvez chercher des stations, effectuer des requêtes et télécharger des fichiers csv. Pour un petit tableau, la tâche est plutôt triviale. Mais ça devient rapidement laborieux à mesure que l’on doit rechercher de nombreuses données.\nLe module weathercan, développé par Steffi LaZerte, permet d’effectuer des requêtes rapidement à partir des coordonnées de votre site expérimental. Par exemple, si je cherche une station météo fournissant des données horaires située à moins de 20 km du sommet du Mont-Bellevue, à Sherbrooke, aux coordonnées [latitude 45.35, longitude -71.90] :\n\nlibrary(\"weathercan\")\nstation_site &lt;- stations_search(coords = c(45.35, -71.90), dist = 20, interval = \"hour\")\n\nThe stations data frame hasn't been updated in over 4 weeks. Consider running `stations_dl()` to check for updates and make sure you have the most recent stations list available\nThe stations data frame hasn't been updated in over 4 weeks. Consider running `stations_dl()` to check for updates and make sure you have the most recent stations list available\n\nstation_site\n\n# A tibble: 4 × 17\n  prov  station_name station_id climate_id WMO_id TC_id   lat   lon  elev tz    \n  &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n1 QC    LENNOXVILLE        5397 7024280     71611 WQH    45.4 -71.8  181  Etc/G…\n2 QC    SHERBROOKE        48371 7028123     71610 YSC    45.4 -71.7  241. Etc/G…\n3 QC    SHERBROOKE A       5530 7028124     71610 YSC    45.4 -71.7  241. Etc/G…\n4 QC    SHERBROOKE A      30171 7028126        NA GSC    45.4 -71.7  241. Etc/G…\n# ℹ 7 more variables: interval &lt;chr&gt;, start &lt;dbl&gt;, end &lt;dbl&gt;, normals &lt;lgl&gt;,\n#   normals_1981_2010 &lt;lgl&gt;, normals_1971_2000 &lt;lgl&gt;, distance &lt;dbl&gt;\n\n\nJe prends en note l’identifiant de la station désirée (ou des stations, disons 5397 et 48371), puis je lance une requête pour obtenir la météo horaire entre les dates désirées.\n\nmont_bellevue &lt;- weather_dl(station_ids = c(5397, 48371),\n                            start = \"2024-02-16\",\n                            end = \"2024-02-23\",\n                            interval = \"hour\",\n                            verbose = TRUE, \n                            time_disp = \"local\")\n\nThe stations data frame hasn't been updated in over 4 weeks. Consider running `stations_dl()` to check for updates and make sure you have the most recent stations list available\n\n\nGetting station: 5397\n\n\nFormatting station data: 5397\n\n\nAdding header data: 5397\n\n\nGetting station: 48371\n\n\nFormatting station data: 48371\n\n\nAdding header data: 48371\n\n\nTrimming missing values before and after\n\n\nAs of weathercan v0.3.0 time display is either local time or UTC\nSee Details under ?weather_dl for more information.\nThis message is shown once per session\n\nmont_bellevue |&gt; head(5)\n\n# A tibble: 5 × 37\n  station_name station_id station_operator prov    lat   lon  elev climate_id\n  &lt;chr&gt;             &lt;dbl&gt; &lt;lgl&gt;            &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n1 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n2 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n3 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n4 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n5 LENNOXVILLE        5397 NA               QC     45.4 -71.8   181 7024280   \n# ℹ 29 more variables: WMO_id &lt;chr&gt;, TC_id &lt;chr&gt;, date &lt;date&gt;, time &lt;dttm&gt;,\n#   year &lt;chr&gt;, month &lt;chr&gt;, day &lt;chr&gt;, hour &lt;chr&gt;, weather &lt;chr&gt;, hmdx &lt;dbl&gt;,\n#   hmdx_flag &lt;chr&gt;, precip_amt &lt;dbl&gt;, precip_amt_flag &lt;chr&gt;, pressure &lt;dbl&gt;,\n#   pressure_flag &lt;chr&gt;, rel_hum &lt;dbl&gt;, rel_hum_flag &lt;chr&gt;, temp &lt;dbl&gt;,\n#   temp_dew &lt;dbl&gt;, temp_dew_flag &lt;chr&gt;, temp_flag &lt;chr&gt;, visib &lt;dbl&gt;,\n#   visib_flag &lt;chr&gt;, wind_chill &lt;dbl&gt;, wind_chill_flag &lt;chr&gt;, wind_dir &lt;dbl&gt;,\n#   wind_dir_flag &lt;chr&gt;, wind_spd &lt;dbl&gt;, wind_spd_flag &lt;chr&gt;\n\n\nEt voilà.\n\nmont_bellevue |&gt;  \n  ggplot(aes(x = time, y = temp)) +\n  geom_line(aes(colour = station_name))\n\n\n\n\n\n9.2.3 Pédométrie avec R\nCette section a été écrite par Michael Leblanc. Je n’y ai appliqué que quelques retouches esthétiques.\nPlusieurs fonctionnalités ont été développées sur R afin d’aider les pédométriciens à visualiser, explorer et traiter les données numériques en science des sols. Voici quelques exemples.\n\n9.2.3.1 Texture du sol\nLa texture du sol est définie par sa composition granulométrique, habituellement représentée par trois fractions (sable, limon, argile), laquelle peut être généralisée en classe texturale. La définition des classes texturales diffère d’un système ou d’un pays à l’autre comme en témoigne l’article Perdus dans le triangle des textures (Richer de Forges et al. 2008). La définition des fractions granulométriques peut également différer selon le domaine d’étude (ingénierie, pédologie) ou le pays. Par exemple, le diamètre du limon est de 0,002 mm à 0,05 mm dans le système canadien, américain et français alors qu’il est de 0,002 mm à 0,02 mm dans le système australien et de 0,002 mm à 0,063 mm dans le système allemand. Il est donc important de vérifier la méthodologie et le système de classification utilisés pour interpréter les données de texture du sol. Le module soilTexture propose des fonctions permettant d’aborder ces multiples définitions.\n\nlibrary(\"soiltexture\")\n\n\n9.2.3.1.1 Les triangles texturaux\nAvec la fonction TT.plot, vous pouvez présenter vos données granulométriques dans un triangle textural tel que défini par les différents systèmes nationaux. Auparavant, créons un objet comprenant des textures aléatoires.\n\nset.seed(848341) # random.org\nrand_text &lt;- TT.dataset(n=100, seed.val=29)\nhead(rand_text)\n\n       CLAY     SILT      SAND          Z\n1 54.650857 40.37101  4.978129 13.2477582\n2 44.745954 40.81782 14.436221 20.8433109\n3 18.192509 48.26752 33.539970  7.1814626\n4 17.750492 40.14405 42.105458 -0.2077358\n5 65.518360 23.36110 11.120538 10.8656027\n6  6.610293 22.45353 70.936173  3.7108567\n\n\nAvec le module soiltexture, les tableaux de texture doivent inclure les intitullés exactes CLAY, SILT et SAND (notez les majuscules). Les points des textures générées peuvent être portés dans des diagrammes ternaires texturaux de différents systèmes de classification, par exemple le système canadioen et le système USDA.\n\npar(mfrow=c(1, 2))\n\nTT.plot(class.sys = \"CA.FR.TT\", \n        tri.data = rand_text,\n        col = \"blue\")\nTT.plot(class.sys = \"USDA.TT\", \n        tri.data = rand_text,\n        col = \"blue\")\n\n\n\n\nLes paramètres de la figure (titres, polices, style de la grille, etc.) peuvent être personnalisés avec les arguments TT.plot.\n\n9.2.3.1.2 Les classes texturales\nLa fonction TT.points.in.classes est utile pour désigner la classe texturale à partir des données granulométriques, en spécifiant bien le système de classification désiré.\n\nTT.points.in.classes(\n  tri.data = rand_text[1:10, ], # \n  class.sys = \"CA.FR.TT\",\n  PiC.type = \"t\"\n)\n\n [1] \"ALi\" \"ALi\" \"L\"   \"L\"   \"ALo\" \"LS\"  \"ALo\" \"A\"   \"LLi\" \"LSA\"\n\n\nPlusieurs autres fonctions sont proposées par soiltexture afin de visualiser, classifier et transformer les données de texture du sol : Functions in soiltexture. Julien Moeys (2018) propose également le tutoriel The soil texture wizard: a tutorial.\n\n9.2.3.2 Profils de sols\nLe profil de sols est une entité décrite par une séquence de couches ou d’horizons avec différentes caractéristiques morphologiques. Le module AQP, pour Algorithms for Quantitative Pedology, propose des fonctions de visualisation, d’agrégation et de classification permettant d’aborder la complexité inhérente aux informations pédologiques.\n\n9.2.3.2.1 La visualisation de profils\nVous devez d’abord structurer vos données dans un tableau (data.frame) incluant minimalement ces trois colonnes :\n\nIdentifiant unique du profil (groupes d’horizons) (id)\nLimites supérieures de l’horizon (top)\nLimites inférieures de l’horizon (down)\n\nVos données morphologiques, physico-chimiques, etc., sont incluses dans les autres colonnes. Chargeons un fichier pédologique à titre d’exemple.\n\nprofils &lt;- read_csv(\"data/08_pedometric-profile.csv\")\n\nRows: 44 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): horizon, hue\ndbl (7): id, top, bottom, value, chroma, pH.CaCl2, C.CNS.pc\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(profils)\n\n# A tibble: 6 × 9\n     id horizon   top bottom hue   value chroma pH.CaCl2 C.CNS.pc\n  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     1 Ap1         0     23 10YR      2      3     4.78     2.71\n2     1 Ap2        23     34 10YR      2      2     4.74     2.2 \n3     1 Bfcj       34     46 7.5YR     4      5     4.79     2.4 \n4     1 BC         46     83 2.5Y      4      5     4.93     0.22\n5     1 C          83    100 2.5Y      5      4     4.82     0.18\n6     2 Ap          0     29 10YR      2      2     4.6      4.22\n\n\nLa fonction munsell2rgb permet de convertir le code de couleur Munsell en format RGB.\n\nlibrary(\"aqp\")\n\nThis is aqp 2.0.2\n\n\n\nAttaching package: 'aqp'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, slice\n\nprofils$soil_color &lt;- with(profils, munsell2rgb(hue, value, chroma))\n\nPréalablement à la visualisation, le tableau est transformé en objet SoilProfileCollection par la fonction depths. Pour ce faire, le tableau doit être un pur data.frame, non pas un tibble.\n\nprofils &lt;- profils %&gt;% as.data.frame()\ndepths(profils) &lt;- id ~ top + bottom\n\nLa fonction plot détectera le type d’objet et appellera la fonction de visualisation en conséquence.\n\npar(mfrow = c(1, 3))\nplot(profils, name=\"horizon\")\ntitle('Couleur des horizons', cex.main=1)\nplot(profils, name=\"horizon\", color='C.CNS.pc', col.label='C total (%)')\nplot(profils, name=\"horizon\", color='pH.CaCl2', col.label='pH CaCl2')\n\n\n\n\nDe multiples figures thématiques peuvent être générées afin de représenter les particuliarités des profils. Pour aller plus loin, consultez les guides Introduction to SoilProfileCollection Objects et Generating Sketches from SPC Objects.\n\n9.2.3.2.2 Les plans verticaux (depth functions)\nLes plans verticaux sont des diagrammes qui permettent d’interpréter les données en fonction de la profondeur. La fonction slab permet le calcul de statistiques descriptives par intervalles de profondeur réguliers, lesquelles permettent de visualiser la variabilité verticale des propriétés des sols.\n\nagg &lt;- slab(profils, fm = ~ C.CNS.pc + pH.CaCl2)\n\nLa visualisation est générée par le module graphique ggplot2\n\nagg %&gt;%\n  ggplot(mapping = aes(x = -top, y = p.q50)) +\n  facet_grid(. ~ variable, scale = \"free\") +\n  geom_ribbon(aes(ymin =  p.q25, ymax = p.q75), fill = \"grey75\", alpha = 0.5) +\n  geom_path() +\n  labs(x = \"Profondeur (cm)\",\n       y = \"Médiane bordée des 25e and 75e percentiles\") +\n  coord_flip()\n\n\n\n\n\n9.2.3.2.3 Le regroupement de profils\nLe calcul des distances de dissimilarité entre les profils avec profile_compare permet la construction de dendrogramme et le regroupement des profils. Notez que nous survolerons au chapitre 10 les concepts de dissimilarité et de partitionnement.\n\nlibrary(\"cluster\")\nlibrary(\"mvtnorm\")\nlibrary(\"sharpshootR\") \n\nRegistered S3 method overwritten by 'vegan':\n  method          from   \n  print.nullmodel parsnip\n\nlibrary(\"igraph\")\n\n\nAttaching package: 'igraph'\n\n\nThe following object is masked from 'package:compositions':\n\n    normalize\n\n\nThe following objects are masked from 'package:dials':\n\n    degree, neighbors\n\n\nThe following objects are masked from 'package:lubridate':\n\n    %--%, union\n\n\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n\n\nThe following objects are masked from 'package:purrr':\n\n    compose, simplify\n\n\nThe following object is masked from 'package:tidyr':\n\n    crossing\n\n\nThe following object is masked from 'package:tibble':\n\n    as_data_frame\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\n# remotes::install_github(\"ncss-tech/sharpshootR\")\nd &lt;- profile_compare(profils, vars=c('C.CNS.pc', 'pH.CaCl2'), k=0, max_d=40)\n\nWarning: 'profile_compare' is deprecated.\nUse 'NCSP' instead.\nSee help(\"Deprecated\")\n\n\nWarning: profile_compare() has been deprecated, please use NCSP()\n\n\nComputing dissimilarity matrices from 10 profiles\n\n\n [0.08 Mb]\n\n\nWarning: SPC / distance matrix IDs out of order, soon to be fixed (#7)\n\nd_diana &lt;- diana(d)\nplotProfileDendrogram(profils, name=\"horizon\", d_diana,\n                      scaling.factor = 0.3, y.offset = 5,\n                      color='pH.CaCl2',  col.label='pH CaCl2')\n\nprofile IDs and clustering IDs are not in the same order\n\n\n\n\n\n\n9.2.3.2.4 Diagramme de relations entre les horizons\nIl est possible de visualiser les transitions d’horizon les plus probables dans un groupe de profils de sols.\n\ntp &lt;- hzTransitionProbabilities(profils, name=\"horizon\")\n\nWarning: ties in transition probability matrix\n\npar(mar = c(0, 0, 0, 0), mfcol = c(1, 2))\nplot(profils, name=\"horizon\")\nplotSoilRelationGraph(tp, graph.mode = \"directed\", edge.arrow.size = 0.5, edge.scaling.factor = 2, vertex.label.cex = 0.75, \n                      vertex.label.family = \"sans\")\n\n\n\n\nConsultez AQP project pour des présentations, des tutoriels et des exemples de figures qui montrent les nombreuses possibilités du package AQP.\n\n9.2.4 Méta-analyses en R\nJe conseille les livres Introduction to Meta-Analysis, Meta-analysis with R et Handbook of Meta-analysis in Ecology and Evolution pour les méta-analyses sur des écosystèmes. Le module metafor est un incournable pour effectuer des métaanalyses en R. On ne passe pas tout à fait à côté si l’on utilise le module meta, lui-même basé en partie sur metafor. Le module meta a touttefois l’avantage d’être simple d’utilisation. Par exemple, pour une méta-analyse d’une réponse continue,\n\nlibrary(\"meta\")\n\nLoading required package: metadat\n\n\nLoading 'meta' package (version 7.0-0).\nType 'help(meta)' for a brief overview.\nReaders of 'Meta-Analysis with R (Use R!)' should install\nolder version of 'meta' package: https://tinyurl.com/dt4y5drs\n\n\n\nAttaching package: 'meta'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    ci\n\ndata(Fleiss1993cont) |&gt; head(5)\n\n[1] \"Fleiss1993cont\"\n\nmeta_analyse &lt;- metacont(n.e = n.psyc, mean.e = mean.psyc, sd.e = sd.psyc, \n                        n.c = n.cont, mean.c = mean.cont, sd.c = sd.cont,\n                        comb.fixed = T, comb.random = T, studlab = study,\n                        data = Fleiss1993cont, sm = \"SMD\")\nmeta_analyse\n\nNumber of studies: k = 5\nNumber of observations: o = 232 (o.e = 106, o.c = 126)\n\n                         SMD             95%-CI     z p-value\nCommon effect model  -0.3434 [-0.6068; -0.0801] -2.56  0.0106\nRandom effects model -0.3434 [-0.6068; -0.0801] -2.56  0.0106\n\nQuantifying heterogeneity:\n tau^2 = 0 [0.0000; 0.7255]; tau = 0 [0.0000; 0.8518]\n I^2 = 0.0% [0.0%; 79.2%]; H = 1.00 [1.00; 2.19]\n\nTest of heterogeneity:\n    Q d.f. p-value\n 3.68    4  0.4514\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hedges' g (bias corrected standardised mean difference; using exact formulae)\n\n\nEt pour effectuer un forest plot,\n\nforest(meta_analyse)\n\n\n\n\n\n9.2.5 Créer des applications avec R\nRStudio vous permet de déployer vos résultats sous forme d’applications web grâce à son module shiny. Pour ce faire, le seul préalable est de savoir programmer en R. En agençant une interface avec des inputs (listes de sélection, des boîtes de dialogue, des sélecteurs, des boutons, etc.) avec des modèles que vous développez, vous pourrez créer des interfaces intéractives.\nPour créer une application shiny, vous devez créer une partie pour l’interface (ui) et une autre pour le calcul (server). Je n’irai pas dans les détails, étant donnée qu’il s’agit d’un sujet à part entière. Pour aller plus loin, visitez le site du projet shiny.\nlibrary(\"shiny\")\n\nui &lt;- basicPage(\n  sliderInput(\"A\", \"Asymptote:\", min = 0, max = 100, value = 50),\n  sliderInput(\"E\", \"Environnement:\", min = -10, max = 100, value = 20),\n  sliderInput(\"R\", \"Taux:\", min = 0, max = 0.1, value = 0.035),\n  sliderInput(\"prix_dose\", \"Prix dose:\", min = 0, max = 5, value = 1),\n  sliderInput(\"prix_vente\", \"Prix vente:\", min = 0, max = 200, value = 100),\n  sliderInput(\"dose\", \"Dose:\", min = 0, max = 300, value = c(0, 200)),\n  plotOutput(\"distPlot\")\n)\n\nserver &lt;- function(input, output) {\n  mitsch_f &lt;- reactive({\n    input$A * (1 - exp(-input$R * (seq(input$dose[1], input$dose[2], length = 100) + input$E)))\n  })\n  \n  mitsch_opt &lt;- reactive({\n    (log((input$A * input$R * input$prix_vente) / input$prix_dose - input$E * input$R) / input$R )\n  })\n  \n  \n  output$distPlot &lt;- renderPlot({\n    plot(seq(input$dose[1], input$dose[2], length = 100), mitsch_f(), type = \"l\", ylim = c(0, 100))\n    abline(v = mitsch_opt() )\n    text(mitsch_opt(), 2, paste(\"Dose optimale:\", round(mitsch_opt(), 0)))\n  })\n}\n\nshinyApp(ui, server)\nUne fois l’application créée, il est possible de la déployer sur le site shninyapps.io. D’abord créer une application shiny dans RStudio: File &gt; New File &gt; Shiny Web App. Écrivez votre code dans le fichier app.R (dans ce cas, ce peut être un copier-coller), puis cliquez sur Run App en haut à droite de la fenêtre d’édition du code. Lorsque l’application fonctionne, vous pourrez la publier via RStudio en cliquant sur le bouton Publish dans la fenêtre Viewer (vous devez au préalable avoir un comte sur shinyapp.io).\nUne application sera publique et sera ouverte. https://essicolo.shinyapps.io/Mitscherlich/\nPour déployer en mode privé, vous devrez débourser pour un forfait ou installer votre propre serveur.\n\n9.2.6 Travailler en Python\nLe chapitre 8 a présenté un module pour les statistiques bayésiennes nécessitant un environnement Python. Il s’agissait de faire fonctionner un module en R qui, à l’interne, effectue ses calculs en Python. Rien ne vous empêche d’effectuer des calculs directement en Python à même l’interface de RStudio.\nNote : Nous avons aussi ajouté une section facultative d’introduction à Python au chapitre 6 si vous souhaitez aller plus loin.\nIl vous faudra d’abord installer Python et les modules de calcul que vous désirez. Il existe plusieurs distributions de Python. Parmi elles, Anaconda est probablement la plus intuitive à installer. Choisissez d’abord Anaconda (~500 Mo) ou Miniconda pour une installation minimale (~60 Mo) - si vous installez Miniconda, vous devrez aussi installer les modules nécessaires pour le calcul. Installez aussi le module reticulate de R, de sorte que vous puissiez communiquer avec Python. Anaconda fonctionne avec des environnements de calcul. Chaque environnement possède sa propre version de Python et ses propres modules: cela vous permet d’isoler vos environnements et de contrôler la version des modules. Vous pouvez connecter R à l’environnement de base créé lors de l’installation d’Anaconda, ou bien en créer un autre. Pour en créer un nouveau, incluant une liste de modules de calcul,\n#{r message=FALSE} library(\"reticulate\") conda_create(envname = \"monprojet\", packages = c(\"python\", \"numpy\", \"scipy\", \"matplotlib\", \"pandas\", \"scikit-learn\"))\nConnectez-vous à votre environnement Python, par exemple j’utilise l’environnement par défaut anaconda3.\n#{r expl-connecter-python} library(\"reticulate\") conda_list() use_condaenv(\"anaconda3\", required = TRUE)\nSupposons que vous travailliez en R markdown. Pour lancer un bloc de code en Python, indiquez python au lieu de r dans l’entête. Il faudra que les modules numpy, pandas et matplotlib soient installé dans votre environnement Python\n#{python expl-py-matplotlib} import numpy as np import pandas as pd import matplotlib.pyplot as plt a = np.linspace(0, 30, 101) b = np.sin(a) plt.plot(a, b) plt.title(\"Un graphique en Matplotlib dans RStudio\")\nPour récupérer une variable Python en R, précédez la variable de py$.\n#{r expl-importer-var-py} plot(py$a, py$b, type = \"l\", main = \"Un graphique en R avec \\n des variables définies en Python\")\nIdem, pour récupérer un objet R en Python, .\n#{python expl-importer-var-r} r.iris.head(6)\nVous aurez ainsi accès aux fonctionnalités de Python et R dans un même flux de travail. Python n’est pas si diférent de R, mais il vous faudra déployer des efforts pour approvoiser le serpent. Pour débuter en Python, je suggère Python Data Science Handbook, de Jake VanderPlas. Pour en savoir plus sur le travail en R et en Python dans RSutdio, référez-vous à la documentation du module reticulate."
  },
  {
    "objectID": "08-explorer.html#bonus-ia-générative",
    "href": "08-explorer.html#bonus-ia-générative",
    "title": "9  Explorer R",
    "section": "\n9.3 Bonus : IA Générative",
    "text": "9.3 Bonus : IA Générative\nEn 2024, les robots conversationnels font les manchettes régulièrement, et nous promettent des Superpouvoirs. En effet, l’IA générative a le potentiel d’accélérer votre flux de travail et de rédiger pour vous des sections de code. Le fonctionnement repose sur des algorithmes d’autoapprentissage : La puissance de calcul permet désormais d’entraîner les modèles avec une quantité phénoménale de documents, si bien que les modèles apprennent de nous. Ils peuvent ensuite générer du texte, des blocs de code, des images; à condition de leur poser les bonnes questions! En effet, les modèles ne sont pas capables (pour l’instant!) de créer de la nouveauté, et ils dépendent de ce qui existe déjà dans leur entraînement. La vitesse de développement de ces outils est exponentielle, si bien que dernièrement, toutes les grandes entreprises semblent vouloir créer leur propre IA.\nToutefois, les risques liés à cette technologie sont tels que la communauté scientifique appelle à une pause dans son développement, le temps de s’organiser pour la recevoir adéquatement. Les principes d’Asilomar, premiers principes de gouvernance en IA, fournissent un premier jet de la direction souhaitée par les experts dans le domaine : Loin de viser des outils non dirigés et autonomes, l’intelligence artificielle devrait être bénéfique, utile, sécuritaire, transparente et non-subversive, tout en profitant au plus grand nombre de personnes possible et au bien commun (je résume ici, mais vous pouvez consulter les principes sur le site mentionné plus haut).\nIl existe plusieurs modèles de language, et la plupart d’entre eux sont pour l’instant propriétaires, incluant le fameux ChatGPT d’OpenAI. À ce sujet, certains scientifiques suggèrent d’éviter l’attrait des modèles propriétaires et de travailler au développement de modèles opensource, le but étant notamment de favoriser une meilleure reproductibilité (les modèles propriétaires sont appelés à être modifiés sans pré-avis, ce qui rend difficile la reproduction des résultats). De plus, les modèles propriétaires ont tendance à recueillir vos données de recherche et vos données personnelles pour s’entraîner, si bien que des questions sur l’éthique et la vie privée se posent.\n\n9.3.1 Utilisation en rédaction scientifique\nL’utilisation de l’IA générative demeure controversée pour une utilisation en rédaction scientifique. En effet, en plus des questionnements éthiques et des erreurs potentielles et fréquentes, les IA génératives sont incapables de comprendre ou de générer de nouvelles informations ou d’effectuer une analyse profonde, ce qui pourrait limiter la discussion dans un article scientifique. Si vous souhaitez tout de même les utiliser, je vous suggère de respecter les recommandations suivantes (ma traduction) :\n\n\nReconnaître leur utilisation dans la section appropriée (typiquement Acknowledgments), indiquer les endroits dans le manuscrit où l’IA a été utilisée et fournir les requêtes (prompt) et les questions posées au modèle dans le matériel supplémentaire.\n\nSouvenez-vous (ainsi que vos co-auteurs) que les sorties des modèles d’IA ne représentent qu’un brouillon très préliminaire, au mieux. La sortie est incomplète, peut comporter de l’information incorrecte, et chaque phrase ainsi que chaque affirmation doit être considérée de façon critique. Vérifiez, vérifiez, et vérifiez encore. Ensuite, vérifiez une fois de plus.\n\nN’utilisez pas les sorties des modèles de façon textuelles. Il ne s’agit pas de vos mots. Le robot pourrait avoir réutilisé du texte provenant d’autre sources, ce qui mènerait à du plagiat involontaire.\n\nToutes les citations recommandées par une IA générative doivent être vérifiées dans le document original puisque les robots conversationnels sont réputés pour générer des citations erronées.\n\nN’incluez pas le robot conversationnel comme un co-auteur. Il ne peut pas générer de nouvelles idées ou rédiger une discussion à partir de nouveaux résultats, éléments qui demeurent du domaine des humains. Il ne s’agit que d’un outil, comme plusieurs autres programmes, pour aider la formulation et la rédaction de manuscrits (et j’ajouterais de codes).\n\nL’IA générative ne peut pas être tenue responsable de toute affirmation ou manquement à l’éthique. Ce sont tous les auteurs d’un manuscrit qui partagent cette responsabilité.\n\nEt le plus important, ne laissez pas les robots conversationnels écraser votre créativité et votre réflexion profonde. Utilisez les pour ouvrir vos horizons et susciter de nouvelles idées!\n\nCes recommandations sont pour l’utilisation d’un texte, mes elles s’appliquent aussi pour votre code.\n\n9.3.2 Et maintenant… Pour l’utiliser sur R?\nSous R, il existe plusieurs options, mais j’ai seulement essayé le module chattr, qui permet d’intégrer les modèles directement dans votre interface RStudio. Vous pouvez simplement suivre les étapes d’installation, puis vous créer un compte et générer une clé secrète d’activation auprès d’OpenAI si vous utilisez les modèles GPTs avec la commande Sys.setenv(\"OPENAI_API_KEY\" = \"####################\").\nJe vous conseille de modifier la version par défaut de modèle puisque GPT-4 est payant (personnellement, j’ai utilisé GPT-3.5-turbo avec la commande suivante : chattr::chattr_use(\"gpt35\")).\nUne fois installé, vous pouvez lancer chattr::chattr_app() pour lancer l’application dans l’ongler Viewer de RStudio (alternativement, vous pouvez créer un raccourci-clavier à partir de l’onglet Outils &gt; Modifier les raccourcis-claviers). Ensuite, vous effectuez vos requêtes et GPT vous répond, comme une conversation. Vous pouvez alors copier-coller les sorties, ou alors générer un script à partir de celles-ci.\nAu début, pour une utilisation simple et des opérations faciles, j’ai été agréablement surpris par les réponses rapides, épurées et bien structurées de GPT-3.5. Comme dans une conversation, je lui fournissais des requêtes (prompts) en lui disant quoi faire, puis le robot me répondait avec une explication, un bloc de code et des commentaires sur les lignes de codes. Venaient ensuite quelques ajustements, par exemple utiliser ggplot au lieu de plot pour réaliser les graphiques, ou alors ajouter des facettes et intégrer toutes les variables sous forme de boxplots.\nJ’ai ensuite décidé de passer au niveau supérieur et de faire des requêtes un peu plus complexes : Fais un modèle linéaire sur telles variables, crée un code pour effectuer tel calcul sur un ensemble de données situées dans un dossier, etc. À cette étape, GPT s’est emballé et a commencé à me fournir des codes en pigeant dans plusieurs modules différents, en mélangeant des méthodes récentes avec des méthodes plus anciennes et parfois désuettes. Les résultats étaient des blocs de codes souvent non fonctionnels, ou alors qui ne faisaient pas du tout ce que j’avais décrit, même après l’ajout de détails dans mes requêtes. Au bout d’un certain temps, j’avais atteint ma limite gratuite de Tokens (et de patience) pour le modèle GPT-3.5.\nAu final, à ce stade-ci, j’ai de la difficulté à recommander cet outil. Les algorithmes d’autoapprentissage qui seront présentés au chapitre 13 sont beaucoup plus intéressants selon moi puisque leur utilisation reste limitée à ce qu’on veut qu’ils fassent et que les dérives scientifiques me semblent moins importantes. L’IA générative a certainement sa place, mais nécessite une bonne connaissance initiale du sujet et des opérations à effectuer. Son rôle devrait être principalement de faciliter le travail de programmeurs ou de scientifiques aguerris, mais ils ne devraient jamais remplacer l’esprit critique, ni faire le travail pour vous."
  },
  {
    "objectID": "09-ordination.html",
    "href": "09-ordination.html",
    "title": "10  Association, partitionnement et ordination",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "10-imputation.html",
    "href": "10-imputation.html",
    "title": "11  Détection de valeurs aberrantes et imputation de données manquantes",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "11-series-temporelles.html",
    "href": "11-series-temporelles.html",
    "title": "12  Les séries temporelles",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "12-autoapprentissage.html",
    "href": "12-autoapprentissage.html",
    "title": "13  Introduction à l’autoapprentissage",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "13-donnees-spatiales.html",
    "href": "13-donnees-spatiales.html",
    "title": "14  Les données géospatiales",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "14-modelisation.html",
    "href": "14-modelisation.html",
    "title": "15  Modélisation de mécanismes écologiques",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "08-explorer.html",
    "href": "08-explorer.html",
    "title": "9  Explorer R",
    "section": "",
    "text": "9.1 R sur le web\nDans un environnement de travail en évolution rapide et constante, il est difficile de considérer que ses compétences sont abouties. Rester informé sur le développement de R vous permettra de trouver et de résoudre des problèmes persistants de manière plus efficace ou par de nouvelles avenues, et vous offrira même l’occasion de dénicher des problèmes dont vous ne soupçonniez pas l’existence. Plusieurs sources d’information vous permettront de vous tenir à jour sur le développement de R, de ses environnements de travail (RStudio, Jupyter, etc.) et des nouveaux modules qui s’y greffent. Plus largement, vous gagnerez à vous informer sur les dernières tendances en calcul scientifique sur d’autres plate-forme que R (Python, Javascript, Julia, etc.). Évidemment, nos tâches quotidiennes ne nous permettent pas de tout suivre. Même si vous pouviez n’attrapper que 1% du défilement, ce serait déjà 1% de plus que rien du tout.\nJe vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez à l’aventure!\nTiré du film The Hobbit: An Unexpected Journey, de Peter Jackson (2012).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Explorer R</span>"
    ]
  },
  {
    "objectID": "09-ordination.html#espaces-danalyse",
    "href": "09-ordination.html#espaces-danalyse",
    "title": "10  Association, partitionnement et ordination",
    "section": "\n10.1 Espaces d’analyse",
    "text": "10.1 Espaces d’analyse\n\n10.1.1 Abondance et occurrence\nL’abondance est le décompte d’espèces observées, tandis que l’occurrence est la présence ou l’absence d’une espèce. Le tableau suivant contient des données d’abondance.\n\nabundance &lt;- tibble('Bruant familier' = c(1, 0, 0, 3),\n                    'Citelle à poitrine rousse' = c(1, 0, 0, 0),\n                    'Colibri à gorge rubis' = c(0, 1, 0, 0),\n                    'Geai bleu' = c(3, 2, 0, 0),\n                    'Bruant chanteur' = c(1, 0, 5, 2),\n                    'Chardonneret' = c(0, 9, 6, 0),\n                    'Bruant à gorge blanche' = c(1, 0, 0, 0),\n                    'Mésange à tête noire' = c(20, 1, 1, 0),\n                    'Jaseur boréal' = c(66, 0, 0, 0))\n\nCe tableau peut être rapidement transformé en données d’occurrence, qui ne comprennent que l’information booléenne de présence (noté 1) et d’absence (noté 0).\n\noccurrence &lt;- abundance |&gt;\n  transmute_all(~if_else(. &gt; 0, 1, 0))\n\nL’espace des espèces (ou des variables ou descripteurs) est celui où les espèces forment les axes et où les sites sont positionnés dans cet espace. Il s’agit d’une perspective en mode R, qui permet principalement d’identifier quels espèces se retrouvent plus couramment ensemble.\n\nabundance |&gt; \n  select(`Bruant chanteur`, Chardonneret, `Mésange à tête noire`)\n\n# A tibble: 4 × 3\n  `Bruant chanteur` Chardonneret `Mésange à tête noire`\n              &lt;dbl&gt;        &lt;dbl&gt;                  &lt;dbl&gt;\n1                 1            0                     20\n2                 0            9                      1\n3                 5            6                      1\n4                 2            0                      0\n\n\nDans l’espace des sites (ou les échantillons ou objets), on transpose la matrice d’abondance. On passe ici en mode Q, où chaque point est une espèce, et où l’on peut observer quels échantillons sont similaires.\n\nabundance |&gt; t()\n\n                          [,1] [,2] [,3] [,4]\nBruant familier              1    0    0    3\nCitelle à poitrine rousse    1    0    0    0\nColibri à gorge rubis        0    1    0    0\nGeai bleu                    3    2    0    0\nBruant chanteur              1    0    5    2\nChardonneret                 0    9    6    0\nBruant à gorge blanche       1    0    0    0\nMésange à tête noire        20    1    1    0\nJaseur boréal               66    0    0    0\n\n\n\n10.1.2 Environnement\nL’espace de l’environnement comprend souvent un autre tableau contenant l’information sur l’environnement où se trouve les espèces: les coordonnées et l’élévation, la pente, le pH du sol, la pluviométrie, etc."
  },
  {
    "objectID": "09-ordination.html#analyse-dassociation",
    "href": "09-ordination.html#analyse-dassociation",
    "title": "10  Association, partitionnement et ordination",
    "section": "\n10.2 Analyse d’association",
    "text": "10.2 Analyse d’association\nNous utiliserons le terme association comme une mesure pour quantifier la ressemblance ou la différence entre deux objets (échantillons) ou variables (descripteurs).\nAlors que la corrélation et la covariance sont des mesures d’association entre des variables (analyse en mode R), la similarité et la distance sont deux types de une mesure d’association entre des objets (analyse en mode Q). Une distance de 0 est mesurée chez deux objets identiques. La distance augmente au fur et à mesure que les objets sont dissociés. Une similarité ayant une valeur de 0 indique aucune association, tandis qu’une valeur de 1 indique une association parfaite. À l’opposé, la dissimilarité est égale à 1-similarité.\nLa distance peut être liée à la similarité par la relation:\n\\[distance=\\sqrt{1-similarité}\\]\nou\n\\[distance=\\sqrt{dissimilarité}\\]\nLa racine carrée permet, pour certains indices de similarité, d’obtenir des propriétés euclidiennes. Pour plus de détails, voyez le tableau 7.2 de Legendre et Legendre (2012).\nLes matrices d’association sont généralement présentées comme des matrices carrées, dont les dimensions sont égales au nombre d’objets (mode Q) ou de variables (mode R) dans le tableau. Chaque élément (“cellule”) de la matrice est un indice d’association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilarité) ou unitaire (similarité), car elle correspond à l’association entre un objet et lui-même.\nPuisque l’association entre A et B est la même qu’entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible d’exprimer une matrice d’association en mode “compact”, sous forme de vecteur. Le vecteur d’association entre des objets A, B et C contiendra toute l’information nécessaire en un vecteur de trois chiffres, [AB, AC, BC], plutôt qu’une matrice de dimension \\(3 \\times 3\\). L’impact sur la mémoire vive peut être considérable pour les calculs comprenant de nombreuses dimensions.\nEn R, les calculs de similarité et de distances peuvent être effectués avec le module vegan. La fonction vegdist permet de calculer les indices d’association en forme carrée.\nNous verrons plus tard les méthodes de mesure de similarité et de distance plus loin. Pour l’instant, utilisons la méthode de Jaccard pour une démonstration sur des données d’occurrence.\n\nlibrary(\"vegan\")\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\n\nThis is vegan 2.6-4\n\nvegdist(occurrence, method = \"jaccard\",\n        diag = TRUE, upper = TRUE)\n\n          1         2         3         4\n1 0.0000000 0.7777778 0.7500000 0.7142857\n2 0.7777778 0.0000000 0.6000000 1.0000000\n3 0.7500000 0.6000000 0.0000000 0.7500000\n4 0.7142857 1.0000000 0.7500000 0.0000000\n\n\nRemarquez que vegdist retourne une matrice dont la diagonale est de 0 (on l’affiche en spécifiant diag = TRUE). La diagonale est l’association d’un objet avec lui-même. Or la similarité d’un objet avec lui-même devrait être de 1! En fait, par convention vegdist retourne des dissimilarités, non pas des similarités. La matrice de distance serait donc calculée en extrayant la racine carrée des éléments de la matrice de dissimilarité:\n\ndissimilarity &lt;- vegdist(occurrence, method = \"jaccard\",\n                         diag = TRUE, upper = TRUE)\ndistance &lt;- sqrt(dissimilarity)\ndistance\n\n          1         2         3         4\n1 0.0000000 0.8819171 0.8660254 0.8451543\n2 0.8819171 0.0000000 0.7745967 1.0000000\n3 0.8660254 0.7745967 0.0000000 0.8660254\n4 0.8451543 1.0000000 0.8660254 0.0000000\n\n\nDans le chapitre sur l’analyse compositionnelle, nous avons abordé les significations différentes que peuvent prendre le zéro. L’information fournie par un zéro peut être différente selon les circonstances. Dans le cas d’une variable continue, un zéro signifie généralement une mesure sous le seuil de détection. Deux tissus dont la concentration en cuivre est nulle ont une affinité sous la perspective de la concentration en cuivre. Dans le cas de mesures d’abondance (décompte) ou d’occurrence (présence-absence), on pourra décrire comme similaires deux niches écologiques où l’on retrouve une espèce en particulier. Mais deux sites où l’on de retrouve pas d’ours polaires ne correspondent pas nécessairement à des niches similaires! En effet, il peut exister de nombreuses raisons écologiques et méthodologiques pour lesquelles l’espèce ou les espèces n’ont pas été observées. C’est le problème des double-zéros (espèces non observées à deux sites), problème qui est amplifié avec les grilles comprenant des espèces rares.\nLa ressemblance entre des objets comprenant des données continues devrait être calculée grâce à des indicateurs symétriques. Inversement, les affinités entre les objets décrits par des données d’abondance ou d’occurrence susceptibles de générer des problèmes de double-zéros devraient être évaluées grâce à des indicateurs asymétriques. Un défi supplémentaire arrive lorsque les données sont de type mixte.\nNous utiliserons la convention de vegan et nous calculerons la dissimilarité, non pas la similarité. Les mesures de dissimilarité sont calculées sur des données d’abondance ou des données d’occurrence. Notons qu’il existe beaucoup de confusion dans la littérature sur la manière de nommer les dissimilarités (ce qui n’est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilarité avec un \\(d\\) minuscule et la distance avec un \\(D\\) majuscule.\n\n10.2.1 Association entre objets (mode Q)\n\n10.2.1.1 Objets: Abondance\nLa dissimilarité de Bray-Curtis est asymétrique. Elle est aussi appelée l’indice de Steinhaus, de Czekanowski ou de Sørensen. Il est important de s’assurer de bien s’entendre la méthode à laquelle on fait référence. L’équation enlève toute ambiguïté. La dissimilarité de Bray-Curtis entre les points A et B est calculée comme suit.\n\\[d_{AB} =  \\frac {\\sum \\left| A_{i} - B_{i} \\right| }{\\sum \\left(A_{i}+B_{i}\\right)}\\]\nUtilisons vegdist pour générer les matrices d’association. Le format “liste” de R est pratique pour enregistrer la collection d’objets, dont les matrice d’association que nous allons créer dans cette section.\n\nassociations_abund &lt;- list()\nassociations_abund[['BrayCurtis']] &lt;- vegdist(abundance, method = \"bray\")\nassociations_abund[['BrayCurtis']]\n\n          1         2         3\n2 0.9433962                    \n3 0.9619048 0.4400000          \n4 0.9591837 1.0000000 0.7647059\n\n\nLa dissimilarité de Bray-Curtis est souvent utilisée dans la littérature. Toutefois, la version originale de Bray-Curtis n’est pas tout à fait métrique (semimétrique). Conséquemment, la dissimilarité de Ruzicka (une variante de la dissimilarité de Jaccard pour les données d’abondance) est métrique, et devrait probablement être préféré à Bray-Curtis (Oksanen, 2006).\n\\[d_{AB, Ruzicka} =  \\frac { 2 \\times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}\\]\n\nassociations_abund[['Ruzicka']] &lt;- associations_abund[['BrayCurtis']] * 2 / (1 + associations_abund[['BrayCurtis']])\n\nLa dissimilarité de Kulczynski (aussi écrit Kulsinski) est asymétrique et semimétrique, tout comme celle de Bray-Curtis. Elle est calculée comme suit.\n\\[d_{AB} = 1-\\frac{1}{2} \\times \\left[ \\frac{\\sum min(A_i, B_i)}{\\sum A_i} + \\frac{\\sum min(A_i, B_i)}{\\sum B_i} \\right]\\]\n\nassociations_abund[['Kulczynski']] &lt;- vegdist(abundance, method = \"kulczynski\")\n\nUne approche commune pour mesurer l’association entre sites décrits par des données d’abondance est la distance de Hellinger. Notez qu’il s’agit ici d’une distance, non pas d’une dissimilarité. Pour l’obtenir, on doit d’abord diviser chaque donnée d’abondance par l’abondance totale pour chaque site pour obtenir les espèces en tant que proportions, puis on extrait la racine carrée de chaque élément. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la généralisation en plusieurs dimensions du théorème de Pythagore, \\(c = \\sqrt{a^2 + b^2}\\).\n\\[D_{AB} = \\sqrt {\\sum \\left( \\frac{A_i}{\\sum A_i} - \\frac{B_i}{\\sum B_i} \\right)^2}\\]\n\n\n\n\n\n\n😱 Attention\n\nLa distance d’Hellinger hérite des biais liées aux données compositionnelles. Elle peut être substituée par une matrice de distances d’Aitchison.\n\n\n\nassociations_abund[['Hellinger']] &lt;- dist(decostand(abundance, method=\"hellinger\"))\n\nToute comme la distance d’Hellinger, la distance de chord est calculée par une distance euclidienne sur des données d’abondance transformées de sorte que chaque ligne ait une longueur (norme) de 1.\n\nassociations_abund[['Chord']] &lt;- dist(decostand(abundance, method=\"normalize\"))\n\nLa métrique du chi-carré, ou \\(\\chi\\)-carré, ou chi-square, donne davantage de poids aux espèces rares qu’aux espèces communes. Son utilisation est recommandée lorsque les espèces rares sont de bons indicateurs de conditions écologiques particulières (Legendre et Legendre, 2012, p. 308).\n\\[  d_{AB} = \\sqrt{\\sum _j \\frac{1}{\\sum y_j} \\left( \\frac{A_j}{\\sum A} - \\frac{B_j}{\\sum B} \\right)^2 }  \\]\nLa métrique peut être transformée en distance en la multipliant par la racine carrée de la somme totale des espèces dans la matrice d’abondance (\\(X\\)).\n\\[ D_{AB} = \\sqrt{\\sum X} \\times d_{AB} \\]\n\nassociations_abund[['ChiSquare']] &lt;- dist(decostand(abundance, method=\"chi.square\"))\n\nUne manière visuellement plus intéressante de présenter une matrice d’association est un graphique de type heatmap.\n\nassociations_abund_df &lt;- list()\n\nfor (i in 1:length(associations_abund)) {\n  associations_abund_df[[i]] &lt;- data.frame(as.matrix(associations_abund[[i]]))\n  colnames(associations_abund_df[[i]]) &lt;- rownames(associations_abund_df[[i]])\n  associations_abund_df[[i]]$row &lt;- rownames(associations_abund_df[[i]])\n  associations_abund_df[[i]] &lt;- associations_abund_df[[i]] |&gt; gather(key=row)\n  associations_abund_df[[i]]$column = rep(1:4, 4)\n  associations_abund_df[[i]]$dist &lt;- names(associations_abund)[i]\n}\nassociations_abund_df &lt;- do.call(rbind, associations_abund_df)\n\nggplot(associations_abund_df, aes(x=row, y=column)) +\n  facet_wrap(. ~ dist, nrow = 2) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = round(value, 2))) +\n  scale_fill_gradient2(low = \"#00ccff\", mid = \"#aad400\", high = \"#ff0066\", midpoint = 2) +\n  labs(x=\"Site\", y=\"Site\")\n\n\n\n\n\n\n\nPeu importe le type d’association utilisée, les heatmaps montrent les mêmes tendances. Les associations de dissimilarité (Bray-Curtis, Kulczynski et Ruzicka) s’étalent de 0 à 1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de zéro, mais n’ont pas de limite supérieure. On note les plus grandes différences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures d’association à l’exception de la dissimilarité de Kulczynski.\n\n10.2.1.2 Objets: Occurrence (présence-absence)\nDes indices d’association différents devraient être utilisés lorsque des données sont compilées sous forme booléenne. En général, les tableaux de données d’occurrence seront compilés avec des 1 (présence) et des 0 (absence).\nLa similarité de Jaccard entre le site A et le site B est la proportion de double 1 (présences de 1 dans A et B) parmi les espèces. La dissimilarité est la proportion complémentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carrée de la dissimilarité.\n\nassociations_occ &lt;- list()\nassociations_occ[['Jaccard']] &lt;- vegdist(occurrence, method = \"jaccard\")\n\nLes distances d’Hellinger, de chord et de chi-carré sont aussi appropriées pour les calculs de distances sur des tableaux d’occurrence.\n\nassociations_occ[['Hellinger']] &lt;- dist(decostand(occurrence, method=\"hellinger\"))\nassociations_occ[['Chord']] &lt;- dist(decostand(occurrence, method=\"normalize\"))\nassociations_occ[['ChiSquare']] &lt;- dist(decostand(occurrence, method=\"chi.square\"))\n\nGraphiquement,\n\nassociations_occ_df &lt;- list()\n\nfor (i in 1:length(associations_occ)) {\n  associations_occ_df[[i]] &lt;- data.frame(as.matrix(associations_occ[[i]]))\n  colnames(associations_occ_df[[i]]) &lt;- rownames(associations_occ_df[[i]])\n  associations_occ_df[[i]]$row &lt;- rownames(associations_occ_df[[i]])\n  associations_occ_df[[i]] &lt;- associations_occ_df[[i]] |&gt; gather(key=row)\n  associations_occ_df[[i]]$column = rep(1:4, 4)\n  associations_occ_df[[i]]$dist &lt;- names(associations_occ)[i]\n}\nassociations_occ_df &lt;- do.call(rbind, associations_occ_df)\n\nggplot(associations_occ_df, aes(x=row, y=column)) +\n  facet_wrap(. ~ dist) +\n  geom_tile(aes(fill = value)) +\n  geom_text(aes(label = round(value, 2))) +\n  scale_fill_gradient2(low = \"#00ccff\", mid = \"#aad400\", high = \"#ff0066\", midpoint = 1) +\n  labs(x=\"Site\", y=\"Site\")\n\n\n\n\n\n\n\nIl est attendu que les matrices d’association sur l’occurrence sont semblables à celles sur l’abondance. Dans ce cas-ci, la distance d’Hellinger donne des résultats semblables à la dissimilarité de Jaccard.\n\n10.2.1.3 Objets: Données quantitatives\nLes données quantitative en écologie peuvent décrire l’état de l’environnement: le climat, l’hydrologie, l’hydrogéochimie, la pédologie, etc. En règle générale, les coordonnées des sites ne sot pas des variables environnementales, à que l’on soupçonne la coordonnée elle-même d’être responsable d’effets sur notre système: mais il s’agira la plupart du temps d’effets confondants (par exemple, on peut mesurer un effet de la latitude sur le rendement des agrumes, mais il s’agira probablement avant tout d’effets dus aux conditions climatiques, qui elles changent en fonction de la latitude). D’autre types de données quantitative pouvant être appréhendées par des distances sont les traits phénologiques, les ionomes, les génomes, etc.\nLa distance euclidienne est la racine carrée de la somme des carrés des distances sur tous les axes. Il s’agit d’une application multidimensionnelle du théorème de Pythagore. La distance d’Aitchison, couverte dans le chapitre 9, est une distance euclidienne calculée sur des données compositionnelles préalablement transformées. La distance euclidienne est sensible aux unités utilisés: utiliser des millimètres plutôt que des mètres enflera la distance euclidienne. Il est recommandé de porter une attention particulière aux unités, et de standardiser les données au besoin (par exemple, en centrant la moyenne à zéro et en fixant l’écart-type à 1).\nOn pourrait, par exemple, mesurer la distance entre des observations des dimensions de différentes espèces d’iris. Ce tableau est inclus dans R par défaut.\n\ndata(iris)\niris |&gt; sample_n(5)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n1          5.3         3.7          1.5         0.2    setosa\n2          6.4         3.1          5.5         1.8 virginica\n3          4.9         3.6          1.4         0.1    setosa\n4          7.9         3.8          6.4         2.0 virginica\n5          4.4         2.9          1.4         0.2    setosa\n\n\nLes mesures du tableau sont en centimètres. Pour éviter de donner davantage de poids aux longueurs des sépales et en même temps de négliger la largeur des pétales, nous allons standardiser le tableau.\n\niris_sc &lt;- iris |&gt;\n  select(-Species) |&gt; \n  scale() |&gt; \n  as_tibble() |&gt; \n  mutate(Species = iris$Species) \niris_sc\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1       -0.898      1.02          -1.34       -1.31 setosa \n 2       -1.14      -0.132         -1.34       -1.31 setosa \n 3       -1.38       0.327         -1.39       -1.31 setosa \n 4       -1.50       0.0979        -1.28       -1.31 setosa \n 5       -1.02       1.25          -1.34       -1.31 setosa \n 6       -0.535      1.93          -1.17       -1.05 setosa \n 7       -1.50       0.786         -1.34       -1.18 setosa \n 8       -1.02       0.786         -1.28       -1.31 setosa \n 9       -1.74      -0.361         -1.34       -1.31 setosa \n10       -1.14       0.0979        -1.28       -1.44 setosa \n# ℹ 140 more rows\n\n\nPour les comparaisons des dimensions, prenons la moyenne des dimensions (mises à l’échelle) par espèce.\n\niris_means &lt;- iris_sc |&gt;\n  group_by(Species) |&gt;\n  summarise_all(mean) |&gt;\n  select(-Species)\niris_means\n\n# A tibble: 3 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1       -1.01        0.850       -1.30       -1.25 \n2        0.112      -0.659        0.284       0.166\n3        0.899      -0.191        1.02        1.08 \n\n\nNous pouvons utiliser la distance euclidienne, commune en géométrie, pour comparer les espèces. La distance euclidienne est calculée comme suit.\n\\[ \\mathcal{E} = \\sqrt{\\Sigma_i \\left( A_i - B_i \\right) ^2 } \\]\n\nassociations_cont = list()\nassociations_cont[['Euclidean']] &lt;- dist(iris_sc |&gt; select(-Species), method=\"euclidean\")\n\nLa distance de Mahalanobis est semblable à la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut être utilisée pour décrire la structure d’un nuage de points. La distance de Mahalanobis se calcule comme suit.\n\\[\\mathcal{M} = \\sqrt{(A - B)^T S^{-1} (A-B)}\\]\nNotez qu’il s’agit d’une généralisation de la distance euclidienne, qui équivaut à une distance de Mahalanobis dont la matrice de covariance est une matrice identité.\nLa distance de Mahalanobis permet de représenter des distances dans un espace fortement corrélé. Elle est couramment utilisée pour détecter les valeurs aberrantes selon des critères de distance à partir du centre d’un jeu de données multivariées.\n\nassociations_cont[['Mahalanobis']] &lt;- vegdist(iris_sc |&gt; select(-Species), 'mahalanobis')\n\nLa distance de Manhattan porte aussi le nom de distance de cityblock ou de taxi. C’est la distance que vous devrez parcourir pour vous rendre du point A au point B à Manhattan, c’est-à-dire selon une séquence de tronçons perpendiculaires.\n\\[ D_{AB} = \\sum _i \\left| A_i - B_i \\right| \\]\nLa distance de Manhattan est appropriée lorsque les gradients (changements d’un état à l’autre ou d’une région à l’autre) ne permettent pas des changements simultanés. Mieux vaut standardiser les variables pour éviter qu’une dimension soit prépondérante.\n\nassociations_cont[['Manhattan']] &lt;- vegdist(iris_sc |&gt; select(-Species), 'manhattan')\n\nAvant de présenter les résultats des espèces d’iris, voici une représentation des distances euclidiennes (rouge), de Mahalanobis (bleu) et de Manhattan (vert), chacune de 1 et 2 unités à partir du centre et, pour ce qui est de la distance de Mahalanobis, selon la covariance.\n\nlibrary(\"car\")\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(\"MASS\")\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nselect &lt;- dplyr::select # éviter les conflits de fonctions entre MASS et dplyr\nfilter &lt;- dplyr::filter\n\nsigma &lt;- matrix(c(1, 0.6, 0.6, 1), ncol = 2) # matrice de covariance\nmu &lt;- c(0, 0) # centre\ndata &lt;- mvrnorm(n = 100, mu, sigma) # générer des données\n\nplot(data, ylim = c(-2, 2), xlim = c(-2, 2), asp = 1)\n\n## cercles\nt &lt;- seq(0,2*pi,length=100)\nc1 &lt;- t(rbind(mu[2] + sin(t)*1, mu[1] + cos(t)*1))\nc2 &lt;- t(rbind(mu[2] + sin(t)*2, mu[1] + cos(t)*2))\nlines(c1, lwd = 2, col = \"red\")\nlines(c2, lwd = 2, col = \"red\")\n\n\n## ellipses\ne1 &lt;- ellipse(mu, sigma, radius=1, add=TRUE)\ne2 &lt;- ellipse(mu, sigma, radius=2, add=TRUE)\n\n## carrés\nlines(c(1, 0, -1, 0, 1), c(0, 1, 0, -1, 0), lwd = 2, col = \"green\")\nlines(c(2, 0, -2, 0, 2), c(0, 2, 0, -2, 0), lwd = 2, col = \"green\")\n\n\n\n\n\n\n\nEt, graphiquement, les résultats des distances des iris.\n\nassociations_cont_df &lt;- list()\n\nfor (i in 1:length(associations_cont)) {\n  associations_cont_df[[i]] &lt;- data.frame(as.matrix(associations_cont[[i]]))\n  colnames(associations_cont_df[[i]]) &lt;- rownames(associations_cont_df[[i]])\n  associations_cont_df[[i]]$row &lt;- rownames(associations_cont_df[[i]])\n  associations_cont_df[[i]] &lt;- associations_cont_df[[i]] |&gt; gather(key=row)\n  associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris))\n  associations_cont_df[[i]]$dist &lt;- names(associations_cont)[i]\n}\nassociations_cont_df &lt;- do.call(rbind, associations_cont_df)\n\nggplot(associations_cont_df, aes(x=row, y=column)) +\n  facet_wrap(. ~ dist) +\n  geom_tile(aes(fill = value), colour = NA) +\n  #geom_text(aes(label = round(value, 2))) +\n  scale_fill_gradient2(low = \"#00ccff\", mid = \"#aad400\", high = \"#ff0066\", midpoint = 5) +\n  labs(x=\"Site\", y=\"Site\")\n\n\n\n\n\n\n\nLe tableau iris est ordonné par espèce. Les distances euclidienne et de Manhattan permettent aisément de distinguer les espèces selon les dimensions des pétales et des sépales. Toutefois, l’utilisation de la covariance avec la distance de Mahalanobis crée des distinction moins tranchées.\n\n10.2.1.4 Objets: Données mixtes\nLes données catégorielles ordinales peuvent être transformées en données continues par gradations linéaires ou quadratiques. Les données catégorielles nominales, quant à elles, peuvent être encodées (encodage catégoriel) en données similaires à des occurrences. Attention toutefois: contrairement à la régression linéaire qui demande d’exclure une catégorie, l’encodage catégoriel doit inclure toutes les catégories. Le comportement par défaut de la fonction model.matrix est d’exclure la catégorie de référence: on doit spécifier que l’intercept est de zéro, c’est-à-dire model.matrix(~ + categorie).\nLa similarité de Gower a été développée pour mesurer des associations entre des objets dont les données sont mixtes: booléennes, catégorielles et continues. La similarité de Gower est calculée en additionnant les distances calculées par colonne, individuellement. Si la colonne est booléenne, on utilise les distances de Jaccard (qui exclue les double-zéro) de manière univariée: une variable à la fois. Pour les variables continues, on utilise la distance de Manhattan divisée par la plage de valeurs de la variable (pour fin de standardisation). Puisqu’elle hérite de la particularité de la distance de Manhattan et de la similarité de Jaccard univariée, la similarité de Gower reste une combinaison linéaire de distances univariées.\n\nX &lt;- tibble(ID = 1:8,\n            age = c(21, 21, 19, 30, 21, 21, 19, 30),\n            gender = c('M','M','N','M','F','F','F','F'),\n            civil_status = c('MARRIED','SINGLE','SINGLE','SINGLE','MARRIED','SINGLE','WIDOW','DIVORCED'),\n            salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0),\n            children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE),\n            available_credit = c(2200,100,22000,1100,2000,100,6000,2200))\nX\n\n# A tibble: 8 × 7\n     ID   age gender civil_status salary children available_credit\n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt; &lt;lgl&gt;               &lt;dbl&gt;\n1     1    21 M      MARRIED        3000 TRUE                 2200\n2     2    21 M      SINGLE         1200 FALSE                 100\n3     3    19 N      SINGLE        32000 TRUE                22000\n4     4    30 M      SINGLE         1800 TRUE                 1100\n5     5    21 F      MARRIED        2900 TRUE                 2000\n6     6    21 F      SINGLE         1100 TRUE                  100\n7     7    19 F      WIDOW         10000 FALSE                6000\n8     8    30 F      DIVORCED       1500 TRUE                 2200\n\n\nIl faut préalablement procéder à l’encodage catégoriel pour les variables catégorielles nominales.\n\nX_dum &lt;- model.matrix(~ 0 + ., X[, -1])\nX_dum\n\n  age genderF genderM genderN civil_statusMARRIED civil_statusSINGLE\n1  21       0       1       0                   1                  0\n2  21       0       1       0                   0                  1\n3  19       0       0       1                   0                  1\n4  30       0       1       0                   0                  1\n5  21       1       0       0                   1                  0\n6  21       1       0       0                   0                  1\n7  19       1       0       0                   0                  0\n8  30       1       0       0                   0                  0\n  civil_statusWIDOW salary childrenTRUE available_credit\n1                 0   3000            1             2200\n2                 0   1200            0              100\n3                 0  32000            1            22000\n4                 0   1800            1             1100\n5                 0   2900            1             2000\n6                 0   1100            1              100\n7                 1  10000            0             6000\n8                 0   1500            1             2200\nattr(,\"assign\")\n [1] 1 2 2 2 3 3 3 4 5 6\nattr(,\"contrasts\")\nattr(,\"contrasts\")$gender\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$civil_status\n[1] \"contr.treatment\"\n\nattr(,\"contrasts\")$children\n[1] \"contr.treatment\"\n\n\nCalculons la dissimilarité de Gower (cette fois le graphique est fait avec pheatmap).\n\nlibrary(\"pheatmap\")\nd_gow &lt;- as.matrix(vegdist(X_dum, 'gower'))\ncolnames(d_gow) &lt;- rownames(d_gow) &lt;- X$ID\npheatmap(d_gow)\n\n\n\n\nLes dendrogrammes apparaissant sur les axes du graphique sont issus d’un processus de partitionnement basé sur la distance, que nous verrons plus loin dans ce chapitre. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diffèrent le plus. Les profils 3 et 4 sont néanmoins plutôt différents.\n\n10.2.2 Associations entre variables (mode R)\nIl existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corrélation. Mais les données d’abondance et d’occurrence demandent des approches différentes.\n\n10.2.2.1 Variables: Abondance\nLa distance du chi-carré est suggérée par Borcard et al. (2011).\n\nabundance_r &lt;- t(abundance)\nD_chisq_R &lt;- as.matrix(dist(decostand(abundance_r, method=\"chi.square\")))\npheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2))\n\n\n\n\nDes coabondances sont notables pour la mésange à tête noire, le jaseur boréal, la citelle à poitrine rousse et le bruant à gorge blanche (tache bleu au centre).\n\n10.2.2.2 Variables: occurrence\nLa dissimilarité de Jaccard peut être utilisée.\n\noccurrence_r &lt;- t(occurrence)\nD_jacc_R &lt;- as.matrix(vegdist(occurrence_r, method = \"jaccard\"))\npheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2))\n\n\n\n\nDes cooccurrences sont notables pour le jaseur boréal, la citelle à poitrine rousse et le bruant à gorge blanche (tache bleu au centre).\n\n10.2.2.3 Variables: Quantités\nLa matrice des corrélations de Pearson peut être utilisée pour les données continues. Quant aux variables ordinales, elles devraient idéalement être liées linéairement ou quadratiquement. Si ce n’est pas le cas, c’est-à-dire que les catégories sont ordonnées par rang seulement, vous pourrez avoir recours aux coefficients de corrélation de Spearman ou de Kendall.\n\niris_cor &lt;- iris |&gt;\n  select(-Species) |&gt;\n  cor()\npheatmap(cor(iris[, -5]), cluster_rows = FALSE, cluster_cols = FALSE,\n         display_numbers = round(iris_cor, 2))\n\n\n\n\n\n10.2.3 Conclusion sur les associations\nIl n’existe pas de règle claire pour déterminer quelle technique d’association utiliser. Cela dépend en premier lieu de vos données. Vous sélectionnerez votre méthode d’association selon le type de données que vous abordez, la question à laquelle vous désirez répondre ainsi l’expérience dans la littérature comme celle de vos collègues scientifiques. S’il n’existe pas de règle clair, c’est qu’il existe des dizaines de méthodes différentes, et la plupart d’entre elles vous donneront une perspective juste et valide. Il faut néanmoins faire attention pour éviter de sélectionner les méthodes qui ne sont pas appropriées."
  },
  {
    "objectID": "09-ordination.html#partitionnement",
    "href": "09-ordination.html#partitionnement",
    "title": "10  Association, partitionnement et ordination",
    "section": "\n10.3 Partitionnement",
    "text": "10.3 Partitionnement\nLes données suivantes ont été générées par Leland McInnes (Tutte institute of mathematics, Ottawa). Êtes-vous en mesure d’identifier des groupes? Combien en trouvez-vous?\n\ndf_mcinnes &lt;- read_csv(\"data/clusterable_data.csv\", col_names = c(\"x\", \"y\"), skip = 1)\n\nRows: 2309 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed()\n\n\n\n\nEn 2D, l’oeil humain peut facilement détecter les groupes. En 3D, c’est toujours possible, mais au-delà de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors d’une aide précieuse. Mais ils transportent en pratique tout un bagage de limitations. Quel est le critère d’association entre les groupes? Combien de groupe devrions-nous créer? Comment distinguer une donnée trop bruitée pour être classifiée?\nLe partitionnement de données (clustering en anglais), et inversement leur regroupement, permet de créer des ensembles selon des critères d’association. On suppose donc que Le partitionnement permet de créer des groupes selon l’information que l’on fait émerger des données. Il est conséquemment entendu que les données ne sont pas catégorisées à priori: il ne s’agit pas de prédire la catégorie d’un objet, mais bien de créer des catégories à partir des objets par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs gènes, etc.\nPlusieurs méthodes sont aujourd’hui offertes aux analystes pour partitionner leurs données. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes.\n\nMéthodes hiérarchique et non hiérarchiques. Dans un partitionnement hiérarchique, l’ensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment l’ultime partitionnement. On pourra alors identifier comment se décline un partitionnement. À l’inverse, un partitionnement non-hiérarchique des algorithmes permettent de créer les groupes non hiérarchisés les plus différents que possible.\nMembership exclusif ou flou. Certaines techniques attribuent à chaque objet une classe unique: l’appartenance sera indiquée par un 1 et la non appartenance par un 0. D’autres techniques vont attribuer un membership flou où le degré d’appartenance est une variable continue de 0 à 1. Parmi les méthodes floues, on retrouve les méthodes probabilistes.\n\n\n10.3.1 Évaluation d’un partitionnement\nLe choix d’une technique de partitionnement parmi de nombreuses disponibles, ainsi que le choix des paramètres gouvernant chacune d’entre elles, est avant tout basé sur ce que l’on désire définir comme étant un groupe, ainsi que la manière d’interpréter les groupes. En outre, le nombre de groupe à départager est toujours une décision de l’analyste. Néanmoins, on peut se fier des indicateurs de performance de partitionnement. Parmis ceux-ci, retenons le score silhouette ainsi que l’indice de Calinski-Harabaz.\n\n10.3.1.1 Score silhouette\nEn anglais, le h dans silhouette se trouve après le l: on parle donc de silhouette coefficient pour désigner le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le sépare des autres points de son groupe (\\(a\\)) ainsi que la distance moyenne qui le sépare des points du groupe le plus rapproché.\n\\[s = \\frac{b-a}{max \\left(a, b \\right)}\\]\nUn coefficient de -1 indique le pire classement, tandis qu’un coefficient de 1 indique le meilleur classement. La moyenne des coefficients silhouette est le score silhouette.\n\n10.3.1.2 Indice de Calinski-Harabaz\nL’indice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus l’indice est élevé, mieux les groupes sont définis. La mathématique est décrite dans la documentation de scikit-learn, un module d’analyse et autoapprentissage sur Python.\nNote. Les coefficients silhouette et l’indice de Calinski-Harabaz sont plus appropriés pour les formes de groupes convexes (cercles, sphères, hypersphères) que pour les formes irrégulières (notamment celles obtenues par la DBSCAN, discutée ci-dessous).\n\n10.3.2 Partitionnement non hiérarchique\nIl peut arriver que vous n’ayez pas besoin de comprendre la structure d’agglomération des objets (ou variables). Plusieurs techniques de partitionnement non hiérarchique sont disponibles sur R. On s’intéressera en particulier aux k-means et au dbscan.\n\n10.3.2.1 Kmeans\nL’objectif des kmeans est de minimiser la distance euclidienne entre un nombre prédéfini de k groupes exclusifs.\n\nL’algorithme commence par placer une nombre k de centroides au hasard dans l’espace d’un nombre p de variables (vous devez fixer k, et p est le nombre de colonnes de vos données).\nEnsuite, chaque objet est étiqueté comme appartenant au groupe du centroïde le plus près.\nLa position du centroïde est déplacée à la moyenne de chaque groupe.\nRecommencer à partir de l’étape 2 jusqu’à ce que l’assignation des objets aux groupes ne change plus.\n\n\nSource: David Sheehan\n\nLa technique des kmeans suppose que les groupes ont des distributions multinormales - représentées par des cercles en 2D, des sphères en 3D, des hypersphères en plus de 3D. Cette limitation est problématique lorsque les groupes se présentent sous des formes irrégulières, comme celles du nuage de points de Leland McInnes, présenté plus haut. De plus, la technique classique des kmeans est basée sur des distances euclidiennes: l’utilisation des kmeans n’est appropriée pour les données comprenant beaucoup de zéros, comme les données d’abondance, qui devraient préalablement être transformées en variables centrées et réduites (Legendre et Legendre, 2012). La technique des mixtures gaussiennes (gaussian mixtures) est une généralisation des kmeans permettant d’intégrer la covariance des groupes. Les groupes ne sont plus des hyper-sphères, mais des hyper-ellipsoïdes.\n\n10.3.2.1.1 Application\nNous pouvons utilisé la fonction kmeans de R. Toutefois, puisque l’on désire ici effectuer des tests de partitionnement pour plusieurs nombres de groupes, nous utiliserons cascadeKM, du module vegan. Notez que de nombreux paramètres par défaut sont utilisés dans les exécutions ci-dessous. Ces notes de cours ne forment pas un travail de recherche scientifique. Lors de travaux de recherche, l’utilsation d’un argument ou d’un autre dans une fonction doit être justifié: qu’un paramètre soit utilisé par défaut dans une fonction n’est a priori pas une justification convaincante.\nPour les kmeans, on doit fixer le nombre de groupes. Le graphique des données de Leland McInnes montrent 6 groupes. Toutefois, il est rare que l’on puisse visualiser des démarcations aussi tranchées que celles de l’exemple, qui plus est dans des cas où l’on doit traiter de plus de deux dimensions. Je vais donc lancer le partitionnement en boucle pour plusieurs nombres de groupes, de 3 à 10 et pour chaque groupe, évaluer le score silhouette et de Calinski-Habaraz. J’utilise un argument random_state pour m’assurer que les groupes seront les mêmes à chaque fois que la cellule sera lancée.\n\nlibrary(\"vegan\")\nmcinnes_kmeans &lt;- cascadeKM(df_mcinnes, inf.gr = 3, sup.gr = 10, criterion = \"calinski\")\nstr(mcinnes_kmeans)\n\nList of 4\n $ partition: int [1:2309, 1:8] 1 1 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2309] \"1\" \"2\" \"3\" \"4\" ...\n  .. ..$ : chr [1:8] \"3 groups\" \"4 groups\" \"5 groups\" \"6 groups\" ...\n $ results  : num [1:2, 1:8] 85.1 2164.5 61.4 2294.6 51.4 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:2] \"SSE\" \"calinski\"\n  .. ..$ : chr [1:8] \"3 groups\" \"4 groups\" \"5 groups\" \"6 groups\" ...\n $ criterion: chr \"calinski\"\n $ size     : int [1:10, 1:8] 1243 561 505 NA NA NA NA NA NA NA ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:10] \"Group 1\" \"Group 2\" \"Group 3\" \"Group 4\" ...\n  .. ..$ : chr [1:8] \"3 groups\" \"4 groups\" \"5 groups\" \"6 groups\" ...\n - attr(*, \"class\")= chr \"cascadeKM\"\n\n\nL’objet mcinnes_kmeans, de type cascadeKM, peut être visualisé directement avec la fonction plot.\n\nplot(mcinnes_kmeans)\n\n\n\n\nOn obtient un maximum de Calinski à 4 groupes, qui correspond à la deuxième simulation effectuée de 3 à 10.\nExaminons les scores silhouette (module: cluster).\n\nlibrary(\"cluster\")\nasw &lt;- c()\nfor (i in 1:ncol(mcinnes_kmeans$partition)) {\n  mcinnes_kmeans_silhouette &lt;- silhouette(mcinnes_kmeans$partition[, i], dist = vegdist(df_mcinnes, method = \"euclidean\"))\n  asw[i] &lt;- summary(mcinnes_kmeans_silhouette)$avg.width\n}\nplot(3:10, asw, type = 'b')\n\n\n\n\n\n\n\nLe score silhouette maximum est à 3 groupes. La forme des groupes n’étant pas convexe, il fallait s’attendre à ce que indicateurs maximaux pour les deux indicateurs soient différents. C’est d’ailleurs souvent le cas. Cet exemple supporte que le choix du nombre de groupe à départager repose sur l’analyste, non pas uniquement sur les indicateurs de performance. Choisissons 6 groupes, puisque que c’est visuellement ce que l’on devrait chercher pour ce cas d’étude.\n\nkmeans_group &lt;- mcinnes_kmeans$partition[, 4]\nmcinnes_kmeans$partition |&gt; head(3)\n\n  3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups 10 groups\n1        1        3        1        5        2        3        1         6\n2        1        3        5        4        2        6        1         6\n3        1        4        1        5        4        3        6        10\n\ndf_mcinnes |&gt; \n  mutate(kmeans_group = kmeans_group) |&gt; # ajouter une colonne de regoupement\n  ggplot(aes(x=x, y=y)) +\n  geom_point(aes(colour = factor(kmeans_group))) +\n  coord_fixed()\n\n\n\n\n\n\n\nL’algorithme kmeans est loin d’être statisfaisant. Cela est attendu, puisque les kmeans recherchent des distribution gaussiennes sur des groupes vraisemblablement non-gaussiens.\nNous pouvons créer un graphique silhouette pour nos 6 groupes. Notez qu’à cause d’un bogue, il n’est pas possible de présenter les données clairement lorsqu’elles sont nombreuses.\n\nsil &lt;- silhouette(mcinnes_kmeans$partition[, 6],\n                  dist = vegdist(df_mcinnes[, ], method = \"euclidean\"))\nsil &lt;- sortSilhouette(sil)\nplot(sil, col = 'black')\n\n\n\n\n\n\n\n\n10.3.2.2 DBSCAN\nLa technique DBSCAN (* Density-Based Spatial Clustering of Applications with Noise) sousentend que les groupes sont composés de zones où l’on retrouve plus de points (zones denses) séparées par des zones de faible densité. Pour lancer l’algorithme, nous devons spécifier une mesure d’association critique (distance ou dissimilarité) d* ainsi qu’un nombre de point critique k dans le voisinage de cette distance.\n\nL’algorithme commence par étiqueter chaque point selon l’une de ces catégories:\n\n\n\nNoyau: le point a au moins k points dans son voisinage, c’est-à-dire à une distance inférieure ou égale à d.\n\nBordure: le point a moins de k points dans son voisinage, mais l’un de des points voisins est un noyau.\n\nBruit: le cas échéant. Ces points sont considérés comme des outliers.\n\n\n\n\n\n\nLes noyaux distancés de d ou moins sont connectés entre eux en englobant les bordures.\n\n\n\n\n\nLe nombre de groupes est prescrit par l’algorithme DBSCAN, qui permet du coup de détecter des données trop bruitées pour être classées.\nDamiani et al. (2014) a développé une approche utilisant la technique DBSCAN pour partitionner des zones d’escale pour les flux de populations migratoires.\n\n10.3.2.2.1 Application\nLa technique DBSCAN n’est pas basée sur le nombre de groupe, mais sur la densité des points. L’argument x ne constitue pas les données, mais une matrice d’association. L’argument minPts spécifie le nombre minimal de points qui l’on doit retrouver à une distance critique d* pour la formation des *noyaux et la propagation des groupes, spécifiée dans l’argument eps. La distance d peut être estimée en prenant une fraction de la moyenne, mais on aura volontiers recours à sont bon jugement.\n\nlibrary(\"dbscan\")\n\n\nAttaching package: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nmcinnes_dbscan &lt;- dbscan(x = vegdist(df_mcinnes[, ], method = \"euclidean\"),\n                         eps = 0.03, minPts = 10)\ndbscan_group &lt;- mcinnes_dbscan$cluster\nunique(dbscan_group)\n\n[1] 1 0 2 6 3 4 5\n\n\nLes paramètres spécifiés donnent 5 groupes (1, 2, ..., 5) et des points trop bruités pour être classifiés (étiquetés 0). Voyons comment les groupes ont été formés.\n\ndf_mcinnes |&gt; \n  mutate(dbscan_group = dbscan_group) |&gt; # ajouter une colonne de regoupement\n  ggplot(aes(x=x, y=y)) +\n  geom_point(aes(colour = factor(dbscan_group))) +\n  coord_fixed()\n\n\n\n\n\n\n\nLe partitionnement semble plus conforme à ce que l’on recherche. Néanmoins, DBSCAN cré quelques petits groupes indésirables (groupe 6, en rose) ainsi qu’un grand groupe (violet) qui auraient lieu d’être partitionné. Ces défaut pourraient être réglés en jouant sur les paramètres eps et minPts.\n\n10.3.3 Partitionnement hiérarchique\nLes techniques de partitionnement hiérarchique sont basées sur les matrices d’association. La technique pour mesurer l’association (entre objets ou variables) déterminera en grande partie le partitionnement des données. Les partitionnements hiérarchiques ont l’avantage de pouvoir être représentés sous forme de dendrogramme (ou arbre) de partition. Un tel dendrogramme présente des sous-groupes qui se joignent en groupes jusqu’à former un seul ensemble.\nLe partitionnement hiérarchique est abondamment utilisé en phylogénie, pour étudier les relations de parenté entre organismes vivants, populations d’organismes et espèces. La phénétique, branche empirique de la phylogénèse interspécifique, fait usage du partitionnement hiérarchique à partir d’associations génétiques entre unités taxonomiques. On retrouve de nombreuses ressources académiques en phylogénétique ainsi que des outils pour R et Python. Toutefois, la phylogénétique en particulier ne fait pas partie de la présente itération de ce manuel.\n\n10.3.3.1 Techniques de partitionnement hiérarchique\nLe partitionnement hiérarchique est typiquement effectué avec une des quatre méthodes suivantes, dont chacune possède ses particularités, mais sont toutes agglomératives: à chaque étape d’agglomération, on fusionne les deux groupes ayant le plus d’affinité sur la base des deux sous-groupes les plus rapprochés.\nSingle link (single). Les groupes sont agglomérés sur la base des deux points parmi les groupes, qui sont les plus proches.\nComplete link (complete). À la différence de la méthode single, on considère comme critère d’agglomération les éléments les plus éloignés de chaque groupe.\nAgglomération centrale. Il s’agit d’une famille de méthodes basées sur les différences entre les tendances centrales des objets ou des groupes.\n\n\nAverage (average). Appelée UPGMA (Unweighted Pair-Group Method unsing Average), les groupes sont agglomérés selon un centre calculés par la moyenne et le nombre d’objet pondère l’agglomération (le poids des groupes est retiré). Cette technique est historiquement utilisée en bioinformatique pour partitionner des groupes phylogénétiques (Sneath et Sokal, 1973).\n\nWeighted (weighted). La version de average, mais non pondérée (WPGMA).\n\nCentroid (centroid). Tout comme average, mais le centroïde (centre géométrique) est utilisé au lieu de la moyenne. Accronyme: UPGMC.\n\nMedian (median). Appelée WPGMC. Devinez! ;)\n\nWard (ward). L’optimisation vise à minimiser les sommes des carrés par regroupement.\n\n10.3.3.2 Quel outil de partitionnement hiérarchique utiliser?\nAlors que le choix de la matrice d’association dépend des données et de leur contexte, la technique de partitionnement hiérarchique peut, quant à elle, être basée sur un critère numérique. Il en existe plusieurs, mais le critère recommandé pour le choix d’une technique de partitionnement hiérarchique est la corrélation cophénétique. La distance cophénétique est la distance à laquelle deux objets ou deux sous-groupes deviennent membres d’un même groupe. La corrélation cophénétique est la corrélation de Pearson entre le vecteur d’association des objets et le vecteur de distances cophénétiques.\n\n10.3.3.3 Application\nLes techniques de partitionnement hiérarchique présentées ci-dessus sont disponibles dans le module stats de R, qui est chargé automatiquement lors de l’ouverture de R. Nous allons classifier les dimensions des iris grâce à la distance de Manhattan.\n\nmcinnes_hclust_distmat &lt;- vegdist(df_mcinnes, method = \"manhattan\")\n\nclustering_methods &lt;- c('single', 'complete', 'average', 'centroid', 'ward')\n\nclust_l &lt;- list()\ncoph_corr_l &lt;- c()\n\nfor (i in seq_along(clustering_methods)) {\n  clust_l[[i]] &lt;- hclust(mcinnes_hclust_distmat, method = clustering_methods[i])\n  coph_corr_l[i] &lt;- cor(mcinnes_hclust_distmat, cophenetic(clust_l[[i]]))\n}\n\nThe \"ward\" method has been renamed to \"ward.D\"; note new \"ward.D2\"\n\ntibble(clustering_methods, coph_corr = coph_corr_l) |&gt; \n  ggplot(aes(x = fct_reorder(clustering_methods, -coph_corr), y = coph_corr)) +\n  geom_col() +\n  labs(x = \"Méthode de partitionnement\", y = \"Corrélation cophénétique\")\n\n\n\n\n\n\n\nLa méthode average retourne la corrélation la plus élevée. Pour plus de flexibilité, enchâssons le nom de la méthode dans une variable. Ainsi, en chageant le nom de cette variable, le reste du code sera conséquent.\n\nnames(clust_l) &lt;- clustering_methods\nbest_method &lt;- \"average\"\n\nLe partitionnement hiérarchique peut être visualisé par un dendrogramme.\n\nplot(clust_l[[best_method]])\n\n\n\n\n\n\n\n\n10.3.3.4 Combien de groupes utiliser?\nLa longueur des lignes verticales est la distance séparant les groupes enfants. Bien que la sélection du nombre de groupe soit avant tout basée sur les besoins du problème, nous pouvons nous appuyer sur certains outils. La hauteur totale peut servir de critère pour définir un nombre de groupes adéquat. On pourra sélectionner le nombre de groupe où la hauteur se stabilise en fonction du nombre de groupe. On pourra aussi utiliser le graphique silhouette, comprenant une collection de largeurs de silhouette, représentant le degré d’appartenance à son groupe. La fonction sklearn.metrics.silhouette_score, du module scikit-learn, s’en occupe.\n\nasw &lt;- c()\nnum_groups &lt;- 3:10\nfor(i in seq_along(num_groups)) {\n  sil &lt;- silhouette(cutree(clust_l[[best_method]], k = num_groups[i]), mcinnes_hclust_distmat)\n  asw[i] &lt;- summary(sil)$avg.width\n}\nplot(num_groups, asw, type = \"b\")\n\n\n\n\n\n\n\nLe nombre optimal de groupes serait de 5. Coupons le dendrorgamme à la hauteur correspondant à 5 groupes avec la fonction cutree.\n\nk_opt &lt;- num_groups[which.max(asw)]\nhclust_group &lt;- cutree(clust_l[[best_method]], k = k_opt)\nplot(clust_l[[best_method]])\nrect.hclust(clust_l[[best_method]], k = k_opt)\n\n\n\n\n\n\n\nLa classification hiérarchique, uniquement basée sur la distance, peut être inappropriée pour définir des formes complexes.\n\ndf_mcinnes |&gt; \n  mutate(hclust_group = hclust_group) |&gt; # ajouter une colonne de regoupement\n  ggplot(aes(x=x, y=y)) +\n  geom_point(aes(colour = factor(hclust_group))) +\n  coord_fixed()\n\n\n\n\n\n\n\n\n10.3.4 Partitionnement hiérarchique basée sur la densité des points\nLa tecchinque HDBSCAN, dont l’algorithme est relativement récent (Campello et al., 2013), permet une partitionnement hiérarchique sur le même principe des zones de densité de la technique DBSCAN. Le HDBSCAN a été utilisée pour partitionner les lieux d’escale d’oiseaux migrateurs en Chine (Xu et al., 2013).\nAvec DBSCAN, un rayon est fixé dans une métrique appropriée. Pour chaque point, on compte le nombre de point voisins, c’est à dire le nombre de point se situant à une distance (ou une dissimilarité) égale ou inférieure au rayon fixé. Avec HDBSCAN, on spécifie le nombre de points devant être recouverts et on calcule le rayon nécessaire pour les recouvrir. Ainsi, chaque point est associé à un rayon critique que l’on nommera \\(d_{noyau}\\). La métrique initiale est ensuite altérée: on remplace les associations entre deux objets A et B par la valeur maximale entre cette association, le rayon critique de A et le rayon critique de B. Cette nouvelle distance est appelée la distance d’atteinte mutuelle: elle accentue les distances pour les points se trouvant dans des zones peu denses. On applique par la suite un algorithme semblable à la partition hiérarchique single link: En s’élargissant, les rayons se superposent, chaque superposition de rayon forment graduellement des groupes qui s’agglomèrent ainsi de manière hiérarchique. Au lieu d’effectuer une tranche à une hauteur donnée dans un dendrogramme de partitionnement, la technique HDBSCAN se base sur un dendrogramme condensé qui discarte les sous-groupes comprenant moins de n objets (\\(n_{gr min}\\)). Dans nouveau dendrogramme, on recherche des groupes qui occupent bien l’espace d’analyse. Pour ce faire, on utilise l’inverse de la distance pour créer un indicateur de persistance (semblable à la similarité), \\(\\lambda\\). Pour chaque groupe hiérarchique dans le dendrogramme condensé, on peut calculer la persistance où le groupe prend naissance. De plus, pour chaque objet d’un groupe, on peut aussi calculer une distance à laquelle il quitte le groupe. La stabilité d’un groupe est la somme des différences de persistance entre la persistance à la naissance et les persistances des objets. On descend dans le dendrogramme. Si la somme des stabilité des groupes enfants est plus grande que la stabilité du groupe parent, on accepte la division. Sinon, le parent forme le groupe. La documentation du module hdbscan pour Python offre une description intuitive et plus exhaustive des principes et algorithme de HDBSCAN.\n\n10.3.4.1 Paramètres\nOutre la métrique d’association dont nous avons discuté, HDBSCAN demande d’être nourri avec quelques paramètres importants. En particulier, le nombre minimum d’objets par groupe, \\(n_{gr min}\\) dépend de la quantité de données que vous avez à votre disposition, ainsi que de la quantité d’objets que vous jugez suffisante pour créer des groupes. Nous utiliserons l’implémentation de HDBSCAN du module dbscan. Si vous désirez davantage d’options, vous préférerez probablement l’implémentation du module largeVis.\n\nmcinnes_hdbscan &lt;- hdbscan(x = vegdist(df_mcinnes, method = \"euclidean\"),\n                           minPts = 20,\n                           gen_hdbscan_tree = TRUE,\n                           gen_simplified_tree = FALSE)\nhdbscan_group &lt;- mcinnes_hdbscan$cluster\nunique(hdbscan_group)\n\n[1] 6 0 4 3 5 1 2\n\n\nNous avons 6 groupes, numérotés de 1 à 6, ainsi que des étiquettes identifiant des objets désignés comme étant du bruit de fond, numéroté 0. Le dendrogramme non condensé peu être produit.\n\nplot(mcinnes_hdbscan$hdbscan_tree)\n\n\n\n\n\n\n\nDifficile d’y voir clair avec autant d’objets. L’objet mcinnes_hdbscan a un nombre minimum d’objets par groupe de 20. Ce qui permet de présenter le dendrogramme de manière condensée.\n\nplot(mcinnes_hdbscan)\n\n\n\n\n\n\n\nEnfin, un aperçu des stratégies de partitionnement utilisés jusqu’ici.\n\nclustering_group &lt;- df_mcinnes |&gt; \n  mutate(kmeans_group,\n         hclust_group,\n         dbscan_group,\n         hdbscan_group) |&gt; \n  gather(-x, -y, key = \"method\", value = \"cluster\")\n\nWarning: attributes are not identical across measure variables; they will be\ndropped\n\nclustering_group$cluster &lt;- factor(clustering_group$cluster)\nclustering_group |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point(aes(colour = cluster)) +\n  facet_wrap(~method, ncol = 2) +\n  coord_equal() +\n  theme_bw()\n\n\n\n\n\n\n\nClairement, le partitionnement avec HDBSCAN donne les meilleurs résultats.\n\n10.3.5 Conclusion sur le partitionnement\nAu chapitre 4, nous avons vu avec le jeu de données “datasaurus” que la visualisation peut permettre de détecter des structures en segmentant les données selon des groupes.\n\n\n\n\n\n\n\n\nOr, si les données n’étaient pas étiquetées, leur structure serait indétectable avec les algorithmes disponibles actuellement. Le partitionnement permet d’explorer des données, de détecter des tendances et de dégager des groupes permettant la prise de décision.\nPlusieurs techniques de partitionnement ont été présentées. Le choix de la technique sera déterminante sur la manière dont les groupes seront partitionnés. La définition d’un groupe variant d’un cas à l’autre, il n’existe pas de règle pour prescrire une méthode ou une autre. La partitionnement hiérarchique a l’avantage de permettre de visualiser comment les groupes s’agglomèrent. Parmi les méthodes de partitionnement hiérarchique disponibles, les méthodes basées sur la densité permettent une grande flexibilité, ainsi qu’une détection d’observations ne faisant partie d’aucun groupe."
  },
  {
    "objectID": "09-ordination.html#ordination",
    "href": "09-ordination.html#ordination",
    "title": "10  Association, partitionnement et ordination",
    "section": "\n10.4 Ordination",
    "text": "10.4 Ordination\nEn écologie, biologie, agronomie comme en foresterie, la plupart des tableaux de données comprennent de nombreuses variables: pH, nutriments, climat, espèces ou cultivars, etc. L’ordination vise à mettre de l’ordre dans des données dont le nombre élevé de variables peut amener à des difficultés d’appréciation et d’interprétation (Legendre et Legendre, 2012). Plus précisément, le terme ordination est utilisé en écologie pour désigner les techniques de réduction d’axe. L’analyse en composante principale est probablement la plus connue de ces techniques. Mais de nombreuses techniques d’ordination ont été développées au cours des dernières années, chacune ayant ses domaines d’application.\nLes techniques de réduction d’axe permettent de dégager l’information la plus importante en projetant une synthèse des relations entre les observations et entre les variables. Les techniques ne supposant aucune structure a priori sont dites non-contraignantes: elles ne comprennent pas de tests statistiques. À l’inverse, les ordinations contraignantes lient des variables descriptives avec une ou plusieurs variables prédictives.\nLa référence en la matière est indiscutablement (Legendre et Legendre, 2012). Cette section en couvrira quelques unes et vous guidera vers la technique la plus appropriée pour vos données.\n\n10.4.1 Ordination non contraignante\nCette section couvrira l’analyse en composantes principales (ACP), l’analyse de correspondance (AC), l’analyse factorielle (AF) ainsi que l’analyse en coordonnées principales (ACoP).\n\n\n\n\n\n\n\nMéthode\nDistance préservée\nVariables\n\n\n\nAnalyse en composantes principales (ACP)\nDistance euclidienne\nDonnées quantitatives, relations linéaires (attention aux double-zéros)\n\n\nAnalyse de correspondance (AC)\nDistance de \\(\\chi^2\\)\n\nDonnées non-négatives, dimentionnellement homogènes ou binaires, abondance ou occurrence\n\n\nPositionnement multidimensionnel (PoMd)\nToute mesure de dissimilarité\nDonnées quantitatives, qualitatives nominales/ordinales ou mixtes\n\n\n\nSource: Adapté de (Legendre et Legendre, 2012, chapitre 9)\n\n10.4.1.1 Analyse en composantes principales\nL’objectif d’une ACP est de représenter les données dans un nombre réduit de dimensions représentant le plus possible la variation d’un tableau de données: elle permet de projeter les données dans un espace où les variables sont combinées en axes orthogonaux dont le premier axe capte le maximum de variance. L’ACP peut par exemple être utilisée pour analyser des corrélations entre variables ou dégager l’information la plus pertinente d’un tableau de données météo ou de signal en un nombre plus retreint de variables.\nL’ACP effectue une rotation des axes à partir du centre (moyenne) du nuage de points effectuée de manière à ce que le premier axe définisse la direction où l’on retrouve la variance maximale. Ce premier axe est une combinaison linéaire des variables et forme la première composante principale. Une fois cet axe définit, on trouve de deuxième axe, orthogonal au premier, où l’on retrouve la variance maximale - cet axe forme la deuxième composante principale, et ainsi de suite jusqu’à ce que le nombre d’axe corresponde au nombre de variables. Les projections des observations sur ces axes principaux sont appelés les scores. Les projections des variables sur les axes principaux sont les vecteurs propres (eigenvectors, ou loadings). La variance des composantes principales diminue de la première à la dernière, et peut être calculée comme une proportion de la variance totale: c’est le pourcentage d’inertie. Par convention, on utilise les valeurs propres (eigenvalues) pour mesurer l’importance des axes. Si la première composante principale a une inertie de 50% et la deuxième a une inertie de 30%, la représentation en 2D des projection représentera 80% de la variance du nuage de points.\nL’hétérogénéité des échelles de mesure peut avoir une grande importance sur les résultats d’une ACP (les données doivent être dimensionnellement homogènes). En effet, la hauteur d’un ceriser aura une variance plus grande que le diamètre d’une cerise exprimé dans les mêmes unités, et cette dernière aura plus de variance que la teneur en cuivre d’une feuille. Il est conséquemment avisé de mettre les données à l’échelle en centrant la moyenne à zéro et l’écart-type à 1 avant de procéder à une ACP.\nL’ACP a été conçue pour projeter en un nombre moindre de dimensions des observations dont les distributions sont multinormales. Bien que l’ACP soit une technique robuste, il est préférable de transformer préalablement les variables dont la distribution est particulièrement asymétriques (Legendre et Legendre, 2012, p. 450). Le cas échéant, les valeurs extrêmes pourraient faire dévier les vecteurs propres et biaiser l’analyse. En particulier, les données ACP menées sur des données compositionnelles sont réputées pour générer des analyses biaisées (Pawlowsky-Glahn and Egozcue, 2006). Le test de Mardia (Korkmaz, 2014) peut être utilisé pour tester la multinormalité. Une distribution multinormale devrait générer des scores en forme d’hypersphère (en forme de cercle sur un biplot: voir plus loin).\n\n10.4.1.1.1 Vecteurs propres et valeurs propres\nUne matrice carrée (comme une matrice de covariance \\(\\Sigma\\)) multipliée par un vecteur propre \\(e\\) est égale aux valeurs propres \\(\\lambda\\) multipliées par les vecteurs propres \\(e\\).\n\\[ \\Sigma e = \\lambda e \\]\nDe manière intuitive, les vecteurs propres indiquent l’orientation de la covariance, et les valeurs propres indique la longueur associée à cette direction. L’ACP est basée sur le calcul des vecteurs propres et des valeurs propres de la matrice de covariance des variables. Pour d’abord obtenir les valeurs propres \\(\\lambda\\), il faut résoudre l’équation\n\\[ det(cov(X) - \\lambda I) = 0 \\],\noù \\(det\\) est l’opération permettant de calculer le déterminant, \\(cov\\) est l’opération pour calculer la covariance, \\(X\\) est la matrice de données, \\(\\lambda\\) sont les valeurs propres et \\(I\\) est une matrice d’identité.\nPour \\(p\\) variables dans votre tableau \\(X\\), vous obtiendrex \\(p\\) valeurs propres. Ensuite, on trouve les vecteurs propres en résolvant l’équation $ e = e $.\nBien qu’il soit possible d’effectuer cette opération à la main pour des cas très simples, vous aurez avantage à utiliser un langage de programmation.\nChargeons les données d’iris, puis isolons seulement les deux dimensions des sépales l’espèce setosa.\n\ndata(\"iris\")\nsetosa_sepal &lt;- iris |&gt; \n  filter(Species == \"setosa\") |&gt; \n  select(starts_with(\"Sepal\"))\nsetosa_sepal\n\n   Sepal.Length Sepal.Width\n1           5.1         3.5\n2           4.9         3.0\n3           4.7         3.2\n4           4.6         3.1\n5           5.0         3.6\n6           5.4         3.9\n7           4.6         3.4\n8           5.0         3.4\n9           4.4         2.9\n10          4.9         3.1\n11          5.4         3.7\n12          4.8         3.4\n13          4.8         3.0\n14          4.3         3.0\n15          5.8         4.0\n16          5.7         4.4\n17          5.4         3.9\n18          5.1         3.5\n19          5.7         3.8\n20          5.1         3.8\n21          5.4         3.4\n22          5.1         3.7\n23          4.6         3.6\n24          5.1         3.3\n25          4.8         3.4\n26          5.0         3.0\n27          5.0         3.4\n28          5.2         3.5\n29          5.2         3.4\n30          4.7         3.2\n31          4.8         3.1\n32          5.4         3.4\n33          5.2         4.1\n34          5.5         4.2\n35          4.9         3.1\n36          5.0         3.2\n37          5.5         3.5\n38          4.9         3.6\n39          4.4         3.0\n40          5.1         3.4\n41          5.0         3.5\n42          4.5         2.3\n43          4.4         3.2\n44          5.0         3.5\n45          5.1         3.8\n46          4.8         3.0\n47          5.1         3.8\n48          4.6         3.2\n49          5.3         3.7\n50          5.0         3.3\n\n\n\nlibrary(\"MVN\")\nsetosa_sepal_mvn &lt;- mvn(setosa_sepal, mvnTest = \"mardia\")\nsetosa_sepal_mvn$multivariateNormality\n\n             Test          Statistic           p value Result\n1 Mardia Skewness  0.759503524380438 0.943793240544741    YES\n2 Mardia Kurtosis 0.0934600553610254 0.925538081956867    YES\n3             MVN               &lt;NA&gt;              &lt;NA&gt;    YES\n\n\nPour considérer la distribution comme multinormale, la p-value de la distortion (Mardia Skewness) et la statistique de Kurtosis (Mardia Kurtosis) doit être égale ou plus élevée que 0.05 (Kormaz, 2019, fiche d’aide de la fonction mvn de R). C’est bien le cas pour les données du tableau setosa_sepal.\nRetirons de la matrice de covariance les valeurs et vecteurs propres avec la fonction eigen.\n\nsetosa_eigen &lt;- eigen(cov(setosa_sepal))\nsetosa_eigenval &lt;- setosa_eigen$values\nsetosa_eigenvec &lt;- setosa_eigen$vectors\n\nLe premier vecteur propre correspond à la première colonne, et le second à la deuxième. Les coordonnées x et y sont les premières et deuxièmes lignes. Les vecteurs propres ont une longueur unitaire (norme de 1). Ils peuvent être mis à l’échelles à la racine carrée des valeurs propres.\n\nsetosa_eigenvec_sc &lt;- setosa_eigenvec %*% diag(sqrt(setosa_eigen$values))\n\nPour effectuer une translation des vecteurs propres au centre du nuage de point, nous avons besoin du centroïde.\n\ncentroid &lt;- setosa_sepal %&gt;% apply(., 2, mean)\n\n\nplot(setosa_sepal, asp = 1)\n\n# vecteurs propres brutes\nlines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]),\n      y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), col = \"green\", lwd = 3) # vecteur propre 1\nlines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]),\n      y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), col = \"green\", lwd = 3) # vecteur propre 1\n\n# vecteurs propres à l'échelle\nlines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]),\n      y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), col = \"red\", lwd = 4) # vecteur propre 1\nlines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]),\n      y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), col = \"red\", lwd = 4) # vecteur propre 1\n\npoints(x=centroid[1], y=centroid[2], pch = 16, cex = 2, col  =\"blue\") # centroid\n\n\n\n\n\n\n\nOn peut observer que, comme je l’ai mentionné plus haut, les vecteurs propres indiquent l’orientation de la covariance, et les valeurs propres indique la longueur associée à cette direction.\n\n10.4.1.1.2 Biplot\nImaginez un nuage de points en 3D, axes y compris. Vous tournez votre nuage de points pour trouver la perspective en 2D qui fera en sorte que vos données soient les plus dispersées possibles. Avec une lampe de poche, vous illuminez votre nuage de points dans l’axe de cette perspective: vous venez d’effectuer une analyse en composantes principales, et l’ombre des points et des axes sur le mur formera votre biplot.\nPour créer un biplot, on juxtapose les descripteurs (variables) en tant que vecteurs propres, représentés par des flèches, et les objets (observations) en tant que scores, représentés par des points. Les résultats d’une ordination peuvent être présentés selon deux types de biplots (Legendre et Legendre, 2012).\n\n\n\n\n\nBiplot de corrélation permettant de visualiser les corrélations entre des variables météorologiques.\n\nDeux types de projection sont couramment utilisés.\nBiplot de distance. Ce type de projection permet de visualiser la position des objets entre eux et par rapport aux descripteurs et d’apprécier la contribution des descripteurs pour créer les composantes principales. Pour créer un biplot de distance, on projette directement les vecteurs propres (\\(U\\)) en guise de descripteurs. Pour ce qui est des objets, on utilise les scores de l’ACP (\\(F\\)). De cette manière,\n\nles distances euclidiennes entre les scores sont des approximations des distances euclidiennes dans l’espace multidimensionnel,\nla projection d’un objet sur un descripteur perpendiculairement à ce dernier est une approximation de la position de l’objet sur le descripteur et\nla projection d’un descripteur sur un axe principal est proportionnelle à sa contribution pour générer l’axe.\n\nBiplot de corrélation. Cette projection permet d’apprécier les corrélations entre les descripteurs. Pour ce faire, les objets et les valeurs propres doivent être transformés. Pour générer les descripteurs, les vecteurs propres (\\(U\\)) doivent être multipliés par la matrice diagonalisée de la racine carrée des valeurs propres (\\(\\Lambda\\)), c’est-à-dire \\(U \\Lambda ^{\\frac{1}{2}}\\). En ce qui a trait aux objets, on multiplie les scores par (\\(F\\)) par la racine carrée négative des valeurs propres diagonalisées, c’est-à-dire \\(F \\Lambda ^{- \\frac{1}{2}}\\). De cette manière,\n\ntout comme c’est le cas pour le biplot de distance, la projection d’un objet sur un descripteur perpendiculairement à ce dernier est une approximation de la position de l’objet sur le descripteur,\nla projection d’un descripteur sur un axe principal est proportionnelle à son écart-type et\nles angles entre les descripteurs sont proportionnelles à leur corrélation (et non pas leur proximité).\n\nEn d’autres mots, le bilot de distances devrait être utilisé pour apprécier la distance entre les objets et le biplot de corrélation devrait être utilisé pour apprécier les corrélations entre les descripteurs. Mais dans tous les cas, le type de biplot utilisé doit être indiqué.\nLe triplot est une forme apparentée au biplot, auquel on ajoute des variables prédictives. Le triplot est utile pour représenter les résultats des ordinations contraignantes comme les analyses de redondance et les analyse de correspondance canoniques.\n\n10.4.1.1.3 Application\nBien que l’ACP puisse être effectuée grâce à des modules de base de R, nous utiliserons le module vegan. Le tableau varechem comprend des données issues d’analyse de sols identifiés par leur composition chimique, leur pH, leur profondeur totale et la profondeur de l’humus publiées dans Väre et al. (1995) et exportées du module vegan.\n\nlibrary(\"vegan\")\ndata(\"varechem\")\nvarechem |&gt; \n  sample_n(5)\n\n      N    P     K    Ca    Mg    S    Al   Fe    Mn   Zn  Mo Baresoil Humdepth\n27 20.6 60.8 233.7 834.0 127.2 40.7  15.4  4.4 132.0 10.7 0.2     18.7      2.9\n28 29.8 73.5 260.0 748.6 105.3 42.5  17.9  2.4 106.6  9.3 0.3     17.6      3.0\n18 19.8 42.1 139.9 519.4  90.0 32.3  39.0 40.9  58.1  4.5 0.3     43.9      2.2\n5  33.1 22.7  43.6 240.3  25.7 14.9  39.0  8.4  26.8  8.4 0.2      8.1      1.0\n6  19.1 26.4  61.1 259.1  37.0 21.4 155.1 81.4  20.6  4.0 0.6      5.8      1.9\n    pH\n27 2.8\n28 2.8\n18 2.7\n5  3.1\n6  3.0\n\n\nComme nous l’avons vu précedemment, les données de concentration sont de type compositionnelles. Les données compositionnelles du tableau varechem mériteraient d’être transformées (Aitchison et Greenacre, 2002). Utilisons les log-ratios centrés (clr).\n\nlibrary(\"compositions\")\n\nWelcome to compositions, a package for compositional data analysis.\nFind an intro with \"? compositions\"\n\n\n\nAttaching package: 'compositions'\n\n\nThe following objects are masked from 'package:stats':\n\n    anova, cor, cov, dist, var\n\n\nThe following object is masked from 'package:graphics':\n\n    segments\n\n\nThe following objects are masked from 'package:base':\n\n    %*%, norm, scale, scale.default\n\nvarecomp &lt;- varechem |&gt;\n  select(-Baresoil, -Humdepth, -pH) %&gt;%\n  mutate(Fv = apply(., 1, function(x) 1e6 - sum(x)))\nvareclr &lt;- varecomp |&gt;\n  acomp() |&gt;\n  clr() |&gt; \n  as_tibble() |&gt; \n  bind_cols(varechem |&gt;\n              select(Baresoil, Humdepth, pH))\nvareclr |&gt; \n  sample_n(5)\n\n# A tibble: 5 × 15\n      N      P     K    Ca      Mg      S    Al     Fe     Mn    Zn    Mo    Fv\n* &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 -1.88 -0.600 0.641  1.48 -0.684  -0.805 1.30  -0.156 -1.14  -2.52 -4.68  9.04\n2 -1.69 -0.624 0.830  1.59 -0.0312 -0.729 0.189 -0.626 -0.331 -2.60 -5.49  9.53\n3 -1.76 -0.553 0.565  2.11  0.575  -0.706 0.159 -1.20  -1.30  -1.95 -4.99  9.05\n4 -1.78 -0.613 0.354  1.72 -0.174  -0.645 0.932 -1.02  -0.935 -2.31 -4.85  9.32\n5 -1.34 -0.760 0.673  2.07  0.549  -0.762 0.181 -1.60  -0.696 -2.26 -5.39  9.34\n# ℹ 3 more variables: Baresoil &lt;dbl&gt;, Humdepth &lt;dbl&gt;, pH &lt;dbl&gt;\n\n\nEffectuons l’ACP. Pour cet exemple, nous standardiserons les données étant données que les colonnes Baresoil, Humedepth et pH ne sont pas à la même échelle que les colonnes des clr.\n\nvareclr_sc &lt;- scale(vareclr)\nvare_pca &lt;- rda(vareclr_sc) # ou bien rda(vareclr, scale = TRUE, mais la mise à l'échelle préalable est plus explicite)\n\nL’objet vareclr_pca contient l’information nécessaire pour mener notre ACP.\n\nsummary(vare_pca, scaling = 2) # scaling = 2 pour obtenir les infos pour les biplots de corrélation\n\n\nCall:\nrda(X = vareclr_sc) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal              15          1\nUnconstrained      15          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                         PC1    PC2    PC3     PC4     PC5     PC6     PC7\nEigenvalue            7.1523 2.4763 2.1122 0.93015 0.57977 0.48786 0.36646\nProportion Explained  0.4768 0.1651 0.1408 0.06201 0.03865 0.03252 0.02443\nCumulative Proportion 0.4768 0.6419 0.7827 0.84473 0.88338 0.91590 0.94034\n                          PC8     PC9    PC10     PC11     PC12     PC13\nEigenvalue            0.29432 0.19686 0.15434 0.107357 0.095635 0.042245\nProportion Explained  0.01962 0.01312 0.01029 0.007157 0.006376 0.002816\nCumulative Proportion 0.95996 0.97308 0.98337 0.990527 0.996902 0.999719\n                           PC14\nEigenvalue            0.0042200\nProportion Explained  0.0002813\nCumulative Proportion 1.0000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  4.309777 \n\n\nSpecies scores\n\n             PC1     PC2     PC3      PC4        PC5       PC6\nN         0.1437  0.7606 -0.6792  0.19837  0.1128526 -0.050149\nP         0.8670 -0.3214 -0.2950 -0.22940  0.1437960 -0.042884\nK         0.9122 -0.3857  0.2357  0.03469  0.2737020  0.075717\nCa        0.9649 -0.3362 -0.2147  0.17757 -0.2188717  0.008051\nMg        0.8263 -0.2723  0.1035  0.52135 -0.1495399 -0.342214\nS         0.8825 -0.3169  0.3539 -0.21216  0.1176279 -0.191386\nAl       -1.0105 -0.2442  0.2146  0.02674 -0.1005560 -0.043569\nFe       -1.0338 -0.2464  0.1492  0.13162  0.1512218  0.081571\nMn        0.9556  0.1041 -0.1256 -0.21300  0.2565831  0.275275\nZn        0.7763 -0.1031 -0.3123 -0.36493 -0.5665691  0.153089\nMo       -0.2152  0.8717  0.4065 -0.33643 -0.2134335 -0.167725\nFv        0.2360  0.5776 -0.8112  0.12736  0.1280097 -0.109737\nBaresoil  0.5147  0.4210  0.4472  0.54980 -0.1438570  0.463148\nHumdepth  0.7455  0.4379  0.5194  0.16493  0.0004757 -0.273056\npH       -0.5754 -0.5864 -0.5957  0.23408 -0.1517661 -0.056641\n\n\nSite scores (weighted sums of species scores)\n\n        PC1       PC2      PC3      PC4      PC5      PC6\n18  0.16862  0.423777  0.46731  0.91175  1.10380  1.06421\n15 -0.09705 -0.097482  0.61143 -0.29049  1.14916  0.40622\n24  0.02831 -0.795737  0.74176 -0.19097 -2.43337 -0.81762\n27  1.39081 -0.354376 -0.19377 -0.45160  0.46020 -0.31446\n23  1.30346  0.357866  0.29887  0.76856  0.20913 -0.64145\n19  0.43636  0.495037  1.21722  1.18128 -0.98242 -0.74474\n22  1.07306  0.467575 -0.32245  0.03717  0.13956 -0.64972\n16  0.02545  0.659714 -0.28861 -0.01424  0.47105  0.45173\n28  1.42005  0.007356 -0.29000 -0.78474  0.97592 -0.80263\n13 -0.50638 -0.220909  1.52981  0.26289  0.42135  0.94054\n14  0.45392  0.649297  0.44573 -0.26620 -0.74522 -0.53228\n20  0.18623  0.259640  0.89112  0.21096 -0.51393  2.24361\n25  1.26264  0.225744 -0.96668 -0.69334  0.61990  0.43312\n7  -1.48685  0.739545 -0.20926  1.09256  0.61856 -0.87999\n5  -0.50622  1.108685 -2.61287 -1.00433 -1.35383  1.21964\n6  -1.28653  0.898663 -0.38778 -0.47556 -0.02449 -0.29419\n3  -1.72773  0.476962 -0.48878  0.71156  1.06398 -1.33473\n4  -0.82844 -0.296515  1.20315 -1.49821 -0.18330  1.05231\n2  -1.00247 -0.609253  0.25185 -0.85420  0.71031  0.14854\n9  -0.43405 -0.338912  0.55348 -1.35776 -0.81986 -1.02468\n12 -0.05083  0.122645 -0.04611 -0.56047 -0.26151 -0.98053\n10  0.17891 -2.315489 -0.69084 -0.19547  0.80628  0.04291\n11 -0.46443 -2.592018 -1.21615  1.56359 -0.62334  0.28748\n21  0.46316  0.728185 -0.49843  1.89726 -0.80791  0.72671\n\n\nLa deuxième ligne de Importance of components, Proportion Explained, indique la proportion de la variance totale captée successivement par les axes principaux. Le premier axe principal comporte 47.68% de la variance. Le deuxième axe principal ajoutant une proportion de 16,51%, une représentation en deux axes principaux présentent 64.19 % de la variance.\n\nprop_expl &lt;- vare_pca$CA$eig / sum(vare_pca$CA$eig)\nprop_expl\n\n         PC1          PC2          PC3          PC4          PC5          PC6 \n0.4768180610 0.1650859388 0.1408156459 0.0620101490 0.0386511040 0.0325238535 \n         PC7          PC8          PC9         PC10         PC11         PC12 \n0.0244303815 0.0196215021 0.0131238464 0.0102890284 0.0071571089 0.0063756951 \n        PC13         PC14 \n0.0028163495 0.0002813359 \n\n\nLa décision du nombre d’axes principaux à retenir est arbitraire. Elle peut dépendre d’un nombre maximal de paramètre à retenir pour éviter de surdimensionner un modèle (curse of dimensionality, section 11) ou d’un seuil de pourcentage de variance minimal à retenir, par exemple 75%. Ou bien, vous retiendrez deux composantes principales si vous désirez présenter un seul biplot.\nL’approche de Kaiser-Guttmann (Borcard et al., 2011) consiste à sélectionner les composantes principales dont la valeur propre est supérieure à leur moyenne.\n\nplot(x = 1:length(vare_pca$CA$eig),\n     y = vare_pca$CA$eig,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\")\nabline(h = mean(vare_pca$CA$eig), col = \"red\", lty = 2)\n\n\n\n\n\n\n\nL’approche du broken stick consiste à couper un bâton d’une longueur de 1 en n tranches. La première tranche est de longueur \\(\\frac{1}{n}\\). La tranche suivante est d’une longueur de la tranche précédente à laquelle on aditionne \\(\\frac{1}{longueur~restante}\\). Puis on place les longueurs en ordre décroissant. On retient les composantes principales dont les valeurs propres cumulées sont plus grandes que le broken stick.\n\nbroken_stick &lt;- function(x) {\n  bsm &lt;- vector(\"numeric\", length = x)\n  bsm[1] &lt;- 1/x\n  for (i in 2:x) {\n    bsm[i] &lt;- bsm[i-1] + 1/(x+1-i)\n  }\n  bsm &lt;- rev(bsm/x)\n  return(bsm)\n}\n\nLe graphique du broken stick:\n\nplot(x = 1:length(vare_pca$CA$eig),\n     y = prop_expl,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\")\nlines(x = 1:length(vare_pca$CA$eig),\n      y = broken_stick(length(vare_pca$CA$eig)),\n      col = \"red\",\n      lty = 2)\n\n\n\n\n\n\n\nLes approches Kaiser-Guttmann et broken stick suggèrent que les trois premières composantes sont suffisantes pour décrire la dispersion des données.\nExaminons les loadings (vecteurs propres) plus en particulier. Dans le langage du module vegan, les vecteurs propres sont les espèces (species) et les scores sont les sites.\n\nvare_eigenvec &lt;- vegan::scores(vare_pca, scaling = 2, display = \"species\", choices = 1:(ncol(vareclr)-1))\nvare_eigenvec\n\n                PC1        PC2        PC3        PC4           PC5          PC6\nN         0.1437343  0.7606006 -0.6792046  0.1983670  0.1128526122 -0.050148980\nP         0.8669892 -0.3213683 -0.2949864 -0.2294036  0.1437959857 -0.042883754\nK         0.9122089 -0.3857245  0.2356904  0.0346904  0.2737019601  0.075717162\nCa        0.9648855 -0.3361651 -0.2147486  0.1775746 -0.2188716732  0.008050762\nMg        0.8263327 -0.2723055  0.1035276  0.5213484 -0.1495399242 -0.342213793\nS         0.8824519 -0.3169039  0.3538854 -0.2121562  0.1176278503 -0.191386377\nAl       -1.0105173 -0.2441785  0.2145614  0.0267422 -0.1005559874 -0.043569364\nFe       -1.0337676 -0.2463987  0.1491865  0.1316173  0.1512218115  0.081571443\nMn        0.9555632  0.1041030 -0.1256178 -0.2130047  0.2565830557  0.275275174\nZn        0.7763480 -0.1030878 -0.3122919 -0.3649341 -0.5665691228  0.153089144\nMo       -0.2152399  0.8717229  0.4064967 -0.3364279 -0.2134335302 -0.167725160\nFv        0.2360040  0.5775863 -0.8111953  0.1273582  0.1280096553 -0.109737235\nBaresoil  0.5147445  0.4209983  0.4472351  0.5497950 -0.1438569673  0.463148072\nHumdepth  0.7455213  0.4379436  0.5193895  0.1649306  0.0004756685 -0.273056212\npH       -0.5753858 -0.5863743 -0.5957495  0.2340826 -0.1517660977 -0.056640816\n                 PC7         PC8          PC9        PC10         PC11\nN        -0.09111164 -0.06122008  0.315645453  0.08090232  0.019251478\nP         0.26894062  0.34111276  0.021124287  0.08756299  0.045741546\nK        -0.21662612 -0.01641260  0.143099440 -0.08737113 -0.183005607\nCa        0.03630015  0.04775616 -0.073609828 -0.10601799 -0.161460554\nMg        0.04617838 -0.12098602 -0.051599273  0.18373857  0.009862571\nS        -0.26825994  0.15822845  0.038378858  0.05100717  0.138785063\nAl       -0.22737412  0.10598673  0.040586196 -0.14473132  0.089462074\nFe        0.10553041 -0.09254655 -0.079426433  0.09908706  0.006376211\nMn        0.20224538 -0.19347804 -0.038859808 -0.07637994  0.083300112\nZn       -0.12332232 -0.14862229  0.024026151  0.02643462  0.064973307\nMo        0.13788948  0.17165900  0.032981311  0.01419924 -0.128814989\nFv       -0.20911147  0.11289753 -0.281443886 -0.08391004  0.012456867\nBaresoil -0.02103009  0.23028292  0.004554036  0.02604286  0.061147847\nHumdepth  0.17061078 -0.11310394  0.027515405 -0.23068827  0.102189307\npH        0.19890884  0.12152266  0.150118818 -0.15240317  0.037691048\n                 PC12        PC13         PC14\nN         0.045420621 -0.05020956  0.002340519\nP         0.145128883  0.03337551 -0.010109130\nK         0.002260341  0.10566808  0.001169065\nCa        0.041210064 -0.14341793  0.007419161\nMg       -0.063493608  0.03782662 -0.023575986\nS        -0.117144869 -0.06075094  0.025874035\nAl        0.058212507 -0.01983102 -0.037901576\nFe        0.049837173  0.01169516  0.036048221\nMn       -0.133353213 -0.02679781 -0.021373612\nZn        0.051057277  0.06538348  0.010896560\nMo       -0.114803631  0.01989539 -0.001335923\nFv       -0.020157331  0.05448619  0.005707928\nBaresoil -0.019696758  0.01640490  0.003823725\nHumdepth  0.109293684  0.02485030  0.016559206\npH       -0.153813168  0.04523353  0.014193061\nattr(,\"const\")\n[1] 4.309777\n\n\nL’ordre d’importance des vecteurs propres est établi en ordre croissant des élément des vecteurs propres associées. Un vecteur propre est une combinaison linéaire des variables. Par exemple, le premier vecteur propre pointe surtout dans la direction du Fe (-1.497) et de l’Al (-1.463). Le deuxième pointe surtout vers le Mo (2.145). Les vecteurs (loadings) d’un biplot de distance présentant les des deux premières composantes principales prendront les coordonnées des deux premières colonnes. Le vecteur Al aura la coordonnée [-1.463 ; -0.601], le vecteur de Fe sera placé à [-1.497 ; -0.606] et le vecteur Mo à [-0.312 ; 2.145]. Il existe différentes fonctions d’affichage des biplots. Notez que leur longueur peut être magnifiée pour améliorer la visualisation.\nLançons la fonction biplot pour créer un biplot de distance et un autre de corrélation.\n\npar(mfrow = c(1, 2))\nbiplot(vare_pca, scaling = 1, main = \"Biplot de distance\")\nbiplot(vare_pca, scaling = 2, main = \"Biplot de corrélation\")\n\n\n\n\n\n\n\nLe biplot de distance permet de dégager les variables qui expliquent davantage la variabilité dans notre tableau: les clr du Fe et de l’Al forment en grande partie le premier axe principal, alors que le clr du Mo forme en grande partie le second axe. Le biplot de corrélation montre que les clr du Fe et du Al sont corrélés dans le même sens, mais das le sens contraire du clr du Mn. L’information sur la teneur en Fe et celle de l’Al est en grande partie redondante. Toutefois, le clr du Mo est presque indépendant du clr du Fe, ceux-ci étant à angle presque droit (~90°). Ces relations peuvent être explorées directement.\n\npar(mfrow = c(1, 2))\nplot(vareclr$Al, vareclr$Fe)\nplot(vareclr$Mo, vareclr$Fe)\n\n\n\n\n\n\n\nNous avons mentionné que l’ACP est une rotation. Prenons un second exemple pour bien en saisir les tenants et aboutissants. Le tableau de données que nous chargerons provient d’un infographie d’un dauphin, intitullée Bottlenose Dolphin, conçu par l’artiste Tarnyloo. Les points correspondent à la surface d’un dauphin. J’ai ajouté une colonne anatomy, qui indique à quelle partie anatomique le point appartient.\n\ndolphin &lt;- read_csv(\"data/07_dolphin.csv\")\n\nRows: 12969 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): anatomy\ndbl (3): x, y, z\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndolphin |&gt; sample_n(5)\n\n# A tibble: 5 × 4\n        x      y       z anatomy   \n    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     \n1  0.0762 -0.521 -0.0803 Head      \n2  0.472   1.00   1.28   Caudal fin\n3 -0.172  -0.361  0.0223 Head      \n4  0.0461 -0.571 -0.0907 Head      \n5  0.0152 -0.710 -0.125  Head      \n\n\nVoici en vue isométrique ce en quoi consiste ce nuage de points.\n\nlibrary(\"scatterplot3d\")\nscatterplot3d(x = dolphin$x, y = dolphin$y, z = dolphin$z, pch = 16, cex.symbols = 0.2)\n\n\n\n\n\n\n\nEffectuons l’ACP sur le dauphin.\n\ndolph_pca &lt;- rda(dolphin |&gt; select(x, y, z), scale = FALSE)\nbiplot(dolph_pca, scaling = 2)\n\n\n\n\n\n\n\nOn n’y voit pas grand chose, mais si l’on extrait les scores et que l’on raccourcit les vecteurs:\n\ndolph_scores &lt;- vegan::scores(dolph_pca, display = \"sites\")\ndolph_loads &lt;- vegan::scores(dolph_pca, display = \"species\")\ndolph_loads\n\n          PC1         PC2\nx -0.02990131  0.01608095\ny -7.13731672 -1.43221776\nz -4.56612084  2.23859843\nattr(,\"const\")\n[1] 9.089026\n\nplot(dolph_scores, pch = 16, cex = 0.24, asp = 1, col = factor(dolphin$anatomy))\nsegments(x0 = rep(0, 3), y0 = rep(0, 3),\n         x1 = dolph_loads[, 1]/50,\n         y1 = dolph_loads[, 2]/50,\n         col = \"chocolate\", lwd = 4)\n\n\n\n\n\n\n\nLa meilleure représentation du dauphin en 2D, selon la variance, est son profil - en effet, il est plus long et haut que large.\n\nNote. Une ACP effectue seulement une rotation des points. Les distances euclidiennes entre les points sont maintenues.\n\n\nNote. L’ACP a été conçue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales (ce n’est évidemment pas le cas du dauphin).\n\n\nNote. Les axes principaux d’une ACP sont des variables aléatoires. Elles peuvent être assujetties à des tests ststistiques, des modèles, du partitionnement de données, etc.\n\n\nExcercice. Effectuez maintenant une ACP avec les données d’iris.\n\n\n10.4.1.2 Analyse de correspondance (AC)\nL’analyse de correspondance (AC) est particulièrement appropriée pour traiter des données d’abondance et d’occurrence. Tout comme l’analyse en composantes principales, les données apportés vers une AC doivent être dimensionnellement homogènes, c’est-à-dire que chaque variable doit être de même métrique: pour des données d’abondance, cela signifie que les décomptes réfèrent tous au même concept: individus, colonies, surfaces occupées, etc. Alors que la distance euclidienne est préservée avec l’ACP, l’AC préserve la distance du \\(\\chi^2\\), qui est insensible aux double-zéros.\nL’AC produit \\(min(n,p)-1\\) axes principaux orthogonaux qui captent non pas le maximum de variance, mais la proportion de mesures aux carré par rapport à la somme des carrés de la matrice. Le biplot obtenu peut être présenté sous forme de biplot de site (scaling 1), où la distance du \\(\\chi^2\\) est préservée entre les sites ou biplot d’espèces (scaling 2), ou la distance du \\(\\chi^2\\) est préservée entre les espèces. L’AC hérite du coup une propriété importate de la distance du \\(\\chi^2\\), qui accorde davantage de distance entre un compte de 0 et de 1 qu’entre 1 et 2, et davantage entre 1 et 2 qu’entre 2 et 3.\nPar exemple, sur ces trois sites, on a compté un individu A de moins que d’individu B.\n\nabundance_0123 = tibble(Site = c(\"Site 1\", \"Site 2\", \"Site 3\"),\n                        A = c(0, 1, 9),\n                        B = c(1, 2, 10))\nabundance_0123\n\n# A tibble: 3 × 3\n  Site       A     B\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Site 1     0     1\n2 Site 2     1     2\n3 Site 3     9    10\n\n\nPourtant, la distance du \\(\\chi^2\\) est plus élevée entre le site 1 et le site 2 qu’entre le site 2 et le site 3.\n\ndist(decostand(abundance_0123 |&gt; select(-Site), method=\"chi.square\"))\n\n          1         2\n2 0.6724111          \n3 0.9555316 0.2831205\n\n\nLa distance du \\(\\chi^2\\) donne davantage d’importance aux espèces rares, ce dont une analyse doit tenir compte. Il pourrait être envisageable de retirer d’un tableau des espèces rare, ou bien prétransformer des données d’abondance par une transformation de chord ou de Hellinger (tel que discuté au chapitre 6), puis procéder à une ACP sur ces données (Legendre et Gallagher, 2001).\n\n10.4.1.2.1 Application\nLe tableau varespec comprend des données de surface de couverture de 44 espèces de plantes en lien avec les données environnementales du tableau varechem. Ces données ont été publiées dans Väre et al. (1995) et exportées du module vegan.\n\ndata(\"varespec\")\nvarespec |&gt;sample_n(5)\n\n   Callvulg Empenigr Rhodtome Vaccmyrt Vaccviti Pinusylv Descflex Betupube\n27     0.00    15.13     2.42     5.92    15.97     0.00     3.70        0\n6      0.30     5.75     0.00     0.00    10.50     0.10     0.00        0\n7      0.00     5.30     0.00     0.00     8.20     0.00     0.05        0\n5      0.00     0.13     0.00     0.00     2.75     0.03     0.00        0\n2      0.05     9.30     0.00     0.00     8.50     0.03     0.00        0\n   Vacculig Diphcomp Dicrsp Dicrfusc Dicrpoly Hylosple Pleuschr Polypili\n27     1.12     0.00      0     3.63     0.00      6.7    58.07     0.00\n6      0.00     0.00      0     0.85     0.00      0.0     0.05     0.03\n7      8.10     0.28      0     0.45     0.03      0.0     0.10     0.00\n5      0.00     0.00      0     0.25     0.03      0.0     0.03     0.18\n2      0.00     0.00      0     0.03     0.00      0.0     0.75     0.00\n   Polyjuni Polycomm Pohlnuta Ptilcili Barbhatc Cladarbu Cladrang Cladstel\n27     0.00     0.13     0.02     0.08     0.08     1.42     7.63     2.55\n6      0.08     0.00     0.00     0.08     0.00    39.00    37.50    11.30\n7      0.25     0.00     0.03     0.00     0.00    35.00    42.50     0.28\n5      0.65     0.00     0.00     0.00     0.00    18.50    59.00     0.98\n2      0.03     0.00     0.00     0.03     0.00     0.48    24.50    75.00\n   Cladunci Cladcocc Cladcorn Cladgrac Cladfimb Cladcris Cladchlo Cladbotr\n27     0.15     0.00     0.38     0.12     0.10     0.03     0.00     0.02\n6      3.45     0.18     0.20     0.25     0.25     0.23     0.03     0.00\n7      0.35     0.08     0.20     0.25     0.18     0.13     0.08     0.00\n5      0.28     0.23     0.23     0.23     0.10     0.05     0.00     0.00\n2      0.20     0.00     0.03     0.03     0.05     0.03     0.03     0.00\n   Cladamau Cladsp Cetreric Cetrisla Flavniva Nepharct Stersp Peltapht Icmaeric\n27     0.00   0.02     0.00        0     0.00      0.0   0.00     0.07     0.00\n6      0.00   0.03     0.35        0     0.08      0.0   0.03     0.00     0.00\n7      0.00   0.00     0.05        0     0.23      0.2   0.93     0.00     0.03\n5      0.03   0.00     0.18        0     0.28      0.0  10.28     0.00     0.10\n2      0.00   0.00     0.00        0     0.00      0.0   0.00     0.00     0.00\n   Cladcerv Claddefo Cladphyl\n27     0.00     0.15        0\n6      0.00     0.28        0\n7      0.00     0.10        0\n5      0.00     0.25        0\n2      0.03     0.03        0\n\n\nPour effectuer l’AC, nous utiliserons, comme pour l’ACP, le module vegan mais cette fois-ci avec la fonction cca. L’AC en scaling 1 est effectuée sur le tableau des abondances avec les espèces comme colonnes et les sites comme lignes. Les matrices d’abondance transposées indique les sites où chaque espèce ont été dénombrées: pour une analyse en scaling 2, on effectue une analyse de correspondance sur la matrice d’abondance (ou d’occurrence) transposée.\nPour chacune des AC, je filtre pour m’assurer que toutes les lignes contiennent au moins une observation. Ce n’est pas nécessaire dans notre cas, mais je le laisse pour l’exemple.\n\nvare_cca &lt;- cca(varespec %&gt;% filter(rowSums(.) &gt; 0))\nsummary(vare_cca, scaling = 1)\n\n\nCall:\ncca(X = varespec %&gt;% filter(rowSums(.) &gt; 0)) \n\nPartitioning of scaled Chi-square:\n              Inertia Proportion\nTotal           2.083          1\nUnconstrained   2.083          1\n\nEigenvalues, and their contribution to the scaled Chi-square \n\nImportance of components:\n                         CA1    CA2    CA3     CA4     CA5     CA6     CA7\nEigenvalue            0.5249 0.3568 0.2344 0.19546 0.17762 0.12156 0.11549\nProportion Explained  0.2520 0.1713 0.1125 0.09383 0.08526 0.05835 0.05544\nCumulative Proportion 0.2520 0.4233 0.5358 0.62962 0.71489 0.77324 0.82868\n                          CA8     CA9    CA10    CA11    CA12    CA13     CA14\nEigenvalue            0.08894 0.07318 0.05752 0.04434 0.02546 0.01710 0.014896\nProportion Explained  0.04269 0.03513 0.02761 0.02129 0.01222 0.00821 0.007151\nCumulative Proportion 0.87137 0.90650 0.93411 0.95539 0.96762 0.97583 0.982978\n                          CA15     CA16     CA17     CA18     CA19      CA20\nEigenvalue            0.010160 0.007830 0.006032 0.004008 0.002865 0.0019275\nProportion Explained  0.004877 0.003759 0.002896 0.001924 0.001375 0.0009253\nCumulative Proportion 0.987855 0.991614 0.994510 0.996434 0.997809 0.9987341\n                           CA21      CA22      CA23\nEigenvalue            0.0018074 0.0005864 0.0002434\nProportion Explained  0.0008676 0.0002815 0.0001168\nCumulative Proportion 0.9996017 0.9998832 1.0000000\n\nScaling 1 for species and site scores\n* Sites are scaled proportional to eigenvalues\n* Species are unscaled: weighted dispersion equal on all dimensions\n\n\nSpecies scores\n\n                CA1       CA2      CA3       CA4        CA5       CA6\nCallvulg  0.0303167 -1.597460  0.11455 -2.894569  0.1376073  2.291129\nEmpenigr  0.0751030  0.379305  0.39303  0.023675  0.8568729 -0.400964\nRhodtome  1.1052309  1.499299  3.04284  0.120106  3.2324306 -0.283510\nVaccmyrt  1.4614812  1.622935  2.72375  0.231688  0.4604556  0.712538\nVaccviti  0.1468014  0.313436  0.14696  0.243505  0.6868371 -0.147815\nPinusylv -0.4820096  0.588517 -0.36020 -0.127094  0.4064754  0.386604\nDescflex  1.5348239  1.218806  1.87562 -0.001340 -1.3136979 -0.070731\nBetupube  0.6694503  1.951826  3.84017  1.389423  7.5959115 -0.244478\nVacculig -0.0830789 -1.629259  1.05063  0.802648 -0.3058811 -1.625341\nDiphcomp -0.5446464 -1.037570  0.52282  0.940275  0.3682126 -1.082929\nDicrsp    1.8120408  0.360290 -4.92082  3.088562  1.3867372  0.157815\nDicrfusc  1.2704743 -0.562978 -0.39718 -2.929542  0.3848272 -2.408710\nDicrpoly  0.7248118  1.409347  0.80341  1.915549  4.5674148  1.295447\nHylosple  2.0062408  1.743883  2.27549  0.928884 -3.7648428  2.254851\nPleuschr  1.3102086  0.583036 -0.01004  0.137298 -1.1216144  0.200422\nPolypili -0.3805097 -1.243904  0.54593  1.477188 -0.7276341 -0.387641\nPolyjuni  1.0133795  0.099043 -2.24697  1.510641  0.7729714 -3.062378\nPolycomm  0.8468241  1.321773  1.13585  1.140723  2.6836594 -0.605038\nPohlnuta -0.0136453  0.589290 -0.35542  0.135481  0.9369707  0.397246\nPtilcili  0.4223631  1.598584  3.43474  1.400065  6.3209491  0.198935\nBarbhatc  0.5018348  2.119334  4.57303  1.693188  8.1101807  0.645995\nCladarbu -0.1531729 -1.483884  0.20024  0.193680  0.0734141  0.358926\nCladrang -0.5502561 -1.084008  0.40552  0.724060 -0.3357992 -0.335924\nCladstel -1.4373146  1.077753 -0.44397 -0.375926 -0.2421525  0.004212\nCladunci  0.8151727 -1.006186 -1.82587 -1.389523  1.6046713  3.675908\nCladcocc -0.2133215 -0.584429 -0.21434 -0.567886 -0.0003788 -0.145303\nCladcorn  0.2631227 -0.177858 -0.44464  0.272422  0.3992282 -0.306738\nCladgrac  0.1956947 -0.311167 -0.23894  0.379013  0.4933026  0.037581\nCladfimb  0.0009213 -0.161418  0.18463 -0.435908  0.4831233 -0.143751\nCladcris  0.3373031 -0.470369 -0.05093 -0.823855  0.7182250  0.636140\nCladchlo -0.6200021  1.207278  0.21889  0.426447  1.9506082  0.120722\nCladbotr  0.5647242  1.047333  2.65330  0.907734  4.4946805  1.201655\nCladamau -0.6598144 -1.512880  0.83251  1.577699 -0.0407227 -1.419139\nCladsp   -0.8209003  0.476164 -0.49752 -0.998241 -0.2393208  0.390785\nCetreric  0.2458192 -0.689228 -1.68427 -0.131681  0.7439412  2.374535\nCetrisla -0.3465221  1.362693  0.85897  0.396752  2.7526968  0.396591\nFlavniva -1.4391907 -0.833589 -0.12919  0.007071 -1.4841375  2.956977\nNepharct  1.6813309  0.199484 -4.33509  2.229917  0.9561223 -5.472858\nStersp   -0.5172793 -2.280900  0.99775  2.377013 -0.8892757 -1.441228\nPeltapht  0.4035858 -0.043265  0.04538  0.711040  0.1824679 -0.841227\nIcmaeric  0.0378754 -2.419595  0.72135  0.361302 -0.3736424 -2.092136\nCladcerv -0.9232858 -0.005233 -1.22058  0.305290 -0.8142627  0.414135\nCladdefo  0.5190399 -0.496632 -0.15271 -0.695927  0.9042143  0.909191\nCladphyl -1.2836161  1.155872 -0.79912 -0.741170 -0.1608002  0.490526\n\n\nSite scores (weighted averages of species scores)\n\n         CA1      CA2       CA3      CA4        CA5      CA6\n18 -0.108122 -0.53705  0.229574  0.24412  0.1405624 -0.14253\n15  0.697118 -0.14441 -0.031788 -0.21743 -0.2738522 -0.08146\n24  0.987603  0.15042 -1.348447  0.80472  0.3095168  0.46773\n27  0.851765  0.49901  0.443559  0.12277 -0.4814871  0.07589\n23  0.359881 -0.05608  0.145813  0.15087  0.2405263 -0.17770\n19  0.003545  0.37017  0.027760  0.06168 -0.1158930 -0.03413\n22  0.860732 -0.11504  0.110869 -1.02169  0.0772348 -0.60530\n16  0.636936 -0.33250  0.001120 -0.79797  0.0130769 -0.54049\n28  1.279352  0.81557  0.670053  0.23137 -0.8929976  0.41783\n13 -0.195009 -0.80564  0.117686 -0.58286 -0.0007212  0.53071\n14  0.528532 -0.70420 -0.517771 -0.86836  0.5713441  0.91671\n20  0.382866 -0.18686 -0.004789  0.10156  0.0458125  0.21087\n25  0.990715  0.11967 -1.110040  0.44929  0.1885902 -0.70694\n7  -0.264704 -1.06013  0.334900  0.45973 -0.0326631 -0.19945\n5  -0.428410 -1.20765  0.374344  0.74970 -0.2596294 -0.30467\n6  -0.330534 -0.77498  0.130760  0.22391  0.0632686  0.09060\n3  -0.899601  0.12075 -0.075742  0.03842 -0.1489585 -0.12031\n4  -0.770294 -0.35351 -0.033779 -0.01795 -0.3007839  0.44303\n2  -0.992193  0.50319 -0.157505 -0.07070 -0.1065172 -0.09928\n9  -0.937173  0.78688 -0.258119 -0.19377 -0.0343535 -0.01259\n12 -0.726413  0.49163 -0.157235 -0.08698 -0.0105774 -0.02801\n10 -1.002083  0.71239 -0.236526 -0.18643 -0.0231666 -0.04928\n11 -0.322647 -0.03871 -0.001297  0.09029 -0.1481448  0.06934\n21  0.259527  0.80746  1.124258  0.36083  1.5437866  0.07051\n\n\n\nvarespec_eigenval &lt;- eigenvals(vare_cca, scaling = 1)\n\nprop_expl &lt;- varespec_eigenval / sum(varespec_eigenval)\n\npar(mfrow = c(1, 2))\nplot(x = 1:length(varespec_eigenval),\n     y = vare_cca$CA$eig,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\")\nabline(h = mean(varespec_eigenval), col = \"red\", lty = 2)\n\nplot(x = 1:length(varespec_eigenval),\n     y = prop_expl,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\")\nlines(x = 1:length(varespec_eigenval),\n      y = broken_stick(length(varespec_eigenval)),\n      col = \"red\",\n      lty = 2)\n\n\n\n\n\n\n\nCréons les biplots.\n\npar(mfrow = c(1, 2))\nplot(vare_cca, scaling = 1, main = \"Biplot des espèces\")\nplot(vare_cca, scaling = 2, main = \"Biplot des sites\")\n\n\n\n\n\n\n\nLe biplot des espèces, à gauche (scaling = 1), montre la distribution des sites selon les espèces. Les emplacements des scores (en noir) montrent les contrastes entre sites selon les espèces qui les recouvrent. Les sites 14 et 15, par exemple, contrastent les sites 19, 20, 21 et 22 selon le 2ième axe principal. Par ailleurs, les axes principaux sont formé de plusieurs espèces dont aucune ne domine clairement.\nLe biplot des sites, à droite (scaling = 2), montre la distribution des recouvrements d’espèces selon les sites. Par exemple, les espèces Betupube (Betula pubescens) et Barbhatc (Barbilophozia hatcheri ) se recouvrent en particulier le site 24. Le site 1 est difficile à identifier, car il est couvert par plusieurs noms d’espèces, au bas au centre. Les sites 3 et 13 se confondent avec Dicrsp (une espèce de Dicranum) qui le recouvre amplement.\nPour les deux types de biplot, les sites où les espèces situés près de l’origine, car ils peuvent être soit près de la moyenne, soit distribués uniformément.\nLe nombre de composantes à retenir peut être évalué par les approches Kaiser-Guttmann et broken-stick.\n\nscaling &lt;- 1\nvarespec_eigenval &lt;- eigenvals(vare_cca, scaling = scaling) # peut être effectué sur les deux types de scaling\n\nprop_expl &lt;- varespec_eigenval / sum(varespec_eigenval)\n\npar(mfrow = c(1, 2))\nplot(x = 1:length(varespec_eigenval),\n     y = vare_cca$CA$eig,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\",\n     main = paste(\"Eigenvalue - Kaiser-Guttmann, scaling =\", scaling))\nabline(h = mean(varespec_eigenval), col = \"red\", lty = 2)\n\nplot(x = 1:length(varespec_eigenval),\n     y = prop_expl,\n     type = \"b\",\n     xlab = \"Rang de la valeur propre\",\n     ylab = \"Valeur propre\",\n     main = paste(\"Proportion - broken stick, scaling =\", scaling))\nlines(x = 1:length(varespec_eigenval),\n      y = broken_stick(length(varespec_eigenval)),\n      col = \"red\",\n      lty = 2)\n\n\n\n\n\n\n\nPour les deux scalings, l’approche Kaiser-Guttmann propose 7 axes, tandis que l’approche broken-stick en propose 5.\nLes représentations biplot d’analyse de correspondance peuvent prendre la forme d’un boomerang, en particulier celles qui sont basées sur des données d’occurrence. Le tableau suivant initialement de Chessel et al. (1987) et est distribué dans le module ade4.\n\nlibrary(\"ade4\")\ndata(\"doubs\")\nfish &lt;- doubs$fish\ndoubs_cca &lt;- cca(fish %&gt;% filter(rowSums(.) &gt; 0))\nplot(doubs_cca, scaling = 2)\n\n\n\n\n\n\n\nLes numéros de sites correspondent à la position dans une rivière, 1 étant en amont et 30 en aval. Le premier axe discrimine l’amont et l’aval, tandis que le deuxième montre deux niches en amont. Bien que l’on observe une discontinuité dans le cours d’eau, il y a une continuité dans les abondances. Cet effet peut être corrigé en retirant la tendance de l’analyse de correspondance par une detrended correspondance analysis. Pour cela, il faudra utiliser la fonction decorana, ce qui ne sera pas couvert ici.\nL’analyse des correspondances multiples (ACM) est utile pour l’ordination des données catégorielles. Le module ade4 est en mesure d’effectuer des AMC, mais n’est pas couvert dans ce manuel.\n\nExcercice. Effectuez et analysez une AC avec les données de recouvrement varespec.\n\n\n10.4.1.3 Positionnement multidimensionnel (PoMd)\nLe positionnement multidimensionnel (PoMd), ou manifold analysis, se base sur les associations entre les objets (mode Q) ou les variables (mode R) pour en réduire les dimensions. Alors que l’analyse en composantes principales conserve la distance euclidienne et que l’analyse de correspondance conserve la distance du \\(\\chi^2\\), le PoMd conserve l’association que vous sélectionnerez à votre convenance. Le PoMd vise à représenter en un nombre limité de dimensions (souvent 2) la distance (ou dissimilarité) qu’ont les objets (ou des variables) les uns par rapport aux autres dans l’espace multidimensionnel.\nIl existe deux types d’AEM. Le PoMd-métrique (metric multidimentional scaling MMDS, parfois le metric est retiré, MDS, et parfois l’on parle de classic MDS) vise à représenter fidèlement la distance entre les objets ou les variables. Le PoMd-métrique ne devrait être utilisée que lorsque la métrique n’est ni euclidienne, ni de \\(\\chi^2\\) et que l’on désire préserver les distances entre les objets. L’PoMd-métrique aussi appelée analyse en coordonnées principales (ACoP ou de l’anglais PCoA) .\nLe PoMd-non-métrique (nonmetric multidimentional scaling, NMDS) vise quant à lui à représenter l’ordre des distances entre les objets ou les variables. C’est une approche par rang: le PoMd-non-métrique vise représenter les objets sont plus proches ou plus éloignées les uns des autres plutôt que de représenter leur similarité dans l’espace multidimentionnelle.\nL’IsoMap, pour isometric feature mapping, est une extension du PoMd qui reconstruit les distances selon les points retrouvés dans le voisinage. Les isomaps sont en mesure d’aplatir des données ayant des formes complexes.\nNous ne traitons pour l’instant que de l’PoMd-métrique (fonction vegan::cmdscale) et des PoMd-non-métrique (fonction vegan::metaMDS).\n\n10.4.1.3.1 Application\nUtilisons les données d’abondance que nous avions au tout début de ce chapitre. La matrice d’association de Bray-Curtis sera utilisée.\n\nassoc_mat &lt;- vegdist(abundance, method = \"bray\")\npheatmap(assoc_mat |&gt; as.matrix(), cluster_rows = FALSE, cluster_cols = FALSE,\n         display_numbers = round(assoc_mat |&gt; as.matrix(), 2))\n\n\n\n\n\n\n\nLes sites 2 et 3 devraient être plus près l’un et l’autre, puis les sites 3 et 4. Les autres associations sont éloignés d’environ la même distance. Lançons le calcul de la PoMd-métrique.\n\npcoa &lt;- cmdscale(assoc_mat, k = nrow(abundance)-1, eig = TRUE)\nspec_scores &lt;- wascores(pcoa$points, abundance)\nordiplot(vegan::scores(pcoa), type = 't', cex = 1.5)\n\nspecies scores not available\n\ntext(spec_scores, row.names(spec_scores), col = \"red\", cex = 0.75)\n\n\n\n\n\n\n\nOn observe en effet que les sites 2 et 3 sont les plus près. Les sites 3 et 4sont plus éloignés. Les sites 1, 2 et 4 font à peu près un triangle équilatéral, ce qui correspond à ce à quoi on devrait s’attendre. Les wa-scores permettent de juxtaposer les espèces sur les sites, pour référence. Le colibri n’est présent que sur le site 2. Le site 1 est populé par des jaseurs et des mésanges, et c’est le seul site où l’on a observé une citelle. On a observé des chardonnerets sur les sites 2 et 3. Sur le site 4, on n’a observé que des bruants, que l’on a aussi observé ailleurs, sauf au site 2.\nLe PoMd-non-métrique (non metric dimensional scaling, NMDS) fonctionne de la même manière que la PoMd-métrique, à la différence que la distance est basée sur les rangs. À cet égard, le site 4 à une distance de 0.76 du site 3, mais plutôt le deuxième plus loin, après le site 2 et avant le site 1. Utilisons la fonction metaMDS.\n\nnmds &lt;- metaMDS(assoc_mat, k = nrow(abundance)-1, eig = TRUE)\n\nRun 0 stress 0 \nRun 1 stress 0 \n... Procrustes: rmse 0.1216366  max resid 0.1634103 \nRun 2 stress 0 \n... Procrustes: rmse 0.1403208  max resid 0.1888214 \nRun 3 stress 0 \n... Procrustes: rmse 0.115492  max resid 0.1523862 \nRun 4 stress 0 \n... Procrustes: rmse 0.1551062  max resid 0.202592 \nRun 5 stress 0 \n... Procrustes: rmse 0.09908794  max resid 0.1384734 \nRun 6 stress 0 \n... Procrustes: rmse 0.0987008  max resid 0.1454209 \nRun 7 stress 0 \n... Procrustes: rmse 0.1164685  max resid 0.1501751 \nRun 8 stress 0 \n... Procrustes: rmse 0.0831961  max resid 0.09943451 \nRun 9 stress 0 \n... Procrustes: rmse 0.1429865  max resid 0.1921223 \nRun 10 stress 0 \n... Procrustes: rmse 0.09337227  max resid 0.1214715 \nRun 11 stress 0 \n... Procrustes: rmse 0.06970139  max resid 0.1001732 \nRun 12 stress 0 \n... Procrustes: rmse 0.08842632  max resid 0.1314237 \nRun 13 stress 0 \n... Procrustes: rmse 0.08200456  max resid 0.1163193 \nRun 14 stress 0 \n... Procrustes: rmse 0.09483784  max resid 0.1239188 \nRun 15 stress 0 \n... Procrustes: rmse 0.1513612  max resid 0.1946446 \nRun 16 stress 0 \n... Procrustes: rmse 0.08664967  max resid 0.1141698 \nRun 17 stress 0 \n... Procrustes: rmse 0.1179912  max resid 0.1610405 \nRun 18 stress 0 \n... Procrustes: rmse 0.1099474  max resid 0.1455303 \nRun 19 stress 0 \n... Procrustes: rmse 0.1063117  max resid 0.1398709 \nRun 20 stress 0 \n... Procrustes: rmse 0.1009788  max resid 0.1310601 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n    20: stress &lt; smin\n\n\nWarning in metaMDS(assoc_mat, k = nrow(abundance) - 1, eig = TRUE): stress is\n(nearly) zero: you may have insufficient data\n\n\nWarning in postMDS(out$points, dis, plot = max(0, plot - 1), ...): skipping\nhalf-change scaling: too few points below threshold\n\nspec_scores &lt;- wascores(nmds$points, abundance)\nordiplot(vegan::scores(nmds), type = 't', cex = 1.5)\n\nspecies scores not available\n\ntext(spec_scores, row.names(spec_scores), col = \"red\", cex = 0.75)\n\n\n\n\n\n\n\nDans ce cas, entre PoMd-métrique et non-métrique, les résultats peuvent être interprétés de manière similaire.\nEn ce qui a trait au dauphin,\n\n\n\n\n\n\n\n\nPour plus de détails, je vous invite à vous référer à Borcard et al. (2011)) ou de consulter l’excellent site GUSTA ME.\n\n10.4.1.4 Conclusion sur l’ordination non contraignante\nLorsque les données sont euclidiennes, l’analyse en composantes principales (ACP) devrait être utilisée. Lorsque la métrique est celle du \\(\\chi^2\\), on préférera l’analyse de correspondance (AC). Si la métrique est autre, le positionnement multidimensionel (PoMd) est préférable. Dans ce dernier cas, si l’on recherche une représentation simplifiée de la distance entre les objets ou variables, on utilisera un PoMd-métrique. À l’inverse, si l’on désire une représentation plus fidèle au rang des distances, on préférera l’PoMd-non-métrique.\n\n10.4.2 Ordination contraignante\nAlors que l’ordination non contraignante vous permet de dresser un portrait de vos variables, l’ordination contraignante (ou canonique) permet de tester statistiquement ainsi que de représenter la relation entre plusieurs variables explicatives (par exemple, des conditions environnementales) et une ou plusieurs variables réponses (par exemple, les espèces observées).\n\nL’analyse discriminante n’a fondamentalement qu’une seulement variable réponse, et celle-ci doit décrire l’appartenance à une catégorie.\nL’analyse de redondance sera préférée lorsque le nombre de variable est plus restreint (variables ionomiques et indicateurs de performance des cultures). Les détails, ainsi que les tenants et aboutissants de ces méthodes, sont présentés dans Numerical Ecology (Legendre et Legendre, 2012).\nL’analyse canonique des corrélations sera préférée lorsque les variables sont parsemées (beaucoup de colonnes avec beaucoup de zéros, comme les variables d’abondance).\n\n\n10.4.2.1 Analyse discriminante\nAlors que l’analyse en composante principale vise à présenter la perspective (les axes) selon laquelle les points sont les plus éclatées, l’analyse discriminante, le plus souvent utilisé dans sa forme linéaire (ADL) et quadratique (ADQ), vise à présenter la perspective selon laquelle les groupes sont les plus éclatés, les groupes formant la variable contraignante. Ces groupes peuvent être connus (e.g. cultivar, région géographique) ou attribués (exemple: par partitionnement). L’ADL est parfois nommée analyse canonique de la variance.\nL’AD vise à représenter des différences entre des groupes aux moyens de combinaisons linéaires (ADL) ou quadratique (ADQ) de variables mesurées. Sa représentation sous forme de biplot permet d’apprécier les différences entre les groupes d’identifier les variables qui sont responsables de la discrimination.\n\n\n\n\n\nBiplot de distance de l’analyse discriminante des ionomes d’espèces de plantes à fruits cultivées sauvages et domestiquées, Source: Parent et al. (2013)\n\nL’ADL a été développée par Fisher (1936), qui à titre d’exemple d’application a utilisé un jeu de données de dimensions d’iris collectées par Edgar Anderson, du Jardin botanique du Missouri, sur 150 spécimens d’iris collectés en Gaspésie (Est du Québec), ma région natale (suis-je assez chauvin?). Ce jeu de données est amplement utilisé à titre d’exemple en analyse multivariée.\nWilliams (1983) a présenté les tenants et aboutissants de l’ADL en écologie. Tout comme les données passant pas une ACP doivent suivre une distribution multinormale pour être statistiquement valide, les distributions des groupes dans une ADL doivent être multinormales et les variances des points par groupe doivent être homogènes… ce qui est rarement le cas en science. Néanmoins:\n\nHeureusement, il y a des évidences dans la littérature que certaines d’entre [ces règles] peuvent être transgressées modérément sans de grands changement dans les taux de classification. Cette conclusion dépends, toutefois, de la sévérité des transgressions, et de facteurs structueaux comme la position relative des moyennes des populations et de la nature des dispersions. - Williams (1983)\n\nL’ADL peut servir autant d’outil d’interprétation que d’outil de classification, c’est à dire de prédire une catégorie selon les variables (chapitre 13). Dans les deux cas, lorsque le nombre de variables approchent le nombre d’observation, les résultats d’une ADL risque d’être difficilement interprétables. Le test approprié pour évaluer l’homodénéité de la covariance est le M-test de Box. Ce test est peu documenté dans la littérature, est rarement utilisé mais a la réputation d’être particulièrement sévère.\nIl est rare que des données écologiques aient des dispersions (covariances) homogènes. Contrairement à l’ADL, l’ADQ ne demande pas à ce que les dispersions (covariances) soient homogènes. Néanmoins, l’ADQ ne génère ni de scores, ni de loadings: il s’agit d’un outil pour prédire des catégories (classification), non pas d’un outil d’ordination.\n\n10.4.2.1.1 Application\nUtilisons les données d’iris.\n\ndata(\"iris\")\n\nTestons la multinormalité par groupe. Rappelons-nous que pour considérer la distribution comme multinormale, la p-value de la distorsion ainsi que la statistique de Kurtosis doivent être égale ou plus élevée que 0.05. La fonction split sépare le tableau en listes et la fonction map applique la fonction spécifiée à chaque élément de la liste. Cela permet d’effectuer des tests de multinormalité sur chacune des espèces d’iris.\n\niris %&gt;%\n  split(.$Species) %&gt;%\n  map(~ mvn(.x %&gt;% select(-Species),\n            mvnTest = \"mardia\")$multivariateNormality)\n\n$setosa\n             Test        Statistic           p value Result\n1 Mardia Skewness 25.6643445196298 0.177185884467652    YES\n2 Mardia Kurtosis 1.29499223711605 0.195322907441935    YES\n3             MVN             &lt;NA&gt;              &lt;NA&gt;    YES\n\n$versicolor\n             Test         Statistic           p value Result\n1 Mardia Skewness  25.1850115362466 0.194444483140265    YES\n2 Mardia Kurtosis -0.57186635893429 0.567412516528727    YES\n3             MVN              &lt;NA&gt;              &lt;NA&gt;    YES\n\n$virginica\n             Test         Statistic           p value Result\n1 Mardia Skewness  26.2705981752915 0.157059707690356    YES\n2 Mardia Kurtosis 0.152614173978342 0.878702546726567    YES\n3             MVN              &lt;NA&gt;              &lt;NA&gt;    YES\n\n\nLe test est passé pour toutes les espèces. Voyons maintenant l’homogénéité de la covariance. Pour ce faire, nous aurons besoin de la fonction boxM, disponible avec le module biotools. Pour que les covariances soient considérées comme égales, la p-vaule doit être supérieure à 0.05.\n\nlibrary(\"heplots\")\n\nLoading required package: broom\n\nboxM(iris |&gt; select(-Species),\n     group = iris$Species)\n\n\n    Box's M-test for Homogeneity of Covariance Matrices\n\ndata:  select(iris, -Species)\nChi-Sq (approx.) = 140.94, df = 20, p-value &lt; 2.2e-16\n\n\nOn est loin d’un cas où les distributions sont homogènes. Nous allons néanmoins procéder à l’analyse discriminante avec le module ade4. Nous aurons d’abord besoin d’effectuer une ACP avec la fonction dudi.pca de ade4 (en spécifiant une mise à l’échelle), que nous projetterons en ADL avec discrimin.\n\nlibrary(\"ade4\")\niris_pca &lt;- dudi.pca(df = iris |&gt; select(-Species),\n                     scannf = FALSE, # ne pas générer de graphique\n                     scale = TRUE)\niris_lda &lt;- discrimin(dudi = iris_pca,\n                      fac = iris$Species,\n                      scannf = FALSE)\n\nLa visualisation peut être effectuée directement sur l’objet issu de la fonction discrimin.\n\nplot(iris_lda)\n\n\n\n\n\n\n\nIl s’agit toutefois d’une visualisation pour le diagnostic davantage que pour la publication. Si l’objectif est la publication, vous pourriez utiliser la fonction plotDA que j’ai conçue à cet effet. J’ai aussi conçu une fonction similaire qui utilise le module graphique de base de R.\n\nsource(\"https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R\")\nplotDA(scores = iris_lda$li,\n       loadings = iris_lda$fa,\n       fac = iris$Species,\n       level=0.95,\n       facname = \"Species\",\n       propLoadings = 1) \n\nLoading required package: ellipse\n\n\n\nAttaching package: 'ellipse'\n\n\nThe following object is masked from 'package:car':\n\n    ellipse\n\n\nThe following object is masked from 'package:graphics':\n\n    pairs\n\n\nLoading required package: grid\n\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\nÀ la différence de l’ACP, l’ADL maximise la séparation des groupes. Nous avions noté avec l’ACP que les dimensions des pétales distinguaient les groupes. Puisque nous avions justement des informations sur les groupes, nous aurions pu procéder directement à un ADL pour obtenir des conclusions plus directes. Si la longueur des pétales permet de distinguer l’espèce setosa des deux autres, la largeur des pétales permet de distinguer virginica et versicolor, bien que les nuages de points se superposent. De manière bivariée, les régions de confiance des moyennes des scores discriminants (petites ellipses) montrent des différence significatives au seuil 0.05.\n\nExcercice. Si l’on effectuait l’ADL sur notre dauphin, avec la colonne anatomy comme variable de regroupement, qu’obtiendrions-nous? Si l’on considère la nageoire codale (queue) comme faisant partie du corps? Quelles sont les limitations?\n\n\n10.4.2.2 Analyse de redondance (RDA)\nEn anglais, on la nomme redundancy analysis, souvent abrégée RDA. Elle est utilisée pour résumer les relations linéaires entre des variables réponse et des variables explicatives. La “redondance” se situe dans l’utilisation de deux tableaux de données contenant de l’information concordante. L’analyse de redondance est une manière élégante d’effectuer une régression linéaire multiple, où la matrice de valeurs prédites par la régression est assujettie à une analyse en composantes principales. Il est ainsi possible de superposer les scores des variables explicatives à ceux des variables réponse.\nPlus précisément, une RDA effectue les étapes suivantes (Borcard et al. (2011)) entre une matrice de variables indépendantes (explicatives) \\(X\\) et une matrice de variables dépendantes (réponse) \\(Y\\).\n\n10.4.2.2.1 1. Régression entre \\(Y\\) et \\(X\\)\n\nPour chacune des variables réponse de \\(Y\\) (\\(y_1\\), \\(y_2\\), , \\(y_j\\)), effectuer une régression linéaire sur les variables explicatives \\(X\\).\n\\[\\hat{y}_j = b_j + m_{1, j} \\times x_1 + m_{2, j} \\times x_2 + ... + m_{i, j} \\times x_i\\]\n\\[\\hat{y}_j = y_j + y_{res, j}\\]\nPour chaque observation (\\(n\\)), nous obtenons une série de valeurs de \\(\\hat{y}_j\\) et de \\(y_{res, j}\\). Donc chaque cellule de la matrice \\(Y\\) a ses pendant \\(\\hat{y}\\) et \\(y_{res}\\). Nous obtenons ainsi une matrice de prédiction \\(\\hat{Y}\\) et une matrice des résidus \\(Y_{res} = Y - \\hat{Y}\\).\n\n10.4.2.2.2 2. Analyse en composantes principales\nEnsuite, on effectue une analyse en composantes principales (ACP) sur la matrice des prédictions \\(\\hat{Y}\\). On obtient ainsi ses valeurs et vecteurs propres. Nommons \\(U\\) ses vecteurs propres. Les fonctions de RDA mettent souvent ces vecteurs à l’échelle avant de les retourner à l’utilisateur. En ordination écologique, ces vecteurs mis à l’échelle sont souvent appelés les scores des espèces, bien qu’il ne s’agisse pas nécessairement d’espèces, mais plus généralement des variables de la matrice dépendante \\(Y\\).\nIl est aussi possible d’effectuer une ACP sur \\(Y_{res}\\).\n\n10.4.2.2.3 3. Calculer les scores\nLes vecteurs propres \\(U\\) sont utilisés pour calculer les scores des sites, \\(Y \\times U\\), ainsi que les contraintes de site \\(\\hat{Y} \\times U\\).\n\n10.4.2.2.4 Application\nNous allons utiliser la fonction rda du module vegan. En ce qui a trait aux données, utilisons les données varespec (matrice Y) et varechem (matrice X). La fonction rda peut fonctionner avec l’interface-formule de R, où à gauche du ~ on retrouve le Y (la matrice de la communauté écologique, i.e. les abondances d’espèces) contre le X (l), à gauche, ce qui peut être pratique pour l’analyse d’interactions. Mais pour comparer deux matrices, nous pouvons définir X et Y. Ce qui est mélangeant, c’est que vegan, contrairement aux conventions, défini X comme étant la matrice réponse et Y comme étant la matrice explicative.\n\nvare_rda &lt;- rda(X = varespec, Y = vareclr, scale = FALSE)\npar(mfrow = c(1, 2))\nordiplot(vare_rda, scaling = 1, type = \"text\", main = \"Scaling 1: triplot de distance\")\nordiplot(vare_rda, scaling = 2, type = \"text\", main = \"Scaling 2: triplot de corrélation\")\n\n\n\n\n\n\n\nLa fonction ordiplot permet de créer un triplot de base. La représentation des wascores est réputée plus robuste (moins susceptible d’être bruitée), mais leur interprétation porte à confusion (Borcard et al. (2011)).\nTriplot de distance (scaling 1). Les angles entre les variables explicatives représentent leur corrélation (non pas les variables réponse).\nTriplot de corrélation (scaling 2). Les angles entre les variables représentent leurs corrélation, que les variables soient réponse ou explicative, ou entre variables réponses et variables explicatives. Les distances entre les objets sur le triplot ne sont pas des approximation de leur distance euclidienne.\nLes triplots montrent que les variables ont toutes un rôle important sur la dispersion des sites autour des axes principaux. Le premier axe principal est composé de manière plus marquée par le clr de l’Al et celui du Fe. Le deuxième axe principal est composé de manière plus marquée par le clr du S, du P et du K. Le triplot de corrélation ne présente pas de tendance appréciable pour la plupart des espèces, qui ne possèdent pas de niche particulière. Toutefois, l’espèce Cladstel, présente surtout dans les sites 9 et 10, est liée à de basses teneurs en N et à de faibles valeurs de Baresoil (sol nu). L’espèce Pleuschr est liée à des sols où l’on retrouve une grande épaisseur d’humus, ainsi que des teneurs élevées en nutriment K, P, S, Ca, Mg et Zn. Elle semble apprécier les sols à bas pH, mais à faible teneur en Fe et Al. La teneur en N lui semble plus indifférente (son vecteur étant presque perpendiculaire).\nOn pourra personnaliser les graphiques en extrayant les scores.\n\nscaling &lt;- 2\nsites &lt;- vegan::scores(vare_rda, display = \"wa\", scaling = scaling)\nspecies &lt;- vegan::scores(vare_rda, display = \"species\", scaling = scaling)\nenv &lt;- vegan::scores(vare_rda, display = \"reg\", scaling = scaling)\n\nplot(0, 0, type = \"n\", xlim = c(-3, 5), ylim = c(-3, 4), asp = 1)\nabline(h=0, v = 0, col = \"grey80\")\ntext(sites/2, labels = rownames(sites), cex = 0.7, col = \"grey50\")\ntext(species/2, labels = rownames(species), col = \"green\", cex = 0.7)\nsegments(x0 = 0, y0 = 0, x1 = env[, 1], y1 = env[, 2], col = \"blue\")\ntext(env, labels = rownames(env), col = \"blue\", cex = 1)\n\n\n\n\n\n\n\nOn pourra effectuer une analyse de Kaiser-Guttmann ou de broken-stick de la même manière que précédemment. Étant une collection de régressions, une RDA est en mesure d’effectuer des tests statistiques sur les coefficients de la régression en utilisant des permutations pour tester la signification des coefficients et des axes d’une RDA. On doit néanmoins obligatoirement effectuer la RDA avec l’interface formule. La variable à gauche du ~ peut être une matrice ou un tableau, et celui de droite est défini dans data. Le . dans l’interface formule signifie “une combinaison linéaire de toutes les variables, sans interaction”.\n\nvare_rda &lt;- rda(varespec ~ ., data = vareclr, scale = FALSE)\nperm_test_term &lt;- anova(vare_rda, by = \"term\")\n#perm_test_axis &lt;- anova(vare_rda, by = \"axis\")\n\nLa signification des axes est difficile à interpréter. Toutefois, celui des variables présente un intérêt.\n\nperm_test_term\n\nPermutation test for rda under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nModel: rda(formula = varespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Fv + Baresoil + Humdepth + pH, data = vareclr, scale = FALSE)\n         Df Variance      F Pr(&gt;F)   \nN         1   216.13 4.8470  0.006 **\nP         1   272.71 6.1159  0.005 **\nK         1   194.97 4.3724  0.012 * \nCa        1    24.92 0.5589  0.661   \nMg        1    52.61 1.1799  0.310   \nS         1   100.07 2.2441  0.089 . \nAl        1   177.91 3.9900  0.008 **\nFe        1   118.59 2.6595  0.050 * \nMn        1    25.96 0.5822  0.638   \nZn        1    35.81 0.8030  0.483   \nMo        1    23.51 0.5273  0.678   \nBaresoil  1    98.64 2.2122  0.103   \nHumdepth  1    43.59 0.9777  0.393   \npH        1    38.93 0.8730  0.447   \nResidual  9   401.31                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa p-value est la probabilité que les pentes calculées pour les variables émergent de distributions dont la moyenne est nulle. Au seuil 0.05, les variables significatives sont (les clr de) l’azote, le phosphore, le potassium et l’aluminium.\nDans le cas des matrices d’abondance (ce n’est pas le cas de varespec, constituée de données de recouvrement), il est préférable avec les RDA de les transformer préalablement avec la transformation compositionnelle, de chord ou de Hellinger (chapitre 9). Une autre option est d’effectuer une RDA sur des matrices d’association en passant par une analyse en coordonnées principales (Legendre et Anderson, 1999). Enfin, les données d’abondance à l’état brutes devraient plutôt passer utiliser une analyse canonique des corrélations.\n\n10.4.2.3 Analyse canonique des correspondances (ACC)\nL’analyse canonique des correspondances (Canonical correspondance analysis), ACC, a été à l’origine conçue pour étudier les liens entre des variables environnementales et l’abondance (décompte) ou l’occurrence (présence-absence) d’espèces (ter Braak, 1986). L’ACC est à la RDA ce que la CA est à l’ACP. Alors que la RDA préserve les distance euclidiennes entre variables dépendantes et indépendantes, l’ACC préserve les distances du \\(\\chi^2\\). Tout comme l’AC, elle hérite du coup une propriété importante de la distance du \\(\\chi^2\\): il y a davantage davantage d’importance aux espèces rares.\nL’analyse des correspondances canoniques est souvent utilisée dans la littérature, mais dans bien des cas une RDA sur des données d’abondance transformées donnera des résultats davantage interprétables (Legendre et Gallagher, 2001).\n\n10.4.2.3.1 Application\nCet exemple d’application concerne des données d’abondance. Nous allons conséquemment utiliser une CCA avec la fonction cca, toujours avec le module vegan.\nLes tableaux doubs_fish et doubs_env comprennent respectivement des données d’abondance d’espèces de poissons et dans différents environnements de la rivière Doubs (Europe) publiées dans Verneaux. (1973) et exportées du module ade4.\n\ndata(\"doubs\")\ndoubs_fish &lt;- doubs$fish\ndoubs_env &lt;- doubs$env\n\nSur le site no 8, aucun poisson n’a pas été observé. Les observations ne comprenant que des zéro doivent être préalablement retirées.\n\ntot_spec &lt;- doubs_fish %&gt;%\n  transmute(tot_spec = apply(., 1, sum))\ndoubs_fish &lt;- doubs_fish %&gt;%\n  filter(tot_spec != 0)\n\nWarning: Using one column matrices in `filter()` was deprecated in dplyr 1.1.0.\nℹ Please use one dimensional logical vectors instead.\n\ndoubs_env &lt;- doubs_env %&gt;%\n  filter(tot_spec != 0)\n\nDe la même manière qu’avec la fonction rda de vegan, nous utilisons cca pour l’ACC.\n\ndoubs_cca &lt;- cca(doubs_fish ~ ., data = doubs_env, scale = FALSE)\n\nComparons les résultats\n\npar(mfrow = c(1, 2))\nordiplot(doubs_cca, scaling = 1, type = \"text\", main = \"CCA - Scaling 1 - Triplot de distance\")\nordiplot(doubs_cca, scaling = 2, type = \"text\", main = \"CCA - Scaling 2 - Triplot de corrélation\")\n\n\n\n\n\n\n\nTriplot de distance (scaling 1).\n\n\nLa projection des variables réponse à angle droit sur les variables explicatives est une approximation de la réponse sur l’explication. (2) Un objet (site ou réponse) situé près d’une variable explicative est plus susceptible d’avoir le décompte 1. (3) Les distances entre les variables (réponse et explicatives) approximent la distance du \\(\\chi^2\\) (traduction adaptée de Borcard et al. (2011)).\n\n\nTriplot de corrélation (scaling 2).\n\n\nLa valeur optmiale de l’espèce sur une variable environnementale quantitative peut être obtenue en projetant l’espèce à angle droit sur la variable. (2) Une espèce se trouvant près d’une variable environnementale est susceptible de se trouver en plus grande abondance aux sites de statut 1 pour cette variable. (3) Les distances n’approximent pas la distance du \\(\\chi^2\\) (traduction adaptée de Borcard et al. (2011))."
  }
]