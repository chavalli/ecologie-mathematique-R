[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyse et modélisation d’agroécosystèmes sur R",
    "section": "",
    "text": "Préface\nCe cours a pour objectif de former les étudiants gradués en génie agroenvironnemental, génie civil, génie écologique, agronomie, biologie, foresterie et écologie en analyse et modélisation de systèmes vivants. Les sujets traités sont l’introduction au langage de programmation R, l’analyse statistique descriptive, la visualisation, la modélisation inférentielle, prédictive et déterministe.\nCe manuel est basé sur le cours Analyse et modélisation d’agroécosystèmes de Essi Parent (voir licence au bas de la page). La version proposée ici tient compte des mises à jour des différents outils présentés dans le manuel original. Elle est construite au format Quarto par Charles Frenette-Vallières et Andrés Felipe Silva Dimaté\nVoici la liste des modifications principales apportées jusqu’à présent par rapport à la version originale :"
  },
  {
    "objectID": "index.html#table-des-matières",
    "href": "index.html#table-des-matières",
    "title": "Analyse et modélisation d’agroécosystèmes sur R",
    "section": "Table des matières",
    "text": "Table des matières\n\nIntroduction\nLa science des données avec R\nOrganisation des données et opérations sur des tableaux\nVisualisation\nScience ouverte et reproductibilité\nIntroduction à Python\nBiostatistiques\nIntroduction à l’analyse bayésienne en écologie\nExplorer R\nAssociation, partitionnement et ordination\nDétection de valeurs aberrantes et imputation de données manquantes\nLes séries temporelles\nIntroduction à l’autoapprentissage\nLes données géospatiales\nModélisation de mécanismes écologiques\n\n\nAnalyse et modélisation d’agroécosystèmes de Essi Parent est mis à disposition selon les termes de la licence Creative Commons Attribution - Pas d’Utilisation Commerciale - Partage dans les Mêmes Conditions 4.0 International\n\nFondé(e) sur une œuvre à https://github.com/essicolo/ecologie-mathematique-R/."
  },
  {
    "objectID": "01-intro.html#définitions",
    "href": "01-intro.html#définitions",
    "title": "1  Introduction",
    "section": "\n1.1 Définitions",
    "text": "1.1 Définitions\nLes mathématiques confèrent aux humains une capacité d’abstraction suffisamment complexe pour leur permettre de toucher les étoiles et les atomes, de comprendre le passé et de prédire le futur, de toucher l’infini et de goûter à l’éternité. À partir des maths, on a pu créer des outils de calcul qui permettent de projeter des images de l’univers, bien au-delà de la Voie lactée. Mais appréhender le vivant, tout près de nous, demeure une tâche complexe.\n\n\n\n\nFigure 1.2: Domaines scientifiques de l’écologie mathématique.\n\n\n\nL’écologie mathématique couvre un large spectre de domaines (Figure 1.2), mais peut être divisée en deux branches: l’écologie théorique et l’écologie quantitative (Legendre et Legendre, 2012). Alors que l’écologie théorique s’intéresse à l’expression mathématique des mécanismes écologiques, l’écologie quantitative, plus empirique, en étudie principalement les phénomènes. La modélisation écologique vise à prévoir une situation selon des conditions données. Faisant partie à la fois de l’écologie théorique et de l’écologie quantitative, elle superpose souvent des mécanismes de l’écologie théorique et des phénomènes empiriques de l’écologie quantitative. L’écologie numérique comprend la branche descriptive de l’écologie quantitative, c’est-à-dire qu’elle s’intéresse à évaluer des effets à partir de données empiriques. L’exploration des données dans le but d’y découvrir des structures passe souvent par des techniques multivariées comme la classification hiérarchique ou la réduction d’axe (par exemple, l’analyse en composantes principales), qui sont davantage heuristiques (dans notre cas, bioheuristique) que statistiques. Les tests d’hypothèses et l’analyse des probabilités, quant à eux, relèvent de la biostatistique.\nLe génie écologique, une discipline intimement liée à l’écologie mathématique, est voué à l’analyse, la modélisation, la conception et la construction de systèmes vivants dans le but de résoudre de manière efficace des problèmes liés à l’écologie et à une panoplie de domaines qui lui sont raccordés. L’agriculture est l’un de ces domaines. C’est d’emblée la discipline qui sera prisée dans ce manuel. Néanmoins, les principes qui seront discutés sont transférables à l’écologie générale."
  },
  {
    "objectID": "01-intro.html#à-qui-sadresse-ce-manuel",
    "href": "01-intro.html#à-qui-sadresse-ce-manuel",
    "title": "1  Introduction",
    "section": "\n1.2 À qui s’adresse ce manuel?",
    "text": "1.2 À qui s’adresse ce manuel?\nLe cours vise à introduire des étudiant.e.s gradué.e.s en agronomie, biologie, écologie, sols, génie agroenvironnemental, génie civil et génie écologique à l’analyse et la modélisation dans leur domaine, tant pour les appuyer pour leurs travaux de recherche que pour leur fournir une trousse d’outils émancipatrice pour leur cheminement professionnel. Plus spécifiquement, vous serez accompagné à découvrir différents outils numériques qui vous permettront d’appréhender vos données, d’en faire émerger l’information et de construire des modèles. L’objectif de ce cours n’est pas de vous former en mathématiques, mais de vous aider à les utiliser. En ce sens, c’est un cours de pilotage, pas un cours de mécanique. Vous ferez tout de même un peu de mécanique pour mieux comprendre les réactions de notre machine.\nBien que des connaissances en programmation et en statistiques aideront grandement les étudiant.e.s à appréhender ce document, une littératie informatique n’est pas requise. Dans tous les cas, quiconque voudra tirer profit de ce manuel devra faire preuve d’autonomie. Vous serez guidés vers des ressources et des références, mais je vous suggère vivement de développer votre propre bibliothèque adaptée à vos besoins et à votre manière de comprendre."
  },
  {
    "objectID": "01-intro.html#les-logiciels-libres",
    "href": "01-intro.html#les-logiciels-libres",
    "title": "1  Introduction",
    "section": "\n1.3 Les logiciels libres",
    "text": "1.3 Les logiciels libres\nTous les outils numériques qui sont proposés dans ce cours sont des logiciels libres:\n\n« Logiciel libre » [free software] désigne des logiciels qui respectent la liberté des utilisateurs. En gros, cela veut dire que les utilisateurs ont la liberté d’exécuter, copier, distribuer, étudier, modifier et améliorer ces logiciels. Ainsi, « logiciel libre » fait référence à la liberté, pas au prix1 (pour comprendre ce concept, vous devez penser à « liberté d’expression », pas à « entrée libre »). - Projet GNU\n\nDonc: codes sources ouverts, développement souvent communautaire, gratuité. Plusieurs raisons éthiques, principalement liées au contrôle de l’environnement virtuel par les utilisateurs et les communautés, peuvent justifier l’utilisation de logiciels libres. Plusieurs raisons pratiques justifient aussi cette orientation. Les logiciels libres vous permettent de transporter vos outils avec vous, d’une entreprise à l’autre, au bureau, ou à la maison, et ce, sans vous soucier d’acheter de coûteuses licences.\nIl existe tout de même des risques liés aux possibles erreurs dans les codes des logiciels communautaires. Ces risques sont d’ailleurs les mêmes que ceux liés aux logiciels propriétaires. Pour les scientifiques, une erreur peut mener à une étude retirée de la littérature et même, potentiellement, des politiques publiques mal avisées. Pour les ingénieurs, les conséquences pourraient être dramatiques. Mais retenez qu’en toute circonstance, comme professionnel.le, vous êtes responsable des outils que vous utilisez: vous devez vous assurer de la bonne qualité d’un logiciel, qu’il soit propriétaire ou communautaire.\nAlors que la qualité des logiciels propriétaires est généralement suivie par audits, celle des logiciels libres est plutôt soumise à la vigilance communautaire. Chaque approche a ses avantages et inconvénients, mais elles ne sont pas exclusives. Ainsi, les logiciels libres peuvent être audités à l’externe par quiconque décide de le faire. Différentes entreprises, souvent concurrentes, participent tant à cette vigilance qu’au développement des logiciels libres: elles en sont même souvent les instigatrices (comme RStudio, Anaconda et Enthought).\nPar ailleurs, ce manuel est distribué librement sous licence Creative commons, selon les termes suivants.\n\n\nAnalyse et modélisation d’agroécosystèmes de Essi Parent est mis à disposition selon les termes de la licence Creative Commons Attribution - Pas d’Utilisation Commerciale - Partage dans les Mêmes Conditions 4.0 International\n\nFondé(e) sur une œuvre à https://github.com/essicolo/ecologie-mathematique-R/."
  },
  {
    "objectID": "01-intro.html#langage-de-programmation",
    "href": "01-intro.html#langage-de-programmation",
    "title": "1  Introduction",
    "section": "\n1.4 Langage de programmation",
    "text": "1.4 Langage de programmation\n\n1.4.1 R\nCe cours est basé sur le langage R. En plus d’être libre, R est un langage de programmation dynamique largement utilisé dans le monde universitaire, et dont l’utilisation s’étend de manière soutenue hors des tours d’ivoire.\n\nR is also the name of a popular programming language used by a growing number of data analysts inside corporations and academia. It is becoming their lingua franca partly because data mining has entered a golden age, whether being used to set ad prices, find new drugs more quickly or fine-tune financial models. New York Times, janvier 2009\n\nSon développement est supporté par la R Foundation for Statistical Computing, basée à l’Université de Vienne. Également, l’équipe de RStudio contribue largement au développement de modules génériques. R est principalement utilisé pour le calcul statistique, mais les récents développements le rendent un outil de choix pour tout ce qui entoure la science des données, de l’interaction avec les bases de données au déploiement d’outils d’intelligence artificielle en passant par la visualisation. Une fois implémenté avec des modules de calcul scientifique spécialisés en biologie, en écologie et en agronomie (que nous couvrirons au long du cours), R devient un outil de calcul convivial, rapide et fiable.\n\n1.4.2 Pourquoi pas Python?\nLa première mouture de ce cours se fondait sur le langage Python. Tout comme R, Python est un langage de programmation dynamique prisé pour le calcul scientifique. Python est un langage générique apprécié pour sa polyvalence et sa simplicité. Python est utilisé autant pour créer des logiciels ou des sites web que pour le calcul scientifique. Ainsi, Python peut être utilisé en interopérabilité avec une panoplie de logiciels libres, comme QGIS pour la cartographie et FreeCAD pour le dessin technique. Il est particulièrement apprécié en ingénierie pour ses modules de calcul par éléments finis (e.g. FeNICS) et en bioinformatique pour ses outils liés au séquençage (scikit-bio), mais ses lacunes en analyse statistique, en particulier en statistiques multivariées m’ont amené à favoriser R.\nBien que leurs possibilités se superposent largement, ce serait une erreur d’aborder R et Python comme des langages rivaux. Les deux langages s’expriment de manière similaire et s’inspirent mutuellement: apprendre à travailler avec l’un revient à apprendre l’autre. Les spécialistes en calcul scientifique tendent à apprendre à travailler avec plus d’un langage de programmation. Par ailleurs, il existe de plus en plus des moyens de travailler en R et en Python dans un même flux de travail. L’interface de calcul RStudio, que nous utiliserons pendant le cours, permet d’inclure des blocs de code en Python.\nDans la version mise à jour du manuel, une courte introduction facultative à Python est proposée.\n\n1.4.3 Pourquoi pas Matlab?\nParce qu’on est en 2024.\n\n1.4.4 Et… SAS?\nParce qu’on est à l’université.\n\n1.4.5 Mais pourquoi pas ______ ?\nD’autres langages, comme Julia, Scala, Javascript et même Ruby sont utilisés en calcul scientifique. Ils sont néanmoins moins garnis et moins documentés que R. Des langages de plus bas niveau, comme Fortran et C++, viennent souvent appuyer les fonctions des autres langages: ces langages sont plus ardus à utiliser au jour le jour, mais leur rapidité de calcul est imbattable."
  },
  {
    "objectID": "01-intro.html#contenu-du-manuel",
    "href": "01-intro.html#contenu-du-manuel",
    "title": "1  Introduction",
    "section": "\n1.5 Contenu du manuel",
    "text": "1.5 Contenu du manuel\nJe favorise une approche intuitive aux développements mathématiques. Nous aborderons l’analyse et la modélisation inférentielle, prédictive et mécanistique appliquée aux agroécosystèmes.\nChapitre 2 - Introduction au langage de programmation R. Qu’est-ce que R? Comment l’aborder? Quelles sont les fonctionnalités de base et comment tirer profit de tout l’écosystème de programmation?\nChapitre 3 - Organisation des données et opérations sur des tableaux. Les tableaux permettent d’enchâsser l’information dans un format prêt-à-porter pour R. Comment les importer, les exporter, les filtrer, et en faire des sommaires?\nChapitre 4 - Visualisation. Comment présenter l’information contenue dans un long tableau en un seul coup d’oeil?\nChapitre 5 - Le travail collaboratif, le suivi de version et la science ouverte. Ce chapitre offre une introduction à l’utilisation des outils de calcul collaboratif, ainsi qu’un aperçu du système de suivi de version git et de son utilisation sur GitHub.\nChapitre 6 - Introduction à Python (section facultative). Une très brève introduction au langage de programmation Python. Ce contenu est externe au cours et est là pour vous fournir des références si vous souhaitez explorer ce langage dans le futur.\nChapitre 7 - Biostatistiques. Il est audacieux de ne consacrer qu’un seul chapitre sur ce vaste sujet. Nous irons à l’essentiel… pour vous donner les outils qui permettront d’approfondir le sujet.\nChapitre 8 - Biostatistiques bayésiennes (section facultative). Une très brève introduction pour qui est intéressé à l’analyse bayésienne.\nChapitre 9 - Explorer R. La science des données évolue rapidement. Vous gagnerez à vous tenir au courrant de son évolution, et immanquablement vous vous buterez sur des opérations qui vous sembleront insolubles. Ce chapitre vous accompagnera à rester à jour sur le développement de R, à poser de bonnes questions et proposera des modules intéressants en écologie mathématique.\nChapitre 10 - Association, partitionnement et ordination. Les écosystèmes diffèrent, mais en quoi sont-ils semblables, et en quoi dffèrent-ils? Ces questions importantes peuvent être abordés par l’écologie numérique, domaine d’étude au sein duquel l’association, le partitionnement et l’ordination sont des outils prédominants.\nChapitre 11 - Détection de valeurs aberrantes et imputation. Une donnée aberrante sortira du lot, pour une raison ou pour une autre. Comment les détecter de manière systématique? D’autre part, que faire lorsqu’une donnée est manquante? Peut-on l’imputer? Comment?\nChapitre 12 - Les séries temporelles. Les capteurs modernes permettent de générer des données en fonction du temps. Que ce soit des données météorologiques enregistrées quotidiennement ou des données de teneur en eau enregistrées au 5 secondes, les données en fonction du temps forment un signal. Comment analyser ces signaux?\nChapitre 13 - L’autoapprentissage. Les applications de l’intelligence artificielle ne sont limitées que par votre imagination. Encore faut-il l’utiliser… intelligemment.\nChapitre 14 - Les données spatiales. Ce chapitre porte sur l’utilisation de R comme système d’information géographique de base. Nous utiliserons aussi l’autoapprentissage et les modèles déterministes comme outils d’interpolation spatial.\nChapitre 15 - La modélisation mécanistique (section facultative). Les modèles sont des maquettes simplifiées. Comment utiliser les équations différentielles ordinaires pour créer ces maquettes?\nSi les chapitres 3 à 5 peuvent être considérés comme fondamentaux pour bien maîtriser R, les autres peuvent être feuilletés à la pièce, bien qu’ils forment une suite logique.\nChaque chapitre de ce manuel est rédigé en format Quarto, dans un environnement RStudio. Pour exécuter les commandes, vous pourrez soit les copier-coller dans R (ou RStudio), soit télécharger les fichiers-sources et exécuter les blocs de code.\nLe manuel original était rédigé au format R Markdown et est toujours disponible à l’adresse suivante."
  },
  {
    "objectID": "01-intro.html#objectifs-généraux",
    "href": "01-intro.html#objectifs-généraux",
    "title": "1  Introduction",
    "section": "\n1.6 Objectifs généraux",
    "text": "1.6 Objectifs généraux\nÀ la fin du cours, vous serez en mesure:\n\nde programmer en langage R\nd’importer, de manipuler (sélection des colonnes, filtres, sommaires statistiques) et d’exporter des tableaux\nde générer des graphiques d’utilisation commune\nde vous assurer que vos calculs soient auditables et reproductibles dans une perspective de science ouverte\nd’appréhender des données écologiques et agronomiques à l’aide de tests statistiques fréquentiels\nd’explorer par vous-même les possibilités offertes par la communauté de développement de modules R\nd’explorer les données à l’aide des outils de l’écologie numérique (association, partitionnement et ordination)\nd’imputer des données manquantes dans un tableau et de détecter des valeurs aberrantes\nde créer un modèle d’autoapprentissage\nd’effectuer une analyse de série temporelle\nd’interpoler des données spatiales\nde modéliser des équations différentielles ordinaires"
  },
  {
    "objectID": "01-intro.html#lectures-complémentaires",
    "href": "01-intro.html#lectures-complémentaires",
    "title": "1  Introduction",
    "section": "\n1.7 Lectures complémentaires",
    "text": "1.7 Lectures complémentaires\n\n1.7.1 Écologie mathématique\n\n\nHow to be a quantitative ecologist. Jason Mathipoulos vous prend par la main pour découvrir les notions de mathématiques fondamentales en écologie, appliquées avec le langage R.\n\n\nNumerical ecology. L’ouvrage hautement détaillé des frères Legendre est non seulement fondamental, mais aussi fondateur d’une science qui évolue encore aujourd’hui: l’analyse des données écologiques.\n\nA practical guide to ecological modelling. Soetaert et Herman portent une attention particulière à la présentation des principes de modélisation dans un langage accessible - ce qui est rarement le cas dans le domaine de la modélisation. Les modèles présentés concernent principalement les bilans de masse, en termes de systèmes de réactions chimiques et de relations biologiques.\n\nModélisation mathématique en écologie. Rare livre en modélisation écologique publié en français, la première partie s’attarde aux concepts mathématiques, alors que la deuxième planche à les appliquer. Si le haut niveau d’abstraction de la première partie vous rebute, n’hésitez pas débuter par la seconde partie et de vous référer à la première au besoin.\n\nA new ecology: systems perspective. Principalement grâce au soleil, la Terre forme un ensemble de gradients d’énergie qui se déclinent en des systèmes d’une étonnante complexité. C’est ainsi que le regretté Sven Erik Jørgensen (1934-2016, Figure 1.3)) et ses collaborateurs décrivent les écosystèmes dans cet ouvrage qui fait suite aux travaux fondateurs de Howard Thomas Odum.\nEcological engineering. Principle and Practice.\nEcological processes handbook.\nModeling complex ecological dynamics\n\n\n\n\n\nFigure 1.3: Sven Erik Jørgensen, Source: Elsevier.\n\n\n\n\n1.7.2 Programmation\n\n\nR for data science (2e). L’analyse de données est une branche importante de l’écologie mathématique. Ce manuel traite des matrices et la manipulation de données chapitre 3), de la visualisation (chapitre 4) ainsi que de l’apprentissage automatique (chapitre 14). R for data science (2e) repasse ces sujets plus en profondeur. En particulier, l’ouvrage de Garrett Grolemund, Hadley Wickham et Mine Çetinkaya-Rundel offre une introduction au module graphique ggplot2 et à tidyverse.\n\nNumerical ecology with R. Daniel Borcard enseigne l’écologie numérique à l’Université de Montréal. Son cours est condensé dans ce livre recettes voué à l’application des principes lourdement décrits dans Numerical ecology.\n\n1.7.3 Divers\n\n\nThe truthful art. Dans cet ouvrage, Alberto Cairo s’intéresse à l’utilisation des données et de leurs présentations pour fournir une information adéquate à différents publics."
  },
  {
    "objectID": "01-intro.html#besoin-daide",
    "href": "01-intro.html#besoin-daide",
    "title": "1  Introduction",
    "section": "\n1.8 Besoin d’aide?",
    "text": "1.8 Besoin d’aide?\nLes ouvrages de référence reconnus vous offrent des bases solides sur lesquelles vous pouvez vous appuyer dans vos travaux. Mais au-delà des principes, au jour le jour, vous vous buterez immanquablement à toutes sortes de petits problèmes. Quel module utiliser pour cette tâche précise? Que veut dire ce message d’erreur? Comment interpréter ce résultat? Pour tous les petits accrocs du quotidien en calcul scientifique, internet offre de nombreuses ressources qui sont très hétérogènes en qualité. Vous apprendrez à reconnaître les ressources fiables à celles qui sont douteuses. Les plateformes basées sur Stack Exchange, comme Stack Overflow et Cross Validated, m’ont souvent été d’une aide précieuse. Vous aurez avantage à vous construire une petite banque d’information avec un logiciel de prise de notes en collectant des liens, en prenant en notes certaines recettes et en suivant des sites d’intérêt avec des flux RSS."
  },
  {
    "objectID": "01-intro.html#à-propos-de-lauteur",
    "href": "01-intro.html#à-propos-de-lauteur",
    "title": "1  Introduction",
    "section": "\n1.9 À propos de l’auteur",
    "text": "1.9 À propos de l’auteur\nJe m’appelle Essi Parent. Je suis ingénieur écologue et professeur adjoint au Département des sols et de génie agroalimentaire de l’Université Laval, Québec, Canada. Je crois que la science est le meilleur moyen d’appréhender le monde pour prendre des décisions avisées."
  },
  {
    "objectID": "01-intro.html#un-cours-complémentaire-à-dautres-cours",
    "href": "01-intro.html#un-cours-complémentaire-à-dautres-cours",
    "title": "1  Introduction",
    "section": "\n1.10 Un cours complémentaire à d’autres cours",
    "text": "1.10 Un cours complémentaire à d’autres cours\nCe cours a été développé pour ouvrir des perspectives mathématiques en écologie et en agronomie à la FSAA de l’Université Laval. Il est complémentaire à certains cours offerts dans d’autres institutions académiques au Québec, dont ceux-ci.\n\n\nBIO2041. Biostatistiques 1, Université de Montréal\n\nBIO2042. Biostatistiques 2, Université de Montréal\n\nBIO109. Introduction à la programmation scientifique, Université de Sherbrooke\n\nBIO500. Méthodes en écologie computationnelle, Université de Sherbrooke."
  },
  {
    "objectID": "01-intro.html#contribuer-au-manuel",
    "href": "01-intro.html#contribuer-au-manuel",
    "title": "1  Introduction",
    "section": "\n1.11 Contribuer au manuel",
    "text": "1.11 Contribuer au manuel\nJe suis ouvert aux commentaires et suggestions. Pour contribuer directement, dirigez-vous sur le dépôt du manuel sur GitHub, puis ouvrez une Issue pour en discuter. Créez une nouvelle branche (fork), effectuez les modifications, puis lancer une requête de fusion (pull request)."
  },
  {
    "objectID": "02-R.html#statistiques-ou-science-des-données",
    "href": "02-R.html#statistiques-ou-science-des-données",
    "title": "2  La science des données avec R",
    "section": "\n2.1 Statistiques ou science des données?",
    "text": "2.1 Statistiques ou science des données?\nSelon Whitlock et Schluter (2015), la statistique est l’étude des méthodes pour décrire et mesurer des aspects de la nature à partir d’échantillon. Pour Grolemund et Wickham (2023), la science des données est une discipline excitante permettant de transformer des données brutes en compréhension, perspectives et connaissances. Oui, excitante! La différence entre les deux champs d’expertise est subtile, et certaines personnes n’y voient qu’une différence de ton.\n\n\n\nData Science is statistics on a Mac.\n\n— Big Data Borat (@BigDataBorat) 27 août 2013\n\nConfinées à ses applications traditionnelles, les statistiques sont davantage vouées à la définition de dispositifs expérimentaux et à l’exécution de tests d’hypothèses, alors que la science des données est moins linéaire, en particulier dans sa phase d’analyse, où de nouvelles questions (donc de nouvelles hypothèses) peuvent être posées au fur et à mesure de l’analyse. Cela arrive généralement davantage lorsque l’on fait face à de nombreuses observations sur lesquelles de nombreux paramètres sont mesurés.\nLa quantité de données et de mesures auxquelles nous avons aujourd’hui accès grâce aux technologies de mesure et de stockage relativement peu dispendieux rend la science des données une discipline particulièrement attrayante, pour ne pas dire sexy."
  },
  {
    "objectID": "02-R.html#débuter-en-r",
    "href": "02-R.html#débuter-en-r",
    "title": "2  La science des données avec R",
    "section": "\n2.2 Débuter en R",
    "text": "2.2 Débuter en R\nR est un langage de programmation dérivé du langage S, qui fut initialement lancé en 1976.\n\n\n\n\nFigure 2.2: Logo officiel du language R.\n\n\n\nR figure parmi les langages de programmation les plus utilisés au monde. Bien qu’il soit basé sur les langages statiques C et Fortran, R est un langage dynamique, c’est-à-dire que le code peut être exécuté ligne par ligne ou bloc par bloc: un avantage majeur pour des activités qui nécessitent des interactions fréquentes. Bien que R soit surtout utilisé pour le calcul statistique, il s’impose de plus en plus comme outil privilégié en sciences des données en raison des récents développements de modules d’analyse, de modélisation et de visualisation, dont plusieurs seront utilisés dans ce manuel.\nUn langage de programmation s’apprend un peu comme une langue. Au début, un code R peut sembler incompréhensible. Et face à son clavier, on ne sait pas trop comment exprimer ce que l’on désire. Au fur et à mesure de l’apprentissage, les symboles, les fonctions et le style deviennent de plus en plus familiers et on apprend tranquillement à traduire en code ce que l’on désire effectuer. Comme une langue s’apprend en la parlant dans la vie de tous les jours, un language de programmation s’apprend avantageusement en solutionnant vos propres problèmes (Figure 2.3).\n\n\n\n\nFigure 2.3: R avant et maintenant, Illustration de Allison Horst"
  },
  {
    "objectID": "02-R.html#préparer-son-flux-de-travail",
    "href": "02-R.html#préparer-son-flux-de-travail",
    "title": "2  La science des données avec R",
    "section": "\n2.3 Préparer son flux de travail",
    "text": "2.3 Préparer son flux de travail\nIl existe de nombreuses manières d’utiliser R. Parmi celles-ci, j’en couvrirai 3:\n\nInstallation classique (installation suggérée)\nInstallation avec Anaconda\nUtilisation infonuagique\n\n\n2.3.1 Installation classique\nInstallation suggérée. Sur Windows ou Mac, dirigez-vous ici, téléchargez et installez. Sur Linux, ouvrez votre gestionnaire d’application, chercher r-base (Ubuntu, Debian), R-base (openSuse) ou R-core (Fedora) et installez-le (assurez-vous que les librairies suivantes sont aussi installées: gcc, gcc-fortran, gcc-c++ et make), vous aurez peut-être besoin d’installer des librairies supplémentaires pour faire fonctionner certains modules.\n\nNote. Les modules présentés dans ce cours devraient être disponibles sur Linux, Windows et Mac. Ce n’est pas le cas pour tous les modules R. La plupart fonctionnent néanmoins sur Linux, dont les systèmes d’opération (je recommande Ubuntu ou l’une de ses dérivées comme elementary OS) sont de bonnes options pour le calcul scientifique.\n\nÀ cette étape, R devrait fonctionner dans un interpréteur de commande . Si vous lancez R dans un terminal (chercher cmd dans le menu si vous êtes sur Windows), vous obtiendrez quelque chose comme ceci.\n\n\n\n\nFigure 2.4: R dans le terminal.\n\n\n\nLe symbole &gt; indique que R attend que vos instructions. Vous voilà dans un état méditatif devant l’indéchiffrable vide du terminal 😵. Ne vous en faites pas: nous commencerons bientôt à jaser avec R.\nAvant cela, installons-nous au salon. Afin de travailler dans un environnement de travail plus confortable, je recommande l’installation de l’interface RStudio, gratuite et open source: téléchargez l’installateur et suivez les instructions. RStudio ressemble à ceci.\n\n\n\n\nFigure 2.5: Fenêtre de RStudio.\n\n\n\nEn haut à droite se trouve un menu Project (None). Il s’agit d’un menu de vos projets. Je recommande d’utiliser ces projets avec RStudio, qui vous permettront de mieux gérer vos sessions de travail, en particulier en lien avec les chemins vers vos données, graphiques, etc., que vous pouvez gérer relativement à l’emplacement de votre dossier de projet plutôt qu’à l’emplacement des fichiers sur votre machine: nous verrons plus en détails au chapitre 5.\n\nEn haut à gauche, vous avez vos feuilles de calcul, qui apparaîtront en tant qu’onglets. Une feuille de calcul R script est une série de commandes que vous lancez en séquence. Il peut aussi s’agir d’un document Quarto si vous choisissez de travailler ainsi. Ce format vous permettra de d’écrire du texte en format Markdown entre des blocs de code. Il est question du format Quarto au chapitre 5).\nEn bas à gauche apparaît la Console, où vous voyez les commandes envoyées à R ainsi que ses sorties.\nEn haut à droite, les différents onglets indiquent où vous en êtes dans vos calculs. En particulier, la liste sous Environment indique les objets qui ont été générés ou chargés jusqu’alors.\nEn bas à droite, on retrouve des onglets de nature variés. Files contient les sous-dossiers et fichiers du dossier de projets. Plots est l’endroit où apparaîtront vos graphiques. Packages contient la liste des modules déjà installés, ainsi qu’un outil de gestion des modules pour leur installation, leur désinstallation et leur mise à jour. Help affiche les fiches d’aide des fonctions (pour obtenir de l’aide sur une fonction dans RStudio, surlignez la fonction dans votre feuille de calcul, puis appuyez sur F1). Enfin, l’onglet Viewer affichera les sorties HTML, en particulier les graphiques interactifs que vous générerez par exemple avec le module plotly, ou alors le rendu de votre fichier Quarto. Si votre environnement de travail était un avion, R serait le moteur et RStudio serait le cockpit!\n\n\n\n\n\nFigure 2.6: Scène de Fifi Brindacier (Astrid Lindgren, 1945).\n\n\n\n\n2.3.2 Installation avec Anaconda\nSi vous cherchez une trousse complète d’analyse de données, comprenant R et Python, vous pourrez préférer Anaconda. Une fois installée, vous pourrez isoler un environnement de travail sur R, ou même isoler des environnements de travail particuliers pour vos projets. Une manière conviviale de créer des environnements de travail est de passer par l’interface Anaconda navigator, que vous lancerez soit dans le menu Windows, soit en ligne de commande anaconda-navigator sous Mac et Linux, puis d’installer r-essentials, rstudio et jupyterlab dans l’onglet Environment. Vous pourrez aussi installer RStudio et Jupyter lab via l’onglet Home de Anaconda navigator. Dans l’environnement de base, installez le package nb_conda_kernels pour vous assurer que tous les noyaux (R, Python, etc.) installés dans les environnements de travail soient automatiquement accessibles dans Jupyter. Si vous désirez utiliser dans Jupyter la version de R installée avec l’installation classique, référez-vous au guide présenté en extra au bas de la page.\n\n\n\n\nFigure 2.7: Anaconda navigator.\n\n\n\nJupyter lab est une interface notebook semblable à Quarto - les format Jupyter (*.ipynb) et Quarto (*.qmd) sont par ailleurs convertibles grâce au module jupytext. L’utilisation de R en Anaconda n’est pas tout à fait au point, et pourrait poser problème pour l’installation de certains modules. Si vous optez pour cette option, préparez-vous à avoir à bidouiller un peu. Plusieurs préfèrent Jupyter à RStudio (ce n’est pas mon cas).\n\n2.3.3 Utilisation infonuagique\nPas besoin d’avoir une machine super puissante pour travailler en R. Il existe une multitude de services infonuagiques (dans le cloud) vous permettant de lancer vos calculs sur des serveurs plutôt que sur votre Chromebook ou votre vieux laptop déglingué. Certains services sont gratuits, et d’autres souvent plus élaborés sont payants. Vous pouvez utiliser gratuitement Azure Notebooks ou un tour de passe-passe pour faire fonctionner Google colab en R. Une option gratuite de CoCalc vient avec un agressant bandeau rouge vif qui disparait avec l’option payante.\nÀ mon avis, le service Nextjournal est celui d’entre tous qui possède en ce moment les meilleures qualités dans sa version gratuite. Vous pourrez y travailler en mode collaboratif, comme dans Google docs. En outre, vous pouvez lancer ces notes de cours en les important dans Nextjournal. Vous devrez toutefois déposer les données dans l’interface, puis à chaque session installer les modules spécialisés. Le service gratuit offre peu de puissance de calcul, mais pour effectuer les applications de base, ça devrait être suffisant. La vidéo ci-dessous monter comment importer les notes de cours dans Nextjournal.\nVideo"
  },
  {
    "objectID": "02-R.html#premiers-pas-avec-r",
    "href": "02-R.html#premiers-pas-avec-r",
    "title": "2  La science des données avec R",
    "section": "\n2.4 Premiers pas avec R",
    "text": "2.4 Premiers pas avec R\nR ne fonctionne pas avec des menus, en faisant danser une souris sous une musique de clics. Vous devrez donc entrer des commandes avec votre clavier, que vous apprendrez par cœur au fur et à mesure, ou que vous retrouverez en lançant des recherches sur internet. Par expérience personnelle, lorsque je travaille avec R, j’ai toujours un navigateur ouvert prêt à recevoir une question.\nLes étapes qui suivent sont des premiers pas. Elles ne feront pas de vous des ceintures noires de la programmation. La plupart des utilisateurs de R ont appris en se pratiquant sur leurs données, en se butant sur des obstacles, en apprenant comment les surmonter ou les contourner…\nPour l’instant, ouvrez seulement un interpréteur de commande, et lancez R. Voyons si R est aussi libre qu’on le prétend.\n\n“La liberté, c’est la liberté de dire que deux et deux font quatre. Si cela est accordé, tout le reste suit.” - George Orwell, 1984\n\n\n2 + 2\n\n[1] 4\n\n\nEt voilà.\n\nLes opérations mathématiques sont effectuées telles que l’on devrait s’attendre.\n\n67.1 - 43.3\n\n[1] 23.8\n\n2 * 4\n\n[1] 8\n\n1 / 2\n\n[1] 0.5\n\n\nL’exposant peut être noté ^, comme c’est le cas dans Excel, ou ** comme c’est le cas en Python.\n\n2^4\n\n[1] 16\n\n\n\n2**4\n\n[1] 16\n\n\n\n1 / 2 # utilisez des espaces de part et d'autre des opérateurs (sauf pour l'exposant) pour éclaircir le code\n\n[1] 0.5\n\n\nR ne lit pas ce qui suit le caractère #. Cela vous laisse l’opportunité de commenter un code comprenant une séquence de plusieurs lignes. Remarquez également que la dernière opération comporte des espaces entre les nombres et l’opérateur /. Dans ce cas (ce n’est pas toujours le cas), les espaces ne signifient rien: ils aident seulement à éclaircir le code. Il existe des guides pour l’écriture de code en R. Je recommande fortement de suivre méticuleusement le guide de style de tidyverse.\nAssigner des objets à des variables est fondamental en programmation. En R, on assigne traditionnellement avec la flèche &lt;-, mais vous verrez parfois le =, qui est davantage utilisé comme standard dans d’autres langages de programmation. Par exemple.\n\na &lt;- 3\n\nTruc. Essayez d’inverser la flèche, e.g. 3 -&gt; a.\nTechniquement, a pointe vers le nombre entier 3. Conséquemment, on peut effectuer des opérations sur a.\n\na * 6\n\n[1] 18\n\n\n\nA + 2\n\nLe message d’erreur nous dit que A n’est pas défini. Sa version minuscule, a, l’est pourtant. La raison est que R considère la case dans la définition des objets. Utiliser la mauvaise case mène donc à des erreurs.\nNote. Les messages d’erreur ne sont pas toujours clairs, mais vous apprendrez à les comprendre. Dans tous les cas, ils sont fait pour vous aider. Lisez-les attentivement!\nEn général, le nom d’une variable doit toujours commencer par une lettre, et ne doit pas contenir de caractères réservés (espaces, +, *). Dans la définition des variables, plusieurs utilisent des symboles . pour délimiter les mots, mais la barre de soulignement _ est à préférer. En effet, dans d’autres langages de programmation comme Python, le . a une autre signification: son utilisation est à éviter autant que possible. De même, évitez l’utilisation de majuscules pour nommer vos objets (voir le guide de style de tidyverse pour nommer les objets).\nNote. À ce stade, vous serez probablement plus à l’aise de copier-coller ces commandes dans votre terminal.\n\nrendement_arbre &lt;- 50 # pomme/arbre\nnombre_arbre &lt;- 300 # arbre\nnombre_pomme &lt;- rendement_arbre * nombre_arbre\nnombre_pomme\n\n[1] 15000\n\n\nComme chez la plupart des langages de programmation, R respecte les conventions des priorités des opérations mathéatiques.\n\n10 - 9^0.5 * 2\n\n[1] 4\n\n\n\n2.4.1 Types de données\nJusqu’à maintenant, nous n’avons utilisé que des nombres entiers (integer ou int) et des nombres réels (numeric ou float64). R inclut d’autres types. La chaîne de caractère (string ou character) contient un ou plusieurs symboles. Elle est définie entre des doubles guillemets \" \" ou des apostrophes ' '. Il n’existe pas de standard sur l’utilisation de l’un ou de l’autre, mais en règle générale, on utilise les apostrophes pour les expressions courtes, contenant un simple mot ou une séquence de lettres, et les guillemets pour les phrases. Une raison pour cela: les guillemets sont utiles pour insérer des apostrophes dans une chaîne de caractère.\n\na &lt;- \"L'ours\"\nb &lt;- \"polaire\"\npaste(a, b)\n\n[1] \"L'ours polaire\"\n\n\nOn colle a et b avec la fonction paste. Notez que l’objet a a été défini précédemment. Il est possible en R de réassigner une variable, mais cela peut porter à confusion, jusqu’à générer des erreurs de calcul si une variable n’est pas assignée à l’objet auquel on voulait référer.\nCombien de caractères contient la chaîne \"L'ours polaire\"? R sait compter. Demandons-lui.\n\nc &lt;- paste(a, b)\nnchar(c)\n\n[1] 14\n\n\nQuatorze, c’est bien cela (comptez “L’ours polaire”, en incluant l’espace). Comme paste, nchar est une fonction incluse par défaut dans l’environnement de travail de R: plus précisément, ces fonctions sont incluses dans le module base, inclut par défaut lorsque R est lancé. La fonction est appelée en écrivant nchar(). Mais une fonction de quoi? Des arguments, qui se trouvent entre les parenthèses. Dans ce cas, il y a un seul argument: c.\nEn calcul scientifique, il est courant de lancer des requêtes déterminant si un résultat est vrai ou faux.\n\na &lt;- 17\na &lt; 10\n\n[1] FALSE\n\na &gt; 10\n\n[1] TRUE\n\na == 10\n\n[1] FALSE\n\na != 10\n\n[1] TRUE\n\na == 17\n\n[1] TRUE\n\n!(a == 17)\n\n[1] FALSE\n\n\nJe viens d’introduire un nouveau type de donnée: les données booléennes (boolean, ou logical), qui ne peuvent prendre que deux états - TRUE ou FALSE. En même temps, j’ai utilisé la fonction print parce que dans mon carnet, seule la dernière opération permet d’afficher le résultat. Si l’on veut forcer une sortie, on utilise print. Puis, on a vu plus haut que le symbole = est réservé pour assigner des objets: pour les tests d’égalité, on utilise le double égal, ==, ou != pour la non-égalité. Enfin, pour inverser une donnée de type booléenne, on utilise le point d’exclamation !.\n\n2.4.2 Les collections de données\nLes exercices précédents ont permis de présenter les types de données offerts par défaut sur R qui sont les plus importants pour le calcul scientifique: int (integer, ou nombre entier), numeric (nombre réel), character (string, ou chaîne de caractère) et logical (booléen). D’autres s’ajouteront tout au long du cours, comme les catégories (factor) et les unités de temps (date-heure).\nLorsque l’on procède à des opérations de calcul en science, nous utilisons rarement des valeurs uniques. Nous préférons les organiser et les traiter en collections. Par défaut, R offre quatre types importants de collections: les vecteurs, les matrices, les listes et les tableaux.\n\n2.4.2.1 Vecteurs\nD’abord, les vecteurs sont une série de variables de même type. Un vecteur est délimité par la fonction c( ) (c pour concaténation). Les éléments de la liste sont séparés par des virgules.\n\nespece &lt;- c(\"Petromyzon marinus\", \"Lepisosteus osseus\", \"Amia calva\", \"Hiodon tergisus\")\nespece\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\n\nPour accéder aux éléments d’une liste, one appelle la liste suivie de la position de l’objet désiré entre crochets.\n\nespece[1]\n\n[1] \"Petromyzon marinus\"\n\nespece[2]\n\n[1] \"Lepisosteus osseus\"\n\nespece[1:3]\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n\nespece[c(1, 3)]\n\n[1] \"Petromyzon marinus\" \"Amia calva\"        \n\n\nOn peut noter que le premier élément de la liste est noté 1, et non 0 comme c’est le cas de la plupart de langages. Le raccourcis 1:3 crée une liste de nombres entiers de 1 à 3 inclusivement, c’est-à-dire l’équivalent de c(1, 2, 3). En effet, on crée une liste d’indices pour soutirer des éléments d’une liste. On peut utiliser le symbole de soustraction pour retirer un ou plusieurs éléments d’un vecteur.\n\nespece[-c(1, 3)]\n\n[1] \"Lepisosteus osseus\" \"Hiodon tergisus\"   \n\n\nPour ajouter un élément à notre liste, on peut utiliser la fonction c( ).\n\nespece &lt;- c(espece, \"Cyprinus carpio\")\nespece\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"    \"Cyprinus carpio\"   \n\n\nNotez que l’on efface l’objet espece par une concaténation de l’objet espece, précédemment définie, et d’un autre élément.\nEn lançant espece[3] &lt;- \"Lepomis gibbosus\", il est possible de changer un élément de la liste.\n\nespece[3] &lt;- \"Lepomis gibbosus\"\nespece\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Lepomis gibbosus\"  \n[4] \"Hiodon tergisus\"    \"Cyprinus carpio\"   \n\n\n\n2.4.2.2 Matrices\nUne matrice est un vecteur de dimension plus élevée que 1. En écologie, on dépasse rarement la deuxième dimension, quoi que les matrices en N dimensions soient courantes en modélisation mathématique. Je ne considérerai pour le moment que des matrices 2D. Comme c’est la cas des vecteurs, les matrices contiennent des valeurs de même type. En R, on peut attribuer aux matrices 2D des noms de ligne et de colonne.\n\nmat &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), \n              ncol = 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\n\ncolnames(mat) &lt;- c(\"A\", \"B\", \"C\")\nrownames(mat) &lt;- c(\"site_1\", \"site_2\", \"site_3\", \"site_4\")\nmat\n\n       A B  C\nsite_1 1 5  9\nsite_2 2 6 10\nsite_3 3 7 11\nsite_4 4 8 12\n\n\nOn peut soutirer les noms de colonne et les noms de ligne. Le résultat est un vecteur.\n\ncolnames(mat)\n\n[1] \"A\" \"B\" \"C\"\n\nrownames(mat)\n\n[1] \"site_1\" \"site_2\" \"site_3\" \"site_4\"\n\n\n\n2.4.2.3 Listes\nLes listes sont des collections hétérogènes dans lesquelles on peut placer les objets désirés, sans distinction: elles peuvent même inclure d’autres listes. Chacun des éléments de la liste peut être identifié par une clé.\n\nma_liste &lt;- list(\n  especes = c(\n    \"Petromyzon marinus\", \"Lepisosteus osseus\",\n    \"Amia calva\", \"Hiodon tergisus\"\n  ),\n  site = \"A101\",\n  stations_meteos = c(\"746583\", \"783786\", \"856363\")\n)\nma_liste\n\n$especes\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\n$site\n[1] \"A101\"\n\n$stations_meteos\n[1] \"746583\" \"783786\" \"856363\"\n\n\nLes éléments de la liste peuvent être soutirés par le nom de la clé ou par l’indice, de cette manière.\n\nma_liste$especes\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\nma_liste[[1]]\n\n[1] \"Petromyzon marinus\" \"Lepisosteus osseus\" \"Amia calva\"        \n[4] \"Hiodon tergisus\"   \n\n\nExercice. Accéder au deuxième élément du vecteur d’espèces dans la liste ma_liste.\n\n2.4.2.4 Tableaux\nEnfin, le type de collection de données le plus important est le tableau, ou data.frame. Techniquement, il s’agit d’une liste composée de vecteurs de même longueur. Chaque colonne peut ainsi prendre un type de donnée indépendamment des autres colonnes.\n\ntableau &lt;- data.frame(\n  espece = c(\n    \"Petromyzon marinus\", \"Lepisosteus osseus\",\n    \"Amia calva\", \"Hiodon tergisus\"\n  ),\n  poids = c(10, 13, 21, 4),\n  longueur = c(35, 44, 50, 8)\n)\ntableau\n\n              espece poids longueur\n1 Petromyzon marinus    10       35\n2 Lepisosteus osseus    13       44\n3         Amia calva    21       50\n4    Hiodon tergisus     4        8\n\n\nEn programmation classique en R (nous verrons plus loin la méthode tidyverse), les éléments d’un tableau se manipulent comme ceux d’une matrice et les colonnes peuvent être appelés comme les éléments d’une liste.\n\ntableau[, 2:3]\n\n  poids longueur\n1    10       35\n2    13       44\n3    21       50\n4     4        8\n\ntableau$poids\n\n[1] 10 13 21  4\n\n\nVous verrez aussi, quoi que rarement, ce format, qui à la différence du format $ génère un tableau.\n\ntableau[\"poids\"]\n\n  poids\n1    10\n2    13\n3    21\n4     4\n\n\nLe tableau est le format de collection à privilégier pour manipuler des données. Récemment, le format de tableau tibble a été créé par l’équipe de RStudio pour offrir un format plus moderne.\n\n2.4.3 Les fonctions\nLorsque vous écrivez une commande suivit de parenthèses, comme data.frame(especes = ...), vous demandez à R de passer à l’action en appelant une fonction. De manière très générale, une fonction transforme quelque chose en quelque chose d’autre (Figure 2.8).\n\n\n\n\nFigure 2.8: Schéma simplifié d’une fonction.\n\n\n\nPar exemple, la fonction mean() prend une collection de nombre comme entrée, puis en sort vous devinez quoi.\n\nmean(tableau$poids)\n\n[1] 12\n\n\nLes entrées sont appelés les arguments de la fonction. Leur définition est toujours disponible dans la documentation.\nExercice. Familiarisez-vous avec la documentation de R en lançant ?mean. Truc: si vous avez pris de l’avance et que vous travaillez déjà en RStudio, mettez le terme en surbrillance, puis appuyez sur F1.\nVous verrez dans la documentation que la fonction mean() demande trois arguments, x, trim et na.rm. Or nous avons seulement placé un vecteur, sans spécifier d’argument!\nEn effet. En l’absence d’une définition des arguments, R supposera que les arguments dans la parenthèse, séparés par une virgule, sont présentés dans le même ordre que celui spécifié dans la définition de la fonction (celle qui est présentée dans le fichier d’aide). Dans le cas qui nous intéresse, mean(tableau$poids) est équivalent à mean(x = tableau$poids).\nMaintenant, selon la fiche d’aide, l’argument na.rm est un valeur logique spécifiant si oui (TRUE) ou non (FALSE) les valeurs manquantes doivent être considérées (une moyenne d’un vecteur comprenant au moins un NA sera de NA). En ne spécifiant rien, R prend la valeur par défaut, telle que spécifiée dans la documentation. Il en va de même pour l’argument trim, qui permet d’élaguer des valeurs extrêmes. Dans la fiche d’aide, mean(x, trim = 0, na.rm = FALSE, ...) signifie que par défaut, l’argument x est vide (il doit donc être spécifié), l’argument trim est de 0 et l’argument na.rm est FALSE.\n\nmean(c(6, 1, 7, 4, 9, NA, 1))\n\n[1] NA\n\nmean(c(6, 1, 7, 4, 9, NA, 1), na.rm = TRUE)\n\n[1] 4.666667\n\n\nVous n’êtes pas emprisonné par les fonctions offertes par R. Vous pouvez installer des modules qui complètent les fonctions de base de R: on le verra un peu plus loin dans ce chapitre. Mais pour l’instant, voyons comment vous pouvez créer vos propres fonctions. Disons que vous voulez créer une fonction qui calcule la sortie de \\(x^3-2y+a\\). Pour obtenir la réponse, on a besoin des arguments x, y et a. La sortie de la fonction est ici triviale: la réponse de l’équation. L’opération function permet de prendre ça en charge.\n\noperation_f &lt;- function(x, y, a = 10) {\n  return(x^3 - 2 * y + a)\n}\n\nNotez que a a une valeur par défaut. La sortie de la fonction est ce qui se trouve entre les parenthèses de return. Vous pouvez maintenant utiliser la fonction operation_f au besoin.\n\noperation_f(x = 2, y = 3, a = 1)\n\n[1] 3\n\n\nUne telle fonction est peu utile. Mais l’utilisation de fonctions personnalisées vous permettra d’éviter de répéter la même opération plusieurs fois dans un flux de travail, en évitant de générer trop de code, donc aussi de potentielles erreurs. Personnellement, j’utilise les fonctions surtout pour générer des graphiques personnalisés.\nExercice. Afin d’acquérir de l’autonomie, vous devrez être en mesure de trouver le nom des commandes dont vous avez besoin pour effectuer la tâche que vous désirez effectuer. Cela peut causer des frustrations, mais vous vous sentirez toujours plus à l’aise avec R jour après jour. L’exercice ici est de trouver par vous-même la commande qui vous permettra mesurer la longueur d’un vecteur.\n\n2.4.4 Les boucles\nLes boucles permettent d’effectuer une même suite d’opérations sur plusieurs objets. Pour faire suite à notre exemple, nous désirons obtenir le résultat de l’opération f pour des paramètres que nous enregistrons dans ce tableau.\n\nparams &lt;- data.frame(\n  x = c(2, 4, 1, 5, 6),\n  y = c(3, 4, 8, 1, 0),\n  a = c(6, 1, 8, 2, 5)\n)\nparams\n\n  x y a\n1 2 3 6\n2 4 4 1\n3 1 8 8\n4 5 1 2\n5 6 0 5\n\n\nNous créons un vecteur vide, puis nous effectuons une itération ligne par ligne en remplissant le vecteur.\n\noperation_res &lt;- c()\nfor (i in 1:nrow(params)) {\n  operation_res[i] &lt;- operation_f(x = params[i, 1], y = params[i, 2], a = params[i, 3])\n}\noperation_res\n\n[1]   8  57  -7 125 221\n\n\nEn faisant varier i sur des valeurs du vecteur donné par la séquence de nombres entiers de 1 au nombre de ligne du tableau de paramètres, nous demandons à R d’effectuer la suite d’opération entre les accolades {}. À chaque boucle, i prend une valeur de la séquence. i est utilisé ici comme indice de la ligne à soutirer du tableau params, qui correspond à l’indice dans le vecteur operation_res.\nAinsi, chaque résultat est calculé dans l’ordre des lignes du tableau de paramètres et l’on pourra très bien y coller nos résultats:\n\nparams$resultats &lt;- operation_res\nparams\n\n  x y a resultats\n1 2 3 6         8\n2 4 4 1        57\n3 1 8 8        -7\n4 5 1 2       125\n5 6 0 5       221\n\n\nNotez que puisque la colonne resultat n’existe pas dans le tableau params, R crée automatiquement une nouvelle colonne.\nLes boucles for vous permettront par exemple de générer en peu de temps 10, 100, 1000 graphiques (autant que vous voulez), chacun issu de simulations obtenues à partir de conditions initiales différentes, et de les enregistrer dans un répertoire sur votre ordinateur. Un travail qui pourrait prendre des semaines sur Excel peut être effectué en R en quelques secondes.\nUn second outil est disponible pour les itérations: les boucles while. Elles effectuent une opération tant qu’un critère n’est pas atteint. Elles sont utiles pour les opérations où l’on cherche une convergence. Je les couvre rapidement puisqu’elles sont rarement utilisées dans les flux de travail courants. En voici un petit exemple.\n\nx &lt;- 100\nwhile (x &gt; 1.1) {\n  x &lt;- sqrt(x)\n  print(x)\n}\n\n[1] 10\n[1] 3.162278\n[1] 1.778279\n[1] 1.333521\n[1] 1.154782\n[1] 1.074608\n\n\nNous avons initié x à une valeur de 100. Puis, tant que (while) le test x &gt; 1.1 est vrai, attribuer à x la nouvelle valeur calculée en extrayant la racine de la valeur précédente de x. Enfin, indiquer la valeur avec print.\n\n2.4.5 Conditions: if, else if, else\n\n\nSi la condition 1 est remplie, effectuer une suite d’instructions 1. Si la condition 1 n’est pas remplie, et si la condition 2 est remplie, effectuer la suite d’instructions 2. Sinon, effectuer la suite d’instruction 3.\n\nVoilà comment on exprime une suite de conditions. Prenons l’exemple simple d’une discrétisation d’une valeur continue. Si \\(x&lt;10\\), il est classé comme faible. Si \\(10 \\leq x &lt;20\\), il est classé comme moyen. Si \\(x \\geq 20\\), il est classé comme élevé. Plaçons cette classification dans une fonction.\n\nclassification &lt;- function(x, lim1 = 10, lim2 = 20) {\n  if (x &lt; lim1) {\n    categorie &lt;- \"faible\"\n  } else if (x &lt; lim2) {\n    categorie &lt;- \"moyen\"\n  } else {\n    categorie &lt;- \"élevé\"\n  }\n  return(categorie)\n}\nclassification(-10)\n\n[1] \"faible\"\n\nclassification(15.4)\n\n[1] \"moyen\"\n\nclassification(1000)\n\n[1] \"élevé\"\n\n\nUne condition est définie avec le if, suivi du test à vrai ou faux entre parenthèses. Si le test retourne un vrai (TRUE), l’instruction entre accolades est exécutée. Si elle est fausse, on passe au suivant.\nExercice. Explorer les commandes ifelse et cut et réfléchissez à la manière qu’elles pourraient être utilisées pour effectuer une discrétisation plus efficacement qu’avec les if et les else.\n\n2.4.6 Installer et charger un module\nLa plupart des opérations d’ordre général (comme les racines carrées, les tests statistiques, la gestion de matrices et de tableau, les graphiques, etc.) sont accessibles grâce aux modules de base de R, qui sont installés et chargés par défaut lors du démarrage de R. Des équipes de travail ont néanmoins développé plusieurs modules pour répondre à leurs besoins spécialisés, et les ont laissés disponibles au grand public dans des modules que vous pouvez installer d’un dépôt CRAN (le AppStore de R), d’un dépôt Anaconda (le AppStore de Anaconda, si vous utilisez cette plate-forme), d’un dépôt Github (dépôts décentralisés), etc.\nRStudio possède un pratique bouton Install qui vous permet d’y inscrire une liste de modules. Le navigateur anaconda offre aussi une interface d’installation. La commande R pour installer un module est install.packages(\"ggplot2\"), si par exemple vous désirez installer ggplot2, le module graphique par excellence en R. C’est la commande que RStudio lancera tout seul si vous lui demandez d’installer ggplot2.\nLes modules sont l’équivalent des applications spécialisées que vous installez sur un téléphone mobile. Pour les utiliser, il faut les ouvrir.\nGénéralement, j’ouvre toutes les applications nécessaires à mon flux de travail au tout début de ma feuille de calcul (la prochaine cellule retournera un message d’erreur si les packages ne sont pas installés).\n\nlibrary(\"tidyverse\") # méta-package qui charge entre autres dplyr et ggplot2\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"vegan\")\n\nLe chargement a nécessité le package : permute\nLe chargement a nécessité le package : lattice\nThis is vegan 2.6-4\n\nlibrary(\"nlme\")\n\n\nAttachement du package : 'nlme'\n\nL'objet suivant est masqué depuis 'package:dplyr':\n\n    collapse\n\n\nLes modules sont installés sur votre ordinateur à un endroit que vous pourrez retrouver avec la commande .libPaths()\nExercice. À partir d’ici jusqu’à la fin du cours, nous utiliserons RStudio. Ouvrez-le et familiarisez-vous avec l’interface! Quelques petits trucs:\n\npour lancer une ligne, placez votre curseur sur la ligne, puis appuyez sur Ctrl+Enter\npour lancer une partie de code précise, mettez le en surbrillance, puis Ctrl+Enter\nutilisez toujours le gestionnaire de projets, en haut à droite!\ninstallez le module tidyverse\n\nlancez data(\"iris\") pour obtenir un tableau d’exercice, puis cliquez sur l’objet dans la fenêtre environnement"
  },
  {
    "objectID": "02-R.html#enfin",
    "href": "02-R.html#enfin",
    "title": "2  La science des données avec R",
    "section": "\n2.5 Enfin…",
    "text": "2.5 Enfin…\nComme une langue, on n’apprend à s’exprimer en un langage informatique qu’en se mettant à l’épreuve, ce que vous ferez tout au long de ce cours. Pour vous encourager, voici quelques trucs pour apprendre à coder en R.\n\n\nR n’aime pas l’ambiguïté. Une simple virgule mal placée et il ne sait plus quoi faire. Cela peut être frustrant au début, mais cette rigidité est nécessaire pour effectuer du calcul scientifique.\n\nLe copier-coller est votre ami. En gardant à l’esprit que vous être responsable de votre code et que vous respectez les droits d’auteur, n’ayez pas peur de copier-coller des lignes de code et de personnaliser par la suite.\n\nL’erreur que vous obtenez: d’autres l’ont obtenue avant vous. Le site de question-réponse stackoverflow est une ressource inestimable où des gens ayant posté des questions ont reçu des réponses d’experts (les meilleures réponses et les meilleures questions apparaissent en premier). Apprenez à chercher intelligemment des réponses en formulant précisément vos questions!\n\nÉtudiez et pratiquez. Les messages d’erreur en R sont courants, même chez les personnes expérimentées. La meilleure manière d’apprendre une langue est de la parler, d’étudier ses susceptibilités, de les tester dans une conversation, etc."
  },
  {
    "objectID": "02-R.html#petit-truc",
    "href": "02-R.html#petit-truc",
    "title": "2  La science des données avec R",
    "section": "\n2.6 Petit truc!",
    "text": "2.6 Petit truc!\nRStudio peut être implémenté avec des extensions. L’une d’elle permet d’ajuster votre style de code. Par exemple, vous voulez vous assurer que toutes les allocations sont bien effectuées avec des &lt;- et non pas des =, qu’il y a bien des espaces de part et d’autre de &lt;-, que les retours de lignes sont bien placés, etc. Installez le module styler, et des options apparaîtront dans le menu Addins comme à la Figure 2.9.\n\n\n\n\nFigure 2.9: L’extension styler permet de formater votre code dans un style particulier"
  },
  {
    "objectID": "02-R.html#extra-jupyter",
    "href": "02-R.html#extra-jupyter",
    "title": "2  La science des données avec R",
    "section": "\n2.7 Extra: Utiliser R avec Jupyter",
    "text": "2.7 Extra: Utiliser R avec Jupyter\nPour utiliser R dans Jupyter notebook ou Jupyter lab, vous devez installer le module IRkernel dans la version de R que vous désirez utiliser avec Jupyter, puis de lancer la commande IRkernel::installspec(). La prochaine fois que vous ouvrirez Jupyter, le noyau de R devrait apparaître.\nJe n’ai aucune expérience sur Mac, mais semble-t-il cela fonctionne comme en Linux. Ouvrez R à partir d’un terminal (R + Enter), puis lancez IRkernel::installspec() après avoir installé IRkernel. Si vous travaillez en Windows, il vous faudra lancer R par son chemin complet dans l’invite de commande de Anaconda (Anaconda Powershell Prompt). Par exemple, ouvrir Anaconda Powershell Prompt, puis, si votre installation de R se trouve dans C:\\Program Files\\R\\R-3.6.2,\n(base) PS C:\\Users\\fifi&gt; cd \"C:\\Program Files\\R\\R-3.6.2\\bin\"\n(base) PS C:\\Program Files\\R\\R-3.6.2\\bin&gt; .\\R.exe\n\nR version 3.6.2 (2019-12-12) -- \"Dark and Stormy Night\"\nCopyright (C) 2019 The R Foundation for Statistical Computing\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\n\nR est un logiciel libre livré sans AUCUNE GARANTIE.\nVous pouvez le redistribuer sous certaines conditions.\nTapez 'license()' ou 'licence()' pour plus de détails.\n\nR est un projet collaboratif avec de nombreux contributeurs.\nTapez 'contributors()' pour plus d'information et\n'citation()' pour la façon de le citer dans les publications.\n\nTapez 'demo()' pour des démonstrations, 'help()' pour l'aide\nen ligne ou 'help.start()' pour obtenir l'aide au format HTML.\nTapez 'q()' pour quitter R.\n\n&gt; install.packages(\"IRkernel\")\nInstallation du package dans 'C:/Users/fifi/Documents/R/win-library/3.6'\n(car 'lib' n'est pas spécifié)\n--- SVP sélectionner un miroir CRAN pour cette session ---\nessai de l'URL 'https://cloud.r-project.org/bin/windows/contrib/3.6/IRkernel_1.1.zip'\nContent type 'application/zip' length 138696 bytes (135 KB)\ndownloaded 135 KB\n\nle package 'IRkernel' a été décompressé et les sommes MD5 ont été vérifiées avec succés\n\nLes packages binaires téléchargés sont dans\n       C:\\Users\\fifi\\AppData\\Local\\Temp\\Rtmp6xJtB3\\downloaded_packages\n\n&gt; IRkernel::installspec()\n[InstallKernelSpec] Installed kernelspec ir in C:\\Users\\fifi\\AppData\\Roaming\\jupyter\\kernels\\ir\n&gt; qui()"
  },
  {
    "objectID": "03-tableaux.html#les-collections-de-données",
    "href": "03-tableaux.html#les-collections-de-données",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.1 Les collections de données",
    "text": "3.1 Les collections de données\nDans le chapitre 2, nous avons survolé différents types d’objets : réels, entiers, chaînes de caractères et booléens. Les données peuvent appartenir à d’autres types : dates, catégories ordinales (ordonnées : faible, moyen, élevé) et nominales (non ordonnées : espèces, cultivars, couleurs, unité pédologique, etc.). Comme mentionné en début de chapitre, une donnée est une valeur associée à une variable. Les données peuvent être organisées en collections.\nNous avons aussi vu au chapitre 2 que la manière privilégiée d’organiser des données était sous forme de tableaux. De manière générale, un tableau de données est une organisation de données en deux dimensions, comportant des lignes et des colonnes. Il est préférable de respecter la convention selon laquelle les lignes sont des observations et les colonnes sont des variables. Ainsi, un tableau est une liste de vecteurs de même longueur, chaque vecteur représentant une variable. Chaque variable est libre de prendre le type de données approprié. La position d’une donnée dans le vecteur correspond à une observation. Lorsque les vecteurs sont posés les uns à côté des autres, la position dans le vecteur devient une ligne qui définit les valeurs des variables d’une observation.\nImaginez que vous consignez des données météorologiques comme les précipitations totales ou la température moyenne pour chaque jour, pendant une semaine sur les sites A, B et C. Chaque site possède ses propres caractéristiques, comme la position en longitude-latitude. Il est redondant de répéter la position du site pour chaque jour de la semaine. Vous préférerez créer deux tableaux : un pour décrire vos observations, et un autre pour décrire les sites. De cette manière, vous créez une collection de tableaux interreliés : une base de données. Nous couvrirons cette notion un peu plus loin. R peut soutirer des données des bases de données grâce au module DBI, qui n’est pas couvert à ce stade de développement du cours.\nDans R, les données structurées en tableaux, ainsi que les opérations sur les tableaux, peuvent être gérées grâce aux modules readr, dplyr et tidyr, tous des modules faisant partie du méta-module tidyverse, qui est un genre de Microsoft Office sur R : plusieurs modules fonctionnant en interopérabilité. Mais avant de se lancer dans l’utilisation de ces modules, voyons quelques règles à suivre pour bien structurer ses données en format tidy, un jargon du tidyverse qui signifie proprement organisé."
  },
  {
    "objectID": "03-tableaux.html#organiser-un-tableau-de-données",
    "href": "03-tableaux.html#organiser-un-tableau-de-données",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.2 Organiser un tableau de données",
    "text": "3.2 Organiser un tableau de données\nAfin de repérer chaque cellule d’un tableau, on attribue à chaque ligne et à chaque colonne un identifiant unique, que l’on nomme indice pour les lignes et entête pour les colonnes.\n\nRègle no 1. Une variable par colonne, une observation par ligne, une valeur par cellule.\n\nLes unités expérimentales sont décrites par une ou plusieurs variables, par des chiffres ou des lettres. Chaque variable devrait être présente en une seule colonne, et chaque ligne devrait correspondre à une unité expérimentale où ces variables ont été mesurées. La règle parait simple, mais elle est rarement respectée. Prenez par exemple le tableau suivant.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nTable 3.1: Rendements obtenus sur les sites expérimentaux selon les traitements.\n\nSite\nTraitement A\nTraitement B\nTraitement C\n\n\n\nSainte-Souris\n4.1\n8.2\n6.8\n\n\nSainte-Fourmi\n5.8\n5.9\nNA\n\n\nSaint-Ours\n2.9\n3.4\n4.6\n\n\n\n\n\n\nQu’est-ce qui cloche avec ce tableau? Chaque ligne est une observation, mais contient plusieurs observations d’une même variable, le rendement, qui devient étalé sur plusieurs colonnes. À bien y penser, le type de traitement est une variable et le rendement en est une autre:\n\n\n\n\nTable 3.2: Rendements obtenus sur les sites expérimentaux selon les traitements.\n\nSite\nTraitement\nRendement\n\n\n\nSainte-Souris\nTraitement A\n4.1\n\n\nSainte-Souris\nTraitement B\n8.2\n\n\nSainte-Souris\nTraitement C\n6.8\n\n\nSainte-Fourmi\nTraitement A\n5.8\n\n\nSainte-Fourmi\nTraitement B\n5.9\n\n\nSainte-Fourmi\nTraitement C\nNA\n\n\nSaint-Ours\nTraitement A\n2.9\n\n\nSaint-Ours\nTraitement B\n3.4\n\n\nSaint-Ours\nTraitement C\n4.6\n\n\n\n\n\n\nPlus précisément, l’expression à bien y penser suggère une réflexion sur la signification des données. Certaines variables peuvent parfois être intégrées dans une même colonne, parfois pas. Par exemple, les concentrations en cuivre, zinc et plomb dans un sol contaminé peuvent être placées dans la même colonne “Concentration” ou déclinées en plusieurs colonnes Cu, Zn et Pb. La première version trouvera son utilité pour créer des graphiques (chapitre 4), alors que la deuxième favorise le traitement statistique (chapitre 7). Il est possible de passer d’un format à l’autre grâce à la fonction pivot_longer() et pivot_wider() du module tidyr.\n\nRègle no 2. Un tableau par unité observationnelle: ne pas répéter les informations.\n\nReprenons la même expérience. Supposons que vous mesurez la précipitation à l’échelle du site.\n\n\n\n\nTable 3.3: Rendements et précipitations obtenus sur les sites expérimentaux selon les traitements.\n\nSite\nTraitement\nRendement\nPrécipitations\n\n\n\nSainte-Souris\nTraitement A\n4.1\n813\n\n\nSainte-Souris\nTraitement B\n8.2\n813\n\n\nSainte-Souris\nTraitement C\n6.8\n813\n\n\nSainte-Fourmi\nTraitement A\n5.8\n642\n\n\nSainte-Fourmi\nTraitement B\n5.9\n642\n\n\nSainte-Fourmi\nTraitement C\nNA\n642\n\n\nSaint-Ours\nTraitement A\n2.9\n1028\n\n\nSaint-Ours\nTraitement B\n3.4\n1028\n\n\nSaint-Ours\nTraitement C\n4.6\n1028\n\n\n\n\n\n\nSegmenter l’information en deux tableaux serait préférable.\n\n\n\n\nTable 3.4: Précipitations sur les sites expérimentaux.\n\nSite\nPrécipitations\n\n\n\nSainte-Souris\n813\n\n\nSainte-Fourmi\n642\n\n\nSaint-Ours\n1028\n\n\n\n\n\n\nLes tableaux Table 3.2 et Table 3.4, ensemble, forment une base de données (collection organisée de tableaux). Les opérations de fusion entre les tableaux peuvent être effectuées grâce aux fonctions de jointure (left_join(), par exemple) du module tidyr. Une jointure de Table 3.4 vers Table 3.2 donnera le tableau Table 3.3.\n\nRègle no 3. Ne pas bousiller les données.\n\nPar exemple.\n\n\nAjouter des commentaires dans des cellules. Si une cellule mérite d’être commentée, il est préférable de placer les commentaires soit dans un fichier décrivant le tableau de données, soit dans une colonne de commentaire juxtaposée à la colonne de la variable à commenter. Par exemple, si vous n’avez pas mesuré le pH pour une observation, n’écrivez pas “échantillon contaminé” dans la cellule, mais annoter dans un fichier d’explication que l’échantillon no X a été contaminé. Si les commentaires sont systématiques, il peut être pratique de les inscrire dans une colonne commentaire_pH.\n\nInscription non systématique. Il arrive souvent que des catégories d’une variable ou que des valeurs manquantes soient annotées différemment. Il arrive même que le séparateur décimal soit non systématique, parfois noté par un point, parfois par une virgule. Par exemple, une fois importés dans votre session, les catégories St-Ours et Saint-Ours seront traitées comme deux catégories distinctes. De même, les cellules correspondant à des valeurs manquantes ne devraient pas être inscrites parfois avec une cellule vide, parfois avec un point, parfois avec un tiret ou avec la mention NA. Le plus simple est de laisser systématiquement ces cellules vides.\n\nInclure des notes dans un tableau. La règle “une colonne, une variable” n’est pas respectée si on ajoute des notes un peu n’importe où sous ou à côté du tableau.\n\nAjouter des sommaires. Si vous ajoutez une ligne sous un tableau comprenant la moyenne de chaque colonne, qu’est-ce qui arrivera lorsque vous importerez votre tableau dans votre session de travail? La ligne sera considérée comme une observation supplémentaire.\n\nInclure une hiérarchie dans les entêtes. Afin de consigner des données de texture du sol, comprenant la proportion de sable, de limon et d’argile, vous organisez votre entête en plusieurs lignes. Une ligne pour la catégorie de donnée, Texture, fusionnée sur trois colonnes, puis trois colonnes intitulées Sable, Limon et Argile. Votre tableau est joli, mais il ne pourra pas être importé conformément dans un votre session de calcul : on recherche une entête unique par colonne. Votre tableau de données devrait plutôt porter les entêtes Texture sable, Texture limon et Texture argile. Un conseil : réserver le travail esthétique à la toute fin d’un flux de travail."
  },
  {
    "objectID": "03-tableaux.html#formats-de-tableau",
    "href": "03-tableaux.html#formats-de-tableau",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.3 Formats de tableau",
    "text": "3.3 Formats de tableau\nPlusieurs outils sont à votre disposition pour créer des tableaux. Je vous présente ici les plus communs.\n\n3.3.1 xls ou xlsx\n\nMicrosoft Excel est un logiciel de type tableur, ou chiffrier électronique. L’ancien format xls a été remplacé par le format xlsx avec l’arrivée de Microsoft Office 2010. Il s’agit d’un format propriétaire, dont l’alternative libre la plus connue est le format ods, popularisé par la suite bureautique LibreOffice. Les formats xls, xlsx ou ods sont davantage utilisés comme outils de calcul que d’entreposage de données. Ils contiennent des formules, des graphiques, du formatage de cellule, etc. Je ne les recommande pas pour stocker des données.\n\n3.3.2 csv\n\nLe format csv, pour comma separated values, est un fichier texte, que vous pouvez ouvrir avec n’importe quel éditeur de texte brut (Bloc note, VSCode, Notepad++, etc.). Chaque colonne doit être délimitée par un caractère cohérent (conventionnellement une virgule, mais en français un point-virgule ou une tabulation pour éviter la confusion avec le séparateur décimal) et chaque ligne du tableau est un retour de ligne. Il est possible d’ouvrir et d’éditer les fichiers csv dans un éditeur texte, mais il est plus pratique de les ouvrir avec des tableurs (LibreOffice Calc, Microsoft Excel, Google Sheets, etc.).\nEncodage des fichiers texte. Puisque le format csv est un fichier texte, un souci particulier doit être porté sur la manière dont le texte est encodé. Les caractères accentués pourraient être importés incorrectement si vous importez votre tableau en spécifiant le mauvais encodage. Pour les fichiers en langues occidentales, l’encodage UTF-8 devrait être utilisé. Toutefois, par défaut, Excel utilise un encodage de Microsoft. Si le csv a été généré par Excel, il est préférable de l’ouvrir avec votre éditeur texte et de l’enregistrer dans l’encodage UTF-8.\n\n3.3.3 json\n\nComme le format csv, le format json indique un fichier en texte clair. En permettant des structures de tableaux emboîtés et en ne demandant pas que chaque colonne ait la même longueur, le format json permet plus de souplesse que le format csv, mais il est plus compliqué à consulter et prend davantage d’espace sur le disque que le csv. Il est utilisé davantage pour le partage de données des applications web, mais en ce qui concerne la matière du cours, ce format est surtout utilisé pour les données géoréférencées. L’encodage est géré de la même manière qu’un fichier csv.\n\n3.3.4 SQLite\nSQLite est une application pour les bases de données relationnelles de type SQL qui n’a pas besoin de serveur pour fonctionner. Les bases de données SQLite sont encodés dans des fichiers portant l’extension db, qui peuvent être facilement partagés.\n\n3.3.5 Suggestion\nEn csv pour les petits tableaux, en sqlite pour les bases de données plus complexes. Ce cours se concentre toutefois sur les données de type csv."
  },
  {
    "objectID": "03-tableaux.html#entreposer-ses-données",
    "href": "03-tableaux.html#entreposer-ses-données",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.4 Entreposer ses données",
    "text": "3.4 Entreposer ses données\nLa manière la plus sécurisée pour entreposer ses données est de les confiner dans une base de données sécurisée sur un serveur sécurisé dans un environnement sécurisé et d’encrypter les communications. C’est aussi… la manière la moins accessible. Des espaces de stockage nuagiques, comme Dropbox ou d’autres options similaires, peuvent être pratiques pour les backups et le partage des données avec une équipe de travail (qui risque en retour de bousiller vos données). Le suivi de version est possible chez certains fournisseurs d’espace de stockage. Mais pour un suivi de version plus rigoureux, les espaces de développement (comme GitHub et GitLab) sont plus appropriés (couverts au chapitre 5). Dans tous les cas, il est important de garder (1) des copies anciennes pour y revenir en cas d’erreurs et (2) un petit fichier décrivant les changements effectués sur les données."
  },
  {
    "objectID": "03-tableaux.html#manipuler-des-données-en-mode-tidyverse",
    "href": "03-tableaux.html#manipuler-des-données-en-mode-tidyverse",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.5 Manipuler des données en mode tidyverse",
    "text": "3.5 Manipuler des données en mode tidyverse\nLe méta-module tidyverse regroupe une collection de précieux modules pour l’analyse de données en R. Il permet d’importer des données dans votre session de travail avec readr, de les explorer avec le module de visualisation ggplot2, de les transformer avec tidyr et dplyr et de les exporter avec readr. Les tableaux de classe data.frame, comme ceux de la plus moderne classe tibble, peuvent être manipulés à travers le flux de travail pour l’analyse et la modélisation. Comme ce sera le cas pour le chapitre sur la visualisation, ce chapitre est loin de couvrir les nombreuses fonctionnalités qui sont offertes dans le tidyverse.\n\n3.5.1 Importer vos données dans votre session de travail\nSupposons que vous avez bien organisé vos données en mode tidy. Pour les importer dans votre session et commencer à les inspecter, vous lancerez une des commandes du module readr, décrites dans la documentation dédiée.\n\n\nread_csv() si le séparateur de colonne est une virgule\n\nread_csv2() si le séparateur de colonne est un point-virgule et que le séparateur décimal est une virgule\n\nread_tsv() si le séparateur de colonne est une tabulation\n\nread_table() si le séparateur de colonne est un espace blanc\n\nread_delim() si le séparateur de colonne est un autre caractère (comme le point-virgule) que vous spécifierez dans l’argument delim = \";\"\n\n\nLes principaux arguments sont les suivants.\n\n\nfile: le chemin vers le fichier. Ce chemin peut aussi bien être une adresse locale (data/…) qu’une adresse internet (https://…).\n\ndelim: le symbole délimitant les colonnes dans le cas de read_delim.\n\ncol_names: si TRUE, la première ligne est l’entête du tableau, sinon FALSE. Si vous spécifiez un vecteur numérique, ce sont les numéros des lignes utilisées pour le nom de l’entête. Si vous utilisez un vecteur de caractères, ce sont les noms des colonnes que vous désirez donner à votre tableau.\n\nna: le symbole spécifiant une valeur manquante. L’argument na='' signifie que les cellules vides sont des données manquantes. Si les valeurs manquantes ne sont pas uniformes, vous pouvez les indiquer dans un vecteur, par exemple na = c(\"\", \"NA\", \"NaN\", \".\", \"-\").\n\nlocal: cet argument prend une fonction local() qui peut inclure des arguments de format de temps, mais aussi d’encodage (voir documentation)\n\nD’autres arguments peuvent être spécifiés au besoin, et les répéter ici dupliquerait l’information de la documentation de la fonction read_csv de readr.\nJe déconseille d’importer des données en format xls ou xlsx. Si toutefois cela vous convient, je vous réfère au module readxl.\nL’aide-mémoire de readr (Figure 3.1) est à afficher près de soi.\n\n\n\n\nFigure 3.1: Aide-mémoire de readr, Source: https://rstudio.github.io/cheatsheets/data-import.pdf\n\n\n\nNous allons charger des données de culture de la chicouté (Rubus chamaemorus), un petit fruit nordique, tiré de Parent et al. (2013). Ouvrons d’abord le fichier pour vérifier les séparateurs de colonnes et de décimales (Figure 3.2).\n\n\n\n\nFigure 3.2: Aperçu brut d’un fichier csv.\n\n\n\nLe séparateur de colonnes est un point-virgule et le décimal est une virgule.\nAvec Atom, mon éditeur texte préféré (il y en a d’autres), je vais dans Edit &gt; Select Encoding et j’obtiens bien le UTF-8 (Figure 3.3).\n\n\n\n\nFigure 3.3: Changer l’encodage d’un fichier csv.\n\n\n\nNous allons donc utiliser read_csv2() avec ses arguments par défaut.\n\nlibrary(\"tidyverse\")\nchicoute &lt;- read_csv2(\"data/chicoute.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 90 Columns: 31\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (5): CodeTourbiere, Ordre, Traitement, DemiParcelle, SousTraitement\ndbl (26): ID, Site, Latitude_m, Longitude_m, Rendement_g_5m2, TotalRamet_nom...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nQuelques commandes utiles inspecter le tableau:\n\n\nhead() présente l’entête du tableau, soit ses 6 premières lignes\n\nstr() et glimpse() présentent les variables du tableau et leur type - glimpse()est la fonction tidyverse et str() est la fonction classique (je préfère str())\n\nsummary() présente des statistiques de base du tableau\n\nnames() ou colnames() sort les noms des colonnes sous forme d’un vecteur\n\ndim() donne les dimensions du tableau, ncol() son nombre de colonnes et nrow() son nombre de lignes\n\nskim est une fonction du module skimr montrant un portrait graphique et numérique du tableau\n\nExtra 1. Plusieurs modules ne se trouvent pas dans les dépôts CRAN, mais sont disponibles sur GitHub. Pour les installer, installez d’abord le module devtools disponible sur CRAN. Vous pourrez alors installer les packages de GitHub comme on le fait avec le package skimr.\nExtra 2. Lorsque je désire utiliser une fonction, mais sans charger le module dans la session, j’utilise la notation module::fonction. Comme dans ce cas, pour skimr.\n\nskimr::skim(chicoute)\n\n\nData summary\n\n\nName\nchicoute\n\n\nNumber of rows\n90\n\n\nNumber of columns\n31\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n26\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nCodeTourbiere\n0\n1.00\n1\n4\n0\n12\n0\n\n\nOrdre\n0\n1.00\n1\n2\n0\n20\n0\n\n\nTraitement\n50\n0.44\n6\n11\n0\n2\n0\n\n\nDemiParcelle\n50\n0.44\n4\n5\n0\n2\n0\n\n\nSousTraitement\n50\n0.44\n1\n7\n0\n3\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n45.50\n26.12\n1.00\n23.25\n45.50\n67.75\n90.00\n▇▇▇▇▇\n\n\nSite\n0\n1.00\n6.33\n5.49\n1.00\n2.00\n4.00\n9.00\n20.00\n▇▃▁▁▁\n\n\nLatitude_m\n0\n1.00\n5701839.86\n1915.50\n5695688.00\n5701868.50\n5702129.00\n5702537.00\n5706394.00\n▁▂▅▇▁\n\n\nLongitude_m\n0\n1.00\n485295.54\n6452.33\n459873.00\n485927.00\n486500.00\n486544.75\n491955.00\n▁▁▁▂▇\n\n\nRendement_g_5m2\n50\n0.44\n13.33\n21.56\n0.00\n0.00\n0.95\n15.63\n72.44\n▇▁▁▁▁\n\n\nTotalRamet_nombre_m2\n0\n1.00\n251.26\n156.06\n40.74\n122.70\n212.92\n347.80\n651.90\n▇▇▃▂▂\n\n\nTotalVegetatif_nombre_m2\n4\n0.96\n199.02\n139.13\n22.92\n86.26\n161.25\n263.78\n580.60\n▇▇▂▂▁\n\n\nTotalFloral_nombre_m2\n4\n0.96\n52.08\n40.41\n4.80\n22.92\n43.00\n69.52\n198.62\n▇▅▂▁▁\n\n\nTotalMale_nombre_m2\n4\n0.96\n24.40\n26.87\n0.00\n3.30\n15.28\n36.51\n104.41\n▇▂▂▁▁\n\n\nTotalFemelle_nombre_m2\n4\n0.96\n27.53\n29.83\n2.55\n10.34\n17.19\n31.96\n187.17\n▇▁▁▁▁\n\n\nFemelleFruit_nombre_m2\n18\n0.80\n19.97\n23.79\n0.40\n7.64\n11.46\n22.83\n157.88\n▇▂▁▁▁\n\n\nFemelleAvorte_nombre_m2\n4\n0.96\n8.49\n14.52\n0.00\n1.27\n3.07\n10.14\n76.80\n▇▁▁▁▁\n\n\nSterileFleur_nombre_m2\n4\n0.96\n0.26\n0.71\n0.00\n0.00\n0.00\n0.00\n3.82\n▇▁▁▁▁\n\n\nC_pourc\n0\n1.00\n50.28\n1.61\n46.72\n49.14\n50.45\n51.58\n53.83\n▃▆▅▇▁\n\n\nN_pourc\n0\n1.00\n2.20\n0.40\n1.53\n1.89\n2.12\n2.58\n3.10\n▃▇▃▃▂\n\n\nP_pourc\n0\n1.00\n0.14\n0.04\n0.07\n0.12\n0.14\n0.16\n0.23\n▃▆▇▂▂\n\n\nK_pourc\n0\n1.00\n0.89\n0.27\n0.35\n0.69\n0.86\n1.13\n1.54\n▃▇▇▇▁\n\n\nCa_pourc\n0\n1.00\n0.39\n0.10\n0.19\n0.32\n0.37\n0.44\n0.88\n▅▇▂▁▁\n\n\nMg_pourc\n0\n1.00\n0.50\n0.08\n0.36\n0.45\n0.48\n0.52\n0.86\n▇▇▂▁▁\n\n\nS_pourc\n0\n1.00\n0.13\n0.04\n0.07\n0.11\n0.13\n0.14\n0.28\n▅▇▂▁▁\n\n\nB_pourc\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▂▅▃▇▃\n\n\nCu_pourc\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▇▁▁▁▁\n\n\nZn_pourc\n0\n1.00\n0.01\n0.00\n0.00\n0.01\n0.01\n0.01\n0.02\n▇▇▂▁▁\n\n\nMn_pourc\n0\n1.00\n0.03\n0.03\n0.00\n0.01\n0.03\n0.05\n0.10\n▇▅▃▂▁\n\n\nFe_pourc\n0\n1.00\n0.02\n0.01\n0.01\n0.01\n0.01\n0.02\n0.05\n▇▂▁▁▁\n\n\nAl_pourc\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n▇▅▁▁▁\n\n\n\n\n\nExercice. Inspectez le tableau.\n\n3.5.2 Comment sélectionner et filtrer des données ?\nOn utilise le terme sélectionner lorsque l’on désire choisir une ou plusieurs lignes et colonnes d’un tableau (la plupart du temps des colonnes). L’action de filtrer signifie de sélectionner des lignes selon certains critères.\n\n3.5.2.1 Sélectionner\nVoici 4 manières de sélectionner une colonne en R.\n\nUne méthode rapide mais peu expressive consiste à indiquer les valeurs numériques de l’indice de la colonne entre des crochets. Il s’agit d’appeler le tableau suivi de crochets. L’intérieur des crochets comprend deux éléments séparés par une virgule. Le premier élément sert à filtrer selon l’indice, le deuxième sert à sélectionner selon l’indice. Ainsi:\n\n\nchicoute[, 1]: sélectionner la première colonne\n\nchicoute[, 1:10]: sélectionner les 10 premières colonnes\n\nchicoute[, c(2, 4, 5)]: sélectionner les colonnes 2, 4 et 5\n\nchicoute[c(10, 13, 20), c(2, 4, 5)]: sélectionner les colonnes 2, 4 et 5 et les lignes 10, 13 et 20.\n\n\nUne autre méthode rapide, mais plus expressive, consiste à appeler le tableau, suivi du symbole $, puis le nom de la colonne, e.g. chicoute$Site.\n\n\nTruc. La plupart des IDE, comme RStudio, peuvent vous proposer des colonnes dans une liste. Après avoir saisi le $, taper sur la touche de tabulation: vous pourrez sélectionner la colonne dans une liste défilante (Figure 3.4).\n\n\n\n\n\nFigure 3.4: Autocomplétion dans RStudio.\n\n\n\n\nVous pouvez aussi inscrire le nom de la colonne, ou du vecteur des colonnes, entre des crochets suivant le nom du tableau, c’est-à-dire chicoute[c(\"Site\", \"Latitude_m\", \"Longitude_m\")].\nEnfin, dans une séquence d’opérations en mode pipeline (chaque opération est mise à la suite de la précédente en plaçant le pipe |&gt; entre chacune), il peut être préférable de sélectionner des colonnes avec la fonction select(), i.e.\n\n\nchicoute |&gt; \n  select(Site, Latitude_m, Longitude_m)\n\n\nNote sur le mode pipeline : Le pipe |&gt; a été introduit dans R-base en 2021. Auparavant, on utilisait la fonction %&gt;% introduite dans le module magrittr, inclus dans tidyverse. La plupart du temps, les deux fonctionnent sensiblement de la même façon, mais il existe quelques différences dans leur interaction avec certaines fonctions. Puisque |&gt; fait partie de R-base, je vous suggère de l’utiliser par défaut, mais il est fort probable que vous trouviez l’ancienne version %&gt;% lors de vos recherches sur internet (ou même dans ce guide si j’oublie d’effectuer les modifications). Pour insérer un pipe, il suffit d’utiliser le raccourci clavier Ctrl + Shift + M. Vous pouvez modifier la forme par défaut dans les options de RStudio, comme sur la Figure 3.5.\n\n\n\n\n\nFigure 3.5: Modifier le pipe par défaut dans RStudio.\n\n\n\nLa fonction select() permet aussi de travailler en exclusion. Ainsi pour enlever des colonnes, on placera un - (signe de soustraction) devant le nom de la colonne.\n⚠️ Attention. Plusieurs modules utilisent la fonction select (et filter, plus bas). Lorsque vous lancez select et que vous obtenez un message d’erreur comme\nError in select(., ends_with(\"pourc\")) : \n  argument inutilisé (ends_with(\"pourc\"))\nil se pourrait bien que R utilise la fonction select d’un autre module. Pour spécifier que vous désirez la fonction select du module dplyr, spécifiez dplyr::select.\nD’autre arguments de select() permettent une sélection rapide. Par exemple, pour obtenir les colonnes contenant des pourcentages:\n\nchicoute |&gt; \n  select(ends_with(\"pourc\")) |&gt; \n  head(3)\n\n# A tibble: 3 × 13\n  C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1    51.5    1.72  0.108     1.21    0.435    0.470  0.0976 0.00258 0.000175\n2    51.3    2.18  0.0985    1.22    0.337    0.439  0.0996 0.00258 0.000407\n3    50.6    2.12  0.0708    1.05    0.373    0.420  0.104  0.00258 0.000037\n# ℹ 4 more variables: Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;,\n#   Al_pourc &lt;dbl&gt;\n\n\n\n3.5.2.2 Filtrer\nComme c’est le cas de la sélection, on pourra filtrer un tableau de plusieurs manières. J’ai déjà présenté comment filtrer selon les indices des lignes. Les autres manières reposent néanmoins sur une opération logique ==, &lt;, &gt; ou %in% (le %in% signifie se trouve parmi et peut être suivi d’un vecteur de valeurs que l’on désire accepter).\nLes conditions booléennes peuvent être combinées avec les opérateurs et, &, et ou, |. Pour rappel,\n\n\nOpération\nRésultat\n\n\n\nVrai et Vrai\nVrai\n\n\nVrai et Faux\nFaux\n\n\nFaux et Faux\nFaux\n\n\nVrai ou Vrai\nVrai\n\n\nVrai ou Faux\nVrai\n\n\nFaux ou Faux\nFaux\n\n\n\n\nLa méthode classique consiste à appliquer une opération logique entre les crochets, par exemple chicoute[chicoute$CodeTourbiere == \"BEAU\", ]\n\nLa méthode tidyverse, plus pratique en mode pipeline, passe par la fonction filter(), i.e.\n\nchicoute |&gt; \n  filter(CodeTourbiere == \"BEAU\")\nCombiner le tout.\n\nchicoute |&gt; \n  filter(Ca_pourc &lt; 0.4 & CodeTourbiere %in% c(\"BEAU\", \"MB\", \"WTP\")) |&gt; \n  select(contains(\"pourc\"))\n\n# A tibble: 4 × 13\n  C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1    51.3    2.18  0.0985   1.22     0.337    0.439  0.0996 0.00258 0.000407\n2    50.6    2.12  0.0708   1.05     0.373    0.420  0.104  0.00258 0.000037\n3    53.8    2.04  0.115    0.947    0.333    0.472  0.106  0.00258 0.000037\n4    52.6    2.11  0.0847   0.913    0.328    0.376  0.111  0.00296 0.000037\n# ℹ 4 more variables: Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;,\n#   Al_pourc &lt;dbl&gt;\n\n\n\n3.5.3 Le format long et le format large\nDans le tableau chicoute, chaque élément possède sa propre colonne. Si l’on voulait mettre en graphique les boxplot des facettes de concentrations d’azote, de phosphore et de potassium dans les différentes tourbières, il faudrait obtenir une seule colonne de concentrations.\nPour ce faire, nous utiliserons la fonction pivot_longer(). L’argument obligatoire (excluant le tableau, qui est implicite dans la chaîne d’opérations), est cols, le nom des colonnes à allonger. Pour obtenir des noms de colonnes allongées personnalisées, on spécifie le nom des variables consistant aux anciens noms de colonnes avec names_to et celui de la nouvelle colonne contenant les valeurs dans values_to. La suite consiste à décrire les colonnes à inclure ou à exclure. Dans le cas qui suit, j’exclue CodeTourbiere de la refonte et j’utilise slice_sample() pour présenter un échantillon aléatoire du résultat. Notez la ligne comprenant la fonction mutate, que l’on verra plus loin. Cette fonction ajoute une colonne au tableau. Dans ce cas-ci, j’ajoute une colonne constituée d’une séquence de nombres allant de 1 au nombre de lignes du tableau (il y en a 90). Cet identifiant unique pour chaque ligne permettra de reconstituer par la suite le tableau initial.\n\nchicoute_long &lt;- chicoute |&gt; \n  select(CodeTourbiere, N_pourc, P_pourc, K_pourc) |&gt; \n  mutate(ID = 1:n())  |&gt;  # mutate ajoute une colonne au tableau\n  # pour l'identifiant, on peut aussi utiliser la commande cur_group_rows()\n  pivot_longer(cols = contains(\"pourc\"), names_to = \"nutrient\", values_to = \"concentration\")\nchicoute_long |&gt;  slice_sample(n = 10)\n\n# A tibble: 10 × 4\n   CodeTourbiere    ID nutrient concentration\n   &lt;chr&gt;         &lt;int&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 WTP              90 P_pourc         0.0848\n 2 MR               39 P_pourc         0.122 \n 3 2                27 N_pourc         2.84  \n 4 2                11 P_pourc         0.138 \n 5 NBM              46 N_pourc         1.60  \n 6 WTP              88 N_pourc         1.63  \n 7 2                18 N_pourc         2.97  \n 8 BP                9 N_pourc         2.16  \n 9 MR               40 P_pourc         0.148 \n10 NBM              48 P_pourc         0.118 \n\n\nL’opération inverse est pivot_wider(), avec laquelle nous sélectionnons une colonne spécifiant les nouvelles colonnes à construire (names_from) ainsi que les valeurs à placer dans ces colonnes (values_from).\n\nchicoute_large &lt;- chicoute_long |&gt; \n  pivot_wider(names_from = nutrient, values_from = concentration)\nchicoute_large |&gt;  slice_sample(n = 10)\n\n# A tibble: 10 × 5\n   CodeTourbiere    ID N_pourc P_pourc K_pourc\n   &lt;chr&gt;         &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 NTP              52    1.88  0.0807   0.394\n 2 SSP              59    1.99  0.136    0.734\n 3 NESP             42    1.92  0.134    1.09 \n 4 1                75    2.55  0.132    0.684\n 5 WTP              90    1.81  0.0848   0.713\n 6 NTP              55    1.88  0.0776   0.419\n 7 BEAU              1    1.72  0.108    1.21 \n 8 1                78    2.31  0.156    0.833\n 9 2                16    2.78  0.167    0.831\n10 1                67    2.22  0.175    0.871\n\n\n\n3.5.4 Combiner des tableaux\nNous avons introduit plus haut la notion de base de données. Nous voudrions peut-être utiliser le code des tourbières pour inclure leur nom, le type d’essai mené à ces tourbières, etc. Importons d’abord le tableau des noms liés aux codes.\n\ntourbieres &lt;- read_csv2(\"data/chicoute_tourbieres.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 11 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (4): Tourbiere, CodeTourbiere, Type, TypeCulture\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntourbieres\n\n# A tibble: 11 × 4\n   Tourbiere               CodeTourbiere Type        TypeCulture\n   &lt;chr&gt;                   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;      \n 1 Beaulieu                BEAU          calibration naturel    \n 2 Brador Path             BP            calibration naturel    \n 3 Lichen (BS2E)           2             validation  cultive sec\n 4 Mannys Brook            MB            calibration naturel    \n 5 Middle Bay Road         MR            calibration naturel    \n 6 North Est of Smelt Pond NESP          calibration naturel    \n 7 North of Blue Moon      NBM           calibration naturel    \n 8 South of Smelt Pond     SSP           calibration naturel    \n 9 Sphaigne (BS2F)         BS2           validation  cultive sec\n10 Sphaigne (BS2F)         1             calibration naturel    \n11 West of Trout Pond      WTP           calibration naturel    \n\n\nNotre information est organisée en deux tableaux, liés par la colonne CodeTourbiere. Comment fusionner l’information pour qu’elle puisse être utilisée dans son ensemble? La fonction left_join effectue cette opération typique avec les bases de données.\n\nchicoute_merge &lt;- left_join(x = chicoute, y = tourbieres, by = \"CodeTourbiere\")\n# ou bien chicoute |&gt;  left_join(y = tourbieres, by = \"CodeTourbiere\")\nchicoute_merge |&gt;  slice_head(n = 4)\n\n# A tibble: 4 × 34\n     ID CodeTourbiere Ordre  Site Traitement DemiParcelle SousTraitement\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;         \n1     1 BEAU          A         1 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n2     2 BEAU          A         2 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n3     3 BEAU          A         3 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n4     4 BEAU          A         4 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n# ℹ 27 more variables: Latitude_m &lt;dbl&gt;, Longitude_m &lt;dbl&gt;,\n#   Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;,\n#   TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;,\n#   TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;,\n#   FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;,\n#   SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, P_pourc &lt;dbl&gt;,\n#   K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, S_pourc &lt;dbl&gt;, …\n\n\nD’autres types de jointures sont possibles, et décrites en détails dans la documentation.\nGarrick Aden-Buie a préparé de jolies animations pour décrire les différents types de jointures.\nleft_join(x, y) colle y à x seulement ce qui dans y correspond à ce que l’on trouve dans x.\n\nright_join(x, y) colle y à x seulement ce qui dans x correspond à ce que l’on trouve dans y.\n\ninner_join(x, y) colle x et y en excluant les lignes où au moins une variable de jointure est absente dans x et y.\n\nfull_join(x, y)garde toutes les lignes et les colonnes de x et y.\n\n\n3.5.5 Opérations sur les tableaux\nLes tableaux peuvent être segmentés en éléments sur lesquels on calculera ce qui nous chante.\nOn pourrait vouloir obtenir :\n\nla somme avec la function sum()\n\nla moyenne avec la function mean() ou la médiane avec la fonction median()\n\nl’écart-type avec la function sd()\n\nles maximum et minimum avec les fonctions min() et max()\n\nun décompte d’occurrence avec la fonction n() ou count()\n\n\nPar exemple,\n\nmean(chicoute$Rendement_g_5m2, na.rm = TRUE)\n\n[1] 13.32851\n\n\nEn mode classique, pour effectuer des opérations sur des tableaux, on utilisera la fonction apply(). Cette fonction prend, comme arguments, le tableau, l’axe (opération par ligne = 1, opération par colonne = 2), puis la fonction à appliquer.\n\napply(chicoute |&gt;  select(contains(\"pourc\")), 2, mean)\n\n     C_pourc      N_pourc      P_pourc      K_pourc     Ca_pourc     Mg_pourc \n5.027911e+01 2.199411e+00 1.388959e-01 8.887000e-01 3.884391e-01 4.980142e-01 \n     S_pourc      B_pourc     Cu_pourc     Zn_pourc     Mn_pourc     Fe_pourc \n1.347177e-01 3.090922e-03 4.089891e-04 6.662155e-03 3.345239e-02 1.514885e-02 \n    Al_pourc \n2.694979e-03 \n\n\nLes opérations peuvent aussi être effectuées par ligne, par exemple une somme (je garde seulement les 10 premiers résultats).\n\napply(chicoute |&gt;  select(contains(\"pourc\")), 1, sum)[1:10]\n\n [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991\n [9] 55.06295 55.16774\n\n\nLa fonction à appliquer peut être personnalisée, par exemple:\n\napply(\n  chicoute |&gt;  select(contains(\"pourc\")), 2,\n  function(x) (prod(x))^(1 / length(x))\n)\n\n     C_pourc      N_pourc      P_pourc      K_pourc     Ca_pourc     Mg_pourc \n50.253429104  2.165246915  0.133754530  0.846193827  0.376192724  0.491763884 \n     S_pourc      B_pourc     Cu_pourc     Zn_pourc     Mn_pourc     Fe_pourc \n 0.129900753  0.003014675  0.000000000  0.006408775  0.024140327  0.014351745 \n    Al_pourc \n 0.002450982 \n\n\nVous reconnaissez cette fonction? C’était la moyenne géométrique (la fonction prod() étant le produit d’un vecteur).\nEn mode tidyverse, on aura besoin principalement des fonction suivantes:\n\n\ngroup_by() pour effectuer des opérations par groupe, l’opération group_by() sépare le tableau en plusieurs petits tableaux, en attendant de les recombiner. C’est un peu l’équivalent des facettes avec le module de visualisation ggplot2, que nous explorons au chapitre 4.\n\nsummarise() pour réduire plusieurs valeurs en une seule, il applique un calcul sur le tableau ou s’il y a lieu sur chaque petit tableau segmenté. Il en existe quelques variantes.\n\n\nsummarise_all() applique la fonction à toutes les colonnes\n\nsummarise_at() applique la fonction aux colonnes spécifiées\n\nsummarise_if() applique la fonction aux colonnes qui ressortent comme TRUE selon une opération booléenne\n\n\n\nmutate() pour ajouter une nouvelle colonne\n\nSi l’on désire ajouter une colonne à un tableau, par exemple le sommaire calculé avec summarise(). À l’inverse, la fonction transmute() retournera seulement le résultat, sans le tableau à partir duquel il a été calculé. De même que summarise(), mutate() et transmute() possèdent leurs équivalents _all(), _at() et _if().\n\n\n\narrange() pour réordonner le tableau\n\nCette fonction est parfois utile lors de la mise en page de tableaux ou de graphiques. Il ne s’agit pas d’une opération sur un tableau, mais plutôt un changement d’affichage en changeant l’ordre d’apparition des données.\n\n\n\nCes opérations sont décrites dans l’aide-mémoire Data Transformation Cheat Sheet (Figure 3.6).\n\n\n\n\nFigure 3.6: Aide-mémoire pour la transformation des données, https://rstudio.github.io/cheatsheets/data-transformation.pdf\n\n\n\nPour effectuer des statistiques par colonne, on utilisera summarise pour des statistiques effectuées sur une seule colonne. summarise peut prendre le nombre désiré de statistiques dont la sortie est un scalaire.\n\nchicoute |&gt; \n  summarise(\n    moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE),\n    ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 2\n  moyenne ecart_type\n    &lt;dbl&gt;      &lt;dbl&gt;\n1    52.1       40.4\n\n\nSi l’on désire un sommaire sur toutes les variables sélectionnées, on utilisera summarise_all(). Pour spécifier que l’on désire la moyenne et l’écart-type, on inscrit les noms des fonctions dans list().\n\nchicoute |&gt; \n  select(contains(\"pourc\")) |&gt; \n  summarise_all(list(mean, sd))\n\n# A tibble: 1 × 26\n  C_pourc_fn1 N_pourc_fn1 P_pourc_fn1 K_pourc_fn1 Ca_pourc_fn1 Mg_pourc_fn1\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1        50.3        2.20       0.139       0.889        0.388        0.498\n# ℹ 20 more variables: S_pourc_fn1 &lt;dbl&gt;, B_pourc_fn1 &lt;dbl&gt;,\n#   Cu_pourc_fn1 &lt;dbl&gt;, Zn_pourc_fn1 &lt;dbl&gt;, Mn_pourc_fn1 &lt;dbl&gt;,\n#   Fe_pourc_fn1 &lt;dbl&gt;, Al_pourc_fn1 &lt;dbl&gt;, C_pourc_fn2 &lt;dbl&gt;,\n#   N_pourc_fn2 &lt;dbl&gt;, P_pourc_fn2 &lt;dbl&gt;, K_pourc_fn2 &lt;dbl&gt;,\n#   Ca_pourc_fn2 &lt;dbl&gt;, Mg_pourc_fn2 &lt;dbl&gt;, S_pourc_fn2 &lt;dbl&gt;,\n#   B_pourc_fn2 &lt;dbl&gt;, Cu_pourc_fn2 &lt;dbl&gt;, Zn_pourc_fn2 &lt;dbl&gt;,\n#   Mn_pourc_fn2 &lt;dbl&gt;, Fe_pourc_fn2 &lt;dbl&gt;, Al_pourc_fn2 &lt;dbl&gt;\n\n\nOn utilisera group_by() pour segmenter le tableau, et ainsi obtenir des statistiques pour chaque groupe.\n\nchicoute |&gt; \n  group_by(CodeTourbiere) |&gt; \n  summarise(\n    moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE),\n    ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)\n  )\n\n# A tibble: 12 × 3\n   CodeTourbiere moyenne ecart_type\n   &lt;chr&gt;           &lt;dbl&gt;      &lt;dbl&gt;\n 1 1                72.1      32.7 \n 2 2                37.1      32.9 \n 3 BEAU            149.       53.2 \n 4 BP               60.4      30.6 \n 5 BS2              27.2      15.5 \n 6 MB               64.7      40.8 \n 7 MR               35.1      10.5 \n 8 NBM              35.1      16.6 \n 9 NESP             21.4       4.88\n10 NTP              47.6      15.9 \n11 SSP              25.7      11.1 \n12 WTP              50.2      28.3 \n\n\nDans le cas de summarise_all, les résultats s’affichent de la même manière.\n\nchicoute |&gt; \n  group_by(CodeTourbiere) |&gt; \n  select(N_pourc, P_pourc, K_pourc) |&gt; \n  summarise_all(list(mean, sd))\n\nAdding missing grouping variables: `CodeTourbiere`\n\n\n# A tibble: 12 × 7\n   CodeTourbiere N_pourc_fn1 P_pourc_fn1 K_pourc_fn1 N_pourc_fn2 P_pourc_fn2\n   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 1                    2.26      0.156        0.880      0.250      0.0193 \n 2 2                    2.76      0.181        1.12       0.178      0.0283 \n 3 BEAU                 2.00      0.0967       1.12       0.179      0.0172 \n 4 BP                   2.05      0.158        0.747      0.161      0.00625\n 5 BS2                  2.08      0.103        1.12       0.420      0.0218 \n 6 MB                   2.15      0.109        0.675      0.114      0.0165 \n 7 MR                   1.99      0.127        0.830      0.0802     0.0131 \n 8 NBM                  2.01      0.127        0.854      0.310      0.0202 \n 9 NESP                 1.76      0.135        0.945      0.149      0.0108 \n10 NTP                  1.83      0.0873       0.402      0.166      0.0103 \n11 SSP                  1.83      0.130        0.700      0.160      0.00383\n12 WTP                  1.79      0.0811       0.578      0.132      0.00587\n# ℹ 1 more variable: K_pourc_fn2 &lt;dbl&gt;\n\n\nPour obtenir des statistiques à chaque ligne, mieux vaut utiliser apply(), tel que vu précédemment. Le point, ., représente le tableau dans une fonction qui n’a pas été conçue pour fonctionner de facto avec dplyr.\n\nchicoute |&gt; \n  select(contains(\"pourc\")) |&gt; \n  apply(1, sum)\n\n [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991\n [9] 55.06295 55.16774 56.41123 55.47917 55.43537 55.79175 55.44561 54.85448\n[17] 54.34262 55.03075 54.40533 51.89319 54.70172 54.62176 54.30250 53.86976\n[25] 53.44731 53.86244 52.43280 54.34978 53.96756 51.46672 55.44267 54.70350\n[33] 55.30711 56.16200 56.64710 55.95499 54.76370 54.32775 54.95419 53.37094\n[41] 53.07855 53.04541 52.09520 52.40456 51.92376 53.33248 56.56405 56.35004\n[49] 56.27185 55.56986 53.81654 55.39638 55.51961 54.88098 54.74774 51.08921\n[57] 51.31462 53.46819 53.15640 52.82020 57.78038 57.94636 56.65558 56.28845\n[65] 55.54463 56.51751 55.36497 56.00594 55.64247 56.56967 56.81674 55.87070\n[73] 55.72308 56.14116 56.42611 55.35650 54.90469 54.03674 53.42991 53.99334\n[81] 53.09085 53.23222 53.28212 53.63192 53.48102 52.31131 51.72026 51.10534\n[89] 51.49055 51.59297\n\n\nPrenons ce tableau des espèces menacées issu de l’Union internationale pour la conservation de la nature distribué par l’OCDE.\n\nlibrary(\"tidyverse\")\nespeces_menacees &lt;- read_csv(\"data/WILD_LIFE_14012020030114795.csv\")\n\nNous exécutons le pipeline suivant.\n\nespeces_menacees |&gt; \n  dplyr::filter(IUCN == \"CRITICAL\", SPEC == \"VASCULAR_PLANT\") |&gt; \n  dplyr::select(Country, Value) |&gt; \n  dplyr::group_by(Country) |&gt; \n  dplyr::summarise(n_critical_plants = sum(Value)) |&gt; \n  dplyr::arrange(desc(n_critical_plants)) |&gt; \n  dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 2\n   Country         n_critical_plants\n   &lt;chr&gt;                       &lt;dbl&gt;\n 1 United States                1222\n 2 Japan                         525\n 3 Canada                        315\n 4 Czech Republic                284\n 5 Spain                         271\n 6 Belgium                       253\n 7 Austria                       172\n 8 Slovak Republic               155\n 9 Australia                     148\n10 Italy                         128\n\n\nCe pipeline consiste à:\nprendre le tableau especes_menacees, puis\n  \nfiltrer pour n'obtenir que les espèces critiques dans la catégorie des plantes vasculaires, puis\n  \nsélectionner les colonnes des pays et des valeurs (nombre d'espèces), puis\n\nsegmenter le tableau en plusieurs tableaux selon le pays, puis\n\nappliquer la fonction sum pour chacun de ces petits tableaux (et recombiner ces sommaires), puis\n\ntrier les pays en nombre décroissant de décompte d'espèces, puis\n\nafficher le top 10\nNotez qu’il aurait aussi été possible d’utiliser la fonction dplyr::slice_max(n_critical_plants, n = 10) pour afficher directement le top 10, sans faire le tri préalable.\n\n3.5.6 Exemple (difficile)\nPour revenir à notre tableau chicoute, imaginez que vous aviez une station météo (station_A) située aux coordonnées (490640, 5702453) et que vous désiriez calculer la distance entre l’observation et la station. Prenez du temps pour réfléchir à la manière dont vous procéderez…\n\nOn pourra créer une fonction qui mesure la distance entre un point x, y et les coordonnées de la station A…\n\ndist_station_A &lt;- function(x, y) {\n  return(sqrt((x - 490640)^2 + (y - 5702453)^2))\n}\n\n… puis ajouter une colonne avec mutate grâce à une fonction prenant les arguments x et y spécifiés.\n\nchicoute |&gt; \n  mutate(dist = dist_station_A(x = Longitude_m, y = Latitude_m)) |&gt; \n  select(ID, CodeTourbiere, Longitude_m, Latitude_m, dist) |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 5\n      ID CodeTourbiere Longitude_m Latitude_m     dist\n   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1    63 BS2                486545    5702197  4103.  \n 2    17 2                  486501    5702627  4143.  \n 3    36 MR                 459875    5701988 30769.  \n 4    51 NTP                487575    5704088  3474.  \n 5     2 BEAU               490634    5702452     6.08\n 6    38 MR                 459880    5701971 30764.  \n 7    39 MR                 459894    5701966 30750.  \n 8    40 MR                 459915    5701994 30728.  \n 9    23 2                  486327    5702544  4314.  \n10    74 1                  486458    5702106  4196.  \n\n\nNous pourrions procéder de la même manière pour fusionner des données climatiques. Le tableau chicoute ne possède pas d’indicateurs climatiques, mais il est possible de les soutirer de stations météo placées près des sites. Ces données ne sont pas disponibles pour le tableau de la chicouté, alors j’utiliserai des données fictives pour l’exemple.\nVoici ce qui pourrait être fait.\n\nCréer un tableau des stations météo ainsi que des indices météorologiques associés à ces stations.\nLier chaque site à une station (à la main où selon la plus petite distance entre le site et la station).\nFusionner les indices climatiques aux sites, puis les sites aux mesures de rendement.\n\nCes opérations demandent habituellement du tâtonnement. Il serait surprenant que même une personne expérimentée soit en mesure de compiler ces opérations sans obtenir de message d’erreur, et retravailler jusqu’à obtenir le résultat souhaité. L’objectif de cette section est de vous présenter un flux de travail que vous pourriez être amenés à effectuer et de fournir quelques éléments nouveaux pour mener à bien une opération. Il peut être frustrant de ne pas saisir toutes les opérations: passez à travers cette section sans jugement. Si vous devez vous frotter à un problème semblable, vous saurez que vous trouverez dans ce manuel une recette intéressante.\n\nmes_stations &lt;- data.frame(\n  Station = c(\"A\", \"B\", \"C\"),\n  Longitude_m = c(490640, 484870, 485929),\n  Latitude_m = c(5702453, 5701870, 5696421),\n  t_moy_C = c(13.8, 18.2, 16.30),\n  prec_tot_mm = c(687, 714, 732)\n)\nmes_stations\n\n  Station Longitude_m Latitude_m t_moy_C prec_tot_mm\n1       A      490640    5702453    13.8         687\n2       B      484870    5701870    18.2         714\n3       C      485929    5696421    16.3         732\n\n\nLa fonction suivante calcule la distance entre des coordonnées x et y et chaque station d’un tableau de stations, puis retourne le nom de la station dont la distance est la moindre.\n\ndist_station &lt;- function(x, y, stations_df) {\n  # stations est le tableau des stations à trois colonnes\n  # 1iere: nom de la station\n  # 2ieme: longitude\n  # 3ieme: latitude\n  distance &lt;- c()\n  for (i in 1:nrow(stations_df)) {\n    distance[i] &lt;- sqrt((x - stations_df[i, 2])^2 + (y - stations_df[i, 3])^2)\n  }\n  nom_station &lt;- as.character(stations_df$Station[which.min(distance)])\n  return(nom_station)\n}\n\nTestons la fonction avec des coordonnées.\n\ndist_station(x = 459875, y = 5701988, stations_df = mes_stations)\n\n[1] \"B\"\n\n\nNous appliquons cette fonction à toutes les lignes du tableau, puis en retournons un échantillon.\n\nchicoute |&gt; \n  rowwise() |&gt; \n  mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) |&gt; \n  select(ID, CodeTourbiere, Longitude_m, Latitude_m, Station) |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 90 × 5\n# Rowwise: \n      ID CodeTourbiere Longitude_m Latitude_m Station\n   &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;  \n 1     1 BEAU               490627    5702454 A      \n 2     2 BEAU               490634    5702452 A      \n 3     3 BEAU               490638    5702461 A      \n 4     4 BEAU               490647    5702453 A      \n 5     5 BEAU               490654    5702445 A      \n 6     6 BP                 484865    5706394 B      \n 7     7 BP                 484054    5706307 B      \n 8     8 BP                 484742    5702280 B      \n 9     9 BP                 484761    5706324 B      \n10    10 BP                 484780    5706364 B      \n# ℹ 80 more rows\n\n\nCela semble fonctionner. On peut y ajouter un left_join() pour joindre les données météo au tableau principal.\n\nchicoute_weather &lt;- chicoute |&gt; \n  rowwise() |&gt; \n  mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) |&gt; \n  left_join(y = mes_stations, by = \"Station\")\nchicoute_weather |&gt;  slice_sample(n = 10)\n\n# A tibble: 90 × 36\n# Rowwise: \n      ID CodeTourbiere Ordre  Site Traitement DemiParcelle SousTraitement\n   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;         \n 1     1 BEAU          A         1 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 2     2 BEAU          A         2 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 3     3 BEAU          A         3 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 4     4 BEAU          A         4 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 5     5 BEAU          A         5 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 6     6 BP            H         1 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 7     7 BP            H         2 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 8     8 BP            H         3 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n 9     9 BP            H         4 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n10    10 BP            H         5 &lt;NA&gt;       &lt;NA&gt;         &lt;NA&gt;          \n# ℹ 80 more rows\n# ℹ 29 more variables: Latitude_m.x &lt;dbl&gt;, Longitude_m.x &lt;dbl&gt;,\n#   Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;,\n#   TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;,\n#   TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;,\n#   FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;,\n#   SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, …\n\n\n\n3.5.7 Exporter un tableau\nSimplement avec write_csv().\n\nwrite_csv(chicoute_weather, \"data/chicoute_weather.csv\")\n\n\n3.5.8 Aller plus loin dans le tidyverse\nLe livre R for data science (2e), de Hadley Wickham et Garrett Grolemund (couverture à la Figure 3.7), est un incontournable.\n\n\n\n\nFigure 3.7: Couverture du libre de Hadley Wickham, Mine Çetinkaya-Rundel et Garrett Grolemund, Source: https://r4ds.hadley.nz/"
  },
  {
    "objectID": "03-tableaux.html#références",
    "href": "03-tableaux.html#références",
    "title": "3  Organisation des données et opérations sur des tableaux",
    "section": "\n3.6 Références",
    "text": "3.6 Références\nParent L.E., Parent, S.É., Herbert-Gentile, V., Naess, K. et Lapointe, L. 2013. Mineral Balance Plasticity of Cloudberry (Rubus chamaemorus) in Quebec-Labrador Bogs. American Journal of Plant Sciences, 4, 1508-1520. DOI: 10.4236/ajps.2013.47183"
  },
  {
    "objectID": "04-visualisation.html#pourquoi-explorer",
    "href": "04-visualisation.html#pourquoi-explorer",
    "title": "4  Visualisation",
    "section": "\n4.1 Pourquoi explorer graphiquement?",
    "text": "4.1 Pourquoi explorer graphiquement?\nLa plupart des graphiques que vous générerez ne seront pas destinés à être publiés. Ils viseront probablement d’abord à explorer des données. Cela vous permettra de mettre en évidence de nouvelles perspectives.\nPrenons par exemple deux variables, \\(X\\) et \\(Y\\). Vous calculez leur moyenne, écart-type et la corrélation entre les deux variables (nous verrons les statistiques plus en détail dans un prochain chapitre).\n\nlibrary(\"tidyverse\")\ndatasaurus &lt;- read_tsv(\"data/DatasaurusDozen.tsv\")\n\ncor_datasaurus &lt;- datasaurus |&gt; \n  group_by(dataset) |&gt; \n  summarise(cor = cor(x = x, y = y, method = \"pearson\"))\n\ndatasaurus |&gt; \n  group_by(dataset) |&gt; \n  summarise_all(list(mean = mean, sd = sd)) |&gt; \n  left_join(cor_datasaurus, by = \"dataset\")\n\n# A tibble: 13 × 6\n   dataset    x_mean y_mean  x_sd  y_sd     cor\n   &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 away         54.3   47.8  16.8  26.9 -0.0641\n 2 bullseye     54.3   47.8  16.8  26.9 -0.0686\n 3 circle       54.3   47.8  16.8  26.9 -0.0683\n 4 dino         54.3   47.8  16.8  26.9 -0.0645\n 5 dots         54.3   47.8  16.8  26.9 -0.0603\n 6 h_lines      54.3   47.8  16.8  26.9 -0.0617\n 7 high_lines   54.3   47.8  16.8  26.9 -0.0685\n 8 slant_down   54.3   47.8  16.8  26.9 -0.0690\n 9 slant_up     54.3   47.8  16.8  26.9 -0.0686\n10 star         54.3   47.8  16.8  26.9 -0.0630\n11 v_lines      54.3   47.8  16.8  26.9 -0.0694\n12 wide_lines   54.3   47.8  16.8  26.9 -0.0666\n13 x_shape      54.3   47.8  16.8  26.9 -0.0656\n\n\nLes moyennes, écarts-types et corrélations sont à peu près les mêmes pour tous les groupes. Peut-on conclure que tous les groupes sont semblables? Pas encore.\nPour démontrer que ces statistiques ne vous apprendront pas grand chose sur la structure des données, Matejka et Fitzmaurice (2017) ont généré 12 jeux de données \\(X\\) et \\(Y\\), ayant chacun pratiquement les mêmes statistiques. Mais avec des structures bien différentes (Figure 4.2)!\n\n\n\n\nFigure 4.2: Animation montrant la progression du jeu de données Datasaurus pour toutes les formes visées. Source: Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing"
  },
  {
    "objectID": "04-visualisation.html#publier-un-graphique",
    "href": "04-visualisation.html#publier-un-graphique",
    "title": "4  Visualisation",
    "section": "\n4.2 Publier un graphique",
    "text": "4.2 Publier un graphique\nVous voilà sensibilisés à l’importance d’explorer les données graphiquement. Mais ce qui ultimement émanera d’un projet sera le rapport que vous déposerez, l’article scientifique que vous ferez publier ou le billet de blogue que vous partagerez sur les réseaux sociaux. Les graphiques inclus dans vos publications méritent une attention particulière pour que votre audience puisse comprendre les découvertes et perspectives offertes par vos travaux. Pour ce faire, un graphique doit répondre honnêtement à la question posée tout en étant attrayant.\n\n4.2.1 Cinq qualités d’un bon graphique\nAlberto Cairo, chercheur spécialisé en visualisation de données, a fait paraître en 2016 le livre The Truthful art. Il note cinq qualités d’une visualisation bien conçue (les citations de cette section proviennent de ma traduction de Alberto Cairo, The Truthful Art (2016), p. 45.).\n\n1- Elle est véritable, puisqu’elle est basée sur une recherche exhaustive et honnête.\n\nCela vaut autant pour les graphiques que pour l’analyse de données. Il s’agit froidement de présenter les données selon l’interprétation la plus exacte. Les pièges à éviter sont le picorage de cerises et la surinterprétation des données. Le picorage, c’est lorsqu’on réduit les perspectives afin de soutenir un argumentaire. Par exemple, retirer des données d’une région ou d’une décennie qui rendraient factice une conclusion fixée a priori. Ceci vaut autant pour les graphiques que pour les statistiques (nous parlerons du p-hacking au prochain chapitre). La surinterprétation, c’est lorsque l’on saute rapidement aux conclusions: par exemple, que l’on génère des corrélations, voire même des relations de causalités à partir de ce qui n’est que du bruit de fond. À ce titre, lors d’une conférence, Heather Krause insiste sur l’importance de faire en sorte que les représentations graphiques répondent correctement aux questions posées dans une étude (Figure 4.3).\n\n\n\n\nFigure 4.3: The F word: Protect your work from four hidden fallacies when working with data, une conférence de Heather Krause, 2018\n\n\n\n\n2- Elle est fonctionnelle, puisqu’elle constitue une représentation précise des données, et qu’elle est construite de manière à laisser les observateurs.trices prendre des initiatives conséquentes.\n\n“La seule chose qui est pire qu’un diagramme en pointe de tarte, c’est d’en présenter plusieurs” (Edward Tufte, designer, cité par Alberto Cairo, 2016, p. 50). Choisir le bon graphique pour représenter vos données est beaucoup moins une question de bon goût qu’une question de démarche rationnelle sur l’objectif visé par la présentation d’un graphique. Je présenterai des lignes guides pour sélectionner le type de graphique qui présentera vos données de manière fonctionnelle en fonction de l’objectif d’un graphique (d’ailleurs, avez-vous vraiment besoin d’un graphique?).\n\n3- Elle est attrayante et intrigante, et même esthétiquement plaisante pour l’audience visée - les scientifiques d’abord, mais aussi le public en général.\n\nEn sciences naturelles, la pensée rationnelle, la capacité à organiser la connaissance et créer de nouvelles avenues sont des qualités qui sont privilégiées au talent artistique. Que vous ayez où non des aptitudes en art visuel, présentez de l’information, pas des décorations. Excel vous permet d’ajouter une perspective 3D à un diagramme en barres. La profondeur contient-elle de l’information? Non. Cette décoration ne fait qu’ajouter de la confusion. Minimalisez, fournissez le plus d’information possible avec le moins d’éléments graphiques possibles. C’est ce que vous proposent les guides graphiques que j’introduirai plus loin.\n\n4- Elle est pertinente, puisqu’elle révèle des évidences scientifiques autrement difficilement accessibles.\n\nIl s’agit de susciter un eurêka, dans le sens qu’elle génère une idée, et parfois une initiative, en un coup d’œil. Le graphique en bâton de hockey est un exemple où l’on a spontanément une idée de la situation. Cette situation peut être la présence d’un phénomène comme l’augmentation de la température globale, mais aussi l’absence de phénomènes pourtant attendus.\n\n5- Elle est instructive, parce que si l’on saisit et accepte les évidences scientifiques qu’elle décrit, cela changera notre perception pour le mieux.\n\nEn présentant cette qualité, Alberto Cairo voulait inciter ses lecteurs.trices à choisir des sujets de discussion visuelle de manière à participer à un monde meilleur. En ce qui nous concerne, il s’agit de bien sélectionner l’information que l’on désire transmettre. Imaginez que vous avez travaillé quelques jours pour créer un graphique, dont vous êtes fier, mais vous (ou un collègue hiérarchiquement favorisé) vous rendez compte que le graphique soutient peu ou pas le propos ou l’objectif de votre thèse/mémoire/rapport/article. Si c’est bien le cas, vous feriez mieux de laisser tomber votre oeuvre et considérer votre démarche comme une occasion d’apprentissage.\nAlberto Cairo résume son livre The Truthful Art dans une entrevue avec le National Geographic."
  },
  {
    "objectID": "04-visualisation.html#choisir-type-graph",
    "href": "04-visualisation.html#choisir-type-graph",
    "title": "4  Visualisation",
    "section": "\n4.3 Choisir le type de graphique le plus approprié",
    "text": "4.3 Choisir le type de graphique le plus approprié\nDe nombreuses manières de présenter les données sont couramment utilisées, comme les nuages de points, les lignes, les histogrammes, les diagrammes en barres et en pointes de tarte. Les principaux types de graphiques seront couverts dans ce chapitre. D’autres types spécialisés seront couverts dans les chapitres appropriés (graphiques davantage orientés vers les statistiques, les biplots, les dendrogrammes, les diagrammes ternaires, les cartes, etc.).\nLa visualisation de données est aujourd’hui devenue un métier pour plusieurs personnes ayant des affinités pour la science, les arts et la communication, dont certaines partagent leur expertise sur le web. À ce titre, le site from data to viz est à conserver dans vos marque-pages. Il comprend des arbres décisionnels qui vous guident vers les options appropriées pour présenter vos données, puis fournissent des exemples pour produire ces visualisations en R. Également, je suggère le site internet de Ann K. Emery, qui présente des lignes guide pour présenter le graphique adéquat selon les données en main. De nombreuses recettes sont également proposées sur r-graph-gallery.com. En ce qui a trait aux couleurs, le choix n’est pas anodin. Si vous avez le souci des détails sur les éléments esthétiques de vos graphiques, je recommande la lecture de ce billet de blog de Lisa Charlotte Rost.\nRetenez néanmois que La couleur est une information. Les couleurs devraient être sélectionnées d’abord pour être lisibles par les personnes ne percevant pas les couleurs (Figure 4.4), selon le support (apte à être photocopié, lisible à l’écran, lisible sur des documents imprimés en noir et blanc) et selon le type de données. Vous pouvez aussi utiliser certains modules comme RColorBrewer comme expliqué dans le billet suivant qui permet d’adopter directement les palettes sélectionnées.\n\nDonnées continues ou catégorielles ordinales: gradient (transition graduelle d’une couleur à l’autre), séquence (transition saccadée selon des groupes de données continues) ou divergentes (transition saccadée d’une couleur à l’autre vers des couleurs divergentes, par exemple orange vers blanc vers bleu).\nDonnées catégorielles nominales: couleurs éloignées d’une catégorie à une autre (plus il y a de catégories, plus les couleurs sont susceptibles de se ressembler).\n\n\n\n\n\nFigure 4.4: Capture d’écran de colorbrewer2.org, qui propose des palettes de couleurs pour créer des cartes, mais l’information est pertinente pour tout type de graphique.\n\n\n\nLe Financial Times offre également ce guide visuel (Figure 4.5).\n\n\n\n\nFigure 4.5: Guide de sélection de graphique du Financial Times\n\n\n\nCairo (2016) propose de procéder en suivant ces étapes:\n\nRéfléchissez au message que vous désirez transmettre: comparer les catégories \\(A\\) et \\(B\\), visualiser une transition ou un changement de \\(A\\) vers \\(B\\), présenter une relation entre \\(A\\) et \\(B\\) ou la distribution de \\(A\\) et \\(B\\) sur une carte.\nEssayez différentes représentations: si le message que vous désirez transmettre a plusieurs volets, il se pourrait que vous ayez besoin de plus d’un graphique.\nMettez de l’ordre dans vos données. C’était le sujet du chapitre 3.\nTestez le résultat. “Hé, qu’est-ce que tu comprends de cela?” Si la personne hausse les épaules, il va falloir réévaluer votre stratégie."
  },
  {
    "objectID": "04-visualisation.html#choisir-son-outil-de-visualisation",
    "href": "04-visualisation.html#choisir-son-outil-de-visualisation",
    "title": "4  Visualisation",
    "section": "\n4.4 Choisir son outil de visualisation",
    "text": "4.4 Choisir son outil de visualisation\nLes modules et logiciels de visualisation sont basés sur des approches que l’on pourrait placer sur un spectre allant de l’impératif au déclaratif.\n\n4.4.1 Approche impérative\nSelon cette approche, vous indiquez comment placer l’information dans un espace graphique. Vous indiquer les symboles, les couleurs, les types de ligne, etc. Peu de choses sont automatisées, ce qui laisse une grande flexibilité, mais demande de vouer beaucoup d’énergie à la manière de coder pour obtenir le graphique désiré. Le module graphique de Excel, ainsi que le module graphique de base de R, utilisent des approches impératives.\n\n4.4.2 Approche déclarative\nLes stratégies d’automatisation graphique se sont grandement améliorées au cours des dernières années. Plutôt que de vouer vos énergies à créer un graphique, il est maintenant possible de spécifier ce que l’on veut présenter.\n\nLa visualisation déclarative vous permet de penser aux données et à leurs relations, plutôt que des détails accessoires.\nJake Vanderplas, Declarative Statistical Visualization in Python with Altair (ma traduction)\n\nL’approche déclarative passe souvent par une grammaire graphique, c’est-à-dire un langage qui explique ce que l’on veut présenter - en mode impératif, on spécifie plutôt comment on veut présenter les données. Le module ggplot2 est le module déclaratif par excellence en R."
  },
  {
    "objectID": "04-visualisation.html#visualisation-en-r",
    "href": "04-visualisation.html#visualisation-en-r",
    "title": "4  Visualisation",
    "section": "\n4.5 Visualisation en R",
    "text": "4.5 Visualisation en R\nEn R, votre trousse d’outils de visualisation mériterait de comprendre les modules suivants.\n\n\nbase. Le module de base de R contient des fonctions graphiques très polyvalentes. Les axes sont générés automatiquement, on peut y ajouter des titres et des légendes, on peut créer plusieurs graphiques sur une même figure, on peut y ajouter différentes géométries (points, lignes et polygones), avec différents types de points ou de traits, différentes couleurs, etc. Les modules spécialisés viennent souvent avec leurs graphiques spécialisés, construits à partir du module de base. En tant que module graphique impératif, on peut tout faire ou presque (pas d’interactivité), mais l’écriture du code est peut expressive.\n\nggplot2. C’est le module graphique par excellence en R (et j’ose dire: en calcul scientifique). ggplot2 se base sur une grammaire graphique. À partir d’un tableau de données, une colonne peut définir l’axe des x, une autre l’axe des y, une autre la couleur des points ou leur dimension. Une autre colonne définissant des catégories peut segmenter la visualisation en plusieurs graphiques alignés horizontalement ou verticalement. Des extensions de ggplot2 permettent de générer des cartes (ggmap), des diagrammes ternaires (ggtern), des animations (gganimate), etc.\n\nplotly. plotly est un module graphique particulièrement utile pour générer des graphiques interactifs. plotly offre une fonction toute simple pour rendre interactif un graphique ggplot2.\n\nNous survolerons rapidement le module de base, irons plus en profondeur avec ggplot2, puis je présenterai brièvement les graphiques interactifs avec plotly."
  },
  {
    "objectID": "04-visualisation.html#module-de-base-pour-les-graphiques",
    "href": "04-visualisation.html#module-de-base-pour-les-graphiques",
    "title": "4  Visualisation",
    "section": "\n4.6 Module de base pour les graphiques",
    "text": "4.6 Module de base pour les graphiques\nNous allons d’abord survoler le module de base, en mode impératif. La fonction de base pour les graphiques en R est plot(). Pour nous exercer avec cette fonction, chargeons d’abord le tableau de données d’exercice iris, publié en 1936 par le célèbre biostatisticien Ronald Fisher.\n\ndata(\"iris\")\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nLe tableau iris contient 5 colonnes, les 4 premières décrivant les longueurs et largeurs des pétales et sépales de différentes espèces d’iris dont le nom apparaît à la 5ième colonne. Vous avez déjà vu au chapitre précédent comment extraire les colonnes d’un tableau; une méthode consiste à appeler le tableau, suivi du $, puis du nom de la colonne, par exemple iris$Species. Pour générer un graphique avec la fonction plot():\n\nplot(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\nPar défaut, le premier argument est le vecteur définissant l’axe des x et le deuxième est celui définissant l’axe des y. Vous rencontrerez souvent de telles utilisations d’arguments implicites, mais je préfère être explicite en définissant bien les arguments: plot(x = iris$Sepal.Length, y = iris$Petal.Length). Le graphique précédent peut être amplement personnalisé en utilisant différents arguments (Figure 4.6).\n\n\n\n\nFigure 4.6: Éléments personnalisables d’un graphique de base\n\n\n\nExercice. Utilisez ces arguments dans la cellule de code de la figure plot(iris$Sepal.Length, iris$Petal.Length).\nRemarquez que la fonction a décidé toute seule de créer un nuage de point. La fonction plot() est conçue pour créer le graphique approprié selon le type des données spécifiées: lignes, boxplot, etc. Si l’on spécifiait les espèces comme argument x:\n\nplot(x = iris$Species, y = iris$Petal.Length)\n\n\n\n# ou bien\n# iris |&gt; \n#   select(Species, Petal.Length) |&gt; \n#   plot()\n\nDe même, la fonction plot() appliquée à un tableau de données générera une représentation bivariée.\n\nplot(iris)\n\n\n\n\nIl est possible d’encoder des attributs grâce à des vecteurs de facteurs (catégories).\n\nplot(iris, col = iris$Species)\n\n\n\n\nL’argument type = \"\" permet de personnaliser l’apparence:\n\n\ntype = \"p\": points\n\ntype = \"l\": ligne\n\ntype = \"o\" et type = \"b\": ligne et points\n\ntype = \"n\": ne rien afficher\n\nCréons un jeu de données.\n\ntime &lt;- seq(from = 0, to = 100, by = 10)\nheight &lt;- abs(time * 0.1 + rnorm(length(time), 0, 2)) \n# abs pour valeur absolue (changement de signe si négatif)\nplot(x = time, y = height, type = \"b\", lty = 2, lwd = 1)\n\n\n\n\nLe type de ligne est spécifié par l’argument lty (qui peut prendre un chiffre ou une chaîne de caractères, i.e. 1 est équivalent de \"solid\", 2 de \"dashed\", 3 de \"dotted\", etc.) et la largeur du trait (valeur numérique), par l’argument lwd.\nLa fonction hist() permet quant à elle de créer des histogrammes. Parmi ses arguments, breaks est particulièrement utile, car il permet d’ajuster la segmentation des incréments.\n\nhist(iris$Petal.Length, breaks = 60)\n\n\n\n\nExercice. Ajustez le titre de l’axe des x, ainsi que les limites de l’axe des x. Êtes-vous en mesure de colorer l’intérieur des barres en bleu?\nLa fonction plot() peut être suivie de plusieurs autres couches comme des lignes (lines() ou abline()), des points (points()), du texte (text()), des polygones (polygon(), des légendes (legend())), etc. On peut aussi personnaliser les couleurs, les types de points, les types de lignes, etc. L’exemple suivant ajoute une ligne au graphique. Ne prêtez pas trop attention aux fonctions predict() et lm() pour l’instant: nous les verrons au chapitre 7.\n\nplot(x = time, y = height)\nlines(x = time, y = predict(lm(height ~ time)))\n\n\n\n\nPour exporter un graphique, vous pouvez passer par le menu Export de RStudio. Mais pour des graphiques destinés à être publiés, je vous suggère d’exporter vos graphiques avec une haute résolution à la suite de la commande png() (ou jpg() ou svg()).\n\nsvg(filename = \"images/mon-graphique.svg\", width = 3000, height = 2000)\n# png(filename = 'images/mon-graphique.png', width = 3000, height=2000, res=300)\nplot(\n  x = iris$Petal.Length,\n  y = iris$Sepal.Length,\n  col = iris$Species,\n  cex = 3, # dimension des points\n  pch = 16 # type de points\n)\ndev.off()\n\npng \n  2 \n\n\nLe format svg crée une version vectorielle du graphique, c’est-à-dire que l’image exportée est un fichier contenant les formes, non pas les pixels. Cela vous permet d’éditer votre graphique dans un logiciel de dessin vectoriel (comme Inkscape).\nDans le bloc de code précédent, j’ai mis en commentaire (# ...) le format d’image png, utile pour les images de type graphique, avec des changements de couleurs drastiques. J’y ai spécifié une haute résolution, à 300 pixels par pouce. Pour les photos, vous préférerez le format jpg. Des éditeurs demanderont peut-être des formats vectoriels comme pdf ou eps. Si vous ne trouvez pas de moyen de modifier un aspect du graphique dans le code (bouger des étiquettes ou des légendes, ajouter des éléments graphiques), vous pouvez exporter votre graphique en format svg et éditer votre graphique dans Inkscape.\nLe module de base de R comprend une panoplie d’autres particularités que je ne couvrirai pas ici, en faveur du module ggplot2."
  },
  {
    "objectID": "04-visualisation.html#la-grammaire-graphique-ggplot2",
    "href": "04-visualisation.html#la-grammaire-graphique-ggplot2",
    "title": "4  Visualisation",
    "section": "\n4.7 La grammaire graphique ggplot2\n",
    "text": "4.7 La grammaire graphique ggplot2\n\nLe module esquisse est une extension de RStudio permettant de générer du code pour le module graphique ggplot2. La vidéo suivant, où j’utilise esquisse, montre ce en quoi consiste une grammaire graphique.\nVideo\nChaque colonne est un élément graphique qui peut être encodé pour former la position en x, en y, la taille des points, leur couleur, ou même le panneau (facet). Mais quelle forme prendra le bidule positionné? Des points, lignes, boxplots, barres? C’est ce que définit une grammaire graphique. Brièvement, une grammaire graphique permet de schématiser des données avec des marqueurs (points, lignes, etc.) sur des attributs visuels (couleurs, dimension, forme). Cette approche permet de dégager 5 composantes.\n\n\nLes données. Votre tableau est bien sûr un argument nécessaire pour générer le graphique.\n\nLes marqueurs. Un terme abstrait pour désigner les points, les lignes, les polygones, les barres, les flèches, etc. En ggplot2, ce sont des géométries, par exemple geom_point() pour définir une géométrie de points.\n\nLes attributs encodés. La position, la dimension, la couleur ou la forme que prendront les géométries. En ggplot2, on les nomme les aesthetics.\n\nLes attributs globaux. Les attributs sont globaux lorsqu’ils sont constants (ils ne dépendent pas d’une variable). Les valeurs par défaut conviennent généralement, mais certains attributs peuvent être spécifiés: par exemple la forme ou la couleur des points, le type de ligne, etc.\n\nLes thèmes. Le thème du graphique permet de personnaliser la manière dont le graphique est rendu. Il existe des thèmes prédéfinis, que vous pouvez ajuster, mais il est possible de créer vos propres thèmes (nous ne couvrirons pas cela dans ce cours).\n\n\n\n\n\nFigure 4.7: Créer une oeuvre d’art avec ggplot2, dessin de @allison_horst.\n\n\n\nLe flux de travail pour créer un graphique à partir d’une grammaire ressemble donc à ceci:\nAvec mon tableau,\nCréer un marqueur (\nencoder(position X = colonne A,\nposition Y = colonne B,\ncouleur = colonne C),\nforme globale = 1)\nAvec un thème noir et blanc\nLe module tidyverse installera des modules utilisés de manière récurrente dans ce cours, comme ggplot2, dplyr, tidyr et readr. Je recommande de le charger au début de vos sessions de travail.\n\nlibrary(\"tidyverse\")\n\nL’approche tidyverse est une grammaire des données. Le module ggplot2, qui en fait partie, est une grammaire graphique (d’où le gg de ggplot)."
  },
  {
    "objectID": "04-visualisation.html#mon-premier-ggplot",
    "href": "04-visualisation.html#mon-premier-ggplot",
    "title": "4  Visualisation",
    "section": "\n4.8 Mon premier ggplot",
    "text": "4.8 Mon premier ggplot\nPour notre premier exercice, je vais charger un tableau depuis le fichier de données abalone.data. Pour plus de détails sur les tableaux de données, consultez le chapitre 3. Le fichier de données porte sur un escargot de mer et comprend le sexe (M: mâle, F: femelle et I: enfant), des poids et dimensions des individus observés, et le nombre d’anneaux comptés dans la coquille.\n\nabalone &lt;- read_csv(\"data/abalone.csv\")\n\nRows: 4177 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Type\ndbl (8): LongestShell, Diameter, Height, WholeWeight, ShuckedWeight, Viscera...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nInspectons l’entête du tableau avec la fonction head().\n\nhead(abalone)\n\n# A tibble: 6 × 9\n  Type  LongestShell Diameter Height WholeWeight ShuckedWeight VisceraWeight\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 M            0.455    0.365  0.095       0.514        0.224         0.101 \n2 M            0.35     0.265  0.09        0.226        0.0995        0.0485\n3 F            0.53     0.42   0.135       0.677        0.256         0.142 \n4 M            0.44     0.365  0.125       0.516        0.216         0.114 \n5 I            0.33     0.255  0.08        0.205        0.0895        0.0395\n6 I            0.425    0.3    0.095       0.352        0.141         0.0775\n# ℹ 2 more variables: ShellWeight &lt;dbl&gt;, Rings &lt;dbl&gt;\n\n\nSuivant la grammaire graphique ggplot2, on pourra créer ce graphique de points comprenant les attributs suivants.\n\n\ndata = abalone, le fichier de données.\n\nmapping = aes(...), spécifié comme attribut de la fonction ggplot(), cet encodage (ou aesthetic) reste l’encodage par défaut pour tous les marqueurs du graphique. Toutefois, l’encodage mapping = aes() peut aussi être spécifié dans la fonction du marqueur (par exemple geom_point()). Dans l’encodage global du graphique, on place en x la longueur de la coquille (x = LongestShell) et on place en y le poids de la coquille (y = ShellWeight).\nPour ajouter une fonction à ggplot, comme une nouvelle couche de marqueur ou des éléments de thème, on utilise le +. Généralement, on change aussi de ligne.\nLe marqueur ajouté est un point, geom_point(), dans lequel on spécifie un encodage de couleur sur la variable Type (colour = Type) et un encodage de dimension du point sur la variable rings (size = Rings). L’attribut alpha = 0.5 se situe hors du mapping et de la fonction aes(): c’est un attribut identique pour tous les points.\n\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)\n\n\n\n\nIl existe plusieurs types de marqueurs:\n\n\ngeom_point() pour les points\n\ngeom_line() pour les lignes\n\ngeom_bar() pour les diagrammes en barre en décompte, geom_col en terme de grandeur et geom_histogram pour les histogrammes\n\ngeom_boxplot() pour les boxplots\n\ngeom_errorbar(), geom_pointrange() ou geom_crossbar() pour les marges d’erreur\n\ngeom_map() pour les cartes\netc.\n\nIl existe plusieurs attributs d’encodage:\n\nla position x, y et z (z pertinent notamment pour le marqueur geom_tile())\nla taille size\n\nla forme des points shape\n\nla couleur, qui peut être discrète ou continue :\n\n\ncolour, pour la couleur des contours\n\nfill, pour la couleur de remplissage\n\n\nle type de ligne linetype\n\nla transparence alpha\n\net d’autres types spécialisés que vous retrouverez dans la documentation des marqueurs\n\nLes types de marqueurs et leurs encodages sont décrits dans la documentation de ggplot2, qui fournit des feuilles aide-mémoire qu’il est commode d’imprimer et d’afficher près de soi (Figure 4.8).\n\n\n\n\nFigure 4.8: Aide-mémoire de ggplot2, source: https://rstudio.github.io/cheatsheets/html/data-visualization.html\n\n\n\n\n4.8.0.1 Les facettes\nDans ggplot2, les facetttes sont un type spécial d’encodage utilisé pour définir des grilles de graphiques. Elles prennent deux formes:\n\nLe collage, facet_wrap(). Une variable catégorielle est utilisée pour segmenter les graphiques en plusieurs graphiques, qui sont placés l’un à la suite de l’autre dans un arrangement spécifié par un nombre de colonnes ou un nombre de lignes.\nLa grille, facet_grid(). Une ou deux variables segmentent les graphiques selon les colonnes et les lignes.\n\nLes facettes peuvent être spécifiées n’importe où dans la chaîne de commande de ggplot2 mais, conventionnellement, on les place tout de suite après la fonction ggplot().\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  facet_wrap(~Type, ncol = 2) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)\n\n\n\n\nLa fonction cut() permet de discrétiser des variables continues en catégories ordonnées - les fonctions peuvent être utilisées à l’intérieur de la fonction ggplot.\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  facet_grid(Type ~ cut(Rings, breaks = seq(0, 30, 5))) +\n  geom_point(mapping = aes(colour = Type), alpha = 0.5)\n\n\n\n\nPar défaut, les axes des facettes, ainsi que leurs dimensions, sont les mêmes. Une telle représentation permet de comparer les facets sur une même échelle. Les axes peuvent être définis selon les données avec l’argument scales, tandis que l’espace des facettes peut être conditionné selon l’argument space - pour plus de détails, voir la fiche de documentation.\nExercice. Personnalisez le graphique avec les données abalone en remplaçant les variables et en réorganisant les facettes.\n\n4.8.1 Plusieurs sources de données\nIl peut arriver que les données pour générer un graphique proviennent de plusieurs tableaux. Lorsqu’on ne spécifie pas la source du tableau dans un marqueur, la valeur par défaut est le tableau spécifié dans l’amorce ggplot(). Il est néanmoins possible de définir une source personnalisée pour chaque marqueur en spécifiant data = ... comme argument du marqueur.\n\nabalone_siteA &lt;- data.frame(\n  LongestShell = c(0.3, 0.8, 0.7),\n  ShellWeight = c(0.05, 0.81, 0.77)\n)\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +\n  geom_point(data = abalone_siteA, size = 8, shape = 4)\n\n\n\n\n\n4.8.2 Exporter avec style\nLe fond gris est une marque distinctive de ggplot2. Il n’est toutefois pas apprécié de tout le monde. D’autres thèmes dits complets peuvent être utilisés (liste des thèmes complets). Les thèmes complets sont appelés avant la fonction theme(), qui permet d’effectuer des ajustements précis dont la liste exhaustive se trouve dans la documentation de ggplot2.\nVous pouvez aussi personnaliser le titre des axes (xlab() et ylab()) ou du graphique (ggtitle()), ou bien tout spécifier dans une même fonction ou bien tout en même temps dans labs(x = \"...\", y = \"...\", title = \"...\"). Il est possible d’utiliser des exposants dans le titre des axes avec la fonction expression(), par exemple labs(x = expression(\"Dose (kg ha\"^\"-1\"~\")\")) pour intituler l’axe des x avec \\(Dose~(kg~ha^{-1})\\). Aussi convient parfois de spécifier les limites (xlim() et ylim(), ou expand_limits(x = c(0, 1), y = c(0, 1))).\nPour exporter un ggplot, on pourra utiliser les commandes de R png(), svg() ou pdf(), ou les outils de RStudio. Toutefois, ggplot2 offre la fonction ggsave(), que l’on place en remorque du graphique, en spécifiant les dimensions (width et height) ainsi que la résolution (dpi). La résolution d’un graphique destiné à la publication est typiquement de plus de 300 dpi.\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +\n  #xlab(\"Length (mm)\") +\n  #ylab(\"Shell weight (g)\") +\n  #ggtitle(\"Abalone\") + # préférablement dans une même ligne\n  labs(x = \"Length (mm)\", y = \"Shell weight (g)\", title = \"Abalone\") +\n  xlim(c(0, 1)) +\n  theme_classic() +\n  theme(\n    axis.title = element_text(size = 20),\n    axis.text = element_text(size = 20),\n    axis.text.y = element_text(size = 20, angle = 90, hjust = 0.5),\n    legend.box = \"horizontal\"\n  )\n\n\n\nggsave(\"images/abalone.png\", width = 8, height = 8, dpi = 300)\n\nNous allons maintenant couvrir différents types de graphiques, accessibles selon différents marqueurs:\n\nles nuages de points\nles diagrammes en ligne\nles boxplots\nles histogrammes\nles diagrammes en barres\n\n4.8.3 Nuages de points\nL’exemple précédent est un nuage de points, que nous avons généré avec le marqueur geom_point(), qui a déjà été passablement introduit. L’exploration de ces données a permis de détecter une croissance exponentielle du poids de la coquille en fonction de sa longueur. Il est clair que les abalones juvéniles (Type I) sont plus petits et moins lourds, mais nous devrons probablement procéder à des tests statistiques pour vérifier s’il y a des différences entre mâles et femelles.\nLe graphique étant très chargé, nous avons utilisé des stratégies pour l’alléger en utilisant de la transparence et des facettes. Le marqueur geom_jitter() peut permettre de mieux apprécier la dispersion des points en ajoutant une dispersion randomisée en x ou en y.\n\nggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +\n  geom_jitter(mapping = aes(colour = Type, size = Rings), alpha = 0.5, width = 0.05, height = 0.1)\n\n\n\n\nDans ce cas-ci, ça ne change pas beaucoup, mais retenons-le pour la suite.\n\n4.8.4 Diagrammes en lignes\nLes lignes sont utilisées pour exprimer des liens entre une suite d’information. Dans la plupart des cas, il s’agit d’une suite d’information dans le temps que l’on appelle les séries temporelles (plus sur ce sujet au chapitre 12. En l’occurrence, les lignes devraient être évitées si la séquence entre les variables n’est pas évidente. Nous allons utiliser un tableau de données de R portant sur la croissance des orangers.\n\ndata(\"Orange\")\nhead(Orange)\n\n  Tree  age circumference\n1    1  118            30\n2    1  484            58\n3    1  664            87\n4    1 1004           115\n5    1 1231           120\n6    1 1372           142\n\n\nLa première colonne spécifie le numéro de l’arbre mesuré, la deuxième son âge et la troisième sa circonférence. Le marqueur geom_line() permet de tracer la tendance de la circonférence selon l’âge. En encodant la couleur de la ligne à l’arbre, nous pourrons tracer une ligne pour chacun d’entre eux.\n\nggplot(data = Orange, mapping = aes(x = age, y = circumference)) +\n  geom_line(aes(colour = Tree))\n\n\n\n\nLa légende ne montre pas les numéros d’arbre en ordre croissant. En effet, la légende (tout comme les facettes) classe les catégories prioritairement selon l’ordre des catégories si elles sont ordinales, ou par ordre alphabétique si les catégories sont nominales. Inspectons la colonne Tree en inspectant le tableau avec la commande str() - la commande glimpse() du tidyverse donne un sommaire moins complet que str().\n\nstr(Orange)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  35 obs. of  3 variables:\n $ Tree         : Ord.factor w/ 5 levels \"3\"&lt;\"1\"&lt;\"5\"&lt;\"2\"&lt;..: 2 2 2 2 2 2 2 4 4 4 ...\n $ age          : num  118 484 664 1004 1231 ...\n $ circumference: num  30 58 87 115 120 142 145 33 69 111 ...\n - attr(*, \"formula\")=Class 'formula'  language circumference ~ age | Tree\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Time since December 31, 1968\"\n  ..$ y: chr \"Trunk circumference\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(days)\"\n  ..$ y: chr \"(mm)\"\n\n\nEn effet, la colonne Tree est un facteur ordinal dont les niveaux sont dans le même ordre que celui la légende.\n\n4.8.5 Les histogrammes\nNous avons vu les histogrammes dans la brève section sur les fonctions graphiques de base dans R: il s’agit de segmenter l’axe des x en incréments, puis de présenter sur l’axe de y le nombre de données que l’on retrouve dans cet incrément. Le marqueur à utiliser est geom_histogram().\nRevenons à nos escargots. Comment présenteriez-vous la longueur de la coquille selon la variable Type? Selon des couleurs ou des facettes? La couleur, dans le cas des histogrammes, est celle du pourtour des barres. Pour colorer l’intérieur des barres, l’argument à utiliser est fill.\n\nggplot(data = abalone, mapping = aes(x = LongestShell)) +\n  geom_histogram(mapping = aes(fill = Type), colour = \"black\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nOn n’y voit pas grand chose. Essayons plutôt les facettes.\n\nggplot(data = abalone, mapping = aes(x = LongestShell)) +\n  facet_grid(Type ~ .) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLes facettes permettent maintenant de bien distinguer la distribution des longueurs des juvéniles. L’argument bins, tout comme l’argument breaks du module graphique de base, permet de spécifier le nombre d’incréments, ce qui peut être très utile en exploration de données.\n\nggplot(data = abalone, mapping = aes(x = LongestShell)) +\n  facet_grid(Type ~ .) +\n  geom_histogram(bins = 60, colour = \"white\")\n\n\n\n\nLe nombre d’incréments est un paramètre qu’il ne faut pas sous-estimer. À preuve, ce tweet de @NicholasStrayer:\n\n\nHistograms are fantastic, but make sure your bin-width/number is chosen well. This is the exact same data, plotted with different bin-widths. Notice that the pattern doesn't necessarily get clearer as bin num increases. #dataviz pic.twitter.com/3MhSFwTVPH\n\n— Nick Strayer (@NicholasStrayer) 7 août 2018\n\n\n4.8.6 Boxplots\nLes boxplots sont une autre manière de visualiser des distributions. L’astuce est de créer une boîte qui s’étend du premier quartile (valeur à laquelle 25% des données ont une valeur inférieure) au troisième quartile (valeur à laquelle 75% des données ont une valeur inférieure). Une barre à l’intérieur de cette boîte est placée à la médiane (qui est en fait le second quartile). De part et d’autre de la boîte, on retrouve des lignes spécifiant l’étendue hors quartiles. Cette étendue peut être déterminée de plusieurs manières, mais dans le cas de ggplot2, il s’agit de 1.5 fois l’étendue de la boîte (l’écart interquartile). Au-delà de ces lignes, on retrouve les points représentant les valeurs extrêmes. Le marqueur à utiliser est geom_boxplot(). L’encodage x est la variable catégorielle et l’encodage y est la variable continue.\n\nggplot(data = abalone, mapping = aes(x = Type, y = LongestShell)) +\n  geom_boxplot()\n\n\n\n\nExercice. On suggère parfois de présenter les mesures sur les boxplots. Utiliser geom_jitter() avec un bruit horizontal.\n\n4.8.7 Les diagrammes en barre\nLes diagrammes en barre représentent une variable continue associée à une catégorie. Les barres sont généralement horizontales et ordonnées. Nous y reviendrons à la fin de ce chapitre, mais retenez pour l’instant que dans tous les cas, les diagrammes en barre doivent inclure le zéro pour éviter les mauvaises interprétations.\nPour les diagrammes en barre, nous allons utiliser les données de l’union internationale pour la conservation de la nature distribuées par l’OCDE.\n\n# Certaines  colonnes de caractères sont considérées comme booléennes\n# mieux vaut définir leur type pour s'assurer que le bon type\n# soit attribué\nespeces_menacees &lt;- read_csv(\"data/WILD_LIFE_14012020030114795.csv\",\n  col_types = list(\n    \"c\", \"c\", \"c\", \"c\",\n    \"c\", \"c\", \"c\", \"c\",\n    \"d\", \"c\", \"c\", \"c\",\n    \"d\", \"c\", \"c\"\n  )\n)\nhead(especes_menacees)\n\n# A tibble: 6 × 15\n  IUCN       `IUCN Category`       SPEC  Species COU   Country `Unit Code` Unit \n  &lt;chr&gt;      &lt;chr&gt;                 &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;\n1 TOT_KNOWN  Total number of know… MAMM… Mammals AUS   Austra… NBR         Numb…\n2 ENDANGERED Number of endangered… MAMM… Mammals AUS   Austra… NBR         Numb…\n3 CRITICAL   Number of critically… MAMM… Mammals AUS   Austra… NBR         Numb…\n4 VULNERABLE Number of vulnerable… MAMM… Mammals AUS   Austra… NBR         Numb…\n5 THREATENED Total number of thre… MAMM… Mammals AUS   Austra… NBR         Numb…\n6 TOT_KNOWN  Total number of know… MAMM… Mammals AUT   Austria NBR         Numb…\n# ℹ 7 more variables: `PowerCode Code` &lt;dbl&gt;, PowerCode &lt;chr&gt;,\n#   `Reference Period Code` &lt;chr&gt;, `Reference Period` &lt;chr&gt;, Value &lt;dbl&gt;,\n#   `Flag Codes` &lt;chr&gt;, Flags &lt;chr&gt;\n\n\nL’exercice consiste à créer un diagramme en barres horizontales du nombre de plantes vasculaires menacées de manière critique pour les 10 pays qui en contiennent le plus. Je vais effectuer quelques opérations sur ce tableau afin d’en arriver avec un tableau que nous pourrons convenablement mettre en graphique: si vous avez bien suivi le dernier chapitre, ces opérations devraient vous être familières!\nNous allons filtrer le tableau pour obtenir le nombre de plantes vasculaires critiquement menacées, sélectionner seulement le pays et le nombre d’espèces, les grouper par pays, additionner toutes les espèces pour chaque pays et enfin sélectionner et arranger les 10 premiers en ordre décroissant. Comme vous le voyez, la création de graphique est liée de près avec la manipulation des tableaux!\n\nespeces_crit &lt;- especes_menacees |&gt; \n  filter(IUCN == \"CRITICAL\", SPEC == \"VASCULAR_PLANT\") |&gt; \n  dplyr::select(Country, Value) |&gt; \n  group_by(Country) |&gt; \n  summarise(n_critical_species = sum(Value)) |&gt; \n  slice_max(n = 10, order_by = n_critical_species)\nespeces_crit\n\n# A tibble: 10 × 2\n   Country         n_critical_species\n   &lt;chr&gt;                        &lt;dbl&gt;\n 1 United States                 1222\n 2 Japan                          525\n 3 Canada                         315\n 4 Czech Republic                 284\n 5 Spain                          271\n 6 Belgium                        253\n 7 Austria                        172\n 8 Slovak Republic                155\n 9 Australia                      148\n10 Italy                          128\n\n\nLe premier type de diagramme en barre que nous allons couvrir est obtenu par le marqueur geom_col().\n\nggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_col()\n\n\n\n\nCe graphique est perfectible. Les barres sont verticales et non ordonnées. Souvenons-nous que ggplot2 ordonne par ordre alphabétique si aucun autre ordre est spécifié. Nous pouvons changer l’ordre en changeant l’ordre des niveaux de la variable Country selon le nombre d’espèces grâce à la fonction fct_reorder.\n\nespeces_crit &lt;- especes_crit %&gt;%\n  mutate(Country = fct_reorder(Country, n_critical_species))\n\nPour faire pivoter le graphique, nous ajoutons coord_flip() à la séquence.\n\nggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\nUne autre méthode, geom_bar(), est un raccourci permettant de compter le nombre d’occurrence d’une variable unique. Par exemple, dans le tableau abalone, le nombre de fois que chaque niveau de la variable Type.\n\nggplot(data = abalone, mapping = aes(x = Type)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\nPersonnellement, j’aime bien passer par un diagramme en lignes avec le marqueur geom_segment(). Cela me donne la flexibilité pour définir un largeur de trait et éventuellement d’ajouter un point au bout pour en faire un diagramme en suçon. Tenez, j’en profite aussi pour y ajouter du texte (décalé horizontalement) et étendre les limtes pour m’assurer que les chiffres apparaissent bien.\n\nggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) +\n  geom_point(size = 5, colour = \"black\") +\n  geom_text(aes(label = n_critical_species), hjust = -0.5) + # si ce ne sont pas des valeurs entières, arrondir avec signif()\n  expand_limits(y = c(0, 1300)) +\n  coord_flip() +\n  theme_bw()\n\n\n\n\nLes diagrammes en barre peuvent être placés en relation avec d’autres. Reprenons notre manipulation de données précédente, mais en incluant tous les pays, pour les trois niveaux d’alerte, pour les poissons.\n\nespeces_pays_iucn &lt;- especes_menacees |&gt; \n  filter(IUCN %in% c(\"ENDANGERED\", \"VULNERABLE\", \"CRITICAL\"), SPEC == \"FISH_TOT\") |&gt; \n  dplyr::select(IUCN, Country, Value) |&gt; \n  group_by(Country, IUCN) |&gt; \n  summarise(n_species = sum(Value)) |&gt; \n  group_by(Country) |&gt; \n  mutate(n_tot = sum(n_species)) |&gt; \n  ungroup() |&gt;  # pour pouvoir modifier Country, non modifiable tant qu'elle est une variable de regroupement (voir group_by)\n  mutate(Country = fct_reorder(Country, n_tot))\n\n`summarise()` has grouped output by 'Country'. You can override using the\n`.groups` argument.\n\nhead(especes_pays_iucn)\n\n# A tibble: 6 × 4\n  Country   IUCN       n_species n_tot\n  &lt;fct&gt;     &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 Australia CRITICAL           8    48\n2 Australia ENDANGERED        16    48\n3 Australia VULNERABLE        24    48\n4 Austria   CRITICAL           6    39\n5 Austria   ENDANGERED        18    39\n6 Austria   VULNERABLE        15    39\n\n\nPour placer les barres les unes à côté des autres, nous spécifions position = \"dodge\".\n\nggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +\n  geom_col(aes(fill = IUCN), position = \"dodge\") +\n  coord_flip()\n\n\n\n\nIl est parfois plus pratique d’utiliser les facettes.\n\nggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +\n  facet_grid(IUCN ~ .) +\n  geom_col() +\n  coord_flip()\n\n\n\n\nPour perfectionner encore ce graphique, on pourrait réordonner les facettes individuellement, mais ne nous égarons par trop.\n\n4.8.8 Exporter un graphique\nPlus besoin d’utiliser la fonction png() en mode ggplot2. Utilisons plutôt ggsave().\n\nggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +\n  facet_grid(IUCN ~ .) +\n  geom_col(aes(fill = IUCN)) +\n  coord_flip()\n\n\n\nggsave(\"images/especes_pays_iucn.png\", width = 6, height = 8, dpi = 300)"
  },
  {
    "objectID": "04-visualisation.html#les-graphiques-comme-outil-dexploration-des-données",
    "href": "04-visualisation.html#les-graphiques-comme-outil-dexploration-des-données",
    "title": "4  Visualisation",
    "section": "\n4.9 Les graphiques comme outil d’exploration des données",
    "text": "4.9 Les graphiques comme outil d’exploration des données\n\n\n\n\nFigure 4.9: Explorer les données avec ggplot2, dessin de @allison_horst.\n\n\n\nLa plupart des graphiques que vous créerez ne seront pas destinés à être publiés, mais serviront d’outil d’exploration des données. Le jeu de données datasaurus, présenté en début de chapitre, permet de saisir l’importance des outils graphiques pour bien comprendre les données.\n\ndatasaurus &lt;- read_tsv(\"data/DatasaurusDozen.tsv\")\n\nRows: 1846 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (1): dataset\ndbl (2): x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(datasaurus)\n\n# A tibble: 6 × 3\n  dataset     x     y\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 dino     55.4  97.2\n2 dino     51.5  96.0\n3 dino     46.2  94.5\n4 dino     42.8  91.4\n5 dino     40.8  88.3\n6 dino     38.7  84.9\n\n\nProjetons d’abord les coordonnées x et y sur un graphique.\n\nggplot(data = datasaurus, mapping = aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\nCe graphique pourrait ressembler à une distribution binormale, ou un coup de 12 dans une porte de grange. Mais on aperçoit des données alignées, parfois de manière rectiligne, parfois en forme d’ellipse. Le tableau datasaurus a une colonne d’information supplémentaire. Utilisons-la comme catégorie pour générer des couleurs différentes.\n\nggplot(data = datasaurus, mapping = aes(x = x, y = y)) +\n  geom_point(mapping = aes(colour = dataset))\n\n\n\n\nCe n’est pas vraiment plus clair. Il y a toutefois des formes qui se dégagent, comme des ellipses et des lignes. Et si je regarde bien, j’y vois une étoile. La catégorisation pourrait-elle être mieux utilisée si on segmentait par facettes au lieu des couleurs?\n\nggplot(data = datasaurus, mapping = aes(x = x, y = y)) +\n  facet_wrap(~dataset, nrow = 2) +\n  geom_point(size = 0.5) +\n  coord_equal()\n\n\n\n\nVoilà! Fait intéressant : ni les statistiques, ni les algorithmes de regroupement ne nous auraient été utiles pour différencier les groupes!\n\n4.9.1 Des graphiques interactifs!\nLes graphiques sont traditionnellement des images statiques. Toutefois, les graphiques n’étant pas dépendants de supports papiers peuvent être utilisés de manière différente, en ajoutant une couche d’interaction. Conçue à Montréal, plotly est un module graphique interactif en soi. Il peut être utilisé grâce à son outil web, tout comme il peut être interfacé avec R, Python, javascript, etc. Mais ce qui retient notre attention ici est son interface avec ggplot2.\nLes graphiques ggplot2 peuvent être enregistrés en tant qu’objets. Il peuvent conséquemment être manipulés par des fonctions. La fonction ggplotly permet de rendre votre ggplot interactif.\n\nlibrary(\"plotly\")\n\n\nAttachement du package : 'plotly'\n\n\nL'objet suivant est masqué depuis 'package:ggplot2':\n\n    last_plot\n\n\nL'objet suivant est masqué depuis 'package:stats':\n\n    filter\n\n\nL'objet suivant est masqué depuis 'package:graphics':\n\n    layout\n\nespeces_crit_bar &lt;- ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +\n  geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) +\n  geom_point(size = 6) +\n  coord_flip()\nggplotly(especes_crit_bar)\n\n\n\n\n\nVous pouvez publier votre graphique plotly en ligne pour le partager ou l’inclure dans une publication web. Il vous faudra créer un compte plotly, puis générer une clé d’utilisation dans Settings &gt; API Keys &gt; Generate key. Pour des raisons de sécurité, la clé du bloc ci-dessous ne fonctionnera pas. J’ai désactivé le bloc de code, mais le résultat se trouve en suivant le lien généré par plotly: https://plot.ly/~essicolo/152/.\nSys.setenv(\"plotly_username\"=\"essicolo\")\nSys.setenv(\"plotly_api_key\"=\"iavd1ycE2iiqOp9YD45I\")\n\nchart_link &lt;- api_create(x = ggplotly(especes_crit_bar), \n                         filename = \"public-graph\",\n                         sharing = \"public\",\n                         fileopt = \"overwrite\")\nchart_link\n\n4.9.2 Des extensions de ggplot2\n\nggplot2 est un module graphique élégant et polyvalent. Il a pourtant bien des limitations. Justement, le module est conçu pour être implémenté avec des extensions. Vous en trouverez plusieurs sur exts.ggplot2.tidyverse.org, mais en trouverez de nombreuses autres en cherchant avec le terme ggplot2 sur github.com, probablement la plate-forme (voire un réseau social) de développement de logiciels la plus utilisée dans le monde. En voici quelques unes.\n\n\nggthemr: spécifier un thème graphique une seule fois dans votre session, et tout le reste suit.\n\ncowplot et patchwork permettent de créer des graphiques prêts pour la publication, par exemple en créant des grilles de plusieurs ggplots, en les numérotant, etc.\nSi les thèmes de base ne vous conviennent pas, vous en trouverez d’autres en installant ggthemes.\n\nggmap et ggspatial sont deux extensions pour créer des cartes. Un chapitre sur les données spatiales est en développement.\n\nggtern permet de créer des diagrammes ternaires, qui sont utiles pour la visualisation de proportions incluant trois composantes, par exemple pour la granulométrie des sols.\n\nggprism permet de personnaliser les ggplots et leur donner un aspect similaire aux graphiques du logiciel statistique prism\n\n\n4.9.3 Aller plus loin avec ggplot2\n\n\n\nClaus O. Wilke est professeur en biologie intégrative à l’Université du Texas à Austin. Son livre Fundamentals of Data Visualization est un guide théorique et pratique pour la visualisation de données avec ggplot2.\nLe site data-to-viz.com vous accompagne dans le choix du graphique à créer selon vos données.\nLe site r-graph-gallery.com offre des recettes pour créer des graphiques avec ggplot2.\nLe livre R Graphics Cookbook, disponible entièrement en ligne, offre aussi des recettes pour réaliser différents graphiques."
  },
  {
    "objectID": "04-visualisation.html#extra-règles-particulières",
    "href": "04-visualisation.html#extra-règles-particulières",
    "title": "4  Visualisation",
    "section": "\n4.10 Extra: Règles particulières",
    "text": "4.10 Extra: Règles particulières\n\nLes mauvais graphiques peuvent survenir à cause de l’ignorance, bien sûr, mais souvent ils existent pour la même raison que la boeuferie [bullhist] verbale ou écrite. Parfois, les gens ne se soucient pas de la façon dont ils présentent les données aussi longtemps que ça appuie leurs arguments et, parfois, ils ne se soucient pas que ça porte à confusion tant qu’ils ont l’air impressionnant. \\(-\\) Carl Bergstorm et Jevin West, Calling Bullshit Read-Along Week 6: Data Visualization\n\nUne représentation visuelle est un outil tranchant qui peut autant présenter un état véritable des données qu’une perspective trompeuse. Bien souvent, une ou plusieurs des 5 qualités ne sont pas respectées. Les occasions d’erreur ne manquent pas - j’en ai fait mention dans la section Choisir le bon type de graphique. Maintenant, notons quelques règles particulières.\n\n4.10.1 Ne tronquez pas inutilement l’axe des \\(y\\)\n\nTronquer l’axe vertical peut amener à porter de fausses conclusions.\n\n\n\n\nFigure 4.10: Effets sur la perception d’utiliser différentes références. Source: Yau (2015), Real Chart Rules to Follow.\n\n\n\n\n\nEffets sur la perception d’utiliser différentes références. Source: Yau (2015), Real Chart Rules to Follow.\n\nLa règle semble simple: les diagrammes en barre (utilisés pour représenter une grandeur) devraient toujours présenter le 0 et les diagrammes en ligne (utilisés pour présenter des tendances) ne requièrent pas nécessairement le zéro (Bergstrom et West, Calling bullshit: Misleading axes on graphs. Mais le zéro n’est pas toujours lié à une quantité particulière : par exemple, la température ou un log-ratio. De plus, avec un diagramme en ligne, on pourra toujours magnifier des tendances en zoomant sur une variation somme toute mineure. On arrive donc moins à une règle qu’une qualité d’un bon graphique, en particulier la qualité no 1 de Cairo: offrir une représentation honnête des données. Par exemple, Nathan Yau, auteur du blogue Flowing Data, propose de présenter des résultats de manière relative à la mesure initiale. C’est d’ailleurs ce qui a été fait pour générer le graphique de Michael Mann et al. au tout début de ce chapitre à la Figure 4.1, où le zéro correspond à la moyenne des températures enregistrées entre 1961 et 1990.\nIl peut être tentant de tronquer l’axe des \\(y\\) lorsque l’on désire superposer deux axes verticaux. Souvent, l’utilisation de plusieurs axes verticaux amène une perception de causalité dans des situations de fausses corrélations. On ne devrait pas utiliser plusieurs axes verticaux.\n\n4.10.2 Utilisez un encrage proportionnel\nCette règle a été proposée par Edward Tufte dans Visual Display of Quantitative Information. Une des raisons pour lesquelles on évite de tronquer l’axe des \\(y\\) en particulier pour les diagrammes en barre est que l’aire représentant une mesure (la quantité d’“encre” nécessaire pour la dessiner) devrait être proportionnelle à sa magnitude. Les diagrammes en barre sont particulièrement sensibles à cette règle, étant donnée que la largeur des barres peuvent amplifier l’aire occupée. Deux solutions dans ce cas: (1) utiliser des barres minces ou (2) préférer des “diagrammes de points” (dot charts, à ne pas confondre aux nuages de points).\nL’encrage a beau être proportionnel, la difficulté que les humains éprouvent à comparer la dimension des cercles, et a fortiori la dimension de parties de cercle, donne peu d’avantage à utiliser des diagrammes en pointe de tarte, souvent utilisés pour illustrer des proportions. Nathan Yau suggère de les utiliser avec suspicions et d’explorer d’autres options.\n\nPour comparer deux proportions, une avenue intéressante est le diagramme en pente, suggéré notamment par Ann K. Emery.\n\nPar extension, le diagramme en pente devient un diagramme en ligne lorsque plusieurs types de proportions sont comparées, ou lorsque des proportions évoluent selon des données continues.\nDe la même manière, les diagrammes en bulles ne devraient pas être représentatifs de la quantité, mais permettent plutôt de contextualiser des données. Justement, le graphique tiré des données de Gap minder présenté plus haut est une contextualisation: l’aire d’un cercle ne permet pas de saisir la population d’un pays, mais de comparer grossièrement la population d’un pays par rapport aux autres.\n\n4.10.3 Publiez vos données\nVous avez peut-être déjà feuilleté un article et voulu avoir accès aux données incluses dans un graphique. Il existe des outils pour digitaliser des graphiques pour en extraire les données. Mais le processus est fastidieux, long, souvent peu précis. De plus en plus, les chercheurs sont encouragés à publier leurs données et leurs calculs. Matplotlib et Seaborn sont des outils graphiques classiques qui devraient être accompagnés des données et calculs ayant servi à les générer. Mais ce n’est pas idéal non plus. En revanche, les outils graphiques modernes comme Plotly et Altair peuvent être exportés en code javascipt, qui contient toutes les informations sur les données et la manière de les représenter graphiquement. Ce chapitre a pour objectif de vous familiariser avec les outils de base les plus communément utilisés en calcul scientifique avec R, mais je vous encourage à explorer la nouvelle génération d’outils graphiques. Nous verrons ça au chapitre 5.\n\n4.10.4 Visitez Junk Charts de temps à autre\nLe statisticien et blogueur Kaiser Fung s’affaire quotidiennement à proposer des améliorations à de mauvais graphiques sur son blogue Junk Charts."
  },
  {
    "objectID": "05-github.html#un-code-reproductible",
    "href": "05-github.html#un-code-reproductible",
    "title": "5  Science ouverte et reproductibilité",
    "section": "\n5.1 Un code reproductible",
    "text": "5.1 Un code reproductible\n\n\n\n\nFigure 5.2: A Guide to Reproducible Code in Ecology and Evolution, BES 2017\n\n\n\nLa British ecological society offre des lignes guide pour créer un flux de travail reproductible (BES, 2017). En outre, les principes suivants doivent être respectés (ma traduction, avec ajouts).\n\nCommencez votre analyse à partir d’une copie des données brutes. Les données doivent être fournies dans un format ouvert (csv, json, sqlite, etc.). Évitez de démarrer une analyse par un chiffrier électronique ou un logiciel propriétaire (qui n’est pas open source). En ce sens, démarrer avec Excel (xls ou xlsx) est à éviter, tout comme le sont les données encodées pour SPSS ou SAS.\nToute opération sur les données, que ce soit du nettoyage, des fusions, des transformations, etc. devrait être effectuée avec du code, non pas manuellement. S’il s’agit d’une erreur de frappe dans un tableau, on peut déroger à la règle. Mais s’il s’agit par exemple d’éliminer des outliers, ne supprimez pas des entrées de vos données brutes. De même, n’effectuez pas de transformation de vos données brutes à l’extérieur du code. En somme, vos calculs devraient être en mesure d’être lancés d’un seul coup, sans opérations manuelles intermédiaires.\nSéparez vos opérations en unités logiques thématiques. Par exemple, vous pourriez séparer votre code en parties: (i) charger, fusionner et nettoyer les données, (ii) analyser les données, (iii) créer des fichiers comme des tableaux et des figures.\nÉliminez la duplication du code en créant des fonctions personnalisées. Assurez-vous de commenter vos fonctions en détails, expliquez ce qui est attendu comme entrées et comme sorties, ce qu’elles font et pourquoi.\nDocumentez votre code et vos données à même les feuilles de calcul ou dans un fichier de documentation séparé.\nTout fichier intermédiaire devrait être séparé de vos données brutes.\n\n\n5.1.1 Structure d’un projet\nUn projet de calcul devrait être contenu en un seul dossier. Si vous n’avez que quelques projets, il est assez facile de garder l’info en mémoire. Toutefois, en particulier en milieu d’entreprise, il se pourrait fort bien que vous ayez à mener plusieurs projets de front. Certaines entreprises créent des numéros de projet: vous aurez avantage à nommer vos dossiers avec ces numéros, incluant une brève description. Pour ma part, j’ordonne mes projets chronologiquement par année, avec un descriptif.\n📁 2019_abeille-canneberge\nNotez que je n’utilise ni espace, ni caractère spécial dans le nom du fichier, pour éviter les erreurs potentielles avec des logiciels capricieux.\nÀ l’intérieur du dossier racine du projet, j’inclus l’information générale: données source (souvent des fichiers Excel), manuscrit (mémoire, thèse, article, etc.) documentation particulière (pour les articles, j’utilise Zotero, un gestionnaire de référence), photos et, évidemment, mon dossier de code (par exemple rstats).\n\n📂 2019_abeille-canneberge\n|-📁 documentation\n|-📁 manuscrit\n|-📁 photos\n|-📁 rstats\n|-📁 source\nSi vous rédigez votre manuscrit à même votre code (en Latex, Lyx, markdown, R markdown ou Quarto que nous verrons plus loin), vous pouvez très bien l’inclure dans votre fichier de calcul.\nÀ l’intérieur du fichier de calcul, vous aurez votre projet RStudio et vos feuilles de calcul séquencées. J’utilise 01-, et non pas 1- pour éviter que le 10- suive le 1- dans le classement en ordre alpha-numérique au cas où j’aurais plus de 10 feuilles de calcul. J’inclus un fichier README.md (extension md pour markdown), qui contient les informations générales de mes calculs. Les données brutes (csv) sont placées dans un dossier data, mes graphiques sont exportés dans un dossier images, mes tableaux sont exportés dans un dossier tables et mes fonctions externes sont exportées dans un dossier lib.\n\n📂 rstats\n|-📁 data\n|-📁 images\n|-📁 lib\n|-📁 tables\n📄 bees.Rproj\n📄 01_clean-data.R\n📄 02_data-mining.R\n📄 03_data-analysis.R\n📄 04_data-modeling.R\n📄 README.md\nJe décris les noms de fichiers dans la langue de communication utile pour le rendu final du projet, souvent en anglais lors de publications académiques. J’évite les noms de fichier qui ne sont pas informatifs, par exemple 01.R ou Rplot1.png, ainsi que les majuscules, les caractères spéciaux et les espaces comme dans Deuxième essai.R (le README.md est une exception).\nPour partager un dossier de projet sur R, on n’a qu’à le compresser (zip), puis à l’envoyer. Pour que le code fonctionne sur un autre ordinateur, les liens vers les fichiers de données à importer ou les graphiques exportés doivent être relatifs au fichier R ouvert dans votre projet, non pas le chemin complet sur votre ordinateur.\n\n\n\n\nFigure 5.3: Retrouvez votre chemin, dessin de @Allison Horst\n\n\n\nTout comme la BSE, l’organisme sans but lucratif rOpenSci offre un guide sur la reproductibilité (le répertoire est maintenant archivé et n’est plus mis à jour depuis 2022, mais vous y avez tout de même accès en lecture seule).\n\n5.1.2 Les formats markdown\n\nUn code reproductible est un code bien décrit. La structure de projet présentée précédemment propose de segmenter le code en plusieurs fichiers R. Cette manière de procéder est optionnelle. Si le fichier de calcul n’est pas trop encombrant, on pourra n’en utiliser qu’un seul, par exemple stats.R. À l’intérieur même des feuilles de calcul R, vous devrez commenter votre code pour en expliquer les étapes, par exemple:\n#############\n## Titre 1 ##\n#############\n\n# Titre 2\n## Titre 3\ndata &lt;- read_csv(\"data/abeilles.csv\") # commentaire particulier\nRStudio a développé une approche plus conviviale avec son format R markdown. Le langage markdown permet de formater un texte avec un minimum de décorations, et R markdown permet d’intégrer du texte et des codes. Le manuel original de notes de cours était par ailleurs entièrement écrit en R markdown.\n\n\n\n\nFigure 5.4: La magie de R markdown, dessin de @Allison Horst\n\n\n\nDepuis quelques années, Quarto a fait son entrée en scène. Il s’agit de la nouvelle version de R markdown qui se veut plus attrayante, accessible, stable et polyvalente. Quarto a la versatilité d’utiliser, dans un seul document, des morceaux de code provenant de différents langages, puis de produire des fichiers sous différents formats en une seule étape. De plus, vous pouvez l’utiliser directement en RStudio ou dans Jupyter. La version actuelle du manuel est montée à l’aide de Quarto, tout comme les diapositives présentées dans les capsules vidéo du cours.\n\n\n\n\n\n(a) La versatilité de Quarto\n\n\n\n\n\n\n\n(b) Les étapes de rendu de Quarto\n\n\n\nFigure 5.5: Dessins de la présentation Hello, Quarto par Julia Lowndes et Mine Çetinkaya-Rundel, présentée à la conférence RStudio de 2022. Illustrés par @Allison Horst.\n\n\n\n5.1.2.1 Le langage markdown\nLe langage markdown est ce qu’on appelle “un langage de balises léger” qui vous permet d’introduire dans votre texte brut des balises simples pour effectuer le formatage. Un fichier portant l’extension .md ou .markdown est un fichier texte clair (que vous pouvez ouvrir et éditer dans votre éditeur texte préféré), tout comme un fichier .R. RStudio permet notamment d’éditer un fichier .md. Il existe aussi de nombreux éditeurs de texte spécialisés en édition markdown - mon préféré est Typora. Les décorations (ou balises) principales en markdown sont les suivantes (les citations utilisées ci-après sont tirées du roman Dune, de Frank Herbert).\nItalique. Pour accentuer en italique, balisez le texte avec des astérisques simples *. Par exemple, “Pourrais-je porter parmi vous le nom de *Paul-Muad'dib*?” devient “Pourrais-je porter parmi vous le nom de Paul-Muad’dib?”\nGras. Pour accentuer en gras, balisez le texte avec des doubles astérisques **. Par exemple, “L'espérance **ternit** l'observation.” devient “L’espérance ternit l’observation”.\nLargeur fixe. Pour un texte à largeur fixe (signifiant du code), balisez le texte avec des accents graves. Par exemple, “Quel nom donnez-vous à la petite `souris`, celle qui saute ?” devient “Quel nom donnez-vous à la petite souris, celle qui saute?”\nListes. Pour effectuer une liste numérotée, utilisez le chiffre 1. Par exemple,\n1. Paul\n1. Leto\n1. Alia\ndevient\n\nPaul\nJessica\nAlia\n\nDe même, pour une liste à puces, changez le 1. par le - ou le *.\nEntêtes. Les titres sont précédés par des #. Un # pour un titre 1, deux ## pour un titre 2, etc. Par exemple,\n\n# Imperium\n## Landsraad\n### Maison des Atréides\n### Maison des Harkonnen\n## CHOAM\n# Guilde des navigateurs\nInsérera les titres appropriés (que je n’insère pas pour ne pas bousiller la structure de ce texte).\nLiens. Pour insérer des liens, le texte est entre crochets directement suivi du lien entre parenthèses. Par exemple, “Longue vie aux [combattants](https://youtu.be/Cv87NJ2xX0k?t=59)” devient “Longue vie aux combattants”.\nÉquations. Les équations suivent la syntaxe Latex entre deux $$ pour les équations sur une ligne et entre des doubles $$ $$ pour les équations sur un paragraphe. Par exemple, $c = \\sqrt{a^2 + b^2}$ devient \\(c = \\sqrt{a^2 + b^2}\\).\nImages. Pour insérer une image, ![nom de l'image](images/spice-must-flow.png).\nUne liste exhaustive des balises markdown est disponible sous forme d’aide-mémoire. L’extension de RStudio remedy, installable de la même façon qu’un module, fera apparaître une section REMEDY dans le menu Addins, où vous trouverez toutes sortes d’options de formatage automatique (Figure 5.6). Toutefois, vous verrez plus loin dans la section sur Quarto que d’autres outils encore plus conviviaux existent désormais.\n\n\n\n\nFigure 5.6: Menu des extensions de RStudio, avec l’extension remedy\n\n\n\n\n5.1.2.2 R markdown\nDans RStudio, ouvrez un R markdown par File &gt; New file &gt; R Markdown. Si le module rmarkdown n’est pas installé, RStudio vous demandera de l’installer. Une fenêtre apparaîtra.\n\n\n\n\nFigure 5.7: Nouveau fichier R markdown\n\n\n\nLes options d’exportation pourront être modifiées par la suite.\nUn fichier d’exemple sera créé, et vous pourrez le modifier. Les parties de texte sont écrits en markdown, et le code R est enchâssé entre les balises ```{r} et ```. Je nommerai ces parties de code des cellules ou des blocs de code. Vous pouvez utiliser le raccourci clavier Ctrl + Alt + I pour insérer rapidement un bloc de code.\nDes options de code peuvent être utilisées à l’intérieur des accolades {r}. Par exemple\n\n\n{r, filtre-outliers} donne le nom filtre-outliers au bloc de code, qui permet nommément de nommer les images créées dans le bloc de code.\n\n{r, eval = FALSE} permet d’activer (TRUE, valeur par défaut) ou de désactiver (FALSE) le calcul de la cellule.\n\n{r, echo = FALSE} permet de n’afficher que la sortie de la cellule de code en n’affichant pas le code, par exemple un graphique ou le sommaire d’une régression.\n\n{r, results = FALSE} permet de n’afficher que le code, mais pas la sortie.\n\n{r, warning = FALSE, message = FALSE, error = FALSE} n’affichera pas les avertissements, les messages automatiques et les messages d’erreur.\n\n{r, fig.width = 10, fig.height = 5, fig.align = \"center\"} affichera les graphiques dans les dimensions voulues, alignée au centre (\"center\"), à gauche (\"left\") ou à droite (\"right\").\n\nNotez que vous pouvez exécuter rapidement du code sur une ligne avec la formulation `r `, par exemple la moyenne des nombres `\\r a&lt;-round(runif(4, 0, 10)); a` est de `\\r mean(a)`, en enlevant les \\ devant les r (ajoutées artificiellement pour éviter que le code soit calculé), sera la moyenne des nombres 7, 2, 2, 8 est de 4.75\nUne fois que vous serez satisfait de votre document, cliquer sur Knit  et le fichier de sortie sera généré. Le guide qui permet de générer le fichier de sortie est tout en haut du fichier. Nous l’appelons le YAML (acronyme récursif de YAML Ain’t Markup Language). Prenez le YAML suivant.\n---\ntitle: \"Dune\"\nauthor: \"Frank Herbert\"\ndate: \"1965-08-01\"\noutput: github_document\n---\nLe titre, l’auteur et la date sont spécifiés. Pour indiquer la date courante, on peut simplement la générer avec R en remplaçant \"1965-08-01\" par 2024-02-11. La spécification output indique le type de document à générer, par exemple html_document pour une page web, pdf_document pour un pdf, ou word_document pour un docx. Dans ce cas-ci, j’indique github_document pour créer un fichier markdown comprenant notamment des liens relatifs vers les images des graphiques générés. Pourquoi un github_document? C’est le sujet de la section 5.2. Mais avant cela, je vous réfère à un autre aide-mémoire.\n\n\n\n\nFigure 5.8: Aide-mémoire pour R Markdown, Source: RStudio\n\n\n\n\n5.1.3 Quarto markdown\nAvant d’utiliser Quarto, il vous faudra l’installer sur votre ordinateur. Lorsque ce sera fait, une fois RStudio redémarré, vous devriez pouvoir créer un document Quarto par File &gt; New file &gt; Quarto Document. Toutefois, je vous recommande plutôt de créer directement un nouveau projet, et de choisir le type de projet s’appliquant à votre situation (typiquement un projet Quarto, mais le manuel de cours par exemple est un livre Quarto).\n\n\n\n\nFigure 5.9: Création d’un nouveau projet Quarto.\n\n\n\nEn créant votre projet dans un nouveau répertoire, vous pouvez choisir le moteur (ici nous utilisons knitr, mais vous pourriez aussi choisir Jupyter si vous préférez) et activer le suivi d’environnement reproductible renv (vous en apprendrez plus sur renv à la section 5.3). De plus, vous pouvez directement créer un répertoire git, ce qui facilitera les étapes d’initialisation de votre répertoire si vous souhaitez en faire un. Pour l’instant, je vous suggère de garder ces cases vides si vous souhaitez seulement utiliser Quarto.\n\n\n\n\nFigure 5.10: Options de création d’un nouveau projet Quarto.\n\n\n\nIl existe un guide très détaillé sur l’utilisation de Quarto sur leur site internet. Je vous suggère pour bien démarrer la lecture suivante.\n\n5.1.3.1 Balises des blocs de code\nEn général, vous pouvez utiliser les mêmes balises dans Quarto qu’avec R markdown. Les mêmes options peuvent être utilisées à l’intérieur des accolades {r} de la même manière (par exemple : {r, important-figure, fig.width = 10, fig.height = 5, fig.align = \"center\"}), ou alors de façon plus claire avec le style YAML de la façon suivante :\n```{r}\n#| label: important-figure\n#| fig-width: 10\n#| fig-height: 5\n#| fig-align: center\n\nknitr::include_graphics(\"images/important-figure.png\")\n```\n\n5.1.3.2 Rendu de documents\nPour créer le rendu de votre document, au lieu d’utiliser le bouton Knit, vous pouvez utiliser Render. Si vos options permettent la sortie de plusieurs formats de fichiers, vous verrez une liste déroulante vous permettant de choisir le format à produire.\n\n\n\n\nFigure 5.11: Créer un rendu de votre document Quarto (Tutorial: Hello Quarto).\n\n\n\nCe bouton est bien pratique lorsque vous avez un seul document simple .qmd dans votre dossier ou alors que vous voulez rapidement visualiser une sortie, mais il est préférable de prendre l’habitude d’utiliser la commande quarto_render pour faire le rendu de tous les fichiers dans le dossier. Certaines options de cette commande vous permettent par exemple de faire le rendu d’un seul format de fichier.\n```{r}\ninstall.packages(\"quarto\")\nquarto::quarto_render(\"index.qmd\")\n```\n\n5.1.3.3 Options YAML\nComme pour R markdown, nous utilisons le guide YAML pour spécifier les options de rendements de documents. On peut l’utiliser directement en haut du fichier .qmd en en-tête, mais en général je recommande plutôt d’utiliser le fichier _quarto.yml qui devrait avoir été créé dans votre dossier de projet lors de la création du projet. Vous pouvez y spécifier par exemple le dossier de sortie des fichiers créés, donner des paramètres pour chaque format de fichiers de sortie, etc. Les informations dans ce fichier sont les consignes qui seront suivies lors de l’interprétation et de la création de vos documents. Voici un exemple du contenu d’un fichier YAML inspiré du manuel des notes de cours.\nproject:\n  type: book\n  output-dir: docs\n  \nbook:\n  title: \"Titre livre\"\n  author:\n  - name: \"Nom auteur 1\"\n  - name: \"Nom auteur 2\"\n  date: today\n  date-format: iso\n  chapters:\n    - href: index.qmd\n      text: Préface\n    - 01-chap1.qmd\n    - 02-chap2.qmd\n  cover-image: images/cover.png\n  \nformat:\n  html:\n    theme: flatly\n    code-link: true\n    css: styles.css\n    toc: true\n  pdf:\n    documentclass: scrreprt\n    toc: true\n\n\nAide-mémoire de Quarto\n\n\n5.1.3.4 Éditeur visuel\nEnfin, un des grands avantages de Quarto en termes d’accessibilité est l’éditeur visuel. Personnellement, je préfère généralement travailler avec l’éditeur de code Source, qui me permet de mieux voir ce qu’il se passe exactement et d’avoir un meilleur contrôle sur le formatage. Toutefois, il m’arrive de passer à l’éditeur visuel lorsque j’ignore comment introduire certains éléments.\n\n\n\n\nFigure 5.12: Éditeur visuel vs source (Tutorial: Hello Quarto).\n\n\n\nEn mode d’édition visuelle, il est très facile d’insérer un nouvel élément : il vous suffit de taper / : une liste déroulante devrait alors apparaître. Vous pouvez alors choisir un élément dans la liste, ou d’abord spécifier votre recherche (comme à la Figure 5.13).\n\n\n\n\nFigure 5.13: Insérer une équation avec l’éditeur visuel. (Tutorial: Authoring).\n\n\n\nPour finir, il existe des modules vous permettant de créer directement un document à partir de formats de référence. Par exemple, quarto-journals vous permet de créer rapidement des fichiers au format de votre revue préférée. Puisque Quarto est plutôt récent, ces outils sont toujours en développement et peuvent parfois être imparfaits (ou inexistants pour votre revue), mais ils seront plus performants avec le temps et des revues devraient être ajoutées."
  },
  {
    "objectID": "05-github.html#sec-intro-git",
    "href": "05-github.html#sec-intro-git",
    "title": "5  Science ouverte et reproductibilité",
    "section": "\n5.2 Introduction à GitHub",
    "text": "5.2 Introduction à GitHub\nLe système de suivi de version git (open source) a été créé par Linus Torvalds, aussi connu pour avoir créé Linux. git prend une photo de votre répertoire de projet à chaque fois que vous commettez un changement. Vous pourrez revenir sans problème sur d’anciennes versions si quelque chose tourne mal, et vous pourrez publier le résultat final sur un service d’hébergement utilisant git.\nIl existe plusieurs services pour rendre git utilisable en ligne, mais GitHub est définitivement le plus utilisé d’entre tous. La plateforme GitHub est presque devenue un réseau social de développement. GitHub, maintenant la propriété de Microsoft, n’est en soi pas open source. Si comme moi vous avez un penchant pour l’open source, je vous redirige vers la plateforme GitLab, qui fonctionne à peu près de la même manière que GitHub, mais dans sa version gratuite GitLab vous octroie autant de répertoires privés que vous désirez. Seul hic, alors que la plateforme GitHub sera fort probablement toujours vivante dans plusieurs années, on en est moins sûr pour GitLab. C’est pourquoi, en règle générale, j’utilise GitHub à des fins professionnelles mais GitLab à des fins personnelles.\nPour suivre cette partie du cours, je vous invite à créer un compte sur GitHub ou GitLab, à votre choix. Créez un nouveau dépôt (New repository).\n\n\n\n\nFigure 5.14: Nouveau dépôt avec GitHub\n\n\n\n\n\n\n\nFigure 5.15: Nouveau dépôt avec GitLab\n\n\n\nPour utiliser git, vous pourrez toujours travailler en ligne de commande (c’est ce que je préfère personnellement mais je ne le conseille pas nécessairement; vous trouverez toute la documentation nécessaire sur le site de GitHub). Il est aussi possible de travailler avec les outils intégrés dans RStudio, mais je vous suggère d’utiliser GitHub Desktop (qui fonctionne aussi sur GitLab) - évidemment, d’autres logiciels similaires existent. Github Desktop vous permettra d’abord de cloner un répertoire en ligne. Le clonage vous permet de créer une copie locale (sur votre ordinateur) du répertoire.\n\n\n\n\nFigure 5.16: Cloner dépôt avec GitHub\n\n\n\n\n\n\n\nFigure 5.17: Cloner dépôt avec GitLab\n\n\n\nUne fois que le dépôt est cloné, il est sur votre ordinateur. Lorsque vous effectuez un changement, vous devez commettre (commit), puis envoyer (push) vos changements vers le dépôt en ligne. Pour que votre document markdown soit lisible par GitHub et GitLab, il doit être exporté au format html. Un fichier .md sera créé, et inclura les détails de votre feuille de calculs, images y compris!\nDe plus, puisque les fichiers HTML générés par Quarto remplacent certains paramètres des fichiers HTML classiques, vous devez ajouter dans le dossier racine de votre projet un fichier vide .nojekyll à partir de votre terminal. Ce fichier indiquera GitHub d’ignorer les procédures Jekyll qu’il lance normalement sur les fichiers HTML (voir site web de Quarto pour plus de détails).\n\n\n\n\n\n\n\nMac/Linux\n\n\nTerminal\n\ntouch .nojekyll\n\n\n\nWindows\n\n\nTerminal\n\ncopy NUL .nojekyll\n\n\n\n\n\n\n\n\nFigure 5.18: Commettre et déployer un dépôt avec GitHub\n\n\n\nL’interface de GitHub Desktop vous permet de revenir en arrière en éliminant des commits précédents.\n\n\n\n\nFigure 5.19: Revenir en arrière avec GitHub desktop\n\n\n\nVous pourrez ajouter des collaborateurs à votre dépôt, pour que plusieurs personnes travaillent de front sur un même dépôt. Il est aussi possible de créer une branche d’un dépôt, fusionner la branche de développement avec la branche principale, commenter les codes, suggérer des changements, etc., mais cela sort du cadre d’un cours sur la reproductibilité.\nSi vous souhaitez créer un site internet comme celui des notes de cours à partir de votre répertoire GitHub, vous devez activer votre page avec GitHub Pages, créer un lien, choisir la branche et le dossier contenant les fichiers HTML et publier.\n\n\n\n\nFigure 5.20: Activer la page avec GitHub Pages\n\n\n\nEnfin, pour renvoyer un article vers votre matériel supplémentaire, insérez le lien dans la section méthodologie. Il peut s’agir du lien complet, ou bien d’un lien raccourci (avec par exemple tinyurl ou bitly). Par exemple,\n\nThe data and the R code used to compute the results are both available as supplementary material at https://git.io/fhHEj.\n\nNotez que RStudio offre une interface pour utiliser git via un onglet afiché en haut à droite dans l’affichage par défaut. Ne l’ayant jamais utilisé, je ne me sens pas à l’aise d’en suggérer l’utilisation, mais libre à vous d’explorer cet outil et de vous l’approprier!\n\n\n\n\nFigure 5.21: L’outil Git de RStudio"
  },
  {
    "objectID": "05-github.html#sec-git-renv",
    "href": "05-github.html#sec-git-renv",
    "title": "5  Science ouverte et reproductibilité",
    "section": "\n5.3 Introduction à renv\n",
    "text": "5.3 Introduction à renv\n\nAlors que les modules sont continuellement mis à jour, on doit s’assurer que l’on sache exactement quelle version a été utilisée si l’on désire être strict sur la reproductibilité. Lorsque je révise un article, je demande à ce que le nom des modules utilisés et leur numéro de version soient explicitement cités et référencés. Par exemple, dans un article sur l’analyse de compositions foliaires de laitues inoculées par une bactérie, j’écrivais:\n\nComputations were performed in the R statistical language version 3.4.1 (R Development Core Team, 2017). The main packages used in the data analysis workflow were the vegan package version 2.4-3 (Oksanen et al., 2017) for ordination, the compositions package version 1.40-1 (van den Boogaart and Tolosana-Delgado, 2013) for ilr transformations, the nlme version 3.1-131 (Pinheiro et al., 2017) package to compute the random experimental effect, the mvoutlier package version 2.0.8 (Filzmoser and Gschwandtner, 2017) for multivariate outlier detection, and the ggplot2 package version 2.2.1 (Wickham and Chang, 2017) for data visualization. The data and computations are publicly available at https://github.com/essicolo/Nicolas-et-al_Infected-lettuce-ionomics. Nicolas et al., 2019\n\nDe cette manière, une personne (que ce soit vos collègues, quiconque voudra auditer ou évaluer votre code ou vous-même dans le futur) pourra reproduire le code publié sur GitHub en installant les versions de R et des modules cités. Mais cela est fastidieux. C’est pourquoi l’équipe de RStudio (oui, encore ceux-là) ont développé le module renv, qui permet d’installer les modules à même votre dossier de projet (le dossier contenant le fichier .Rproj).\nPour l’utiliser à tout moment en cours de projet, il suffit de lancer la commande renv::init(). Cette commande configure l’infrastructure du projet, installe les libraries utilisées dans le dossier renv à l’intérieur du dossier de projet et bloque leurs versions actuelles dans le fichier renv.lock, crée un fichier .Rprofile qui traquera les installations futures de librairies supplémentaires, puis redémarre la session R. Ouf!\n\n\n\n\nFigure 5.22: L’outil renv de RStudio\n\n\n\nDans le dossier renv, le .gitignore contient tous les documents et les types de documents qui sont ignorés par git. L’option par défaut est d’ignorer le dossier library, qui contient les modules installés, mais de garder les fichiers activate.R et settings.json, qui contiennent le script d’installation des modules non installés (qui devront être installés par les autres personnes utilisant votre projet) ainsi que les paramètres du projet. Mieux vaut garder les options par défaut. Initialiser renv revient à scanner vos documents de projet pour trouver les modules utilisés et créer un paquet contenant tout cela à même votre projet, dans un dossier renv.\n\n📂 rstats\n|-📁 data\n|-📁 docs\n|-📁 images\n|-📁 lib\n|-📁 renv\n|-📁 tables\n📄 _quarto.yml\n📄 sentier-d-or.Rproj\n📄 stats.qmd\n📄 README.md\n📄 renv.lock\n📄 .Rprofile\n📄 .gitignore\nCe dossier contiendra tout ce qu’il faut pour utiliser les modules du projet d’une personne que l’on nommera Leto. Lorsqu’une autre personne, appellons-la Ghanima, utilisera le projet de Leto, RStudio vérifiera si le module renv est bien installé, et l’installera s’il ne l’est pas. Pour utiliser les modules du projet et non pas les modules de son ordinateur, Ghanima lancera la fonction renv::restore(), qui installera les versions décrites dans le lockfile (renv.lock). Si Leto décide de mettre à jour ses modules en cours de projet, il lancera la fonction renv::install() pour installer de nouveaux modules, renv::update() pour faire la mise à jour de tous les modules utilisés, puis renv::snapshot() pour que ces nouveaux modules soit intégrés à son projet dans le lockfile. Lorsque Leto commettra (commit) ses changements dans git et les publiera (push) sur GitHub, puis lorsque Ghanima mettra à jour (fetch) son dépôt local git lié au dépôt GitHub, elle devra à nouveau lancer renv::restore() pour que les modules soient bel et bien ceux utilisés par Leto.\nNotez qu’avec renv, vous n’installez pas réellement les modules complets à chaque fois : renv garde en mémoire cache l’installation des modules sur votre ordinateur, si bien que lorsque vous installez plusieurs la même version d’un même module, vous ne compilez pas à chaque fois et vous n’utilisez pas plus d’espace sur votre disque dur qu’il n’en faut."
  },
  {
    "objectID": "05-github.html#pour-terminer-le-reprex",
    "href": "05-github.html#pour-terminer-le-reprex",
    "title": "5  Science ouverte et reproductibilité",
    "section": "\n5.4 Pour terminer, le reprex\n",
    "text": "5.4 Pour terminer, le reprex\n\nLorsque j’ai découvert un bogue dans le module weathercan, j’ai ouvert une issue sur GitHub en indiquant le message d’erreur obtenu, en espérant que l’origine du bogue puisse être facilement déduit. Un développeur de weathercan m’a demandé un reprex. J’ai été déçu lorsque j’ai compris que le reprex n’était pas une espèce de dinosaure, mais plutôt un exemple reproductible (reproducible example).\n\n📗 Reprex: Un exemple reproductible.\n\n\nJ’ai essayé d’isoler le problème pour reproduire l’erreur avec le minimum de code possible. À partir d’un code de plus de 7000 lignes (les présentes notes de cours), j’en suis arrivé à ceci:\n\nstations &lt;- data.frame(A = 1)\n\nlibrary(\"weathercan\")\nmont_bellevue &lt;- weather_dl(\n  station_ids = c(5397, 48371),\n  start = \"2019-02-01\",\n  end = \"2019-02-07\",\n  interval = \"hour\",\n  verbose = TRUE\n)\n\n, qui me retournait l’erreur\nGetting station: 5397\nFormatting station data: 5397\nError in strptime(xx, f, tz = tz) : valeur 'tz' incorrecte\nLe bogue: la fonction weather_dl() utilisait à l’interne un objet nommé stations, qui entrait en conflit avec un objet stations s’il était défini hors de la fonction.\nSynthétiser une question n’est pas facile (créer cet exemple reprductible m’a pris près de 2 heures). Mais répondre à une question non synthétisée, c’est encore plus difficile. C’est pourquoi on (moi y compris) vous demandera systématiquement un reprex lorsque vous poserez une question liée à une erreur systématique, le plus souvent en programmation.\n\nUn exemple reproductible permet à quelqu’un de recréer l’erreur que vous avez obtenue simplement en copiant-collant votre code. - Hadley Wickham\n\nSelon Hadley Wickham (gourou de R), un reprex devrait comprendre quatre éléments (je joue à l’hérétique en me permettant d’adapter le document du gourou):\n\nLes modules devraient être chargés en début de code.\nPuis vous chargez des données, qui peuvent être des données d’exemple ou des données incluses à même le code R (comme des données générées au hasard).\nAssurez-vous que votre code est un exemple minimal (retirer le superflu) et qu’il soit facilement lisible.\nIncluez la sortie de la fonction sessionInfo(), qui indique la plateforme matérielle et logicielle sur laquelle vous avez généré l’erreur. Ceci est important en particulier s’il s’agit d’un bogue.\n\nLorsque vous pensez avoir généré votre reprex, redémarrez R (Session &gt; Restart R dans RStudio), puis lancez votre code pour vous assurer que l’erreur puisse être générée dans un nouvel environnement tout propre.\nLa librarie reprex de tidyverse vous aide à générer et à tester des exemples reproductibles en vous fournissant un bloc de code au format markdown que vous pouvez directement copier-coller sur GitHub, StackOverflow ou Discourse (les lieux fréquents où vous poserez vos questions)."
  },
  {
    "objectID": "06-python.html#quest-ce-que-python",
    "href": "06-python.html#quest-ce-que-python",
    "title": "6  Introduction à Python",
    "section": "6.1 Qu’est-ce que Python ?",
    "text": "6.1 Qu’est-ce que Python ?\nPython est un langage de programmation de haute niveau (comme R), ce langage est apparu en février 1991 et a été créé par Guido van Rossum. Un des objectifs principaux de Van Rossum était de créer un langage libre, simple et intuitive, mais puissant comme d’autres langages déjà existants. Python a été amplement adopté partout dans le monde et est devenu un des langages de programmation plus populaires d’après différentes rangs comme les indices TIOBE et PYPL et les tendances des questions dans Stack Overflow.\nDans les dernières années Python est devenu l’un des outils les plus utilisés pour le calcul scientifique et pour l’analyse de données, même si ce langage n’était pas conçu spécifiquement pour ces tâches. L’utilisation de Python dans la science de données a été poussé par le développement de différents modules qui permettent la manipulation et l’analyse de données, certains des modules les plus populaires pour l’analyse de données en python sont :\n\nnumpy : Ce module permet de manipuler et de stocker de façon efficiente les données dans des objets connus comme tableaux (en anglais, array).\npandas : Permet de travailler avec des données tabulaires avec des étiquettes de file et de colonne, l’objet primaire de ce module est le DataFrame.\nmatplotlib : C’est le module le plus utilisé pour la visualisation de données sur Python, il peut être considéré comme le module de base pour la visualisation en Python.\nSciPy : Ce module a différentes fonctions pour la computation scientifique comme le calcul numérique, le traitement de signaux et d’images, et certains statistiques.\nscikit-learn : C’est une des modules les plus utilisés pour l’apprentissage automatique. scikit-learn a des innombrables algorithmes d’apprentissage supervisé et non-supervisé utilisés pour la classification ou la régression."
  },
  {
    "objectID": "06-python.html#installation",
    "href": "06-python.html#installation",
    "title": "6  Introduction à Python",
    "section": "6.2 Installation",
    "text": "6.2 Installation\nIl y a différentes façons d’installer Python dans un ordinateur. Dans le cas de Mac et Linux Python vient par défaut avec ces systèmes d’exploitation. Dans le cas de windows il faut le télécharger et l’installer. Une façon d’installer Python sur windows est de télécharger le fichier d’installation directement du site web de Python, une fois le fichier téléchargé, l’exécuter et suivre les instructions pour l’installation.\nUne autre alternative pour télécharger Python est à partir de Anaconda. Anaconda c’est une distribution de Python, dont l’objectif est de simplifier la gestion et le déploiement des modules. Le gestionnaire de modules de Anaconda s’appelle conda. Il y a deux options pour installer Anaconda sur un ordinateur :\n\nTélécharger la version complète qui vient avec Python, conda et 1 500 modules pre-installés.\nTélécharger une version minimale appellé Miniconda qui ne vient qu’avec Python et conda.\n\nLes deux différences principales entre Anaconda et Miniconda sont (1) l’espace réquis pour l’installation qui est de 3 GO et 400 MO, respectivement, et (2) le temps d’installation puisque ça prends moins de temps à installer Miniconda que Anaconda. L’utilisateur peut choisir Anaconda si n’a pas d’expérience et s’il ne veut pas se préoccuper à installer des modules. L’installation de Miniconda est récommandé pour des utilisateurs qui sont plus experimentés et qui savent déjà quels modules ils vont utiliser. Enfin, si l’ordinateur n’a pas beaucoup d’espace, il est recommandé d’installer Miniconda. Dans ce manuel, les exemples d’éxecution de Python et l’installation des modules se fera à partir d’un terminal de commande nommé « Anaconda prompt »."
  },
  {
    "objectID": "06-python.html#chapitre-en-construction",
    "href": "06-python.html#chapitre-en-construction",
    "title": "6  Introduction à Python",
    "section": "6.3 Chapitre en construction",
    "text": "6.3 Chapitre en construction"
  },
  {
    "objectID": "06-python.html#anaconda-prompt",
    "href": "06-python.html#anaconda-prompt",
    "title": "6  Introduction à Python",
    "section": "6.4 Anaconda prompt",
    "text": "6.4 Anaconda prompt\n\nconda bash\n“Bonjour monde”\nRstudio et Python\nVS Code\nOperations Python\nTypes d’objets\nBoucles\nFonctions\nLes gestionnaires de modules\nNumpy, Pandas, Visualisation\nFin"
  },
  {
    "objectID": "07a-biostats.html#populations-et-échantillons",
    "href": "07a-biostats.html#populations-et-échantillons",
    "title": "7  Biostatistiques",
    "section": "\n7.1 Populations et échantillons",
    "text": "7.1 Populations et échantillons\nLe principe d’inférence consiste à généraliser des conclusions à l’échelle d’une population à partir d’échantillons issus de cette population. Alors qu’une population contient tous les éléments étudiés, un échantillon d’une population est une observation unique. Une expérience bien conçue fera en sorte que les échantillons soient représentatifs de la population qui, la plupart du temps, ne peut être observée entièrement pour des raisons pratiques.\nLes principes d’expérimentation servant de base à la conception d’une bonne méthodologie sont présentés dans le cours Dispositifs expérimentaux (BVG-7002). Également, je recommande le livre Principes d’expérimentation: planification des expériences et analyse de leurs résultats de Pierre Dagnelie (2012), disponible en ligne en format PDF. Un bon aperçu des dispositifs expérimentaux est aussi présenté dans Introductory Statistics with R, de Peter Dalgaard (2008), que vous pouvez télécharger du site de la bibliothèque de l’Université Laval vous avez un identifiant autorisé.\nUne population est échantillonnée pour induire des paramètres: un rendement typique dans des conditions météorologiques, édaphiques et managériales données, la masse typique des faucons pèlerins, mâles et femelles, le microbiome typique d’un sol agricole ou forestier, etc. Une statistique est une estimation d’un paramètre calculée à partir des données, par exemple une moyenne et un écart-type, ou une ordonnée à l’origine (intercept) et une pente.\nPar exemple, la moyenne (\\(\\mu\\)) et l’écart-type (\\(\\sigma\\)) d’une population sont estimés par les moyennes (\\(\\bar{x}\\)) et écarts-types (\\(s\\)) calculés sur les données issues de l’échantillonnage.\nChaque paramètre est liée à une perspective que l’on désire connaître chez une population. Ces angles d’observations sont les variables."
  },
  {
    "objectID": "07a-biostats.html#les-variables",
    "href": "07a-biostats.html#les-variables",
    "title": "7  Biostatistiques",
    "section": "\n7.2 Les variables",
    "text": "7.2 Les variables\nNous avons abordé au chapitre 3 la notion de variable par l’intermédiaire d’une donnée. Une variable est l’observation d’une caractéristique décrivant un échantillon. Si la charactéristique varie d’un échantillon à un autre sans que vous en expliquiez la raison (i.e. si identifier la source de la variabilité ne fait pas partie de votre expérience), on parlera de variable aléatoire. Même le hasard est régi par certaines lois: ce qui est aléatoire dans une variable peut être décrit par des lois de probabilité, que nous verrons plus bas.\nMais restons aux variables pour l’instant. Par convention, on peut attribuer aux variables un symbole mathématique. Par exemple, on peut donner à la masse volumique d’un sol (qui est le résultat d’une méthodologie précise) le symbole \\(\\rho\\). Lorsque l’on attribue une valeur à \\(\\rho\\), on parle d’une donnée. Chaque donnée d’une observation a un indice qui lui est propre, que l’on désigne souvent par \\(i\\), que l’on place en indice \\(\\rho_i\\). Pour la première donnée, on a \\(i=1\\), donc \\(\\rho_1\\). Pour un nombre \\(n\\) d’échantillons, on aura \\(\\rho_1\\), \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_n\\), formant le vecteur \\(\\rho = \\left[\\rho_1, \\rho_2, \\rho_3, ..., \\rho_n \\right]\\).\nEn R, une variable est associée à un vecteur ou une colonne d’un tableau.\n\nrho &lt;- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D\ndata &lt;- data.frame(rho = rho) # tableau\ndata\n\n   rho\n1 1.34\n2 1.52\n3 1.26\n4 1.43\n5 1.39\n\n\nIl existe plusieurs types de variables, qui se regroupent en deux grandes catégories: les variables quantitatives et les variables qualitatives.\n\n7.2.1 Variables quantitatives\nCes variables peuvent être continues dans un espace échantillonnal réel ou discrètes dans un espace échantillonnal ne considérant que des valeurs fixes. Notons que la notion de nombre réel est toujours une approximation en sciences expérimentales comme en calcul numérique, étant donnée que l’on est limité par la précision des appareils comme par le nombre d’octets à utiliser. Bien que les valeurs fixes des distributions discrètes ne soient pas toujours des valeurs entières, c’est bien souvent le cas en biostatistiques comme en démographie, où les décomptes d’individus sont souvent présents (et où la notion de fraction d’individus n’est pas acceptée).\n\n7.2.2 Variables qualitatives\nOn exprime parfois qu’une variable qualitative est une variable impossible à mesurer numériquement: une couleur, l’appartenance à une espèce ou à une série de sol. Pourtant, dans bien des cas, les variables qualitatives peuvent être encodées en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile à un loam sableux, qui autrement est décrit par la classe texturale d’un sol. Pour une couleur, on peut lui associer une longueur d’onde ou des pourcentages de rouge, vert et bleu, ainsi qu’un ton. En ce qui a trait aux variables ordonnées, il est possible de supposer un étalement. Par exemple, une variable d’intensité faible-moyenne-forte peut être transformée linéairement en valeurs quantitatives -1, 0 et 1. Attention toutefois, l’étalement peut parfois être quadratique ou logarithmique. Les séries de sol peuvent être encodées par la proportion de gleyfication (Parent et al., 2017). Quant aux catégories difficilement transformables en quantités, on pourra passer par l’encodage catégoriel, souvent appelé dummyfication, que nous verrons plus loin. L’analyse qualitative consiste en l’analyse de verbatims, essentiellement utile en sciences sociales: nous n’en n’aurons pas besoin ici. Nous considérerons les variables qualitatives comme des variables quantitatives qui n’ont pas subi de prétraitement."
  },
  {
    "objectID": "07a-biostats.html#les-probabilités",
    "href": "07a-biostats.html#les-probabilités",
    "title": "7  Biostatistiques",
    "section": "\n7.3 Les probabilités",
    "text": "7.3 Les probabilités\n\n« Nous sommes si éloignés de connaître tous les agens de la nature, et leurs divers modes d’action ; qu’il ne serait pas philosophique de nier les phénomènes, uniquement parce qu’ils sont inexplicables dans l’état actuel de nos connaissances. Seulement, nous devons les examiner avec une attention d’autant plus scrupuleuse, qu’il paraît plus difficile de les admettre ; et c’est ici que le calcul des probabilités devient indispensable, pour déterminer jusqu’à quel point il faut multiplier les observations ou les expériences, afin d’obtenir en faveur des agens qu’elles indiquent, une probabilité supérieure aux raisons que l’on peut avoir d’ailleurs, de ne pas les admettre. » — Pierre-Simon de Laplace\n\nUne probabilité est la vraisemblance qu’un évènement se réalise chez un échantillon. Les probabilités forment le cadre des systèmes stochastiques, c’est-à-dire des systèmes trop complexes pour en connaître exactement les aboutissants, auxquels on attribue une part de hasard. Ces systèmes sont prédominants dans les processus vivants.\nOn peut dégager deux perspectives sur les probabilités: l’une passe par une interprétation fréquentielle, l’autre bayésienne.\n\nL’interprétation fréquentielle représente la fréquence des occurrences après un nombre infini d’évènements. Par exemple, si vous jouez à pile ou face un grand nombre de fois, le nombre de pile sera égal à la moitié du nombre de lancers. L’approche fréquentielle teste si les données concordent avec un modèle du réel. Il s’agit de l’interprétation communément utilisée.\nL’interprétation bayésienne vise à quantifier l’incertitude des phénomènes. Dans cette perspective, plus l’information s’accumule, plus l’incertitude diminue. Cette approche gagne en notoriété notamment parce qu’elle permet de décrire des phénomènes qui, intrinsèquement, ne peuvent être répétés infiniment (absence d’asymptote), comme ceux qui sont bien définis dans le temps ou sur des populations limitées. L’approche bayésienne évalue la probabilité que le modèle soit réel.\n\nUne erreur courante consiste à aborder des statistiques fréquentielles comme des statistiques bayésiennes. Par exemple, si l’on désire évaluer la probabilité de l’existence de vie sur Mars, on devra passer par le bayésien, car avec les stats fréquentielles, on devra plutôt conclure si les données sont conformes ou non avec l’hypothèse de la vie sur Mars (exemple tirée du blogue Dynamic Ecology).\nDes rivalités factices s’installent entre les tenants des différentes approches, dont chacune, en réalité, répond à des questions différentes dont il convient réfléchir sur les limitations. Bien que les statistiques bayésiennes soient de plus en plus utilisées, nous ne couvrirons dans ce chapitre que l’approche fréquentielle. L’approche bayésienne est néanmoins traitée dans le chapitre 8, qui est facultatif au cours."
  },
  {
    "objectID": "07a-biostats.html#les-distributions",
    "href": "07a-biostats.html#les-distributions",
    "title": "7  Biostatistiques",
    "section": "\n7.4 Les distributions",
    "text": "7.4 Les distributions\nUne variable aléatoire peut prendre des valeurs selon des modèles de distribution des probabilités. Une distribution est une fonction mathématique décrivant la probabilité d’observer une série d’évènements. Ces évènements peuvent être des valeurs continues, des nombres entiers, des catégories, des valeurs booléennes (Vrai/Faux), etc. Dépendemment du type de valeur et des observations obtenues, on peut associer des variables à différentes lois de probabilité. Toujours, l’aire sous la courbe d’une distribution de probabilité est égale à 1.\nEn statistiques inférentielles, les distributions sont les modèles, comprenant certains paramètres comme la moyenne et la variance pour les distributions normales, à partir desquelles les données sont générées.\nIl existe deux grandes familles de distribution: discrètes et continues. Les distributions discrètes sont contraintes à des valeurs prédéfinies (finies ou infinies), alors que les distributions continues prennent nécessairement un nombre infini de valeurs, dont la probabilité ne peut pas être évaluée ponctuellement, mais sur un intervalle.\nL’espérance mathématique est une fonction de tendance centrale, souvent décrite par un paramètre. Il s’agit de la moyenne d’une population pour une distribution normale. La variance, quant à elle, décrit la variabilité d’une population, i.e. son étalement autour de l’espérance. Pour une distribution normale, la variance d’une population est aussi appelée variance, souvent présentée par l’écart-type (égal à la racine carrée de la variance).\n\n7.4.1 Distribution binomiale\nEn tant que scénario à deux issues possibles, des tirages à pile ou face suivent une loi binomiale, comme toute variable booléenne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la présence/absence d’une espèce, d’une maladie, d’un trait phylogénétique, ainsi que les catégories encodées. Lorsque l’opération ne comprend qu’un seul échantillon (i.e. un seul tirage à pile ou face), il s’agit d’un cas particulier d’une loi binomiale que l’on nomme une loi de Bernouilli.\nPour 25 tirages à pile ou face indépendants (i.e. dont l’ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilités est de 1. La fonction dbinom est une fonction de distribution de probabilités. Les fonctions de distribution de probabilités discrètes sont appelées des fonctions de masse.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nx &lt;- 0:25\ny &lt;- dbinom(x = x, size = 25, prob = 0.5)\nprint(paste('La somme des probabilités est de', sum(y)))\n\n[1] \"La somme des probabilités est de 1\"\n\nggplot(data = tibble(x, y), mapping = aes(x, y)) +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = \"grey50\") +\n  geom_point()\n\n\n\n\n\n7.4.2 Distribution de Poisson\nLa loi de Poisson (avec un P majuscule, introduite par le mathématicien français Siméon Denis Poisson et non pas l’animal) décrit des distributions discrètes de probabilité d’un nombre d’évènements se produisant dans l’espace ou dans le temps. Les distributions de Poisson décrivent ce qui tient du décompte. Il peut s’agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants d’asclépiades se trouvant sur une terre cultivée, ou du nombre d’évènements de précipitation au mois de juin, etc. La distribution de Poisson n’a qu’un seul paramètre, \\(\\lambda\\), qui décrit la moyenne des décomptes.\nPar exemple, en un mois de 30 jours, et une moyenne de 8 évènements de précipitation pour ce mois, on obtient la distribution suivante.\n\nx &lt;- 1:30\ny &lt;- dpois(x, lambda = 8)\nprint(paste('La somme des probabilités est de', sum(y)))\n\n[1] \"La somme des probabilités est de 0.999664536835124\"\n\nggplot(data = data.frame(x, y), mapping = aes(x, y)) +\n  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = \"grey50\") +\n  geom_point()\n\n\n\n\n\n7.4.3 Distribution uniforme\nLa distribution la plus simple est probablement la distribution uniforme. Si la variable est discrète, chaque catégorie est associée à une probabilité égale. Si la variable est continue, la probabilité est directement proportionnelle à la largeur de l’intervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour décrire des a priori vagues pour l’analyse bayésienne (ce sujet est traité dans le chapitre 8). Nous utilisons la fonction dunif. À la différence des distributions discrètes, les fonctions de distribution de probabilités continues sont appelées des fonctions de densité d’une loi de probabilité (probability density function).\n\nincrement &lt;- 0.01\nx &lt;- seq(-4, 4, by = increment)\ny1 &lt;- dunif(x, min = -3, max = 3)\ny2 &lt;- dunif(x, min = -2, max = 2)\ny3 &lt;- dunif(x, min = -1, max = 1)\n\nprint(paste('La somme des probabilités est de', sum(y3 * increment)))\n\n[1] \"La somme des probabilités est de 1.005\"\n\ngg_unif &lt;- data.frame(x, y1, y2, y3) |&gt;  \n  pivot_longer(-x, names_to = \"variable\", values_to = \"value\")\n\nggplot(data = gg_unif, mapping = aes(x = x, y = value)) +\n  geom_line(aes(colour = variable))\n\n\n\n\n\n7.4.4 Distribution normale\nLa plus répandue de ces lois est probablement la loi normale, parfois nommée loi gaussienne et plus rarement loi laplacienne. Il s’agit de la distribution classique en forme de cloche.\nLa loi normale est décrite par une moyenne, qui désigne la tendance centrale, et une variance, qui désigne l’étalement des probabilités autour de la moyenne. La racine carrée de la variance est l’écart-type.\nLes distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approximées par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne d’une loi log-normale est la moyenne géométrique.\n\nincrement &lt;- 0.01\nx &lt;- seq(-10, 10, by = increment)\ny1 &lt;- dnorm(x, mean = 0, sd = 1)\ny2 &lt;- dnorm(x, mean = 0, sd = 2)\ny3 &lt;- dnorm(x, mean = 0, sd = 3)\n\nprint(paste('La somme des probabilités est de', sum(y3 * increment)))\n\n[1] \"La somme des probabilités est de 0.999147010743368\"\n\ngg_norm &lt;- data.frame(x, y1, y2, y3) |&gt;  \n  pivot_longer(-x, names_to = \"variable\", values_to = \"value\")\n\nggplot(data = gg_norm, mapping = aes(x = x, y = value)) +\n  geom_line(aes(colour = variable))\n\n\n\n\nQuelle est la probabilité d’obtenir le nombre 0 chez une observation continue distribuée normalement dont la moyenne est 0 et l’écart-type est de 1? Réponse: 0. La loi normale étant une distribution continue, les probabilités non-nulles ne peuvent être calculées que sur des intervalles. Par exemple, la probabilité de retrouver une valeur dans l’intervalle entre -1 et 2 est calculée en soustrayant la probabilité cumulée à -1 de la probabilité cumulée à 2.\n\nincrement &lt;- 0.01\nx &lt;- seq(-5, 5, by = increment)\ny &lt;- dnorm(x, mean = 0, sd = 1)\n\nprob_between &lt;- c(-1, 2)\n\ngg_norm &lt;- data.frame(x, y)\ngg_auc &lt;- gg_norm |&gt; \n  filter(x &gt; prob_between[1], x &lt; prob_between[2]) |&gt; \n  rbind(c(prob_between[2], 0)) |&gt; \n  rbind(c(prob_between[1], 0))\n\nggplot(data.frame(x, y), aes(x, y)) +\n  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadécimal\n  geom_line()\n\n\n\nprob_norm_between &lt;- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1)\nprint(paste(\"La probabilité d'obtenir un nombre entre\",\n            prob_between[1], \"et\",\n            prob_between[2], \"est d'environ\",\n            round(prob_norm_between, 2) * 100, \"%\"))\n\n[1] \"La probabilité d'obtenir un nombre entre -1 et 2 est d'environ 82 %\"\n\n\nLa courbe normale peut être utile pour évaluer la distribution d’une population. Par exemple, on peut calculer les limites de région sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et d’autre de la moyenne. Il s’agit ainsi de l’intervalle de confiance sur la déviation de la distribution.\n\nincrement &lt;- 0.01\nx &lt;- seq(-5, 5, by = increment)\ny &lt;- dnorm(x, mean = 0, sd = 1)\n\nalpha &lt;- 0.05\nprob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1),\n                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1))\n\ngg_norm &lt;- data.frame(x, y)\ngg_auc &lt;- gg_norm |&gt; \n  filter(x &gt; prob_between[1], x &lt; prob_between[2]) |&gt; \n  rbind(c(prob_between[2], 0)) |&gt; \n  rbind(c(prob_between[1], 0))\n\nggplot(data = data.frame(x, y), mapping = aes(x, y)) +\n  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadécimal\n  geom_line() +\n  geom_text(data = data.frame(x = prob_between,\n                              y = c(0, 0),\n                              labels = round(prob_between, 2)),\n            mapping = aes(label = labels))\n\n\n\n\nOn pourrait aussi être intéressé à l’intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont l’écart-type est noté erreur standard. On calcule cette erreur en divisant la variance par le nombre d’observation, ou en divisant l’écart-type par la racine carrée du nombre d’observations. Ainsi, pour 10 échantillons:\n\nincrement &lt;- 0.01\nx &lt;- seq(-5, 5, by = increment)\ny &lt;- dnorm(x, mean = 0, sd = 1)\n\nalpha &lt;- 0.05\nprob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10),\n                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10))\n\ngg_norm &lt;- data.frame(x, y)\ngg_auc &lt;- gg_norm |&gt; \n  filter(x &gt; prob_between[1], x &lt; prob_between[2]) |&gt; \n  rbind(c(prob_between[2], 0)) |&gt; \n  rbind(c(prob_between[1], 0))\n\nggplot(data = data.frame(x, y), mapping = aes(x, y)) +\n  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadécimal\n  geom_line() +\n  geom_text(data = data.frame(x = prob_between,\n                              y = c(0, 0),\n                              labels = round(prob_between, 2)),\n            mapping = aes(label = labels))"
  },
  {
    "objectID": "07a-biostats.html#statistiques-descriptives",
    "href": "07a-biostats.html#statistiques-descriptives",
    "title": "7  Biostatistiques",
    "section": "\n7.5 Statistiques descriptives",
    "text": "7.5 Statistiques descriptives\nOn a vu comment générer des statistiques sommaires en R avec la fonction summary(). Reprenons les données d’iris.\n\ndata(\"iris\")\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nPour précisément effectuer une moyenne et un écart-type sur un vecteur, passons par les fonctions mean() et sd().\n\nmean(iris$Sepal.Length)\n\n[1] 5.843333\n\nsd(iris$Sepal.Length)\n\n[1] 0.8280661\n\n\nPour effectuer un sommaire de tableau piloté par une fonction, nous passons par la gamme de fonctions summarise(), de dplyr. Dans ce cas, avec group_by(), nous fragmentons le tableau par espèce pour effectuer un sommaire sur toutes les variables.\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise_all(mean)\n\n# A tibble: 3 × 5\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n1 setosa             5.01        3.43         1.46       0.246\n2 versicolor         5.94        2.77         4.26       1.33 \n3 virginica          6.59        2.97         5.55       2.03 \n\n\nVous pourriez être intéressé par les quartiles à 25, 50 et 75%. Mais la fonction summarise() n’autorise que les fonctions dont la sortie est d’un seul objet, alors faisons sorte que l’objet soit une liste - lorsque l’on imbrique une fonction funs, le tableau à insérer dans la fonction est indiqué par un ..\n\niris |&gt; \n  group_by(Species) |&gt; \n  summarise_all(list(q25 = ~ quantile(., probs = 0.25),\n                     q50 = ~ quantile(., probs = 0.50),\n                     q75 = ~ quantile(., probs = 0.75)))\n\n# A tibble: 3 × 13\n  Species    Sepal.Length_q25 Sepal.Width_q25 Petal.Length_q25 Petal.Width_q25\n  &lt;fct&gt;                 &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n1 setosa                 4.8             3.2               1.4             0.2\n2 versicolor             5.6             2.52              4               1.2\n3 virginica              6.22            2.8               5.1             1.8\n# ℹ 8 more variables: Sepal.Length_q50 &lt;dbl&gt;, Sepal.Width_q50 &lt;dbl&gt;,\n#   Petal.Length_q50 &lt;dbl&gt;, Petal.Width_q50 &lt;dbl&gt;, Sepal.Length_q75 &lt;dbl&gt;,\n#   Sepal.Width_q75 &lt;dbl&gt;, Petal.Length_q75 &lt;dbl&gt;, Petal.Width_q75 &lt;dbl&gt;\n\n\nEn mode programmation classique de R, on pourra générer les quartiles à la pièce.\n\nquantile(iris$Sepal.Length[iris$Species == 'setosa'])\n\n  0%  25%  50%  75% 100% \n 4.3  4.8  5.0  5.2  5.8 \n\nquantile(iris$Sepal.Length[iris$Species == 'versicolor'])\n\n  0%  25%  50%  75% 100% \n 4.9  5.6  5.9  6.3  7.0 \n\nquantile(iris$Sepal.Length[iris$Species == 'virginica'])\n\n   0%   25%   50%   75%  100% \n4.900 6.225 6.500 6.900 7.900 \n\n\nLa fonction table() permettra d’obtenir des décomptes par catégorie, ici par plages de longueurs de sépales. Pour obtenir les proportions du nombre total, il s’agit d’encapsuler le tableau croisé dans la fonction prop.table().\n\ntableau_croise &lt;- table(iris$Species,\n                        cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length)))\ntableau_croise\n\n            \n             (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9]\n  setosa            35        14         0         0\n  versicolor         4        20        17         9\n  virginica          1         5        18        26\n\n\n\nprop.table(tableau_croise)\n\n            \n               (4.3,5.1]   (5.1,5.8]   (5.8,6.4]   (6.4,7.9]\n  setosa     0.234899329 0.093959732 0.000000000 0.000000000\n  versicolor 0.026845638 0.134228188 0.114093960 0.060402685\n  virginica  0.006711409 0.033557047 0.120805369 0.174496644"
  },
  {
    "objectID": "07a-biostats.html#tests-dhypothèses-à-un-et-deux-échantillons",
    "href": "07a-biostats.html#tests-dhypothèses-à-un-et-deux-échantillons",
    "title": "7  Biostatistiques",
    "section": "\n7.6 Tests d’hypothèses à un et deux échantillons",
    "text": "7.6 Tests d’hypothèses à un et deux échantillons\nUn test d’hypothèse permet de décider si une hypothèse est confirmée ou rejetée à un seuil de probabilité prédéterminé.\nCette section est inspirée du chapitre 5 de Dalgaard, 2008.\n\nInformation: l’hypothèse nulle. Les tests d’hypothèse évaluent des effets statistiques (qui ne sont pas nécessairement des effets de causalité). L’effet à évaluer peut être celui d’un traitement, d’indicateurs météorologiques (e.g. précipitations totales, degré-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est menée pour évaluer l’hypothèse que l’on retrouve des différences entre des unités expérimentales. Par convention, l’hypothèse nulle (écrite \\(H_0\\)) est l’hypothèse qu’il n’y ait pas d’effet (c’est l’hypothèse de l’avocat du diable 😈) à l’échelle de la population (et non pas à l’échelle de l’échantillon). À l’inverse, l’hypothèse alternative (écrite \\(H_1\\)) est l’hypothèse qu’il y ait un effet à l’échelle de la population.\n\nÀ titre d’exercice en stats, on débute souvent en testant si deux vecteurs de valeurs continues proviennent de populations à moyennes différentes ou si un vecteur de valeurs a été généré à partir d’une population ayant une moyenne donnée. Dans cette section, nous utiliserons la fonction t.test() pour les tests de t et la fonction wilcox.test() pour les tests de Wilcoxon (aussi appelé de Mann-Whitney).\n\n7.6.1 Test de t à un seul échantillon\nNous devons assumer, pour ce test, que l’échantillon est recueillit d’une population dont la distribution est normale, \\(\\mathcal{N} \\sim \\left( \\mu, \\sigma^2 \\right)\\), et que chaque échantillon est indépendant l’un de l’autre. L’hypothèse nulle est souvent celle de l’avocat du diable, que la moyenne soit égale à une valeur donnée (donc la différence entre la moyenne de la population et une moyenne donnée est de zéro): ici, que \\(\\mu = \\bar{x}\\). L’erreur standard sur la moyenne (ESM) de l’échantillon, \\(\\bar{x}\\) est calculée comme suit.\n\\[ESM = \\frac{s}{\\sqrt{n}}\\]\noù \\(s\\) est l’écart-type de l’échantillon et \\(n\\) est le nombre d’échantillons.\nPour tester l’intervalle de confiance de l’échantillon, on multiplie l’ESM par l’aire sous la courbe de densité couvrant une certaine proportion de part et d’autre de l’échantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et d’autre.\n\nset.seed(33746)\nx &lt;- rnorm(20, 16, 4)\n\nlevel &lt;-  0.95\nalpha &lt;- 1-level\n\nx_bar &lt;- mean(x)\ns &lt;- sd(x)\nn &lt;- length(x)\n\nerror &lt;- qnorm(1 - alpha/2) * s / sqrt(n)\nerror\n\n[1] 1.483253\n\n\nL’intervalle de confiance est l’erreur de part et d’autre de la moyenne.\n\nc(x_bar - error, x_bar + error)\n\n[1] 14.35630 17.32281\n\n\nSi la moyenne de la population est de 16, un nombre qui se situe dans l’intervalle de confiance on accepte l’hypothèse nulle au seuil 0.05. Si le nombre d’échantillon est réduit (généralement &lt; 30), on passera plutôt par une distribution de t, avec \\(n-1\\) degrés de liberté.\n\nerror &lt;- qt(1 - alpha/2, n-1) * s / sqrt(n)\nc(x_bar - error, x_bar + error)\n\n[1] 14.25561 17.42351\n\n\nPlus simplement, on pourra utiliser la fonction t.test() en spécifiant la moyenne de la population. Nous avons généré 20 données avec une moyenne de 16 et un écart-type de 4. Nous savons donc que la vraie moyenne de l’échantillon est de 16. Mais disons que nous testons l’hypothèse que ces données sont tirées d’une population dont la moyenne est 18 (et implicitement que son écart-type est de 4).\n\nt.test(x, mu = 18)\n\n\n    One Sample t-test\n\ndata:  x\nt = -2.8548, df = 19, p-value = 0.01014\nalternative hypothesis: true mean is not equal to 18\n95 percent confidence interval:\n 14.25561 17.42351\nsample estimates:\nmean of x \n 15.83956 \n\n\nLa fonction retourne la valeur de t (t-value), le nombre de degrés de liberté (\\(n-1 = 19\\)), une description de l’hypothèse alternative (alternative hypothesis: true mean is not equal to 18), ainsi que l’intervalle de confiance au niveau de 95%. Le test contient aussi la p-value.\n\n\n7.6.1.1 Information: la p-value\n\nLa p-value, ou valeur-p ou p-valeur, est utilisée pour trancher si, oui ou non, un résultat est significatif. En langage scientifique, le mot significatif ne devrait être utilisé que lorsque l’on réfère à un test d’hypothèse statistique. Vous retrouverez des p-values partout en stats. Les p-values indiquent la probabilité que les données ait été échantillonnées d’une population où un effet est observable selon le modèle statistique utilisé.\n\nLa p-value est la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l’hypothèse nulle est vraie.\n\nUne p-value élevée indique que le modèle appliqué à vos données concorde avec la conclusion que l’hypothèse nulle est vraie, et inversement si la p-value est faible. Le seuil arbitraire utilisée en écologie et en agriculture, comme dans plusieurs domaines, est de 0.05. L’utilisation d’un seuil est toutefois contestée avec raison. Une enquête menée dans la littérature scientifiques a révélé que 49% des 791 articles étudiés interprétaient un effet non significatif comme un effet nul (Amrhein et al., 2019). En effet, une catégorisation de la p-value avec un seuil de significativité brouille le jugement sur l’importance des effets et de leur incertitude. Les six principes de l’American Statistical Association guident l’interprétation des p-values. [ma traduction]\n\nLes p-values indiquent l’ampleur de l’incompatibilité des données avec le modèle statistique\nLes p-values ne mesurent pas la probabilité que l’hypothèse étudiée soit vraie, ni la probabilité que les données ont été générées uniquement par la chance.\nLes conclusions scientifiques et décisions d’affaire ou politiques ne devraient pas être basées sur l’atteinte d’une p-value à un seuil spécifique.\nUne inférence appropriée demande un rapport complet et transparent.\nUne p-value, ou une signification statistique, ne mesure pas l’ampleur d’un effet ou l’importance d’un résultat.\nEn tant que tel, une p-value n’offre pas une bonne mesure des évidences d’un modèle ou d’une hypothèse.\n\n\nDans le cas précédent, la p-value était de 0.01014. Pour aider notre interprétation, prenons l’hypothèse alternative: true mean is not equal to 18. L’hypothèse nulle était bien que la vraie moyenne est égale à 18. Insérons la p-value dans la définition: la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l’hypothèse nulle est vraie est de 0.01014. Il est donc très peu probable que les données soient tirées d’un échantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette l’hypothèse nulle et l’on conclut qu’à ce seuil de confiance, l’échantillon ne provient pas d’une population ayant une moyenne de 18.\n\n7.6.2 Attention: mauvaises interprétations des p-values\n\n\n“La p-value n’a jamais été conçue comme substitut au raisonnement scientifique” Ron Wasserstein, directeur de l’American Statistical Association [ma traduction].\n\nUn résultat montrant une p-value plus élevée que 0.05 est-il pertinent?\nLors d’une conférence, Dr Evil ne présente que les résultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importants… En écartant ces résultats, Dr Evil commet 3 erreurs:\n\nLa p-value n’est pas un bon indicateur de l’importance d’un test statistique. L’importance d’une variable dans un modèle devrait être évaluée par la valeur de son coefficient. Son incertitude devrait être évaluée par sa variance. Une manière plus intuitive d’évaluer la variance est l’écart-type ou l’intervalle de confiance. À un certain seuil d’intervalle de confiance, la p-value traduira la probabilité qu’un coefficient réellement nul ait pu générer des données démontrant un coefficient égal ou supérieur.\nIl est tout aussi important de savoir que le traitement fonctionne que de savoir qu’il ne fonctionne pas. Les résultats démontrant des effets sont malheureusement davantage soumis aux journaux et davantage publiés que ceux ne démontrant pas d’effets (Decullier et al., 2005).\nLe seuil de 0.05 est arbitraire.\n\n\n\n7.6.2.1 Attention au p-hacking\n\nLe p-hacking (ou data dredging) consiste à manipuler les données et les modèles pour faire en sorte d’obtenir des p-values favorables à l’hypothèse testée et, éventuellement, aux conclusions recherchées. À éviter dans tous les cas. Toujours. Toujours. Toujours.\n\n\n\n\nUn sketch humoristique de John Oliver sur le p-hacking, Last week tonight, 2016 (en anglais)\n\n\n\n\n7.6.3 Test de Wilcoxon à un seul échantillon\nLe test de t suppose que la distribution des données est normale… ce qui est rarement le cas, surtout lorsque les échantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: c’est un test non-paramétrique basé sur le tri des valeurs.\n\nwilcox.test(x, mu = 18)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x\nV = 39, p-value = 0.01208\nalternative hypothesis: true location is not equal to 18\n\n\nLe V est la somme des rangs positifs. Dans ce cas, la p-value est semblable à celle du test de t, et les mêmes conclusions s’appliquent.\n\n7.6.4 Tests de t à deux échantillons\nLes tests à un échantillon servent plutôt à s’exercer: rarement en aura-t-on besoin en recherche, où plus souvent, on voudra comparer les moyennes de deux unités expérimentales. L’expérience comprend donc deux séries de données continues, \\(x_1\\) et \\(x_2\\), issues de lois de distribution normale \\(\\mathcal{N} \\left( \\mu_1, \\sigma_1^2 \\right)\\) et \\(\\mathcal{N} \\left( \\mu_2, \\sigma_2^2 \\right)\\), et nous testons l’hypothèse nulle que \\(\\mu_1 = \\mu_2\\). La statistique t est calculée comme suit.\n\\[t = \\frac{\\bar{x_1} - \\bar{x_2}}{ESDM}\\]\nL’ESDM est l’erreur standard de la différence des moyennes:\n\\[ESDM = \\sqrt{ESM_1^2 + ESM_2^2}\\]\nSi vous supposez que les variances sont identiques, l’erreur standard (\\(s\\)) est calculée pour les échantillons des deux groupes, puis insérée dans le calcul des ESM. La statistique t sera alors évaluée à \\(n_1 + n_2 - 2\\) degrés de liberté. Si vous supposez que la variance est différente (procédure de Welch), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degrés de liberté calculé à partir des erreurs standards et du nombre d’échantillon dans les groupes: cette procédure est considérée comme plus prudente (Dalgaard, 2008, page 101).\nPrenons les données d’iris pour l’exemple en excluant l’iris setosa étant donnée que les tests de t se restreignent à deux groupes. Nous allons tester la longueur des pétales.\n\niris_pl &lt;- iris |&gt; \n    filter(Species != \"setosa\") |&gt; \n    select(Species, Petal.Length)\nslice_sample(iris_pl, n = 5)\n\n     Species Petal.Length\n1  virginica          5.1\n2 versicolor          4.0\n3  virginica          5.0\n4 versicolor          4.6\n5 versicolor          4.1\n\n\nDans la prochaine cellule de code, nous introduisons l’interface-formule de R, où l’on retrouve typiquement le ~, entre les variables de sortie à gauche et les variables d’entrée à droite. Dans notre cas, la variable de sortie est la variable testée, Petal.Length, qui varie en fonction du groupe Species, qui est la variable d’entrée (variable explicative) - nous verrons les types de variables plus en détails dans la section sur les modèles statistiques (chapitre 7.8).\n\nt.test(formula = Petal.Length ~ Species,\n       data = iris_pl, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  Petal.Length by Species\nt = -12.604, df = 95.57, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group versicolor and group virginica is not equal to 0\n95 percent confidence interval:\n -1.49549 -1.08851\nsample estimates:\nmean in group versicolor  mean in group virginica \n                   4.260                    5.552 \n\n\nNous obtenons une sortie similaire aux précédentes. L’intervalle de confiance à 95% exclut le zéro, ce qui est cohérent avec la p-value très faible, qui nous indique le rejet de l’hypothèse nulle au seuil 0.05. Les données montrent que les groupes ont des moyennes de longueur de pétales différentes.\n\n\n7.6.4.1 Enregistrer les résultats d’un test\nIl est possible d’enregistrer un test dans un objet.\n\ntt_pl &lt;- t.test(formula = Petal.Length ~ Species,\n                data = iris_pl, var.equal = FALSE)\nsummary(tt_pl)\n\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \np.value     1      -none- numeric  \nconf.int    2      -none- numeric  \nestimate    2      -none- numeric  \nnull.value  1      -none- numeric  \nstderr      1      -none- numeric  \nalternative 1      -none- character\nmethod      1      -none- character\ndata.name   1      -none- character\n\nstr(tt_pl)\n\nList of 10\n $ statistic  : Named num -12.6\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 95.6\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 4.9e-22\n $ conf.int   : num [1:2] -1.5 -1.09\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num [1:2] 4.26 5.55\n  ..- attr(*, \"names\")= chr [1:2] \"mean in group versicolor\" \"mean in group virginica\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"difference in means between group versicolor and group virginica\"\n $ stderr     : num 0.103\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Welch Two Sample t-test\"\n $ data.name  : chr \"Petal.Length by Species\"\n - attr(*, \"class\")= chr \"htest\"\n\n\n\n7.6.5 Comparaison des variances\nPour comparer les variances, on a recours au test de F (F pour Fisher).\n\nvar.test(formula = Petal.Length ~ Species,\n         data = iris_pl)\n\n\n    F test to compare two variances\n\ndata:  Petal.Length by Species\nF = 0.72497, num df = 49, denom df = 49, p-value = 0.2637\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.411402 1.277530\nsample estimates:\nratio of variances \n         0.7249678 \n\n\nIl semble que l’on pourrait relancer le test de t sans la procédure Welch, avec var.equal = TRUE.\n\n7.6.6 Tests de Wilcoxon à deux échantillons\nCela ressemble au test de t!\n\nwilcox.test(formula = Petal.Length ~ Species,\n       data = iris_pl, var.equal = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Petal.Length by Species\nW = 44.5, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n7.6.7 Les tests pairés\nLes tests pairés sont utilisés lorsque deux échantillons proviennent d’une même unité expérimentale: il s’agit en fait de tests sur la différence entre deux observations.\n\nset.seed(2555)\n\nn &lt;- 20\navant &lt;- rnorm(n, 16, 4)\napres &lt;- rnorm(n, 18, 3)\n\nIl est important de spécifier que le test est pairé, la valeur par défaut de paired étant FALSE.\n\nt.test(avant, apres, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  avant and apres\nt = -1.5168, df = 19, p-value = 0.1458\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -4.5804586  0.7311427\nsample estimates:\nmean difference \n      -1.924658 \n\n\nL’hypothèse nulle qu’il n’y ait pas de différence entre l’avant et l’après traitement est acceptée au seuil 0.05.\nExercice. Effectuer un test de Wilcoxon pairé."
  },
  {
    "objectID": "07a-biostats.html#lanalyse-de-variance",
    "href": "07a-biostats.html#lanalyse-de-variance",
    "title": "7  Biostatistiques",
    "section": "\n7.7 L’analyse de variance",
    "text": "7.7 L’analyse de variance\nL’analyse de variance consiste à comparer des moyennes de plusieurs groupes distribués normalement et de même variance. Cette section sera élaborée prochainement plus en profondeur. Considérons-la pour le moment comme une régression sur une variable catégorielle.\n\npl_aov &lt;- aov(Petal.Length ~ Species, iris)\nsummary(pl_aov)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  437.1  218.55    1180 &lt;2e-16 ***\nResiduals   147   27.2    0.19                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa prochaine section, justement, est vouée aux modèles statistiques explicatifs, qui incluent la régression."
  },
  {
    "objectID": "07a-biostats.html#sec-bios-models",
    "href": "07a-biostats.html#sec-bios-models",
    "title": "7  Biostatistiques",
    "section": "\n7.8 Les modèles statistiques",
    "text": "7.8 Les modèles statistiques\nLa modélisation statistique consiste à lier de manière explicite des variables de sortie \\(y\\) (ou variables-réponse / variables dépendantes) à des variables explicatives \\(x\\) (ou variables prédictives / indépendantes / covariables). Les variables-réponse sont modélisées par une fonction des variables explicatives ou prédictives.\nPourquoi garder les termes explicatives et prédictives? Parce que les modèles statistiques (basés sur des données et non pas sur des mécanismes) sont de deux ordres. D’abord, les modèles prédictifs sont conçus pour prédire de manière fiable une ou plusieurs variables-réponse à partir des informations contenues dans les variables qui sont, dans ce cas, prédictives (par exemple : Les séries temporelles au chapitre 12). Lorsque l’on désire tester des hypothèses pour évaluer quelles variables expliquent la réponse, on parlera de modélisation (et de variables) explicative. En inférence statistique, on évaluera les corrélations entre les variables explicatives et les variables-réponse. Un lien de corrélation n’est pas un lien de causalité. L’inférence causale peut en revanche être évaluée par des modèles d’équations structurelles.\nCette section couvre la modélisation explicative. Les variables qui contribuent à créer les modèles peuvent être de différentes natures et distribuées selon différentes lois de probabilité. Alors que les modèles linéaires simples (lm) impliquent une variable-réponse distribuée de manière continue, les modèles linéaires généralisés peuvent aussi expliquer des variables de sorties discrètes.\nDans les deux cas, on distinguera les variables fixes et les variables aléatoires. Les variables fixes sont les variables testées lors de l’expérience: dose du traitement, espèce/cultivar, météo, etc. Les variables aléatoires sont les sources de variation qui génèrent du bruit dans le modèle: les unités expérimentales ou le temps lors de mesures répétées. Les modèles incluant des effets fixes seulement sont des modèles à effets fixes. Généralement, les modèles incluant des variables aléatoires incluent aussi des variables fixes: on parlera alors de modèles mixtes. Nous couvrirons ces deux types de modèles.\n\n7.8.1 Modèles à effets fixes\nLes tests de t et de Wilcoxon, explorés précédemment, sont des modèles statistiques à une seule variable. Nous avons vu dans l’interface-formule qu’une variable-réponse peut être liée à une variable explicative avec le tilde ~. En particulier, le test de t est une régression linéaire univariée (à une seule variable explicative) dont la variable explicative comprend deux catégories. De même, l’anova est une régression linéaire univariée dont la variable explicative comprend plusieurs catégories. Or l’interface-formule peut être utilisé dans plusieurs circonstances, notamment pour ajouter plusieurs variables de différents types: on parlera de régression multivariée.\nLa plupart des modèles statistiques peuvent être approximés comme une combinaison linéaire de variables: ce sont des modèles linéaires. Les modèles non-linéaires impliquent des stratégies computationnelles complexes qui rendent leur utilisation plus difficile à manœuvrer.\nUn modèle linéaire univarié prendra la forme \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), où \\(\\beta_0\\) est l’intercept et \\(\\beta_1\\) est la pente et \\(\\epsilon\\) est l’erreur.\nVous verrez parfois la notation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\). La notation avec le chapeau \\(\\hat{y}\\) exprime qu’il s’agit des valeurs générées par le modèle. En fait, \\(y = \\hat{y} - \\epsilon\\).\n\n7.8.1.1 Modèle linéaire univarié avec variable continue\nPrenons les données lasrosas.corn incluses dans le module agridat, où l’on retrouve le rendement d’une production de maïs à dose d’azote variable, en Argentine.\n\nlibrary(\"agridat\")\ndata(\"lasrosas.corn\")\nslice_sample(lasrosas.corn, n = 10)\n\n   year       lat      long yield nitro topo     bv rep nf\n1  1999 -33.05207 -63.84230 69.57   0.0   LO 185.67  R1 N0\n2  1999 -33.05137 -63.84383 67.41  53.0    E 175.12  R2 N2\n3  1999 -33.05104 -63.84323 68.33  29.0   LO 168.70  R3 N1\n4  1999 -33.05162 -63.84456 68.06  53.0    E 171.71  R1 N2\n5  1999 -33.05180 -63.84386 63.99   0.0   LO 172.46  R1 N0\n6  2001 -33.05065 -63.84578 35.85  50.6   HT 194.85  R1 N2\n7  1999 -33.05170 -63.84553 58.89 131.5   HT 187.98  R1 N5\n8  2001 -33.05077 -63.84502 50.95 124.6   HT 184.66  R2 N5\n9  1999 -33.05181 -63.84202 78.75 106.0   LO 169.25  R2 N4\n10 1999 -33.05154 -63.84468 68.58  29.0    E 169.35  R1 N1\n\n\nCes données comprennent plusieurs variables. Prenons le rendement (yield) comme variable de sortie et, pour le moment, ne retenons que la dose d’azote (nitro) comme variable explicative: il s’agit d’une régression univariée. Les deux variables sont continues. Explorons d’abord le nuage de points de l’une et l’autre.\n\nggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) +\n    geom_point()\n\n\n\n\nL’hypothèse nulle est que la dose d’azote n’affecte pas le rendement, c’est à dire que le coefficient de pente et nul. Une autre hypothèse est que l’intercept est nul, c’est à dire qu’à une dose de 0, le rendement est de 0. Un modèle linéaire à variable de sortie continue est créé avec la fonction lm(), pour linear model.\n\nmodlin_1 &lt;- lm(yield ~ nitro, data = lasrosas.corn)\nsummary(modlin_1)\n\n\nCall:\nlm(formula = yield ~ nitro, data = lasrosas.corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.183 -15.341  -3.079  13.725  45.897 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 65.843213   0.608573 108.193  &lt; 2e-16 ***\nnitro        0.061717   0.007868   7.845 5.75e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.66 on 3441 degrees of freedom\nMultiple R-squared:  0.01757,   Adjusted R-squared:  0.01728 \nF-statistic: 61.54 on 1 and 3441 DF,  p-value: 5.754e-15\n\n\nLe diagnostic du modèle comprend plusieurs informations. D’abord, la formule utilisée est affichée pour la traçabilité. Vient ensuite un aperçu de la distribution des résidus. La médiane devrait s’approcher de la moyenne des résidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en considération l’échelle de y, et ce -3.079 est exprimé en terme de rendement, ici en quintaux (i.e. 100 kg) par hectare. La distribution des résidus mérite d’être davantage investiguée. Nous verrons cela un peu plus tard.\nLes coefficients apparaissent ensuite. Les estimés sont les valeurs des effets. R fournit aussi l’erreur standard associée, la valeur de t ainsi que la p-value (la probabilité d’obtenir cet effet ou un effet plus extrême si en réalité il y avait absence d’effet). L’intercept est bien sûr plus élevé que 0 (à dose nulle, on obtient un rendement de 65.8 quintaux par hectare en moyenne). La pente de la variable nitro est de ~0.06: pour chaque augmentation d’un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de maïs. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que l’intercept. Soulignons que l’ampleur du coefficient est très important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait qu’elle est inférieure à 0.05 (ce qui arrive souvent dans la littérature), serait très insuffisant pour l’interprétation des statistiques. La p-value nous indique néanmoins qu’il serait très improbable qu’une telle pente ait été générée alors que celle-ci est nulle en réalité. Les étoiles à côté des p-values indiquent l’ampleur selon l’échelle Signif. codes indiquée en-dessous du tableau des coefficients.\nSous ce tableau, R offre d’autres statistiques. En outre, les R² et R² ajustés indiquent si la régression passe effectivement par les points. Le R² prend un maximum de 1 lorsque la droite passe exactement sur les points.\nEnfin, le test de F génère une p-value indiquant la probabilité que les coefficients de pente ait été générés si les vrais coefficients étaient nuls. Dans le cas d’une régression univariée, cela répète l’information sur l’unique coefficient.\nOn pourra également obtenir les intervalles de confiance avec la fonction confint().\n\nconfint(modlin_1, level = 0.95)\n\n                  2.5 %      97.5 %\n(Intercept) 64.65001137 67.03641474\nnitro        0.04629164  0.07714271\n\n\nOu soutirer l’information de différentes manières, comme avec la fonction coefficients().\n\ncoefficients(modlin_1)\n\n(Intercept)       nitro \n65.84321305  0.06171718 \n\n\nÉgalement, on pourra exécuter le modèle sur les données qui ont servi à le générer:\n\npredict(modlin_1)[1:5]\n\n       1        2        3        4        5 \n73.95902 73.95902 73.95902 73.95902 73.95902 \n\n\nOu sur des données externes.\n\nnouvelles_donnees &lt;- data.frame(nitro = seq(from = 0, to = 100, by = 5))\npredict(modlin_1, newdata = nouvelles_donnees)[1:5]\n\n       1        2        3        4        5 \n65.84321 66.15180 66.46038 66.76897 67.07756 \n\n\n\n7.8.1.2 Analyse des résidus\nLes résidus sont les erreurs du modèle. C’est le vecteur \\(\\epsilon\\), qui est un décalage entre les données et le modèle. Le R² est un indicateur de l’ampleur du décalage, mais une régression linéaire explicative en bonne et due forme devrait être accompagnée d’une analyse des résidus. On peut les calculer par \\(\\epsilon = y - \\hat{y}\\), ou alors simplement utiliser la fonction residuals().\n\nres_df &lt;- data.frame(nitro = lasrosas.corn$nitro,\n                     residus_lm = residuals(modlin_1),\n                     residus_calcul = lasrosas.corn$yield - predict(modlin_1))\nslice_sample(res_df, n = 10)\n\n     nitro residus_lm residus_calcul\n2931 124.6  24.666827      24.666827\n1793 124.6  11.126827      11.126827\n2006  99.8  25.417413      25.417413\n116   66.0 -11.636547     -11.636547\n1235 131.5  11.460978      11.460978\n2426  75.4 -18.686688     -18.686688\n1132  29.0  -1.763011      -1.763011\n15   131.5 -11.289022     -11.289022\n1691 131.5  -5.639022      -5.639022\n38   131.5 -13.129022     -13.129022\n\n\nDans une bonne régression linéaire, on ne retrouvera pas de structure identifiable dans les résidus, c’est-à-dire que les résidus sont bien distribués de part et d’autre du modèle de régression.\n\nggplot(res_df, aes(x = nitro, y = residus_lm)) +\n  geom_point() +\n  labs(x = \"Dose N\", y = \"Résidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nBien que le jugement soit subjectif, on peut dire avec confiance qu’il n’y a pas structure particulière. En revanche, on pourrait générer un \\(y\\) qui varie de manière quadratique avec \\(x\\), un modèle linéaire montrera une structure évidente.\n\nset.seed(36164)\nx &lt;- 0:100\ny &lt;- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50)\nmodlin_2 &lt;- lm(y ~ x)\nggplot(data.frame(y, residus = residuals(modlin_2)),\n       aes(x = x, y = residus)) +\n  geom_point() +\n  labs(x = \"x\", y = \"Résidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\n\n\n\nÉgalement, les résidus ne devraient pas croître avec \\(x\\).\n\nset.seed(3984)\nx &lt;- 0:100\ny &lt;-  10 + x + x * rnorm(length(x), 0, 2)\nmodlin_3 &lt;- lm(y ~ x)\nggplot(data.frame(x, residus = residuals(modlin_3)),\n       aes(x = x, y = residus)) +\n  geom_point() +\n  labs(x = \"x\", y = \"Résidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\n\n\n\nOn pourra aussi inspecter les résidus avec un graphique de leur distribution. Reprenons notre modèle de rendement du maïs.\n\nggplot(res_df, aes(x = residus_lm)) +\n  geom_histogram(binwidth = 2, color = \"white\") +\n  labs(x = \"Residual\")\n\n\n\n\nL’histogramme devrait présenter une distribution normale. Les tests de normalité comme le test de Shapiro-Wilk peuvent aider, mais ils sont généralement très sévères.\n\nshapiro.test(res_df$residus_lm)\n\n\n    Shapiro-Wilk normality test\n\ndata:  res_df$residus_lm\nW = 0.94868, p-value &lt; 2.2e-16\n\n\nL’hypothèse nulle que la distribution est normale est rejetée au seuil 0.05. Dans notre cas, il est évident que la sévérité du test n’est pas en cause car les résidus semblent générer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilité de la variable-réponse.\n\n7.8.1.3 Régression multiple\nComme c’est le cas pour bien des phénomènes en écologie, le rendement d’une culture n’est certainement pas expliqué seulement par la dose d’azote.\nLorsque l’on combine plusieurs variables explicatives, on crée un modèle de régression multivariée, ou une régression multiple. Bien que les tendances puissent sembler non-linéaires, l’ajout de variables et le calcul des coefficients associés reste un problème d’algèbre linéaire.\nOn pourra en effet généraliser les modèles linéaires, univariés et multivariés, de la manière suivante.\n\\[ y = X \\beta + \\epsilon \\]\noù:\n\\(X\\) est la matrice du modèle à \\(n\\) observations et \\(p\\) variables.\n\\[ X = \\left( \\begin{matrix}\n1 & x_{11} & \\cdots & x_{1p}  \\\\\n1 & x_{21} & \\cdots & x_{2p}  \\\\\n\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n1 & x_{n1} & \\cdots & x_{np}\n\\end{matrix} \\right) \\]\n\\(\\beta\\) est la matrice des \\(p\\) coefficients, \\(\\beta_0\\) étant l’intercept qui multiplie la première colonne de la matrice \\(X\\).\n\\[ \\beta = \\left( \\begin{matrix}\n\\beta_0  \\\\\n\\beta_1  \\\\\n\\vdots \\\\\n\\beta_p\n\\end{matrix} \\right) \\]\n\\(\\epsilon\\) est l’erreur de chaque observation.\n\\[ \\epsilon = \\left( \\begin{matrix}\n\\epsilon_0  \\\\\n\\epsilon_1  \\\\\n\\vdots \\\\\n\\epsilon_n\n\\end{matrix} \\right) \\]\n\n7.8.1.4 Modèles linéaires univariés avec variable catégorielle nominale\n\nUne variable catégorielle nominale (non ordonnée) utilisée à elle seule dans un modèle comme variable explicative, est un cas particulier de régression multiple. En effet, l’encodage catégoriel (ou dummyfication) transforme une variable catégorielle nominale en une matrice de modèle comprenant une colonne désignant l’intercept (une série de 1) désignant la catégorie de référence, ainsi que des colonnes pour chacune des autres catégories désignant l’appartenance (1) ou la non appartenance (0) de la catégorie désignée par la colonne.\n\n7.8.1.4.1 L’encodage catégoriel\nUne variable à \\(C\\) catégories pourra être déclinée en \\(C\\) variables dont chaque colonne désigne par un 1 l’appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour l’exemple, créons un vecteur désignant le cultivar de pomme de terre.\n\ndata &lt;- data.frame(cultivar = factor(c('Superior', 'Superior', 'Superior', 'Russet', 'Kenebec', 'Russet')))\nmodel.matrix(~cultivar, data)\n\n  (Intercept) cultivarRusset cultivarSuperior\n1           1              0                1\n2           1              0                1\n3           1              0                1\n4           1              1                0\n5           1              0                0\n6           1              1                0\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$cultivar\n[1] \"contr.treatment\"\n\n\nNous avons trois catégories, encodées en trois colonnes. La première colonne est un intercept et les deux autres décrivent l’absence (0) ou la présence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que l’appartenance à une catégorie est mutuellement exclusive, c’est-à-dire qu’un échantillon ne peut être assigné qu’à une seule catégorie, on peut déduire une catégorie à partir de l’information sur toutes les autres. Par exemple, si cultivar_Russet et cultivar_Superior sont toutes deux égales à \\(0\\), on conclura que cultivar_Kenebec est nécessairement égal à \\(1\\). Et si l’un d’entre cultivar_Russet et cultivar_Superior est égal à \\(1\\), cultivar_Kenebec est nécessairement égal à \\(0\\). L’information contenue dans un nombre \\(C\\) de catégories peut être encodée dans un nombre \\(C-1\\) de colonnes. C’est pourquoi, dans une analyse statistique, on désignera une catégorie comme une référence, que l’on détecte lorsque toutes les autres catégories sont encodées avec des \\(0\\): cette référence sera incluse dans l’intercept. La catégorie de référence par défaut en R est la première catégorie dans l’ordre alphabétique. On pourra modifier cette référence avec la fonction relevel().\n\ndata$cultivar &lt;- relevel(data$cultivar, ref = \"Superior\")\nmodel.matrix(~cultivar, data)\n\n  (Intercept) cultivarKenebec cultivarRusset\n1           1               0              0\n2           1               0              0\n3           1               0              0\n4           1               0              1\n5           1               1              0\n6           1               0              1\nattr(,\"assign\")\n[1] 0 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$cultivar\n[1] \"contr.treatment\"\n\n\nPour certains modèles, vous devrez vous assurer vous-même de l’encodage catégoriel. Pour d’autre, en particulier avec l’interface par formule de R, ce sera fait automatiquement.\n\n7.8.1.4.2 Exemple d’application\nPrenons la topographie du terrain, qui peut prendre plusieurs niveaux.\n\nlevels(lasrosas.corn$topo)\n\n[1] \"E\"  \"HT\" \"LO\" \"W\" \n\n\nExplorons le rendement selon la topographie.\n\nggplot(lasrosas.corn, aes(x = topo, y = yield)) +\n    geom_boxplot()\n\n\n\n\nLes différences sont évidentes, et la modélisation devrait montrer des effets différents.\nL’encodage catégoriel peut être visualisé en générant la matrice de modèle avec la fonction model.matrix() et l’interface-formule - sans la variable-réponse.\n\nmodel.matrix(~ topo, data = lasrosas.corn) |&gt; \n    as_tibble() |&gt;  # as_tibble pour transformer la matrice en tableau\n    slice_sample(n = 10)\n\n# A tibble: 10 × 4\n   `(Intercept)` topoHT topoLO topoW\n           &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1             1      0      0     0\n 2             1      0      1     0\n 3             1      0      0     1\n 4             1      0      0     1\n 5             1      0      1     0\n 6             1      0      0     1\n 7             1      1      0     0\n 8             1      0      1     0\n 9             1      0      0     1\n10             1      0      0     0\n\n\nDans le cas d’un modèle avec une variable catégorielle nominale seule, l’intercept représente la catégorie de référence, ici E. Les autres colonnes spécifient l’appartenance (1) ou la non-appartenance (0) de la catégorie pour chaque observation.\nCette matrice de modèle utilisée pour la régression donnera un intercept, qui indiquera l’effet de la catégorie de référence, puis les différences entre les catégories subséquentes et la catégorie de référence.\n\nmodlin_4 &lt;- lm(yield ~ topo, data = lasrosas.corn)\nsummary(modlin_4)\n\n\nCall:\nlm(formula = yield ~ topo, data = lasrosas.corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.371 -11.933  -1.593  11.080  44.119 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  78.6653     0.5399 145.707   &lt;2e-16 ***\ntopoHT      -30.0526     0.7500 -40.069   &lt;2e-16 ***\ntopoLO        6.2832     0.7293   8.615   &lt;2e-16 ***\ntopoW       -11.8841     0.7039 -16.883   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.59 on 3439 degrees of freedom\nMultiple R-squared:  0.4596,    Adjusted R-squared:  0.4591 \nF-statistic:   975 on 3 and 3439 DF,  p-value: &lt; 2.2e-16\n\n\nLe modèle linéaire est équivalent à l’anova, mais les résultats de lm sont plus élaborés.\n\nsummary(aov(yield ~ topo, data = lasrosas.corn))\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \ntopo           3 622351  207450     975 &lt;2e-16 ***\nResiduals   3439 731746     213                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nL’analyse de résidus peut être effectuée de la même manière.\n\n7.8.1.5 Modèles linéaires univariés avec variable catégorielle ordinale\n\nBien que j’introduise la régression sur variable catégorielle ordinale à la suite de la section sur les variables nominales, nous revenons dans ce cas à une régression simple, univariée. Voyons un cas à 5 niveaux.\n\nstatut &lt;- c(\"Totalement en désaccord\",\n            \"En désaccord\",\n            \"Ni en accord, ni en désaccord\",\n            \"En accord\",\n            \"Totalement en accord\")\nstatut_o &lt;- factor(statut, levels = statut, ordered=TRUE)\nmodel.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) où 5 est le nombre de niveaux\n\n  (Intercept)    statut_o.L statut_o.Q    statut_o.C statut_o^4\n1           1 -6.324555e-01  0.5345225 -3.162278e-01  0.1195229\n2           1 -3.162278e-01 -0.2672612  6.324555e-01 -0.4780914\n3           1 -3.510833e-17 -0.5345225  1.755417e-16  0.7171372\n4           1  3.162278e-01 -0.2672612 -6.324555e-01 -0.4780914\n5           1  6.324555e-01  0.5345225  3.162278e-01  0.1195229\nattr(,\"assign\")\n[1] 0 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$statut_o\n[1] \"contr.poly\"\n\n\nLa matrice de modèle a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres désignant différentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils linéairement? De manière quadratique, cubique ou plus loin dans des distributions polynomiales?\n\nmodmat_tidy &lt;- data.frame(statut, model.matrix(~statut_o)[, -1]) |&gt; \n    pivot_longer(-statut, names_to = \"variable\", values_to = \"valeur\")\nmodmat_tidy$statut &lt;- factor(modmat_tidy$statut,\n                             levels = statut,\n                             ordered=TRUE)\nggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) +\n    facet_wrap(. ~ variable) +\n    geom_point() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nRègle générale, pour les variables ordinales, on préférera une distribution linéaire, et c’est l’option par défaut de la fonction lm(). L’utilisation d’une autre distribution peut être effectuée à la mitaine en utilisant dans le modèle la colonne désirée de la sortie de la fonction model.matrix().\n\n7.8.1.6 Régression multiple à plusieurs variables\nReprenons le tableau de données du rendement de maïs.\n\nhead(lasrosas.corn)\n\n  year       lat      long yield nitro topo     bv rep nf\n1 1999 -33.05113 -63.84886 72.14 131.5    W 162.60  R1 N5\n2 1999 -33.05115 -63.84879 73.79 131.5    W 170.49  R1 N5\n3 1999 -33.05116 -63.84872 77.25 131.5    W 168.39  R1 N5\n4 1999 -33.05117 -63.84865 76.35 131.5    W 176.68  R1 N5\n5 1999 -33.05118 -63.84858 75.55 131.5    W 171.46  R1 N5\n6 1999 -33.05120 -63.84851 70.24 131.5    W 170.56  R1 N5\n\n\nPour ajouter des variables au modèle dans l’interface-formule, on additionne les noms de colonne. La variable lat désigne la latitude, la variable long désigne la longitude et la variable bv (brightness value) désigne la teneur en matière organique du sol (plus bv est élevée, plus faible est la teneur en matière organique).\n\nmodlin_5 &lt;- lm(yield ~ lat + long + nitro + topo + bv,\n               data = lasrosas.corn)\nsummary(modlin_5)\n\n\nCall:\nlm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.405 -11.071  -1.251  10.592  40.078 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.946e+05  3.309e+04   5.882 4.45e-09 ***\nlat          5.541e+03  4.555e+02  12.163  &lt; 2e-16 ***\nlong         1.776e+02  4.491e+02   0.395    0.693    \nnitro        6.867e-02  5.451e-03  12.597  &lt; 2e-16 ***\ntopoHT      -2.665e+01  1.087e+00 -24.520  &lt; 2e-16 ***\ntopoLO       5.565e+00  1.035e+00   5.378 8.03e-08 ***\ntopoW       -1.465e+01  1.655e+00  -8.849  &lt; 2e-16 ***\nbv          -5.089e-01  3.069e-02 -16.578  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.47 on 3435 degrees of freedom\nMultiple R-squared:  0.5397,    Adjusted R-squared:  0.5387 \nF-statistic: 575.3 on 7 and 3435 DF,  p-value: &lt; 2.2e-16\n\n\nL’ampleur des coefficients est relatif à l’échelle de la variable. En effet, un coefficient de 5541 sur la variable lat n’est pas comparable au coefficient de la variable bv, de -0.5089, étant donné que les variables ne sont pas exprimées avec la même échelle. Pour les comparer sur une même base, on peut centrer (soustraire la moyenne) et réduire (diviser par l’écart-type).\n\nlasrosas.corn_sc &lt;- lasrosas.corn |&gt; \n  mutate_at(c(\"lat\", \"long\", \"nitro\", \"bv\"), scale)\n\nmodlin_5_sc &lt;- lm(yield ~ lat + long + nitro + topo + bv,\n               data = lasrosas.corn_sc)\nsummary(modlin_5_sc)\n\n\nCall:\nlm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.405 -11.071  -1.251  10.592  40.078 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  78.9114     0.6666 118.376  &lt; 2e-16 ***\nlat           3.9201     0.3223  12.163  &lt; 2e-16 ***\nlong          0.3479     0.8796   0.395    0.693    \nnitro         2.9252     0.2322  12.597  &lt; 2e-16 ***\ntopoHT      -26.6487     1.0868 -24.520  &lt; 2e-16 ***\ntopoLO        5.5647     1.0347   5.378 8.03e-08 ***\ntopoW       -14.6487     1.6555  -8.849  &lt; 2e-16 ***\nbv           -4.9253     0.2971 -16.578  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.47 on 3435 degrees of freedom\nMultiple R-squared:  0.5397,    Adjusted R-squared:  0.5387 \nF-statistic: 575.3 on 7 and 3435 DF,  p-value: &lt; 2.2e-16\n\n\nTypiquement, les variables catégorielles, qui ne sont pas mises à l’échelle, donneront des coefficients plus élevées, et devrons être évaluées entre elles et non comparativement aux variables mises à l’échelle. Une manière conviviale de représenter des coefficients consiste à utiliser la fonction tidy du module broom, qui génère un tableau contenant les coefficients ainsi que leurs intervalles de confiance, que nous pourrons ensuite porter graphiquement.\n\nlibrary(\"broom\") # ou bien charger le méta-module tidymodels\nintervals &lt;- tidy(modlin_5_sc, conf.int = TRUE, conf.level = 0.95)\nintervals\n\n# A tibble: 8 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   78.9       0.667   118.    0            77.6      80.2 \n2 lat            3.92      0.322    12.2   2.34e- 33     3.29      4.55\n3 long           0.348     0.880     0.395 6.93e-  1    -1.38      2.07\n4 nitro          2.93      0.232    12.6   1.33e- 35     2.47      3.38\n5 topoHT       -26.6       1.09    -24.5   1.74e-122   -28.8     -24.5 \n6 topoLO         5.56      1.03      5.38  8.03e-  8     3.54      7.59\n7 topoW        -14.6       1.66     -8.85  1.39e- 18   -17.9     -11.4 \n8 bv            -4.93      0.297   -16.6   1.92e- 59    -5.51     -4.34\n\n\nLa valeur par défaut de l’argument conf.level est de 0.95, mais je vous suggère de toujours l’écrire de manière explicite, ne serait-ce que pour rappeler à vous-même ainsi qu’à vos collègues que cette valeur est arbitraire: il s’agit d’une décision d’analyse, non pas d’une valeur à utiliser par convention.\nPour le graphique, on aura avantage à séparer les effets catégoriels aux effets numériques pour mieux interpréter leurs effets entre eux. J’utilise la fonction dplyr::case_when() pour créer une nouvelle colonne qui catégorisera les termes de l’équation. Cette catégorie me permettra d’effectuer un facet_wrap().\n\nintervals |&gt; \n  mutate(type = case_when(\n    term %in% c(\"topoHT\", \"topoLO\", \"topoW\") ~ \"Catégorie\", # condition ~ résultat\n    term == \"(Intercept)\" ~ \"Intercept\",  # condition ~ résultat\n    TRUE ~ \"numérique\" # pour toute autre condition (TRUE) ~ résultat\n  )) |&gt; \n  ggplot(mapping = aes(x = estimate, y = term)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  geom_segment(mapping = aes(x = conf.low, xend = conf.high, yend = term)) +\n  geom_point() +\n  labs(x = \"Coefficient standardisé\", y = \"\") +\n  facet_wrap(~type, scales = \"free\", ncol = 1, strip.position = \"right\")\n\n\n\n\nOn y voit qu’à l’exception de la variable long, tous les coefficients sont éloignés de 0. Le coefficient bv est négatif, indiquant que plus la valeur de bv est élevé (donc plus le sol est pauvre en matière organique), plus le rendement est faible. Plus la latitude est élevée (plus on se dirige vers le Nord de l’Argentine), plus le rendement est élevé. La dose d’azote a aussi un effet statistique positif sur le rendement.\nQuant aux catégories topographiques, elles sont toutes éloignées de la catégorie E, placée à zéro. De plus, les intervalles de confiance à 0.95 ne se chevauchant pas, on peut conclure que la variabilité du phénomène échantillonné n’est pas suffisante pour expliquer les différences importantes entre elles.\nOn pourra retrouver des cas où l’effet combiné de plusieurs variables diffère de l’effet des deux variables prises séparément. Par exemple, on pourrait évaluer l’effet de l’azote et celui de la topographie dans un même modèle, puis y ajouter une interaction entre l’azote et la topographie, qui définira des effets supplémentaires de l’azote selon chaque catégorie topographique. C’est ce que l’on appelle une interaction.\nDans l’interface-formule, l’interaction entre l’azote et la topographie est notée nitro:topo. Pour ajouter cette interaction, la formule deviendra yield ~ nitro + topo + nitro:topo. Une approche équivalente est d’utiliser le raccourci yield ~ nitro * topo.\n\nmodlin_5_sc &lt;- lm(yield ~ nitro * topo,\n               data = lasrosas.corn_sc)\nsummary(modlin_5_sc)\n\n\nCall:\nlm(formula = yield ~ nitro * topo, data = lasrosas.corn_sc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-47.984 -11.985  -1.388  10.339  40.636 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   78.6999     0.5322 147.870  &lt; 2e-16 ***\nnitro          1.8131     0.5351   3.388 0.000711 ***\ntopoHT       -30.0052     0.7394 -40.578  &lt; 2e-16 ***\ntopoLO         6.2026     0.7190   8.627  &lt; 2e-16 ***\ntopoW        -11.9628     0.6939 -17.240  &lt; 2e-16 ***\nnitro:topoHT   1.2553     0.7461   1.682 0.092565 .  \nnitro:topoLO   0.5695     0.7186   0.792 0.428141    \nnitro:topoW    0.7702     0.6944   1.109 0.267460    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.38 on 3435 degrees of freedom\nMultiple R-squared:  0.4756,    Adjusted R-squared:  0.4746 \nF-statistic: 445.1 on 7 and 3435 DF,  p-value: &lt; 2.2e-16\n\n\nLes résultats montrent des effets de l’azote et des catégories topographiques, mais il y a davantage d’incertitude sur les interactions, indiquant que l’effet statistique de l’azote est sensiblement le même indépendamment des niveaux topographiques.\nDans le cas des régressions multiples, les résidus ne peuvent pas être présentés selon une variable explicative \\(x\\), puisqu’il y en a plusieurs. On fera l’analyse des résidus selon la variable réponse \\(y\\).\n\ntibble(\n  y = lasrosas.corn_sc$yield,\n  residus = residuals(modlin_5_sc)\n) |&gt; \n  ggplot(aes(x = y, y = residus)) +\n  geom_point() +\n  labs(x = \"y\", y = \"Résidus\") +\n  geom_hline(yintercept = 0, col = \"red\", size = 1)\n\n\n\n\nDans ce modèle, il y a clairement une structure qui nous échappe! L’ajout d’autres variables nous permettrait éventuellement d’obtenir une distribution qui s’approche d’un bruit.\n\n7.8.1.7 Les interactions\nUne interaction est un effet supplémentaire qui est investigué pour des combinaisons de variables. L’interaction entre l’azote et la topographie est une nouvelle variable créée par la multiplication de l’azote, une variable numérique, et de la topographie, qui ici est une variable catégorielle.\n\nmodel.matrix(~ nitro * topo, data = lasrosas.corn_sc) |&gt; head()\n\n  (Intercept)    nitro topoHT topoLO topoW nitro:topoHT nitro:topoLO\n1           1 1.571194      0      0     1            0            0\n2           1 1.571194      0      0     1            0            0\n3           1 1.571194      0      0     1            0            0\n4           1 1.571194      0      0     1            0            0\n5           1 1.571194      0      0     1            0            0\n6           1 1.571194      0      0     1            0            0\n  nitro:topoW\n1    1.571194\n2    1.571194\n3    1.571194\n4    1.571194\n5    1.571194\n6    1.571194\n\n\nL’entête de la matrice modèle montre que l’interaction est l’addition de trois variables, qui sont nulles si la catégorie topographique est absente, mais qui prend la dose d’azote pour la catégorie présente seulement.\nL’interprétation d’une interaction est spécifique au modèle utilisé. Une manière de l’interpréter est de se demander dans quelles unités elle est exprimée. Dans notre exemple, il s’agit de kg/ha standardisés.\nPrenons un autre exemple, cette fois-ci avec des données fictives. Une enquête a été menée où des personnes évaluaient le karma (échelle 0 à 10) de pieds nus, en bas (chaussettes) et/ou en sandales.\n\nkarma_df &lt;- read_csv(\"data/karma_df.csv\")\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): ID, sandales, bas, karma\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNous désirons savoir quelle est l’effet des bas et des sandales sur le karma, donc 💖 ~ 👡 + 🧦.\n\ntidy(lm(karma ~ sandales + bas, karma_df))\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.67     0.147      25.0 1.11e-94\n2 sandales        2.29     0.166      13.8 7.99e-38\n3 bas             1.77     0.168      10.5 6.31e-24\n\n\nÀ partir du scénario à pieds nus d’un karma de 3.67, les sandales ajoutent 2.29 de points de karma, alors que les bas en ajoutent 1.8. Mais ce modèle est incomplet, cas on n’évalue pas l’effet des bas ET des sandales, donc 💖 ~ 👡 * 🧦.\n\ntidy(lm(karma ~ sandales * bas, karma_df))\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      2.68     0.134      20.0 2.22e-68\n2 sandales         3.68     0.159      23.2 4.07e-85\n3 bas              3.20     0.162      19.8 1.40e-67\n4 sandales:bas    -5.25     0.309     -17.0 3.79e-53\n\n\nLe modèle est plus clair. Sans interaction, les effets sur le karma des bas et des sandales étaient négativement affectés par l’effet d’interaction sandales:bas, le karma étant poussé à la baisse par le bas blanc dans vos sandales.\nIl est possible d’ajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a d’interactions, plus votre modèle comprendra de variables et vos tests d’hypothèse perdront en puissance statistique.\n\n7.8.1.8 Les modèles linéaires généralisés\nDans un modèle linéaire ordinaire, un changement constant dans les variables explicatives résulte en un changement constant de la variable-réponse. Cette supposition ne serait pas adéquate si la variable-réponse était un décompte, si elle est booléenne ou si, de manière générale, la variable-réponse ne suivait pas une distribution continue. De manière plus spécifique, elle ne s’applique pas aux cas où il n’y a pas moyen de retrouver une distribution normale des résidus. On pourra bien sûr transformer les variables. Mais il pourrait s’avérer impossible ou tout simplement non souhaitable de transformer les variables. Le modèle linéaire généralisé (MLG, ou generalized linear model - GLM) est une généralisation du modèle linéaire ordinaire chez qui la variable-réponse peut être caractérisé par une distribution de Poisson, de Bernouilli, etc.\nPrenons d’abord le cas d’un décompte de vers fil-de-fer (worms) retrouvés dans des parcelles sous différents traitements (trt). Les décomptes sont typiquement distribués selon une loi de Poisson.\n\ncochran.wireworms |&gt; ggplot(aes(x = worms)) + geom_histogram(bins = 10)\n\n\n\n\nExplorons les décomptes selon les traitements.\n\ncochran.wireworms |&gt; ggplot(aes(x = trt, y = worms)) + geom_boxplot()\n\n\n\n\nLes traitements semblent à première vue avoir un effet comparativement au contrôle. Lançons un MLG avec la fonction glm(), et spécifions que la sortie est une distribution de Poisson. Bien que la fonction de lien (link = \"log\") soit explictement imposée, le log est la valeur par défaut pour les distributions de Poisson. Ainsi, les coefficients du modèles devront être interprétés selon un modèle \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\).\n\nmodglm_1 &lt;- glm(worms ~ trt, cochran.wireworms, family = stats::poisson(link=\"log\"))\nsummary(modglm_1)\n\n\nCall:\nglm(formula = worms ~ trt, family = stats::poisson(link = \"log\"), \n    data = cochran.wireworms)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.1823     0.4082   0.447 0.655160    \ntrtM          1.6422     0.4460   3.682 0.000231 ***\ntrtN          1.7636     0.4418   3.991 6.57e-05 ***\ntrtO          1.5755     0.4485   3.513 0.000443 ***\ntrtP          1.3437     0.4584   2.931 0.003375 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 64.555  on 24  degrees of freedom\nResidual deviance: 38.026  on 20  degrees of freedom\nAIC: 125.64\n\nNumber of Fisher Scoring iterations: 5\n\n\nL’interprétation spécifique des coefficients d’une régression de Poisson doit passer par la fonction de lien \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). Le traitement de référence (K), qui correspond à l’intercept, sera accompagné d’un nombre de vers de \\(exp \\left(0.1823\\right) = 1.20\\) vers, et le traitement M, à \\(exp \\left(1.6422\\right) = 5.17\\) vers. Cela correspond à ce que l’on observe sur les boxplots plus haut.\nIl est très probable (p-value de ~0.66) qu’un intercept (traitement K) de 0.18 ayant une erreur standard de 0.4082 ait été généré depuis une population dont l’intercept est nul. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils être considérés comme équivalents?\n\nintervals &lt;- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l'intercept\n                    LL = confint(modglm_1)[, 1], # [-1, ] enlever la première ligne, celle de l'intercept\n                    UL = confint(modglm_1)[, 2],\n                    variable = names(coefficients(modglm_1)))\n\nAttente de la réalisation du profilage...\nAttente de la réalisation du profilage...\n\nintervals\n\n# A tibble: 5 × 4\n  Estimate     LL    UL variable   \n     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n1    0.182 -0.740 0.888 (Intercept)\n2    1.64   0.840 2.62  trtM       \n3    1.76   0.972 2.74  trtN       \n4    1.58   0.766 2.56  trtO       \n5    1.34   0.509 2.34  trtP       \n\n\n\nggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) +\n    geom_vline(xintercept = 0, lty = 2) +\n    geom_segment(mapping = aes(x = LL, xend = UL,\n                               y = variable, yend = variable)) +\n    geom_point() +\n    labs(x = \"Coefficient\", y = \"\")\n\n\n\n\nLes intervalles de confiance se superposant, on ne peut pas conclure qu’un traitement est lié à une réduction plus importante de vers qu’un autre, au seuil 0.05.\nMaintenant, à défaut de trouver un tableau de données plus approprié, prenons le tableau mtcars, qui rassemble des données sur des modèles de voitures. La colonne vs, pour v-shaped, inscrit 0 si les pistons sont droits et 1 s’ils sont placés en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du véhicule (wt)?\n\nmtcars |&gt; slice_sample(n = 6)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nFiat 128          32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nVolvo 142E        21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nChrysler Imperial 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n\n\n\nmtcars |&gt; \n    ggplot(aes(x = wt, y = vs)) + geom_point()\n\n\n\n\nIl semble y avoir une tendance: les véhicules plus lourds ont plutôt des pistons droits (vs = 0). Vérifions cela.\n\nmodglm_2 &lt;- glm(vs ~ wt, data = mtcars, family = stats::binomial())\nsummary(modglm_2)\n\n\nCall:\nglm(formula = vs ~ wt, family = stats::binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   5.7147     2.3014   2.483  0.01302 * \nwt           -1.9105     0.7279  -2.625  0.00867 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.860  on 31  degrees of freedom\nResidual deviance: 31.367  on 30  degrees of freedom\nAIC: 35.367\n\nNumber of Fisher Scoring iterations: 5\n\n\nExercice. Analyser les résultats.\n\n7.8.1.9 Les modèles non-linéaires\nLa hauteur d’un arbre en fonction du temps n’est typiquement pas linéaire. Elle tend à croître de plus en plus lentement jusqu’à un plateau. De même, le rendement d’une culture traitée avec des doses croissantes de fertilisants tend à atteindre un maximum, puis à se stabiliser.\nCes phénomènes ne peuvent pas être approximés par des modèles linéaires. Examinons les données du tableau engelstad.nitro.\n\nengelstad.nitro |&gt; slice_sample(n = 10)\n\n         loc year nitro yield\n1  Knoxville 1966     0  63.0\n2  Knoxville 1965   335  61.2\n3    Jackson 1965   335  73.0\n4    Jackson 1966   201  61.3\n5    Jackson 1966   335  59.8\n6  Knoxville 1964     0  60.9\n7  Knoxville 1964    67  75.9\n8    Jackson 1966    67  45.2\n9    Jackson 1962   201  73.1\n10   Jackson 1964   335  67.8\n\n\n\nengelstad.nitro |&gt; \n    ggplot(aes(x = nitro, y = yield)) +\n        facet_grid(year ~ loc) +\n        geom_line() +\n        geom_point()\n\n\n\n\nLe modèle de Mitscherlich pourrait être utilisé.\n\\[ y = A \\left( 1 - e^{-R \\left( E + x \\right)} \\right) \\]\noù \\(y\\) est le rendement, \\(x\\) est la dose, \\(A\\) est l’asymptote vers laquelle la courbe converge à dose croissante, \\(E\\) est l’équivalent de dose fourni par l’environnement et \\(R\\) est le taux de réponse.\nExplorons la fonction.\n\nmitscherlich_f &lt;- function(x, A, E, R) {\n    A * (1 - exp(-R*(E + x)))\n}\n\nx &lt;- seq(0, 350, by = 5)\ny &lt;- mitscherlich_f(x, A = 75, E = 30, R = 0.02)\n\nggplot(tibble(x, y), aes(x, y)) +\n    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +\n    geom_line() + ylim(c(0, 100))\n\n\n\n\nExercice. Changez les paramètres pour visualiser comment la courbe réagit.\nNous pouvons décrire le modèle grâce à l’interface formule dans la fonction nls(). Notez que les modèles non-linéaires demandent des stratégies de calcul différentes de celles des modèles linéaires. En tout temps, nous devons identifier des valeurs de départ raisonnables pour les paramètres dans l’argument start. Vous réussirez rarement à obtenir une convergence du premier coup avec vos paramètres de départ. Le défi est d’en trouver qui permettront au modèle de converger. Parfois, le modèle ne convergera jamais. D’autres fois, il convergera vers des solutions différentes selon les variables de départ choisies.\nmodnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))),\n                data = engelstad.nitro,\n                start = list(A = 50, E = 10, R = 0.2))\nLe modèle ne converge pas (le bloc de calcul est désactivé). Essayons les valeurs prises plus haut, lors de la création du graphique, qui semblent bien s’ajuster.\n\nmodnl_1 &lt;-  nls(yield ~ A * (1 - exp(-R*(E + nitro))),\n                data = engelstad.nitro,\n                start = list(A = 75, E = 30, R = 0.02))\n\nBingo! Voyons maintenant le sommaire.\n\nsummary(modnl_1)\n\n\nFormula: yield ~ A * (1 - exp(-R * (E + nitro)))\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nA 75.023427   3.331860  22.517   &lt;2e-16 ***\nE 66.164111  27.251591   2.428   0.0184 *  \nR  0.012565   0.004881   2.574   0.0127 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.34 on 57 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 8.058e-06\n\n\nLes paramètres sont différents de zéro, et donnent la courbe suivante.\n\nx &lt;- seq(0, 350, by = 5)\ny &lt;- mitscherlich_f(x,\n                    A = coefficients(modnl_1)[1],\n                    E = coefficients(modnl_1)[2],\n                    R = coefficients(modnl_1)[3])\n\nggplot(tibble(x, y), aes(x, y)) +\n    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +\n    geom_line() + ylim(c(0, 100))\n\n\n\n\nEt les résidus…\n\ntibble(res = residuals(modnl_1)) |&gt; \n    ggplot(aes(x = res)) + geom_histogram(bins = 20)\n\n\n\n\n\ntibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) |&gt; \n    ggplot(aes(x = nitro, y = res)) +\n        geom_point() +\n        geom_hline(yintercept = 0, colour = \"red\")\n\n\n\n\nLes résidus ne sont pas distribués normalement, mais semblent bien partagés de part et d’autre de la courbe.\n\n7.8.2 Modèles à effets mixtes\nLorsque l’on combine des variables fixes (testées lors de l’expérience) et des variables aléatoire (variation des unités expérimentales), on obtient un modèle mixte. Les modèles mixtes peuvent être univariés, multivariés, linéaires (ordinaires ou généralisés) ou non linéaires.\nÀ la différence d’un effet fixe, un effet aléatoire sera toujours distribué normalement avec une moyenne de 0 et une certaine variance. Dans un modèle linéaire où l’effet aléatoire est un décalage d’intercept, cet effet s’additionne aux effets fixes:\n\\[ y = X \\beta + Z b + \\epsilon \\]\noù:\n\\(Z\\) est la matrice du modèle à \\(n\\) observations et \\(p\\) variables aléatoires. Les variables aléatoires sont souvent des variables nominales qui subissent un encodage catégoriel.\n\\[ Z = \\left( \\begin{matrix}\nz_{11} & \\cdots & z_{1p}  \\\\\nz_{21} & \\cdots & z_{2p}  \\\\\n\\vdots & \\ddots & \\vdots  \\\\\nz_{n1} & \\cdots & z_{np}\n\\end{matrix} \\right) \\]\n\\(b\\) est la matrice des \\(p\\) coefficients aléatoires.\n\\[ b = \\left( \\begin{matrix}\nb_0  \\\\\nb_1  \\\\\n\\vdots \\\\\nb_p\n\\end{matrix} \\right) \\]\nLe tableau lasrosas.corn, utilisé précédemment, contenait trois répétitions effectuées au cours de deux années, 1999 et 2001. Étant donné que la répétition R1 de 1999 n’a rien à voir avec la répétition R1 de 2001, on dit qu’elle est emboîtée dans l’année.\nLe module nlme nous aidera à monter notre modèle mixte.\n\nlibrary(\"nlme\")\n\n\nAttachement du package : 'nlme'\n\n\nL'objet suivant est masqué depuis 'package:dplyr':\n\n    collapse\n\nmmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv,\n                 random = ~ 1|year/rep,\n                 data = lasrosas.corn,\n                 control = lmeControl(opt = \"optim\"))\n\nÀ ce stade vous devriez commencer à être familier avec l’interface formule et vous devriez saisir l’argument fixed, qui désigne l’effet fixe. L’effet aléatoire est random, suit un tilde ~. À gauche de la barre verticale |, on place les variables désignant les effets aléatoire sur la pente. Nous n’avons pas couvert cet aspect, alors nous le laissons à 1. À droite, on retrouve un structure d’emboîtement désignant l’effet aléatoire: le premier niveau est l’année, dans laquelle est emboîtée la répétition.\n\nsummary(mmodlin_1)\n\nLinear mixed-effects model fit by REML\n  Data: lasrosas.corn \n       AIC      BIC    logLik\n  26535.37 26602.93 -13256.69\n\nRandom effects:\n Formula: ~1 | year\n        (Intercept)\nStdDev:    20.35426\n\n Formula: ~1 | rep %in% year\n        (Intercept) Residual\nStdDev:    11.17447 11.35617\n\nFixed effects:  yield ~ lat + long + nitro + topo + bv \n                 Value Std.Error   DF    t-value p-value\n(Intercept) -1379436.9  55894.55 3430 -24.679273   0.000\nlat           -25453.0   1016.53 3430 -25.039084   0.000\nlong           -8432.3    466.05 3430 -18.092988   0.000\nnitro              0.0      0.00 3430   1.739757   0.082\ntopoHT           -27.7      0.92 3430 -30.122438   0.000\ntopoLO             6.8      0.88 3430   7.804733   0.000\ntopoW            -16.7      1.40 3430 -11.944793   0.000\nbv                -0.5      0.03 3430 -19.242424   0.000\n Correlation: \n       (Intr) lat    long   nitro  topoHT topoLO topoW \nlat     0.897                                          \nlong    0.866  0.555                                   \nnitro   0.366  0.391  0.247                            \ntopoHT  0.300 -0.017  0.582  0.024                     \ntopoLO -0.334 -0.006 -0.621 -0.038 -0.358              \ntopoW   0.403 -0.004  0.762  0.027  0.802 -0.545       \nbv     -0.121 -0.012 -0.214 -0.023 -0.467  0.346 -0.266\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-4.32360269 -0.66781575 -0.07450856  0.61587533  3.96434001 \n\nNumber of Observations: 3443\nNumber of Groups: \n         year rep %in% year \n            2             6 \n\n\nLa sortie est semblable à celle de la fonction lm().\n\n7.8.2.1 Modèles mixtes non-linéaires\nLe modèle non linéaire créé plus haut liait le rendement à la dose d’azote. Toutefois, les unités expérimentales (le site loc et l’année year) n’étaient pas pris en considération. Nous allons maintenant les considérer.\nNous devons décider la structure de l’effet aléatoire, et sur quelles variables il doit être appliqué - la décision appartient à l’analyste. Il me semble plus convenable de supposer que le site et l’année affectera le rendement maximum plutôt que l’environnement et le taux: les effets aléatoires seront donc affectés à la variable A. Les effets aléatoires n’ont pas de structure d’emboîtement. L’effet de l’année sur A sera celui d’une pente et l’effet de site sera celui de l’intercept. La fonction que nous utiliserons est nlme().\n\nmm &lt;- nlme(yield ~ A * (1 - exp(-R*(E + nitro))),\n           data = engelstad.nitro,\n           start = c(A = 75, E = 30, R = 0.02),\n           fixed = list(A ~ 1, E ~ 1, R ~ 1),\n           random = A ~ year | loc)\nsummary(mm)\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: yield ~ A * (1 - exp(-R * (E + nitro))) \n  Data: engelstad.nitro \n       AIC     BIC    logLik\n  477.2286 491.889 -231.6143\n\nRandom effects:\n Formula: A ~ year | loc\n Structure: General positive-definite, Log-Cholesky parametrization\n              StdDev       Corr  \nA.(Intercept)  2.608588499 A.(In)\nA.year         0.003066584 -0.556\nResidual      11.152757993       \n\nFixed effects:  list(A ~ 1, E ~ 1, R ~ 1) \n                 Value Std.Error DF   t-value p-value\nA.(Intercept) 74.58222  4.722715 56 15.792235  0.0000\nE             65.56721 25.533994 56  2.567840  0.0129\nR              0.01308  0.004808 56  2.720215  0.0087\n Correlation: \n  A.(In) E     \nE  0.379       \nR -0.483 -0.934\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.83373140 -0.89293039  0.07418166  0.68353578  1.82434344 \n\nNumber of Observations: 60\nNumber of Groups: 2 \n\n\nEt sur graphique:\n\nengelstad.nitro |&gt; \n  ggplot(aes(x = nitro, y = yield)) +\n  facet_grid(year ~ loc) +\n  geom_line(data = tibble(nitro = engelstad.nitro$nitro,\n                          yield = predict(mm, level = 0)),\n            colour = \"grey35\") +\n  geom_point() +\n  ylim(c(0, 95))\n\n\n\n\nLes modèles mixtes non linéaires peuvent devenir très complexes lorsque les paramètres, par exemple A, E et R, sont eux-même affectés linéairement par des variables (par exemple A ~ topo). Pour aller plus loin, consultez Parent et al. (2017) ainsi que les calculs associés à l’article. Ou écrivez-moi un courriel pour en discuter!\nNote. L’interprétation de p-values sur les modèles mixtes est controversée. À ce sujet, Douglas Bates a écrit une longue lettre à la communauté de développement du module lme4, une alternative à nlme, qui remet en cause l’utilisation des p-values, ici. De plus en plus, pour les modèles mixtes, on se tourne vers les statistiques bayésiennes, couvertes dans le chapitre 8 avec le module greta. Mais en ce qui a trait aux modèles mixtes, le module brms automatise bien des aspects de l’approche bayésienne.\n\n7.8.3 Aller plus loin\n\n7.8.3.1 Statistiques générales:\n\nThe analysis of biological data\n\n7.8.3.2 Statistiques avec R\n\nDisponibles en version électronique à la bibliothèque de l’Université Laval:\n\nIntroduction aux statistiques avec R: Introductory statistics with R\n\nApprofondir les statistiques avec R: The R Book, Third edition\n\nApprofondir les modèles à effets mixtes avec R: Mixed Effects Models and Extensions in Ecology with R\n\n\n\n\nModernDive, un livre en ligne offrant une approche moderne avec le package moderndive."
  },
  {
    "objectID": "07b-bayes.html",
    "href": "07b-bayes.html",
    "title": "8  Introduction à l’analyse bayésienne en écologie",
    "section": "",
    "text": "Objectifs spécifiques:\nCe chapitre est un extra. Il ne fait pas partie des objectifs du cours. Il ne sera pas évalué.\nÀ la fin de ce chapitre, vous\n\nserez en mesure de définir ce que sont les statistiques bayésiennes\nserez en mesure de calculer des statistiques descriptives de base en mode bayésien avec le module greta.\n\n\nÀ venir!"
  },
  {
    "objectID": "08-explorer.html",
    "href": "08-explorer.html",
    "title": "9  Explorer R",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "09-ordination.html",
    "href": "09-ordination.html",
    "title": "10  Association, partitionnement et ordination",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "10-imputation.html",
    "href": "10-imputation.html",
    "title": "11  Détection de valeurs aberrantes et imputation de données manquantes",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "11-series-temporelles.html",
    "href": "11-series-temporelles.html",
    "title": "12  Les séries temporelles",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "12-autoapprentissage.html",
    "href": "12-autoapprentissage.html",
    "title": "13  Introduction à l’autoapprentissage",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "13-donnees-spatiales.html",
    "href": "13-donnees-spatiales.html",
    "title": "14  Les données géospatiales",
    "section": "",
    "text": "À venir"
  },
  {
    "objectID": "14-modelisation.html",
    "href": "14-modelisation.html",
    "title": "15  Modélisation de mécanismes écologiques",
    "section": "",
    "text": "À venir"
  }
]