# Introduction √† l'autoapprentissage {#sec-ia}

---

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- saurez √©tablir un plan de mod√©lisation par autoapprentissage
- saurez d√©finir le sous-apprentissage et le surapprentissage
- serez en mesure d'effectuer un autoapprentissage avec les techniques des *k*-proches voisins, les arbres de d√©cision, les for√™ts al√©atoires, les r√©seaux neuronaux et les processus gaussiens

***

Plusieurs cas d'esp√®ces en sciences et g√©nies peuvent √™tre approch√©s en liant un variable avec une ou plusieurs autres √† l'aide de r√©gressions lin√©aires, polynomiales, sinuso√Ødales, exponentielle, sigmo√Ødales, [etc](https://dl.sciencesocieties.org/publications/aj/pdfs/107/2/786). Encore faut-il s'assurer que ces formes pr√©√©tablies repr√©sentent le ph√©nom√®ne de mani√®re fiable.

Lorsque la forme de la r√©ponse est difficile √† envisager, en particulier dans des cas non-lin√©aires ou impliquant plusieurs variables, on pourra faire appel √† des mod√®les dont la structure n'est pas contr√¥l√©e par une √©quation rigide gouvern√©e par des param√®tres (comme la pente ou l'intercept).

L'**autoapprentissage**, apprentissage automatique, ou *machine learning*, vise √† d√©tecter des structures complexes √©mergeant d'ensembles de donn√©es √† l'aide des math√©matiques et de processus automatis√©s afin de pr√©dire l'√©mergence de futures occurrences. Comme ensemble de techniques empiriques, l'autoapprentissage est un cas particulier de l'**intelligence artificielle**, qui elle inclut aussi les m√©canismes d√©terministes et des ensembles d'op√©rations logiques. Par exemple, les premiers ordinateurs √† comp√©titionner aux √©checs se basaient sur des r√®gles de logique (si la reine noire est positionn√©e en c3 et qu'un le fou blanc est en position f6 et que ... alors bouge la tour en g5 - j'√©cris n'importe quoi). Un jeu simple d'intelligence artificielle consiste √† lancer une marche al√©atoire, par exemple bouger √† chaque pas d'une distance au hasard en x et y, puis de recalculer le pas s'il arrive dans une bo√Æte d√©finie (figure @fig-ml_random_walk). Dans les deux cas, il s'agit d'intelligence artificielle, mais pas d'autoapprentissage. 

```{r}
#| label: fig-ml_random_walk
#| out-width: 100%
#| fig-align: center
#| fig-cap: "Petite tortue, n'entre pas dans la bo√Æte!"
#| echo: false

set.seed(68538)
n_step <- 100
y <- x <- vector(length = n_step)
y[1] <- x[1] <- 0

box <- c(xg = 0, xd = 4, yb = 0, yh = 2)

for (i in 2:n_step) {
  x[i] <- x[i-1] + runif(1, -1, 1)
  y[i] <- y[i-1] + runif(1, -1, 1)
  while (x[i] > box[1] & x[i] < box[2] & y[i] > box[3] & y[i] < box[4]) {
    x[i] <- x[i-1] + runif(1, -1, 1)
    y[i] <- y[i-1] + runif(1, -1, 1)
  }
}

plot(x, y, type = 'l', col = rgb(0, 0, 1, 0.5), asp = 1)
for (i in 1:(n_step-1)) {
  arrows(x0 = x[i], y0 = y[i],
         x1 = x[i+1], y1 = y[i+1], 
         col = rgb(i/n_step, 0, 1-i/n_step),
         length = 0.08)
}
polygon(x = c(box[1], box[2], box[2], box[1]),
        y = c(box[3], box[3], box[4], box[4]),
        col = rgb(1, 0, 0, 0.25))
points(x, y,  pch = "üê¢")

```

L'autoapprentissage passera davantage par la simulation de nombreuses parties et d√©gagera la structure optimale pour l'emporter consid√©rant les positions des pi√®ces sur l'√©chiquier.

## Lexique

L'autoapprentissage poss√®de son jargon particulier. Puisque certains termes peuvent porter √† confusion, voici quelques d√©finitions de termes que j'utiliserai dans ce chapitre.

- **R√©ponse**. La variable que l'on cherche √† obtenir. Il peut s'agir d'une variable continue comme d'une variable cat√©gorielle. On la nomme aussi la *cible*.
- **Pr√©dicteur**. Une variable utilis√©e pour pr√©dire une r√©ponse. Les pr√©dicteurs sont des variables continues. Les pr√©dicteurs de type cat√©goriel doivent pr√©alablement √™tre dummifi√©s (voir chapitre 5). On nomme les pr√©dicteurs les *entr√©es*.
- **Apprentissage supervis√©** et **non-supervis√©**. Si vous avez suivi le cours jusqu'ici, vous avez d√©j√† utilis√© des outils entrant dans la grande famille de l'apprentissage automatique. La r√©gression lin√©aire, par exemple, vise √† minimiser l'erreur sur la r√©ponse en optimisant les coefficients de pente et l'intercept. Un apprentissage supervis√© a une cible, comme c'est le cas de la r√©gression lin√©aire. En revanche, un apprentissage non supervis√© n'en a pas: on laisse l'algorithme le soin de d√©tecter des structures int√©ressantes. Nous avons d√©j√† utilis√© cette approche. Pensez-y un peu... l'analyse en composante principale ou en coordonn√©es principales, ainsi que le partitionnement hi√©rarchique ou non, couverts au chapitre @sec-ord, sont des exemples d'apprentissage non supervis√©. En revanche, l'analyse de redondance a une r√©ponse. L'analyse discriminante aussi, bien que sa r√©ponse soit cat√©gorielle. L'apprentissage non supervis√© ayant d√©j√† √©t√© couvert (sans le nommer) au chapitre @sec-ord, ce chapitre ne s'int√©resse qu'√† l'apprentissage supervis√©.
- **R√©gression** et **Classification**. Alors que la r√©gression est un type d'apprentissage automatique pour les r√©ponses continues, la classification vise √† pr√©dire une r√©ponse cat√©gorielle. Il existe des algorithmes uniquement application √† la r√©gression, uniquement applicables √† la classification, et plusieurs autres adaptable aux deux situations.
- **Donn√©es d'entra√Ænement** et **donn√©es de test**. Lorsque l'on g√©n√®re un mod√®le, on d√©sire qu'il sache comment r√©agir √† ses pr√©dicteurs. Cela se fait avec des donn√©es d'entra√Ænement, sur lesquelles on **calibre** et **valide** le mod√®le. Les donn√©es de test servent √† v√©rifier si le mod√®le est en mesure de pr√©dire des r√©ponses sur lesquelles il n'a pas √©t√© entra√Æn√©.
- **Fonction de perte**. Une fonction qui mesure l'erreur d'un mod√®le.

## D√©marche

La premi√®re t√¢che est d'explorer les donn√©es, ce que nous avons couvert au chapitres \@ref(chapitre-tableaux) et @sec-visual.

### Pr√©traitement

Pour la plupart des techniques d'autoapprentissage, le choix de l'√©chelle de mesure est d√©terminant sur la mod√©lisation subs√©quente. Par exemple, un algorithme bas√© sur la distance comme les *k* plus proches voisins ne mesurera pas les m√™mes distances entre deux observations si l'on change l'unit√© de mesure d'une variable du m√®tre au kilom√®tre. Il est donc important d'effectuer, ou d'envisager la possibilit√© d'effectuer un pr√©traitement sur les donn√©es. Je vous r√©f√®re au chapitre @sec-expl pour plus de d√©tails sur le pr√©traitement.

### Entra√Ænement et test

Vous connaissez peut-√™tre l'expression sportive "avoir l'avantage du terrain". Il s'agit d'un principe pr√©tendant que les athl√®tes performent mieux en terrain connu. Idem pour les mod√®les ph√©nom√©nologiques. Il est possible qu'un mod√®le fonctionne tr√®s bien sur les donn√©es avec lesquelles il a √©t√© entra√Æn√©, mais tr√®s mal sur des donn√©es externes. De mauvaises pr√©dictions effectu√©es √† partir d'un mod√®le qui semblait bien se comporter peut mener √† des d√©cisions qui, pourtant prises de mani√®re confiante, se r√©v√®lent fallacieuses au point d'aboutir √† de graves cons√©quences. C'est pourquoi, **en mode pr√©dictif, on doit √©valuer la pr√©cision et l'exactitude d'un mod√®le sur des donn√©es qui n'ont pas √©t√© utilis√©s dans son entra√Ænement**.

En pratique, il convient de s√©parer un tableau de donn√©es en deux: un tableau d‚Äôentra√Ænement et un tableau de test. Il n'existe pas de standards sur la proportion √† utiliser dans l'un et l'autre. Cela d√©pend de la prudence de l'analyse et de l'ampleur de son tableau de donn√©es. Dans certains cas, nous pr√©f√©rerons couper le tableau √† 50%. Dans d'autres, nous pr√©f√©rerons r√©server le deux-tiers des donn√©es pour l'entra√Ænement, ou 70%, 75%. Rarement, toutefois, r√©servera-t-on moins plus de 50% et moins de 20% √† la phase de test.

Si les donn√©es sont peu √©quilibr√©es (par exemple, on retrouve peu de donn√©es de l'esp√®ce $A$, que l'on retrouve peu de donn√©es √† un pH inf√©rieur √† 5 ou que l'on a peu de donn√©es crois√©es de l'esp√®ce $A$ √† pH inf√©rieur √† 5), il y a un danger qu'une trop grande part, voire toute les donn√©es, se retrouvent dans le tableau d'entra√Ænement (certaines situations ne seront ainsi pas test√©es) ou dans le tableau de test (certaines situations ne seront pas couvertes par le mod√®le). L'analyste doit s'assurer de s√©parer le tableau au hasard, mais de mani√®re consciencieuse.

### Sousapprentissage et surapprentissage

Une difficult√© en mod√©lisation ph√©nom√©nologique est de discerner ce qui tient de la structure de ce qui tient du bruit. Lorsque l'on consid√®re une structure comme du bruit, on est dans un cas de sousapprentissage. Lorsque, au contraire, on interpr√®te du bruit comme une structure, on est en cas de surapprentissage. Les graphiques de la figure @fig-mesapprentissage pr√©sentent ces deux cas, avec au centre un cas d'apprentissage conforme.

```{r}
#| label: fig-mesapprentissage
#| fig-align: center
#| fig-cap: "Cas de figure de m√©sapprentissage. √Ä gauche, sous-apprentissage. Au centre, apprentissage valide. √Ä droite, surapprentissage."
#| echo: false

set.seed(35473)
n <- 50
x <- seq(0, 20, length = n) 
y <- 500 + 0.4 * (x-10)^3 + rnorm(n, mean=10, sd=80) # le bruit est g√©n√©r√© par rnorm()

par(mfrow = c(1, 3))
plot(x, y, main = "Sousapprentissage", col = "#46c19a", pch=16)
lines(x, predict(lm(y~x)), col = "#b94a73")

plot(x, y, main = "Apprentissage conforme", col = "#46c19a", pch=16)
lines(x, 
      predict(lm(y~x + I(x^2) + I(x^3))),
      col = "#b94a73")

plot(x, y, main = "Surapprentissage", col = "#46c19a", pch=16)
lines(x, 
      predict(lm(y~x + I(x^2) + I(x^3) + I(x^4) + 
                   I(x^5) + I(x^6) + I(x^7) + I(x^8) +
                   I(x^9) + I(x^10) + I(x^11) + I(x^12) +
                   I(x^13) + I(x^14) + I(x^15) + I(x^16))),
      col = "#b94a73")

```

Il est n√©anmoins difficile d'inspecter un mod√®le comprenant plusieurs entr√©es. On d√©tectera le m√©sapprentissage lorsque la pr√©cision d'un mod√®le est lourdement alt√©r√©e en phase de test. Une mani√®re de limiter le *m√©sapprentissage* est d'avoir recours √† la validation crois√©e.

### Validation crois√©e

Souvent confondue avec le fait de s√©parer le tableau en phases d'entra√Ænement et de test, la validation crois√©e est un principe incluant plusieurs algorithmes qui consistent √† entra√Æner le mod√®le sur un √©chantillonnage al√©atoire des donn√©es d'entra√Ænement. La technique la plus utilis√©e est le *k-fold*, o√π l'on s√©pare al√©atoirement le tableau d'entra√Ænement en un nombre *k* de tableaux. √Ä chaque √©tape de la validation crois√©e, on calibre le mod√®le sur tous les tableaux sauf un, puis on valide le mod√®le sur le tableau exclu. La performance du mod√®le en entra√Ænement est jug√©e sur les validations.

### Choix de l'algorithme d'apprentissage

Face aux [centaines d‚Äôalgorithmes d'apprentissages qui vous sont offertes](https://topepo.github.io/caret/available-models.html), choisir l'algorithme (ou les algorithmes) ad√©quats pour votre probl√®me n'est pas une t√¢che facile. Ce choix sera motiv√© par les tenants et aboutissants des algorithmes, votre exp√©rience, l'exp√©rience de la litt√©rature, l'exp√©rience de vos coll√®gues, etc. √Ä moins d'√™tre particuli√®rement surdou√©.e, il vous sera pratiquement impossible de ma√Ætriser la math√©matique de chacun d'eux. Une approche raisonnable est de tester plusieurs mod√®les, de retenir les mod√®les qui semblent les plus pertinents, et d'approfondir si ce n'est d√©j√† fait la math√©matique des options retenues. Ajoutons qu'il existe des algorithmes g√©n√©tiques, qui ne sont pas couverts ici, qui permettent de s√©lectionner des mod√®les d'autoapprentissage optimaux. Un de ces algorithmes est offert par le module Python [`tpot`](https://epistasislab.github.io/tpot/).

### D√©ploiement

Nous ne couvrirons pas la phase de d√©ploiement d'un mod√®le. Notons seulement qu'il est possible, en R, d'exporter un mod√®le dans un fichier `.Rdata`, qui pourra √™tre charg√© dans un autre environnement R. Cet environnement peut √™tre une feuille de calcul comme une interface visuelle mont√©e, par exemple, avec [Shiny](https://shiny.posit.co/) ([chapitre @sec-expl]).

----

En r√©sum√©,

1. Explorer les donn√©es
1. S√©lectionner des algorithmes
1. Effectuer un pr√©traitement
1. Cr√©er un ensemble d'entra√Ænement et un ensemble de test
1. Lisser les donn√©es sur les donn√©es d'entra√Ænement avec validation crois√©e
1. Tester le mod√®le
1. D√©ployer le mod√®le

## L'autoapprentissage en R

Plusieurs options sont disponibles.

1. Les modules que l'on retrouve en R pour l'autoapprentissage sont nombreux, et parfois sp√©cialis√©s. Il est possible de les utiliser individuellement.
1. Chacun de ces modules fonctionne √† sa fa√ßon. Le module **`caret`** de R a √©t√© con√ßu pour donner acc√®s √† des centaines de fonctions d'autoapprentissage via une interface commune. **`caret`** est tr√®s efficace, mais prend de l'√¢ge. Une refonte compl√®te, nomm√©e [**`parsnip`**](https://tidymodels.github.io/parsnip/) (panais en fran√ßais) est en cours sous l'ombrelle du m√©ta-module de mod√©lisation [**`tidymodels`**](https://github.com/tidymodels/tidymodels), mais n'est √† ce jour pas encore aboutie.
1. Le module **`mlr3`**, connu ancienment comme **`mlr`** (**M**achine **L**earning in **R**), occupe sensiblement le m√™me cr√©neau que **`caret`**, mais utilise plut√¥t une approche par objets connect√©s. Le livre ¬´ [Applied machine learning using mlr3 in R](https://mlr3book.mlr-org.com/) ¬ª disponible sur internet, offre une introduction √† ce module avec des exemples pratiques.
1. En Python, le module **`scikit-learn`** offre un interface unique pour l'utilisation de nombreuses techniques d'autoapprentissage. Il est possible d'appeler des fonctions de Python √† partir de R gr√¢ce au module **`reticulate`**.

Dans ce chapitre, nous verrons comment fonctionnent certains algorithmes s√©lectionn√©s, puis nous les appliquerons avec le module respectif qui m'a sembl√© le plus appropri√©. Nous utiliserons **`caret`** ainsi que quelques outils de **`tidymodels`**, dont les recettes pour le pr√©traitement (module **`recipes`**).

```{r}
#| label: ml-loadpackages

library("tidyverse") # √©videmment
library("tidymodels")
library("caret")
```

## Algorithmes

Il existe des centaines d'algorithmes d'autoapprentissage. Je n'en couvrirai que quatre, qui me semblent √™tre appropri√©s pour la mod√©lisation ph√©nom√©nologique en agro√©cologie, et utilisables autant pour la r√©gression et la classification.

- Les *k* plus proches voisins 
- Les arbres de d√©cision
- Les r√©seaux neuronaux
- Les processus gaussiens

### Les *k* plus proches voisins

```{r}
#| label: fig-ml_les-voisons
#| fig-align: center
#| fig-cap: "<< Le... l'id√©e en arri√®re pour √™tre... euh... simpliste, l√† c'est que c'est un peu de... euhmm... de la vitamine de vinyle.>> - Georges ([Les voisins](https://youtu.be/-RpYi_Vuviw?t=6m40s), une pi√®ce de Claude Meunier)."
#| echo: false

knitr::include_graphics("images/11_les-voisins.jpg")
```

Pour dire comme Georges, le... l'id√©e en arri√®re des KNN pour √™tre... euh... *simpliste*, c'est qu'un objet va ressembler √† ce qui se trouve dans son voisinage. Les KNN se basent en effet sur une m√©trique de distance pour rechercher un nombre *k* de points situ√©s √† proximit√© de la mesure. Les *k* points les plus proches sont retenus, *k* √©tant un entier non nul √† optimiser. Un autre param√®tre parfois utilis√© est la distance maximale des voisins √† consid√©rer: un voisin trop √©loign√© pourra √™tre discart√©. La r√©ponse attribu√©e √† la mesure est calcul√©e √† partir de la r√©ponse des *k* voisins retenus. Dans le cas d'une r√©gression, on utiliser g√©n√©ralement la moyenne. Dans le cas de la classification, la mesure prendra la cat√©gorie qui sera la plus pr√©sente chez les *k* plus proches voisins.

L'algorithme des *k* plus proches voisins est relativement simple √† comprendre. Certains pi√®ges sont, de m√™me, peuvent √™tre contourn√©s facilement. Imaginez que vous rechercher les points les plus rapproch√©s dans un syst√®me de coordonn√©es g√©ographiques o√π les coordonn√©es $x$ sont exprim√©es en m√®tres et les coordonn√©es $y$, en centim√®tres. Vous y projetez trois points (figure @fig-ml_knn1).

```{r}
#| label: fig-ml_knn1
#| fig-align: center
#| fig-cap: "Distances entre les points pour utilisation avec les KNN"
#| echo: false

data <- data.frame(X = c(0, 1, 0),
                   Y = c(0, 0, 1),
                   row.names = c('A', 'B', 'C'))
par(pty="s")
plot(data, cex=3,
     xlab = 'Position X (m)', ylab = 'Position Y (cm)')
text(data, labels = rownames(data))
```

Techniquement la distance A-B est 100 fois plus √©lev√©e que la distance A-C, mais l'algorithme ne se soucie pas de la m√©trique que vous utilisez (figure @fig-ml_knn1). Il est primordial dans ce cas d'utiliser la m√™me m√©trique. Cette strat√©gie est √©vidente lorsque les variables sont comparables. C'est rarement le cas, que ce soit lorsque l'on compare des dimensions physionomiques (la longueur d'une phalange ou celle d'un f√©mur) mais lorsque les variables incluent des m√©langes de longueurs, des pH, des d√©comptes, etc., il est important de bien identifier la m√©trique et le type de distance qu'il convient le mieux d'utiliser. En outre, la standardisation des donn√©es √† une moyenne de z√©ro et √† un √©cart-type de 1 est une approche couramment utilis√©e.

#### Exemple d'application

Pour ce premier exemple, je pr√©senterai un cheminement d'autoapprentissage, du pr√©traitement au test. Nous allons essayer de classer les esp√®ces de dragon selon leurs dimensions.

```{r}
#| label: fig-ml_dragons
#| out-width: 50%
#| fig-align: center
#| fig-cap: "Dimensions mesur√©s sur les dragons captur√©s."
#| echo: false

knitr::include_graphics("images/11_dragon.png")
```

```{r}
#| label: fig-ml_load-dragons
dragons <- read_csv("data/11_dragons.csv")
```

Assurons-nous que les donn√©es sont toutes √† l'√©chelle. Nous pourrions utiliser la fonction `scale()`. Toutefois, si je capture un nouveau dragon, je n'aurai pas l'information pour convertir mes nouvelles dimensions dans la m√™me m√©trique que celle utilis√©e pour lisser mon mod√®le. Prenez donc soin de conserver la moyenne et l'√©cart-type pour subs√©quemment calculer des mises √† l'√©chelle.

```{r}
#| label: ml_sc_dragons

dim_means <- dragons %>% 
  dplyr::select(starts_with("V")) %>% 
  summarise_all(mean, na.rm = TRUE)

dim_sds <- dragons %>% 
  dplyr::select(starts_with("V")) %>% 
  summarise_all(sd, na.rm = TRUE)

dragons_sc <- dragons %>% 
  dplyr::select(starts_with("V")) %>%
  scale(.) %>% 
  as_tibble() %>% 
  mutate(Species = dragons$Species)
```

S√©parons les donn√©es en entra√Ænement (`_tr`) et en test (`_te`) avec une proportion 70/30 (`p = 0.7`). Il est essentiel d'utiliser `set.seed()` pour s'assurer que la partition soit la m√™me √† chaque session de code (pour la reproductibilit√©) - j'ai l'habitude de taper n'importe quel num√©ro √† environ 6 chiffres, mais lors de publications, je vais sur [random.org](https://www.random.org/) et je g√©n√®re un num√©ro au hasard, sans biais.

```{r}
#| label: ml_dragons_tr_te_split

set.seed(68017)
id_tr <- createDataPartition(dragons_sc$Species, p = 0.7, list = FALSE)[, 1]
dragons_tr <- dragons_sc[id_tr, ]
dragons_te <- dragons_sc[-id_tr, ]
```

Avant de lancer nos calculs, allons vois sur la [page de `caret`](https://topepo.github.io/caret/available-models.html) les modules qui effectuent des KNN pour la classification. Nous trouvons **`knn`** et **`kknn`**. Si les modules n√©cessaires aux calculs ne sont pas install√©s sur votre ordinateur, **`caret`** vous demandera de les installer. Prenons le module **`kknn`**, qui demande le param√®tre `kmax`, soit le nombre de voisins √† consid√©rer, ainsi qu'un param√®tre de `distance` (sp√©cifiez 1 pour la distance de Mahattan et 2 pour la distance euclidienne), et un `kernel`, qui est une fonction pour mesurer la distance. Comment choisir les bons param√®tres? Une mani√®re de proc√©der est de cr√©er une grille de param√®tres.

```{r}
#| label: ml-dragons-grid

kknn_grid <- expand.grid(kmax = 3:6,
                         distance = 1:2,
                         kernel = c("rectangular", "gaussian", "optimal"))
```

Les noms des colonnes de la grille doivent correspondre aux noms des param√®tres du mod√®le. Nous allons mod√©liser avec une validation crois√©e √† 5 plis.

```{r}
#| label: ml-dragons-ctrl

ctrl <- trainControl(method="repeatedcv", repeats = 5)
```

Pour finalement lisser le mod√®le.

```{r}
#| label: ml-dragons-fit

set.seed(8961704)
clf <- train(Species ~ .,
             data = dragons_tr,
             method = "kknn",
             tuneGrid = kknn_grid,
             trainControl = ctrl)
clf
```

Nous obtenons les param√®tres du mod√®le optimal. Pr√©disons l'esp√®ce de dragons selon ses dimensions pour chacun des tableaux.

```{r}
#| label: ml-dragons-predict

pred_tr <- predict(clf)
pred_te <- predict(clf, newdata = dragons_te)
```

Une mani√®re d'√©valuer la pr√©diction est d'afficher un tableau de contingence.

```{r}
#| label: ml-dragons-diag-tr

table(dragons_tr$Species, pred_tr)
```

```{r}
#| label: ml-dragons-diag-te

table(dragons_te$Species, pred_te)
```

Les esp√®ces de dragon sont toutes bien class√©es tant entra√Ænement qu'en test (c'est rarement le cas dans les situations r√©elles).

### Les arbres d√©cisionnels

```{r}
#| label: ml-ents
#| out-width: 100%
#| fig-align: center
#| fig-cap: "Les Ents, tir√© du film le Seigneur des anneaux, qui prennent trop de temps avant de se d√©cider - paradoxalement, les abrbres de d√©cisions sont dot√©s d'algorithmes rapides."
#| echo: false

knitr::include_graphics("images/11_Entmoot.jpg")
```

Un arbre d√©cisionnel est une collection hi√©rarchis√©e de d√©cisions, le plus souvent binaires. Chaque embranchement est un test √† vrai ou faux sur une variable. La r√©ponse, que ce soit une cat√©gorie ou une valeur num√©rique, se trouve au bout de la derni√®re branche. Les suites de d√©cisions sont organis√©es de mani√®re √† ce que la pr√©cision de la r√©ponse soit optimis√©e. Ils ont l'avantage de pouvoir √™tre exprim√©s en un sch√©ma simple et imprimable.

```{r}
#| label: ml_jj_dt
#| out-width: 50%
#| fig-align: center
#| fig-cap: "Exemple d'arbre de d√©cision, tir√© du [blogue de Jeremy Jordon](https://www.jeremyjordan.me/decision-trees/)."
#| echo: false

knitr::include_graphics("https://www.jeremyjordan.me/content/images/2017/03/Screen-Shot-2017-03-11-at-10.15.37-PM.png")
```

Les arbres sont notamment param√©tr√©s par le nombre maximum d'embranchements, qui s'il est trop √©lev√© peut mener √† du surapprentissage. [Il existe de nombreux algorithmes d'arbres de d√©cision](https://topepo.github.io/caret/available-models.html).

Une collection d'arbres devient une for√™t. Les for√™ts al√©atoires (*random forest*) sont une cat√©gorie d'algorithmes compos√©s de plusieurs arbres de d√©cision optimis√©s sur des donn√©es r√©pliqu√©es al√©atoirement par *bagging*. Allons-y par √©tape. √Ä partir des donn√©es existantes compos√©es de *n* observations (donc *n* lignes) s√©lectionn√©es pour l'entra√Ænement, √©chantillonnons au hasard *avec remplacement* un nombre *n* de nouvelles observations. Le remplacement implique qu'on retrouvera fort probablement dans notre nouveau tableau des lignes identiques. Lissons un arbre sur notre tableau al√©atoire. Effectuons un nouveau tirage, puis un autre arbre. Puis encore, et encore, disons 10 fois. Nous obtiendrons une for√™t de 10 arbres. Pour une nouvelle observation √† pr√©dire, nous obtenons donc 10 pr√©dictions, sur lesquelles nous pouvons effectuer un moyenne s'il s'agit d'une variable num√©rique, ou bien prenons la cat√©gorie la plus souvent pr√©dite dans le cas d'une classification. Les for√™ts al√©atoires peuvent √™tre constitu√©s de 10, 100, 1000 arbres: autant qu'il en est n√©cessaire.

#### Exemple d'application

Utilisons toujours nos donn√©es de dimensions de dragons. Bien qu'il en existe plusieurs, le module conventionnel pour effectuer un arbre de d√©cision est **`rpart2`**. [Sur la page de **`caret`**](https://topepo.github.io/caret/available-models.html), nous trouvons **`rpart2`**, apte pour les classifications et les r√©gressions, [qui n'a besoin que du param√®tre `maxdepth`](https://topepo.github.io/caret/train-models-by-tag.html#random-forest).

```{r}
#| label: ml-tree-grid

rpart2_grid <- expand.grid(maxdepth = 3:10) # expand_grid n'est pas n√©cessaire ici
```

Prenons 5 plis encore une fois.

```{r}
#| label: nl-tree-ctrl

ctrl <- trainControl(method="repeatedcv", repeats = 5)
```

Pour finalement lisser le mod√®le.

```{r}
#| label: ml-tree-fit

set.seed(3468973)
clf <- train(Species ~ .,
             data = dragons_tr,
             method = "rpart2",
             tuneGrid = rpart2_grid)
clf
```

Nous obtenons les param√®tres du mod√®le optimal: `maxdepth = 3` - puisque c'est √† la limite inf√©rieure de la grille, mieux vaudrait √©tendre la grille, mais passons pour l'exemple. Comme je l'ai mentionn√©, un arbre de d√©cision est un outil convivial √† visualiser.

```{r}
#| label: ml-tree-plot

plot(clf$finalModel)
text(clf$finalModel)
```

Ou en plus beau, je vous laisse essayer.

```{r}
#| label: ml-tree-fancy
#| eval: false

library("rattle")
fancyRpartPlot(clf$finalModel)
```

Tout comme pour les KNN, pr√©disons l'esp√®ce de dragons selon ses dimensions pour chacun des tableaux.

```{r}
#| label: ml-tree-predict

pred_tr <- predict(clf)
pred_te <- predict(clf, newdata = dragons_te)
```

En ce qui a trait aux tableaux de contingence...

```{r}
#| label: ml-tree-diag-tr

table(dragons_tr$Species, pred_tr)
```

```{r}
#| label: ml-tree-diag-te

table(dragons_te$Species, pred_te)
```

Les esp√®ces de dragon sont toutes bien class√©es en entra√Ænement et en test... sauf pour les dragons de caverne, qui (l'avez-vous remarquez?) n'apparaissent pas dans l'arbre de d√©cision!

Le module **`caret`** vient avec la fonction `varImp()` qui offre une appr√©ciation de l'importance des variables dans le mod√®le final. La notion d'importance varie d'un mod√®le √† l'autre, et reste √† ce jour mal document√©. Mieux vaut en examiner les tenants et aboutissants avant d'interpr√©ter excessivement la sortie de cette fonction.

```{r}
#| label: ml-tree-varimp
varImp(clf) %>% plot(.)
```

On pourra effectuer de la m√™me mani√®re une for√™t al√©atoire, mais cette fois-ci avec le module **`rf`**.

```{r}
#| label: ml-tree-dragons-fit

set.seed(3468973)
ctrl <- trainControl(method="repeatedcv", repeats = 5)
clf <- train(Species ~ .,
             data = dragons_tr,
             method = "rf")
clf
```

Et les r√©sultats.

```{r}
#| label: ml-tree-dragons-pred

pred_tr <- predict(clf)
pred_te <- predict(clf, newdata = dragons_te)
table(dragons_te$Species, pred_te)
table(dragons_tr$Species, pred_tr)
```

Notez que les for√™ts al√©atoires ne g√©n√®re par de visuel.

### Les r√©seaux neuronaux

Apr√®s les KNN et les *random forests*, nous passons au domaine plus complexe des r√©seaux neuronaux. Le terme *r√©seau neuronal* est une m√©taphore li√©e √† une perception que l'on avait du fonctionnement du cerveau humain lorsque la technique des r√©seaux neuronaux a √©t√© d√©velopp√©e dans les ann√©es 1950. Un r√©seau neuronal comprend une s√©rie de bo√Ætes d'entr√©es li√©e √† des fonctions qui transforment et acheminent successivement l'information jusqu'√† la sortie d'une ou plusieurs r√©ponses. Il existe plusieurs formes de r√©seaux neuronnaux, dont la plus simple manifestation est le *perceptron multicouche*. Dans l'exemple de la figure @fig-ml_nn1, on retrouve 4 variables d'entr√©e et trois variables de sortie entre lesquelles on retrouve 5 couches dont le nombre de neurones varient entre 3 et 6.

```{r}
#| label: fig-ml_nn1
#| out-width: 50%
#| fig-align: center
#| fig-cap: "R√©seau neuronal sch√©matis√©, Source: [Neural designer](https://www.neuraldesigner.com/)."
#| echo: false

knitr::include_graphics("images/11_deep_neural_network.png")
```

Entre la premi√®re couche de neurones (les variables pr√©dictives) et la derni√®re couche (les variables r√©ponse), on retrouve des *couches cach√©es*. Chaque neurone est reli√© √† tous les neurones de la couche suivante.

Les liens sont des poids, qui peuvent prendre des valeurs dans l'ensemble des nombres r√©els. √Ä chaque neurone suivant la premi√®re couche, on fait la somme des poids multipli√©s par la sortie du neurone. Le nombre obtenu entre dans chaque neurone de la couche. Le neurone est une fonction, souvent tr√®s simple, qui transforme le nombre. La fonction plus utilis√©e est probablement la fonction ReLU, pour *rectified linear unit*, qui expulse le m√™me nombre aux neurones de la prochaine couche s'il est positif: sinon, il expulse un z√©ro.

**Exercice**. Si tous les neurones sont des fonctions ReLU, calculez la sortie de ce petit r√©seau neuronal.

<img src="images/11_nn_ex1_Q.jpg" width="600px">

Vous trouverez la r√©ponse sur l'image `images/11_nn_ex1_R.jpg`.

Il est aussi possible d'ajouter un *biais* √† chaque neurone, qui est un nombre r√©el additionn√© √† la somme des neurones pond√©r√©e par les poids.

L'optimisation les poids pour chaque lien et les biais pour chaque neurone (gr√¢ce √† des algorithmes dont le fonctionnement sort du cadre de ce cours) constitue le processus d'apprentissage. Avec l'aide de logiciels et de modules sp√©cialis√©s, la construction de r√©seaux de centaines de neurones organis√©s en centaines de couches vous permettra de capter des patrons complexes dans des ensembles de donn√©es.

Vous avez peut-√™tre d√©j√† entendu parler d'apprentissage profond (ou *deep learning*). Il s'agit simplement d'une appellation des r√©seaux neuronaux modernis√© pour insister sur la pr√©sence de nombreuses couches de neurones. C'est un terme √† la mode.

#### Les r√©seaux neuronaux sur R avec **`neuralnet`**

Plusieurs modules sont disponibles sur R pour l'apprentissage profond. Certains utilisent le module [H2O.ia](https://github.com/h2oai/h2o-3), propuls√© en Java, d'autres utilisent plut√¥t [Keras](https://keras.rstudio.com/), propuls√© en Python par l'interm√©diaire de [Tensorflow](https://www.tensorflow.org/). J'ai une pr√©f√©rence pour Keras, puisqu'il supporte les r√©seaux neuronaux classiques (perceptrons multicouche) autant que convolutifs ou r√©currents. Keras pourrait n√©anmoins √™tre difficile √† installer sur Windows, o√π Python ne vient pas par d√©faut. Sur Windows, Keras ne fonctionne qu'avec Anaconda: vous devez donc installez [Anaconda ou Miniconda](https://www.anaconda.com/download/#windows) (Miniconda offre une installation minimaliste).

Donc, pour ce cours, nous utiliserons le module **`neuralnet`**. Il est possible de l'utilser gr√¢ce √† l'interface de **`caret`**, mais son utilisation directe permet davantage de flexibilit√©. Chargeons les donn√©es d'iris.

```{r}
#| label: ml-load-iris

library("neuralnet")
data("iris")
```

Prenons soin de segmenter nos donn√©es en entra√Ænement et en test.

```{r}
#| label: ml-iris-tr-te-split

set.seed(8453668)
iris_tr_index <- createDataPartition(y=iris$Species, p = 0.75, list = FALSE)
```

Nous pouvons ainsi cr√©er nos tableaux d'entra√Ænement et de test pour les variables pr√©dictives.

Les r√©seaux neuronaux sont aptes √† g√©n√©rer des sorties multiples. Nous d√©sirons pr√©dire une cat√©gorie, et **`neuralnet`** ne s'occupe pas de les transformer de facto. Lors de la pr√©diction d'une cat√©gorie, nous devons g√©n√©rer des sorties multiples qui permettront de d√©cider de l'appartenance exclusive √† une cat√©gorie ou une autre. Nous avons abord√© l'encodage cat√©goriel aux chapitres [@sec-bios] et [@sec-expl]. C'est ce que nous ferons ici.

```{r}
#| label: ml-iris-preprocess

species_oh <- model.matrix(~ 0 + Species, iris)
colnames(species_oh) <- levels(iris$Species)
iris_oh <- iris %>% 
  cbind(species_oh)
iris_tr <- iris_oh[iris_tr_index, ]
iris_te <- iris_oh[-iris_tr_index, ]
```

Lan√ßons le r√©seau neuronal avec l'interface-formule de R (neuralnet n'accepte pas le `.` pour indiquer *prend toutes les variables √† l'exeption de celles utilis√©es en y*): nous allons les inclure √† la main.  L'argument `hidden` est un vecteur qui indique le nombre de neurones pour chaque couche. L'argument `linear.input` indique si l'on d√©sire travailler en r√©gression (`linear.output = TRUE`) ou en classification (`linear.output = FALSE`). Lorsque les donn√©es sont nombreuses, patience, le calcul prend pas mal de temps. Dans ce cas-ci, nous avons un tout petit tableau.

```{r}
#| label: ml-iris-fit

nn <- neuralnet(setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
                data = iris_tr, 
                hidden = c(5, 5),
                act.fct = "tanh",
                linear.output = FALSE)
```

Un r√©seau neuronal peu complexe peut √™tre lisible.

```{r}
#| label: ml-iris-nn-plot

plot(nn)
```

Il n'existe pas de r√®gle stricte sur le nombre de couche et le nombre de n≈ìuds par couche. Il est n√©anmoins conseill√© de g√©n√©rer d'abord un mod√®le simple, puis au besoin de le complexifier graduellement en terme de nombre de n≈ìuds, puis de nombre de couches. Si vous d√©sirez aller plus loin et utiliser keras, le module [`autokeras`](https://autokeras.com/), disponible seulement en Python, est con√ßu pour optimiser un mod√®le Keras.

La sortie du r√©seau neuronal est une valeur pr√®s de 1 ou une valeur pr√®s de 0. Voici une mani√®re de g√©n√©rer un vecteur cat√©goriel.

```{r}
#| label: ml-iris-predict

compute_te <- compute(nn, iris_te)
pred_te <- compute_te$net.result %>%
  as_tibble() %>% 
  apply(., 1, which.max) %>% 
  levels(iris$Species)[.] %>%
  as.factor()
```

La fonction `caret::confusionMatrix()` permet de g√©n√©rer les statistiques du mod√®le.

```{r}
#| label: ml-iris-predict-diag

confusionMatrix(iris_te$Species, pred_te)
```

Encore une fois, c'est rarement le cas mais nous obtenons une classification parfaite.

#### Pour aller plus loin

En une heure divis√©e en [4 vid√©os](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi), Grant Sanderson explique les r√©seaux neuronaux de mani√®re intuitive. En ce qui a trait √† Keras, je recommande le livre [Deep learning with R, de Fran√ßois Allaire](https://www.safaribooksonline.com/library/view/deep-learning-with/9781617295546/?ar), auquel vous avez acc√®s avec un IDUL de l'Universit√© Laval. Si vous vous sentez √† l'aise √† utiliser Keras avec le langage Python, je vous recommande le cours gratuit en ligne [*Applications of deep neural networks*, de Jeff Heaton](https://www.youtube.com/watch?v=sRy26qWejOI&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN).

Des types de r√©seaux neuronaux sp√©cialis√©s ont √©t√© d√©velopp√©s. Je les pr√©sente sans aller dans les d√©tails.

- **R√©seaux neuronaux convolutif**. Ce type de r√©seau neuronal est surtout utilis√© en reconnaissance d'image. Les couches de neurones convolutifs poss√®dent, en plus des fonctions des perceptrons classiques, des filtres permettant d'int√©grer les variables descriptives connexes √† l'observation: dans le cas d'une image, il s'agit de scanner les pixels au pourtour du pixel trait√©. [Une br√®ve introduction sur Youtube](https://www.youtube.com/watch?v=YRhxdVk_sIs).
- **R√©seaux neuronaux r√©currents**. Pr√©dire des occurrences futures √† partir de s√©ries temporelles implique que la r√©ponse au temps t d√©pend non seulement de conditions externes, mais aussi de la r√©ponse au temps t-1. Les r√©seaux neuronaux r√©currents. Vous devrez ajouter des neurones particuliers pour cette t√¢che, qui pourra √™tre pris en charge par Keras gr√¢ce aux couches de type [*Long Short-Term Memory network*, ou LSTM](https://www.youtube.com/watch?v=UnclHXZszpw).
- **R√©seaux neuronaux probabilistes**. Les r√©seaux neuronaux non-probabilistes offre une estimation de la variable r√©ponse. Mais quelle est la cr√©dibilit√© de la r√©ponse selon les variables descriptives? Question qui pourrait se r√©v√©ler cruciale en m√©decine ou en ing√©nierie, √† la laquelle on pourra r√©pondre en mode probabiliste. Pour ce faire, on pose des distributions *a priori* sur les poids du r√©seau neuronal. Le module [`edward`](http://edwardlib.org/), programm√© et distribu√© en Python, offre cette possibilit√©. Vous pourrez acc√©der √† `edward` gr√¢ce au module `reticulate`, mais √† ce stade mieux vaudra basculer en Python. Pour en savoir davantage, consid√©rez [cette conf√©rence de Andrew Rowan](https://www.youtube.com/watch?v=I09QVNrUS3Q).

### Les processus gaussiens

Les sorties des techniques que soit les KNN, les arbres ou les for√™ts ainsi que les r√©seaux neuronaux sont (classiquement) des nombres r√©els ou des cat√©gories. Dans les cas o√π la cr√©dibilit√© de la r√©ponse est importante, il devient pertinent que la sortie soit probabiliste: les pr√©dictions seront alors pr√©sent√©es sous forme de distributions de probabilit√©. Dans le cas d‚Äôune classification, la sortie du mod√®le sera un vecteur de probabilit√© qu‚Äôune observation appartienne √† une classe ou √† une autre. Dans celui d‚Äôune r√©gression, on obtiendra une distribution continue.

Les **processus gaussiens** tirent profit des statistiques bay√©siennes pour effectuer des pr√©dictions probabilistes. D‚Äôautres techniques peuvent √™tre utilis√©es pour effectuer des pr√©dictions probabilistes, comme les [r√©seaux neuronaux probabilistes](http://edwardlib.org/iclr2017), que j'ai introduits pr√©c√©demment.

Bien que les processus gaussiens peuvent √™tre utilis√©s pour la classification, son fonctionnement s'explique favorablement, de mani√®re intuitive, par la r√©gression.

#### Un approche intuitive

Ayant acquis de l'exp√©rience en enseignement des processus gaussiens, [John Cunningham](http://stat.columbia.edu/~cunningham/) a d√©velopp√© une approche intuitive permettant de saisir les m√©canismes des processus gaussiens. lors de conf√©rences disponible sur YouTube ([1](https://youtu.be/BS4Wd5rwNwE), [2](https://www.youtube.com/watch?v=Jv25sg-IYHU)), il aborde le sujet par la n√©cessit√© d'effectuer une r√©gression non-lin√©aire.

G√©n√©rons d'abord une variable pr√©dictive `x`, l'heure, et une variable r√©ponse `y`, le rythme cardiaque d'un individu en battements par minute (bpm).

```{r}
#| label: ml-gp-data

x <- c(7, 8, 10, 14, 17)
y <- c(61, 74, 69, 67, 78)

plot(x, y, xlab="Heure", ylab="Rythme cardiaque (bpm)")
abline(v=12, lty=3, col='gray50');text(12, 67, '?', cex=2)
abline(v=16, lty=3, col='gray50');text(16, 72, '?', cex=2)
```

Poser un probl√®me par un processus gaussien, c'est se demander les valeurs cr√©dibles qui pourraient √™tre obtenues hors du domaine d'observations (par exemple, dans la figure ci-dessus, √† `x=12` et `x=16`)? Ou bien, de mani√®re plus g√©n√©rale, *quelles fonctions ont pu g√©n√©rer les variables r√©ponse √† partir d'une structure dans les variables pr√©dictives?*

Les distributions normales, que nous appellerons *gaussiennes* dans cette section par concordance avec le terme *processus gaussien*, sont particuli√®rement utiles pour r√©pondre √† cette question.

Nous avons vu pr√©c√©demment ce que sont les distributions de probabilit√©: des outils math√©matiques permettant d'appr√©hender la structure des processus al√©atoires. Une distribution gaussienne repr√©sente une situation o√π l'on tire au hasard des valeurs continues. Une distribution gaussienne de la variable al√©atoire $X$ de moyenne $0$ et de variance de $1$ est not√©e ainsi:

$$ X \sim \mathcal{N} \left( 0, 1\right)$$

Par exemple, une courbe de distribution gaussienne du rythme cardiaque √† 7:00 pourrait prendre la forme suivante.

$$ bpm \sim \mathcal{N} \left( 65, 5\right)$$

En `R`:

```{r}
#| label: ml-gp-xseq

x_sequence <- seq(50, 80, length=100)
plot(x_sequence,
     dnorm(x_sequence, mean=65, sd=5),
     type="l",
     xlab="Rythme cardiaque (bpm)",
     ylab="Densit√©")
```

Une distribution **bi**normale, un cas particulier de la distribution **multi**normale, comprendra deux vecteurs, $x_1$ et $x_2$. Elle aura donc deux moyennes. Puisqu'il s'agit d'une distribution binormale, et non pas deux distributions normales, les deux variables ne sont pas ind√©pendantes et l'on utilisera une matrice de covariance au lieu de deux variances ind√©pendantes.

$$
\binom{x_1}{x_2} \sim \mathcal{N}
\Bigg( 
\binom{\mu_1}{\mu_2},
\left[ {\begin{array}{cc}
\Sigma_{x_1} & \Sigma_{x_1,x_2} \\
\Sigma_{x_1,x_2}^T & \Sigma_{x_2} \\
\end{array} } \right]
\Bigg)
$$

La matrice $\Sigma$, dite de *variance-covariance*, indique sur sa diagonale les variances des variables ($\Sigma_{x_1}$ et $\Sigma_{x_2}$). Les covariances $\Sigma_{x_1,x_2}$ et $\Sigma_{x_1,x_2}^T$ sont sym√©triques et indiquent le lien entre les variables.

On pourrait supposer que le rythme cardiaque √† 8:00 soit corr√©l√© avec celui √† 7:00. Mises ensembles, les distributions gaussiennes √† 7:00 et √† 8:00 formeraient une distribution gaussienne binormale.

$$
\binom{bpm_7}{bpm_8} \sim \mathcal{N}
\Bigg( 
\binom{65}{75},
\left[ {\begin{array}{cc}
10 & 6 \\
6 & 15 \\
\end{array} } \right]
\Bigg)
$$

En `R`:

```{r}
#| label: ml-gp-ellipse

library("ellipse")
means_vec <- c(65, 75)
covariance_mat <- matrix(c(10, 6, 6, 15), ncol=2)
par(pty='s')
plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), 
     type='l',
     xlab="Rythme cardiaque √† 7:00 (bpm)",
     ylab="Rythme cardiaque √† 8:00 (bpm)")
#lines(ellipse(x=covariance_mat, centre=means_vec, level=0.8))
```

On peut se poser la question: √©tant donn√©e que $x_1 = 68$, quelle serait la distribution de $x_2$? Dans ce cas bivari√©e, la distribution marginale serait univari√©e, mais dans le cas multivari√© en $D$ dimensions, la distribution marginale o√π l'on sp√©cifie $m$ variables serait de $D-m$. Une propri√©t√© fondamentale d'une distribution gaussienne est que peu importe l'endroit o√π l'angle selon lequel on la tranche, la distribution marginale sera aussi gaussienne. Lorsque l'on retranche une ou plusieurs variables en sp√©cifiant la valeur qu'elles prennent, on applique un *conditionnement* √† la distribution.

```{r}
#| label: ml-gp-multnorm
#| echo: false

library("condMVNorm")

condition_x1 <- 61 # changer ce chiffre pour visualiser l'effet

cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat,
                           dependent=2, given=1, X.given=condition_x1)
cond_mean <- cond_parameters$condMean
cond_sd <- sqrt(cond_parameters$condVar)
x2_sequence <- seq(50, 90, length=100)
x2_dens <- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd)

par(pty='s')
plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type='l',
     xlab="Rythme cardiaque √† 7:00 (bpm)",
     ylab="Rythme cardiaque √† 8:00 (bpm)")
abline(v=condition_x1, col='#f8ad00', lwd=2, lty=2)
lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col="#f8ad00", lwd=2)
lines(x = c(condition_x1, condition_x1),
      y = c(cond_mean-cond_sd, cond_mean+cond_sd),
      lwd=3, col='#46c19a')
points(condition_x1, cond_mean, 
       col='#46c19a', pch=16, cex=2)

n_sample <- 20
points(x = rep(condition_x1, n_sample),
       y = rnorm(n_sample, cond_mean, cond_sd),
       pch=4, col = rgb(0, 0, 0, 0.5))
```

Les points sur l'axe (symbole x) conditionn√©s sont des √©chantillons tir√©s au hasard dans la distribution conditionn√©e.

Une autre mani√®re de visualiser la distribution gaussienne binormale est de placer $x_1$ et $x_2$ c√¥te √† c√¥te en abscisse, avec leur valeur en ordonn√©e. Le bloc de code suivant peut sembler lourd au premier coup d‚Äô≈ìil: pas de panique, il s'agit surtout d'instructions graphiques. Vous pouvez vous amuser √† changer les param√®tres de la distribution binormale (section 1) ainsi que la valeur de $x_1$ √† laquelle est conditionn√©e la distribution de $x_2$ (section 2).

```{r}
#| label: ml-gp-distr1
#| echo: false

source("lib/plot_matrix.R")

# 1. Distribution
means_vec <- c(65, 65)
covariance_mat <- matrix(c(10, 6, 6, 15), ncol=2)

# 2. Condition
condition_x1 <- 61 # changer ce chiffre pour visualiser l'effet

# 3. Densit√© conditionn√©e
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat,
                           dependent=2, given=1, X.given=condition_x1)
cond_mean <- cond_parameters$condMean
cond_sd <- sqrt(cond_parameters$condVar)
x2_sequence <- seq(50, 90, length=100)
x2_dens <- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd)
x2_draw <- rnorm(1, cond_mean, cond_sd)

# 4. Graphiques
options(repr.plot.width = 8, repr.plot.height = 5)
layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2))
par(mar=c(4, 4, 1, 1), pty='s')

## 4.1 Ellipse
plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), 
     type='l', xlab="BPM √† 7:00", ylab="BPM √† 8:00")
abline(v=condition_x1, col='#f8ad00', lwd=1)
lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col="#f8ad00", lwd=1)
lines(x = c(condition_x1, condition_x1),
      y = c(cond_mean-cond_sd, cond_mean+cond_sd),
      lwd=2, col='#46c19a')
points(condition_x1, cond_mean, 
       col='#46c19a', pch=16, cex=1)
points(condition_x1, x2_draw, pch=16, col="#b94a73")

## 4.2 Covariance
plot_matrix(covariance_mat)

## 4.3 S√©rie
plot(c(1, 2), c(condition_x1, x2_draw), xlim=c(0, 6), ylim=c(55, 75), type='l',
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")
points(1, condition_x1, pch=16, col='#46c19a', cex=3)
points(2, x2_draw, pch=16, col='#b94a73', cex=3)
```

Les valeurs que peuvent prendre le rythme cardiaque en $x_2$ sont tir√©es al√©atoirement d'une distribution conditionn√©e. Sautons maintenant au cas multinormal, incluant 6 variables (*hexanormal*!). Afin d'√©viter de composer une matrice de covariance √† la mitaine, je me permets de la g√©n√©rer avec une fonction. Cette fonction particuli√®re est nomm√©e *fonction de base radiale* ou *exponentiel de la racine*.

$$K_{RBF} \left( x_i, x_j \right) = \sigma^2 exp \left( -\frac{\left( x_i - x_j \right)^2}{2 l^2}  \right) $$

```{r}
#| label: ml-gp-rbf

RBF_kernel <- function(x, sigma, l) {
  n <- length(x)
  k <- matrix(ncol = n, nrow = n)
  for (i in 1:n) {
    for (j in 1:n) {
      k[i, j] = sigma^2 * exp(-1/(2*l^2) * (x[i] - x[j])^2)
    }
  }
  colnames(k) <- paste0('x', 1:n)
  rownames(k) <- colnames(k)
  return(k)
}
```

Dans la fonction `RBF_kernel`, `x` d√©signe les dimensions, `sigma` d√©signe un √©cart-type commun √† chacune des dimensions et `l` est la longueur d√©signant l'amplification de la covariance entre des dimensions √©loign√©es (dans le sens que la premi√®re dimension est √©loign√©e de la derni√®re). Pour 6 dimensions, avec un √©cart-type de 4 et une longueur de 2.

```{r}
#| label: ml-gp-rbf_6d

covariance_6 <- RBF_kernel(1:6, sigma=4, l=2)
round(covariance_6, 2)
```

Changez la valeur de `l` permet de bien saisir son influence sur la matrice de covariance. Avec un `l` de 1, la covariance entre $x_1$ et $x_6$ est pratiquement nulle: elle est un peut plus √©lev√©e avec `l=2`. Pour reprendre l'exemple du rythme cardiaque, on devrait en effet s'attendre √† retrouver une plus grande corr√©lation entre celles mesur√©es aux temps 4 et 5 qu'entre les temps 1 et 6.

De m√™me que dans la situation o√π nous avions une distribution binormale, nous pouvons conditionner une distribution multinormale. Dans l'exemple suivant, je conditionne la distribution multinormale de 6 dimensions en sp√©cifiant les valeurs prises par les deux premi√®res dimensions. Le r√©sultat du conditionnement est une distribution en 4 dimensions. Puisqu'il est difficile de pr√©senter une distribution en 6D, le graphique en haut √† gauche ne comprend que les dimensions 1 et 6. Remarquez que la corr√©lation entre les dimensions 1 et 6 est faible, en concordance avec la matrice de covariance g√©n√©r√©e par la fonction `RBF_kernel`. Lancez plusieurs fois le code et voyez ce qui advient des √©chantillonnages dans les dimensions 3 √† 6 selon le conditionnement en 1 et 2.

```{r}
#| label: ml-gp-cond_6d

library("MASS")

# 1. Distribution
means_vec <- rep(65, 6)
covariance_mat <- covariance_6

# 2. Condition
conditions_x <- c(61, 74) # changer ces chiffres pour visualiser l'effet

# 3. Densit√© conditionn√©e
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat, 
                           dependent.ind = 3:6, given.ind=1:2,
                           X.given=conditions_x)
cond_mean <- cond_parameters$condMean
cond_sd <- sqrt(cond_parameters$condVar)
x6_sequence <- seq(50, 90, length=100)
x6_dens <- dnorm(x2_sequence, mean=cond_mean[4], sd=cond_sd[4, 4])

x_3.6_draw <- mvrnorm(n = 1, mu = cond_mean, Sigma = cond_sd^2)

# 4. Graphiques
layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2))
par(mar=c(4, 4, 1, 1))

## 4.1 Ellipse
plot(ellipse(x=covariance_mat[c(1, 6), c(1, 6)], centre=means_vec[c(1, 6)], levels=0.95), 
     type='l', xlab="BPM √† 7:00", ylab="BPM √† 8:00")
abline(v=conditions_x[1], col='#f8ad00', lwd=1)
lines(x=condition_x1 + x6_dens*40, y=x2_sequence, col="#f8ad00", lwd=1)
lines(x = c(conditions_x[1], conditions_x[1]),
      y = c(cond_mean[4]-cond_sd[4, 4], cond_mean[4]+cond_sd[4, 4]),
      lwd=2, col='#46c19a')
points(conditions_x[1], cond_mean[4],
       col='#46c19a', pch=16, cex=1)
points(conditions_x[1], x_3.6_draw[4], pch=16, col="#b94a73")

## 4.2 Covariance
plot_matrix(covariance_mat, cex=0.8)

## 4.3 S√©rie
plot(1:6, c(conditions_x, x_3.6_draw), xlim=c(0, 6), ylim=c(60, 85), type='l',
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")
points(c(1, 2), conditions_x, pch=16, col='#46c19a', cex=3)
points(3:6, x_3.6_draw, pch=16, col='#b94a73', cex=3)
```

La structure de la covariance assure que les dimensions proches prennent des valeurs similaires, assurant une courbe lisse et non en dents de scie. Pourquoi s'arr√™ter √† 6 dimensions? Prenons-en plusieurs, puis g√©n√©rons plus d'un √©chantillon. Ensuite, utilisons ces simulations pour de calculer la moyenne et l'√©cart-type de chacune des dimensions.

```{r}
#| label: ml-gp-cond_65
#| echo: false

# 1. Distribution
n <- 20
means_vec <- rep(65, n)
covariance_mat <- RBF_kernel(x = 1:n, sigma = 10, l = 2)

# 2. Condition
conditions_x <- c(61, 74) # changer ces chiffres pour visualiser l'effet

# 3. Densit√© conditionn√©e
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat, 
                           dependent.ind = 3:n, given.ind=1:2,
                           X.given=conditions_x)
cond_mean <- cond_parameters$condMean
cond_sd <- cond_parameters$condVar

# 4. Graphiques
par(mar=c(4, 4, 1, 1))

## 4.3 S√©rie
plot(0, 0, xlim=c(0, n), ylim=c(40, 95), type='l',
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")

samples <- 50
x_3.n_draw <- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd)
for (i in 1:samples) {
  lines(1:n, c(conditions_x, x_3.n_draw[i, ]), col = rgb(0, 0, 0, 0.15))
}
x_3.n_draw_mean <- apply(x_3.n_draw, 2, mean)
x_3.n_draw_sd <- apply(x_3.n_draw, 2, stats::sd)

lines(1:n, c(conditions_x, x_3.n_draw_mean), lwd = 2)
lines(1:n, c(conditions_x, x_3.n_draw_mean + x_3.n_draw_sd), col = "#b94a73", lwd = 2)
lines(1:n, c(conditions_x, x_3.n_draw_mean - x_3.n_draw_sd), col = "#b94a73", lwd = 2)
points(c(1, 2), conditions_x, pch=16, col='#46c19a', cex=2)
```

Revenons au rythme cardiaque. On pourra utiliser le conditionnement aux temps observ√©s, soit 7:00, 8:00, 10:00, 14:00 et 17:00 pour estimer la distribution √† 12:00 et 16:00, o√π √† des dimensions artificielles quelconques ici fix√©es aux demi-heures.

```{r}
#| label: ml-gp-cond-bpm
#| echo: false

# 1. Distribution
n <- 21
means_vec <- rep(65, n)
covariance_mat <- RBF_kernel(x = 1:n, sigma = 5, l = 2)

# 2. Condition
conditions_x <- c(61, 74, 69, 67, 78)
conditions_indices <- c(1, 3, 7, 15, 21)
dependent_indices <- (1:20)[! 1:20 %in% conditions_indices]

# 3. Densit√© conditionn√©e
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat, 
                           dependent.ind = dependent_indices,
                           given.ind=conditions_indices,
                           X.given=conditions_x)
cond_mean <- cond_parameters$condMean
cond_sd <- cond_parameters$condVar
samples <- 100
x_draw <- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd)
means_draw <- apply(x_draw, 2, mean)
sd_draw <- apply(x_draw, 2, stats::sd)

# 4. Graphiques
par(mar=c(4, 4, 1, 1))

## 4.1 Combiner les pr√©dictions
bpm <- rep(NA, n)
bpm[conditions_indices] <- conditions_x
bpm[dependent_indices] <- means_draw

bpm_sd <- rep(NA, n)
bpm_sd[conditions_indices] <- 0
bpm_sd[dependent_indices] <- sd_draw


## 4.2 Combiner les tirages et les donn√©es
x_draw_all <- matrix(ncol = n, nrow = samples)
for (i in 1:length(conditions_x)) x_draw_all[, conditions_indices[i]] <- conditions_x[i]
x_draw_all[, dependent_indices] <- x_draw


## 4.3 S√©rie
plot(1:n, bpm, xlim=c(0, n), ylim=c(40, 90), type='l', lwd = 2,
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")
for (i in 1:samples) {
  lines(1:n, x_draw_all[i, ], col = rgb(0, 0, 0, 0.1))
}
lines(1:n, bpm+bpm_sd, col = "#b94a73", lwd = 2)
lines(1:n, bpm-bpm_sd, col = "#b94a73", lwd = 2)
points(conditions_indices, bpm[conditions_indices], pch=16, col='#46c19a', cex=2)

```

Comme on devrait s'y attendre, la r√©gression r√©sultant de la mise en indices de la distribution est pr√©cise aux mesures, et impr√©cise aux indices peu garnis en mesures. Nous avions utilis√© 21 dimensions. **Lorsque l'on g√©n√©ralise la proc√©dure √† une quantit√© infinie de dimensions, on obtient un *processus gaussien*.** 

![](https://media.giphy.com/media/12R2bKfxceemNq/giphy.gif)

L'indice de la variable devient ainsi une valeur r√©elle. Un processus gaussien, $\mathcal{GP}$, est d√©fini par une fonction de la moyenne, $m \left( x \right)$, et une autre de la covariance que l'on nomme *noyau* (ou *kernel*), $K \left( x, x' \right)$. Un processus gaussien est not√© de la mani√®re suivante:

$$\mathcal{GP} \sim \left( m \left( x \right), K \left( x, x' \right) \right)$$

La fonction d√©finissant la moyenne peut √™tre facilement √©cart√©e en s'assurant de centrer la variable r√©ponse √† z√©ro ($y_{centr√©} = y - \hat{y}$). Ainsi, par convention, on sp√©cifie une fonction de moyenne comme retournant toujours un z√©ro. Quant au noyau, il peut prendre diff√©rentes fonctions de covariance ou combinaisons de fonctions de covariance. R√®gle g√©n√©rale, on utilisera un noyau permettant de d√©finir deux param√®tres: la hauteur ($\sigma$) et la longueur de l'ondulation ($l$) (figure @fig-ml_gp_hyperp).

```{r}
#| label: fig-ml_gp_hyperp
#| out-width: 100%
#| fig-align: center
#| fig-cap: "Hyperparam√®tres d'un noyau RBF."
#| echo: false

hyperparameters <- expand.grid(l=c(1, 3, 9), sigma=1:3)

# Graphique
n <- 100

samples_list <- list()
for (i in 1:nrow(hyperparameters)) {
  sample <- mvrnorm(n = 1, mu = rep(0, n), 
                    Sigma = RBF_kernel(x=1:n,
                                       sigma = hyperparameters$sigma[i],
                                       l = hyperparameters$l[i]))
  samples_list[[i]] <- data.frame(sigma = paste("sigma =", hyperparameters$sigma[i]),
                                  l = paste("l =", hyperparameters$l[i]),
                                  x = 1:n,
                                  sample = sample)
  
}
samples_df <- do.call(rbind.data.frame, samples_list)
samples_df %>%
  ggplot(mapping = aes(x = x, y = sample)) +
  geom_line() +
  facet_grid(l ~ sigma)
```

On pourra ajouter √† ce noyau un bruit blanc, c'est-√†-dire une variation purement al√©atoire, sans covariance (noyau g√©n√©rant une matrice diagonale).

Le noyau devient ainsi un *a priori*, et le processus gaussien conditionn√© aux donn√©es devient un *a posteriori* probabiliste.

Finalement, les processus gaussiens peuvent √™tre extrapol√©s √† plusieurs variables descriptives.

### Les processus gaussiens en `R`

Pas de souci, vous n'aurez pas √† programmer vos propres fonctions pour lancer des processus gaussiens. Vous pourrez [passer par `caret`](https://topepo.github.io/caret/train-models-by-tag.html#gaussian-process). Vous pourriez, comme c'est le cas avec les r√©seaux neuronaux, obtenir davantage de contr√¥le sur l'autoapprentissage en utilisant directement la fonction `gausspr()` du package **`kernlab`**.

```{r}
#| label: ml-gp-kernlab

library("kernlab")
x <- c(7, 8, 10, 14, 17)
y <- c(61, 74, 69, 67, 78)
y_sc <- (y - mean(y)) / sd(y)

m <- gausspr(x, y_sc, 
             kernel = 'rbfdot', # le noyau: diff√©rents types disponibles (?gausspr)
             kpar = list(sigma = 4), # hyperparam√®tre du noyau (l est optimis√©)
             variance.model = TRUE, # pour pouvoir g√©n√©rer les √©carts-type
             scaled = TRUE, # mettre √† l'√©chelle des variables
             var = 0.01, # bruit blanc
             cross = 2) # nombre de plis de la validation crois√©e

xtest <- seq(6, 18, by = 0.1)
y_sc_pred_mean <- predict(m, xtest, type="response")
y_pred_mean <- y_sc_pred_mean * sd(y) + mean(y)
y_sc_pred_sd <- predict(m, xtest, type="sdeviation") # "sdeviation" en r√©gression et "probabilities" pour la classification
y_pred_sd <- y_sc_pred_sd * sd(y)

plot(x, y, xlim = c(6, 18), ylim = c(45, 90))
lines(xtest, y_pred_mean)
lines(xtest, y_pred_mean + y_pred_sd, col="red")
lines(xtest, y_pred_mean - y_pred_sd, col="red")
abline(v=12, lty=3, col='gray50');text(12, 67, '?', cex=2)
abline(v=16, lty=3, col='gray50');text(16, 72, '?', cex=2)
```

#### Application pratique

Les processus gaussiens sont utiles pour effectuer des pr√©dictions sur des ph√©nom√®ne sur lesquels on d√©sire √©viter de se commettre sur la structure. Les s√©ries temporelles ou les signaux spectraux en sont des exemples. Aussi, j'ai utilis√© les processus gaussiens pour mod√©liser des courbes de r√©ponse aux fertilisants. Prenons ces donn√©es g√©n√©r√©es au hasard, comprenant l'identifiant de la mesure, le bloc du test, la dose de fertilisant, trois variables environnementales ainsi que la performance de la culture en terme de rendement.

```{r}
#| label: ml-gp-pratique1
#| message: false

fert <- read_csv("data/11_response_fert.csv")
```

```{r}
#| label: ml-gp-pratique2

fert %>% 
  ggplot(aes(x = Dose, y = Yield)) +
  geom_line(aes(group = Block), colour = rgb(0, 0, 0, 0.5))
```

Les blocs 1 √† 30 serviront d'entra√Ænement, les autres de test. Le rendement est mis √† l'√©chelle pour la mod√©lisation.

```{r}
#| label: ml-gp-pratique3

environment <- fert %>% 
  dplyr::select(Dose, var1, var2, var3)
yield_sc <- (fert$Yield - mean(fert$Yield)) / sd(fert$Yield)

environment_tr <- environment[fert$Block <= 30, ]
environment_te <- environment[fert$Block > 30, ]
yield_tr <- yield_sc[fert$Block <= 30]
yield_te <- yield_sc[fert$Block > 30]
```

Je pourrais optimiser les hyperparam√®tres en cr√©ant une grille puis en lan√ßant plusieurs processus gaussiens en boucle. Mais pour l'exemple j'utilise des hyperparam√®tres quelconque.

```{r}
#| label: ml-gp-pratique4-fit

yield_gp <- gausspr(environment_tr, yield_tr, kernel = 'rbfdot',
                    kpar = list(sigma = 0.1),
                    variance.model = TRUE,
                    scaled = TRUE,
                    var = 0.1,
                    cross = 10)

# Rendements pr√©dits dans l'√©chelle originale
gp_pred_tr <- predict(yield_gp, environment_tr, type="response") * sd(fert$Yield) + mean(fert$Yield)
gp_pred_te <- predict(yield_gp, environment_te, type="response") * sd(fert$Yield) + mean(fert$Yield)

# Rendements r√©els dans l'√©chelle originale
yield_tr_os <- yield_tr * sd(fert$Yield) + mean(fert$Yield)
yield_te_os <- yield_te * sd(fert$Yield) + mean(fert$Yield)

par(mfrow = c(1, 2))
plot(yield_tr_os, gp_pred_tr, main = "train")
abline(0, 1, col = "red")
plot(yield_te_os, gp_pred_te, main = "test")
abline(0, 1, col = "red")
```

La pr√©diction semble bien fonctionner en entra√Ænement comme en test. Pour une application √† un cas d'√©tude, disons que pour mon site j'ai des variables environnementales de valeurs du bloc 50, et que je cherche la dose optimale.

```{r}
#| label: ml-gp-pratique-slice

fert %>% 
  dplyr::filter(Block == 50) %>% 
  dplyr::select(var1, var2, var3) %>% 
  dplyr::slice(1)
```

Je peux cr√©er un tableau comprenant des environnements √©gaux pour chaque ligne, mais comprenant des incr√©ments de dose, puis pr√©dire la courbe de r√©ponse ainsi que son incertitude. Et puisque c'est un cas document√©, je peux afficher les r√©sultats de l'essai pour v√©rifier si le mod√®le est cr√©dible.

```{r}
#| label: ml-gp-pratique-rep-table

environment_appl <- data.frame(Dose = seq(0, 200, 5), var1 = 1.57, var2 = 101.5, var3 = -10.7)
yield_appl_sc <- predict(yield_gp, environment_appl, type="response")
y_sc_pred_sd_sc <- predict(yield_gp, environment_appl, type="sdeviation")

yield_appl <- yield_appl_sc * sd(fert$Yield) + mean(fert$Yield)
yield_appl_sd <- y_sc_pred_sd_sc * sd(fert$Yield)

plot(environment_appl$Dose, yield_appl, type = "l", ylim = c(0, 35))
points(x = fert[fert$Block == 50, ]$Dose, y = fert[fert$Block == 50, ]$Yield)
lines(environment_appl$Dose, yield_appl + yield_appl_sd, col = "red")
lines(environment_appl$Dose, yield_appl - yield_appl_sd, col = "red")
```

Pour chaque incr√©ment de dose de la courbe de r√©ponse, il est possible de calculer un rendement √©conomique et/ou √©cologique en fonction du prix de la dose pond√©r√© par un co√ªt environnemental, puis de soutirer une performance optimale en terme de fertilisation.

**Exercice**. Changez les valeurs des variables environnementales pour g√©n√©rer le tableau `environment_appl` avec des valeurs qui sortent du lot (voir figure @fig-ml_variables_env). Qu'observez-vous? Pourquoi?

```{r}
#| label: fig-ml_variables_env
#| out-width: 100%
#| fig-align: center
#| fig-cap: "Variables environnementales du tableau fictif `fert`."
#| echo: false

fert %>% 
  gather(key = "variable", value = "value", var1, var2, var3) %>% 
  ggplot(aes(x = value)) +
  facet_wrap(~variable, scales = "free_x") +
  geom_histogram(bins = 10)
```

**Exercice**. Effectuer la pr√©diction du rendement avec d'autres techniques, comme des r√©seaux neuronaux. Comment les mod√®les se comportent-ils?

```{r}
#| label: ml-gp-exercice-nn
#| echo: false
#| eval: false

# r√©seau neuronal
yield_nn <- neuralnet(Yield ~ Dose + var1 + var2 + var3,
                      data = bind_cols(Yield = yield_tr, environment_tr), 
                      hidden = c(20, 100, 100, 20),
                      threshold = 0.05,
                      stepmax = 100000,
                      linear.output = TRUE)

# calcul des r√©ponses
yield_tr_nn_compute <- compute(yield_nn, environment_tr)
yield_tr_nn <- yield_tr_nn_compute$net.result * sd(fert$Yield) + mean(fert$Yield)

yield_te_nn_compute <- compute(yield_nn, environment_te)
yield_te_nn <- yield_te_nn_compute$net.result * sd(fert$Yield) + mean(fert$Yield)

par(mfrow = c(1, 2))
plot(yield_tr_os, yield_tr_nn, main = "train")
abline(0, 1, col = "red")
plot(yield_te_os, yield_te_nn, main = "test")
abline(0, 1, col = "red")

environment_appl <- data.frame(Dose = seq(0, 200, 5), var1 = 1.57, var2 = 101.5, var3 = -10.7)
yield_appl_sc <- compute(yield_nn, environment_appl)$net.result
yield_appl <- yield_appl_sc * sd(fert$Yield) + mean(fert$Yield)

plot(environment_appl$Dose, yield_appl, type = "l", ylim = c(0, 50))
points(x = fert[fert$Block == 50, ]$Dose, y = fert[fert$Block == 50, ]$Yield)
```

```{r}
#| label: ml-rm

rm(list = ls())
```
